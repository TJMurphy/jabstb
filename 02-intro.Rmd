# The Big Picture {#bigpic}

*To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher*
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(Hmisc)
```

Let's start by listing out some characteristics that biomedical experiments share in common.

Biomedical research experiments...

* tend to involve the use of relatively small sample sizes. 
*	are usually highly exploratory in nature. 
*	generate data that are either discrete counts or scalar measurements.
* are structured by a small group of fairly common experimental designs.
* are usually interpreted in a binary way; as having “worked”, or not.
* test hypotheses (though too often these are unstated).
* aspire for rigor, replicability and reproducibility.
* aspire to be unbiased.

The stakes can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge and opening the door to real opportunities to create solutions for human diseases and suffering. 

But no less motivating are the issues related to the professional practice. These include ego, a natural impulse to seek validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. 

Scientific discovery is driven by ideals and by biases. The statistics covered in this course, it turns out, were invented to with these drivers in mind. 

The statistical design and analysis of experiments will be taught as a framework. Researchers who operate within this framework are well-positioned to make discoveries. This framework not only helps detect the signals in the noise of experimental data, but also provides a method to keep biases in check.

We'll discuss statistics in terms of experimental design. The idea here is to front load all of the statistical decision making in the planning stages. It turns out that there is a role for software 


The RA Fisher quote above really needs to be taken to heart. How often is an experiment revealed as hopelessly flawed in its design when the statistical analysis isn't considered until all of the data are collected? The answer is, "way too often".

For that reason we'll emphasize planning out the statistical analysis ahead of an experiment. The unbiased researcher only has to follow the plan. 

In fact, we'll learn how simulating the data and the analysis *a priori* generates incredible insight into the two things any scientist would want to know before running an experiment: is it properly designed and is it feasible?

## What are experimental statistics?

Experimental statistics were invented about a century ago to solve several problems that researchers of the day faced in dealing with data.

Then, as now, experimental statistics are used 

* to summarize data into simpler descriptive models.
* as procedures to draw inferences from samples. 
* as procedures that guide the design of experiments.
* to serve as framework for conducting unbiased research with a modicum of integrity.

What did you think statistics or "biostats" is before you read that? Chances are you thought it was one or two of those bullets, and probably not the latter two.

### Descriptive modeling

Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. 

For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics, such as the sample mean and standard deviation, or median and ranges or percentiles. The alternative is to discuss individual replicates in explicit, gory detail. 

What most people think of as statisical tests are also a way of conveying descriptive information about the experimental model. 

If you say, "I analyzed this by a two-tailed paired t-test," say no more. I know exactly what kind of experiment you ran! 

Regression models are used to describe patterns of data behavior. For example, the well-known Michalis-Menten model that describes product formation as a function of substrate concentration.  
$$ [P]=\frac{[S][Emax]}{[S]+Km}$$
It is a special case of a more general hyperbolic model that a large number of nonlinear biological processes can be fit to. 
$$[Y]=\frac{[X][\beta_0]}{[X]+\beta_1} $$

Several other nonlinear regression models are useful to describe other common phenomena such as multimeric binding relationships or processes that are modeled as time series, such as growth or decay. 

In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. 

Take a simple example of two groups shown here. Each group has been fit to a simple mean and standard deviation model. Clearly, that model fits the control group much better compared to the treatment group fit. 

Why? For the latter most of the data values are greater than the mean of the group. The data are skewed. Sure, a mean can be calculated from any group of values. But does it summarize the data accurately? Perhaps some other model (or group of statistical parameters) would better convey to you how that data look?


```{r echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="This is a caption"}

Control <- c(111, 113, 92, 103, 91, 101, 90, 93, 105, 91)
Treatment <- c(111, 113, 92, 103, 91, 101, 90, 93, 1, 2)
data <- data.frame(Control, Treatment)
data <- gather(data, Group, Serum_glucose)

ggplot(data, aes(Group, Serum_glucose))+
  geom_jitter(width=0.1, size=4, color="blue", alpha = 0.2) +
  stat_summary(fun.data="mean_sdl", fun.args =list(mult=1))

```

### Statistical inference

There are two types of inference for the experimentalist. 

One type is to infer whether an experiment “worked” or not. This familiar process involves calculating a test statistic from the data (t-test, F-tests, etc). These values are then located on a null distribution of the test statistic to derive p-values. In essence, we set up threshold rules based upon these p-values that allow us to decide whether the data, transformed into a test statistic, are so extremely unexpected they warrant rejection of a null hypothesis.

A second type of inference is to extrapolate from a sample some **estimate** for the values of the variables within the population that was sampled. This is usually important when we’re measuring standardized variables. Whether it is serum glucose levels, or ligand affinity for a protein, or rate constants. By standardized variables I imply stuff that should have the same value no matter where on planet earth they are measured. In some situations, the precision and accuracy by which we estimate these values becomes fairly important.

Estimation can also be important when dealing with variables on relative scales, too. For example, we might expect the effect size for some treatment, relative to control, be the same no matter where on the planet it is measured.

### Experimental design

Statistically rigorous experiments are planned out in advance.  

As will become clear in the history section of this chapter, poor experimental planning by researchers of his time is what inspired RA Fisher to invent much of the statistical methods and procedures that we use today. 

Experimental design is the researcher’s most potent weapon against bias. It involves establishing a clear, testable statistical hypothesis and establishing series of decision rules in advance of the research. These range from subject selection and arrangement, predetermination of replicate number using *a priori* power analysis, setting data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, on up to what analysis will be performed on the data. .

Experimental design is very common in prospective clinical research. Statisticians are usually consulted during the design phase of clinical trials, where all of the sampling and analysis protocols are carefully vetted. The statistical design and analysis of clinical studies is extremely important in the drug and medical device discovery process. 

Unfortunately, very few basic biomedical scientists practice anything remotely like this. The vast majority of the published work today is retrospective, if the approach that research teams take to sampling and analysis is taken into account. Yet, they all tend to use statistics that are largely intended for prospective designs.

### A role for Monte Carlo simulation

I am convinced that Monte Carlo simulations of experiments have a lot of utility in experimental design and planning and for that reason they are something I emphasize in the course. We'll use them to show how sample sizes can be predetermine. But they are also really useful to visualize expected output. By forcing a researcher to go through an analysis they can help expose flaws in the experiment before any data are actually produced and to truly assess the feasibility of a planned experiment.

Monte Carlo simulations are actually pretty simple to run with software like R. All that is needed is a rough idea of the expected values for your outcome variables. One needs some idea of what would be considered a minimally viable or scientifically meaningful effect size. This information is usually available in preliminary experiments. Having that, one simply simulates a dataset and runs the expected statistical analysis on it. A cycle of data simulation and testing is repeated a thousand times to get a long run average of how well the experiment will work! 

This course will emphasize the importance of the following questions, which should be given a high priority by biomedical researchers when planning experiments:

* What statistical approaches should I use in the planning stages?
* What statistical methods should I use to analyze the data? 
* Why should I analyze the data in the way that was planned?
* How should the analysis be interpreted?

I am aware that many researchers arrive at this point having skipped the first of these question and who have no intention to ask of the third.

### Anti-bias framework

If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct.

The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn't provide us a magic woo-woo shield to protect us from our biases. 

Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias.

The following are some of the biggies.

###Cognitive biases

From a statistical point of view biases can be classified into two major groupings. The first are [Cognitive biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases). These are how we think (or fail to think) about our experiments and our data. 

These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, "how could this not work!" you are in the throes of a pretty deep bias and probably don't realize it.  In bench research, these cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or telling glitches in the data suggesting something else we are not considering is going on.

###Systematic biases

The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed. 

Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line.

###Scientific misconduct

To limit bias to just these two categories is not to admit malicious biases exist, too. These include [forbidden practices such as data fabrication and falsification](https://grants.nih.gov/policy/research_integrity/index.htm). This is obviously a problem of integrity, but very few scientists working today are immune from the high stakes careerism drivers that seem to expose weaknesses with integrity. 

In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them.

The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite liveable. 

If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you've given it your best shot. That is hard to conclude when the latter occurs.

## Brief History of Experimental Design

Researchers in the pre-statistics days lacked the statistical framework that today's researchers take for granted! Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they were weak in designing experiments, in summarizing data, and in drawing inference from it. They just lacked a reliable framework for all of that.

A statistical approach to experimental design was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. 

This is to say that at the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. His career took that trajectory by accident.

For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwin’s theory of evolution, the hot discovery of the day. Fisher's side hustle was mathematical problems related to genetics and inspired by evolution. Today, we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. That's certainly where his career ambitions seem laid. 

One big contribution he made during this early stage in his illustrious career was no small feat. He defined variance as the square of the standard deviation. He proposed that variance is useful as a descriptive statistic for the variability within a population (and much more...it would soon become the foundation of multigroup exprimental designs). 

In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. 

After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of it all. 

Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the station’s data, some of which represented serial experiments that had been running for decades.

Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of most of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds familiar, it was meant to.

Here's when the paradigm shifted. Fisher began to think about the process by which experimental data **should** be collected.  Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. His invention of the **analysis of variance** extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well.

It was a truly remarkable period. In 1925 Fisher published a small book, [*Statistical Methods for Research Workers*](https://psychclassics.yorku.ca/Fisher/Methods/).  In 1934 he published its extension, [*Design of Experiments*](https://www.phil.vt.edu/dmayo/PhilStatistics/b%20Fisher%20design%20of%20experiments.pdf). In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would cross oceans...and then disciplines.

Today, experiments that we would recognize as statistically rigorous are those in which Fisher’s early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. 

There's much more to this history, including many additional players. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today.
