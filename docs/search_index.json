[
["index.html", "JABSTB: Statistical Design and Analysis of Experiments with R Preface", " JABSTB: Statistical Design and Analysis of Experiments with R TJ Murphy PhD, Department of Pharmacology and Chemical Biology, School of Medicine, Emory University, Atlanta, GA biostats538@gmail.com 2019-02-25 Preface This book is a resource for students enrolled in my biostats course. The students are primarily in one of Emory’s biomedical and biological sciences PhD programs. The class usually includes the occasional Emory honors program undergrads, students from Emory’s public health school, and a few Georgia Tech graduate students. I’d been wrestling with the decision to switch my course over to R for a few years, but never really found a book that covers the subject the way I like to emphasize. When I finally made the switch I needed to prepare a bunch of handouts. I did that using R Markdown. Then I decided to write some intro handouts. Then I found Bookdown. Before I knew it, I had a “book”. This is that. JABSTB. Not included in this book are additional materials for the course (eg, take home and group exercises, slide decks, data sets, my extensive collection of stats cartoons, etc). The scope of the book is to provide some background on statistical fundamentals that are most relevant to the biomedical researcher and to provide examples for running and interpreting various statistical functions. These people test ideas by generating data after manipulating some independent variable(s). They need to know principles of sampling, error, statistical hypotheses, types of data and experimental design. Each chapter has a corresponding R Markdown document. If you wish to grab those documents (and any data sets read in those Markdowns) instead of using this material as HTML, go grab it on Github. Simply fork, clone or download them from the Github jabstb repo. This book is a living document, subject to a lot of on-the-fly revision. Stuff will be added and eliminated over time. As I write these words, in Dec 2018, my main disclaimer is that it is definitely an MVP. If you find errors, have any suggestions, or would otherwise like to contribute, please submit a pull request and/or contact me by email. I welcome your additions. Copyright 2018-2019 © TJ Murphy MIT license. "],
["author.html", "Chapter 1 About the author", " Chapter 1 About the author I learned this material as a graduate student at Mizzou. There I took several stats courses as electives. The ones that impacted me the most were taught by the late Gary Krause, then a professor and statistician in Mizzou’s agricultural college. The light turned on for me during Gary’s Experimental Design course. That’s when the fog of mathematical statistics cleared enough so I could finally “get” the pragmatic value of statistics for the researcher. Why it didn’t hit me earlier, I don’t know. I’d been involved in plenty of research and data collection by then. What became most clear to me is that experimental design is a statistical framework for conducting unbiased research. That concept permeates my course and this book and it is the one thing I most want my students to take away from this. Here’s how it hit me. I was working on my PhD in pharmacology within the medical school. But most of my classmates in Gary’s courses were from the other side of campus, working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. The problem my classmates shared, which was not a problem that really affected me, is having only a single growing or mating season by which to run a fully replicated experiment. Figure 1.1: One shot! They only had one shot. Which changes everything about the approach to research. Planning was a priority for them. They needed to map out their experimental design well in advance and with a lot of statistical thought on the front end. Once the experiment began, any new wrinkles or oversights would have to wait until the next growing season. They didn’t have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups, to tweak it. They don’t have the luxury we have in the biomedical sciences with our typically much easier access to biological material. Their planning was based upon statistical design principles, often in consultation with Gary. For them, statistics were a priori planning and post-hoc tests. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the planned approach. An experiment set in motion according to a plan could only follow the plan. In contrast, it is fair to say that most biomedical scientists fail to incorporate statistical design into their plans at all. That failure opens up a whole can of worms that can generally be characterized as running statistics in ways our statistics were never meant to be run. All too common is the biomedical researchers who takes a more “fly by the seat of their pants” approach to running experiments and collecting data. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation of what the data obviously show. Oops. Unfortunately, that approach is riddled with biases, if not flat out lacking in integrity. A lot has been written in recent years about the replication and reproducibility crisis in biomedical research. The reproducibility problem is a problem of process, and fairly simple to solve. Just be more explicit about how you arrived at your statistical solution, and I should be able to arrive at the same value, given your data. I’m completely agnostic about whether a given type of statistics (Bayesian vs frequentist) has anything to do with the replication problem or offers a better approach. What I do think has been left largely unsaid is the failure of most biomedical researchers to embrace a statistical design framework in their experimental planning. It doesn’t make any sense to use the final stage tools of hypothesis testing, no matter if it is a posterior or p-value, if the hypothesis is being stated for the first time only AFTER all the data are in and have been inspected for meaning! And that’s pretty much what’s happening out there, by and large. Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made. Therefore, the focus of this course is in teaching statistics as an experimental design framework. The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance, to follow it (and to feel really, really guilty when they don’t) and to appreciate why this is so important to reduce bias. That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs and data types. Most of which she will forget immediately. But she will no where to go to relearn it and what to do when she gets there. And since I emphasize pre-planning, this book is full of simulations. Other than that it serves as a reproducible environment, that’s the really great advantage of using R to teach biostats, in my view. I’m not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work “under the hood”. When I do, it is mostly for stuff I think should be helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don’t believe I’m the person who should be teaching it. Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That’s why operating within a statistical design framework is so important. For the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments and prolonged time to degrees. "],
["history.html", "Chapter 2 A Brief History of Experimental Design", " Chapter 2 A Brief History of Experimental Design Researchers in the pre-statistics days lacked the statistical framework that today’s researchers take for granted. Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they struggled with designing experiments, in summarizing the data, and in drawing unbiased inference from it. The statistical approach to experimental design we use today was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. Figure 2.1: RA Fisher in 1913, from the Adelaide Digital Archive At the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. He took that trajectory by accident. For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwin’s theory of evolution, which was the hot discovery of the day, of course. Fisher’s side hustle was to work on mathematical problems related to evolutionary genetics. Today, we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. That’s certainly where his career ambitions seem laid. He never lost an interest in evolution and would go on to become, unfortunately, a prominent eugenicist. The take-away from that, alone, is that statistics is not a fool-proof antibias framework. Still, one big contribution he made during this early stage was no small feat. He defined variance and its relationship to the mean of a population. He proposed that variance is useful as a descriptive statistic for the variability within a population. Further developed, it would soon become the foundation of the powerful multigroup experimental designs that are called ANOVA, the analysis of variance, which are widely used today in the biomedical sciences. In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar (WWI) agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of this data they already had on hand. Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher early in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the station’s data, some of which represented serial agricultural experiments that had been running for decades. As he dug in Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of much of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds at all familiar then I’ve achieved my objective for mentioning it. There’s nothing worse than spending months of painstaking effort collecting data that can’t be analyzed. Here’s when the paradigm shifted. Fisher began to think about the process by which experimental data should be collected. Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. As I mentioned above, his invention of the analysis of variance extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well. It was a truly remarkable period. In 1925 Fisher published a small book, Statistical Methods for Research Workers. In 1934 he published its extension, Design of Experiments. In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would soon cross oceans…and then disciplines. It is telling that the first academic statistics department was at Iowa State University, in the heart of the corn belt, in the 1920’s. George Snedecor started it. George Snedecor is also the author of the F-distribution and the F-test, which is used to make decisions on ANOVA experimental designs. By the 1950’s statistical methods were spreading rapidly through the medical literature. Today, experiments that we would recognize as statistically rigorous are those in which Fisher’s early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. There’s much more to this history, including many additional players and plenty of controversy that remains unsettled to this day. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today. "],
["software.html", "Chapter 3 Software 3.1 My code is your code 3.2 Install R and RStudio 3.3 Getting started with R 3.4 Other resources", " Chapter 3 Software library(swirl) library(tidyverse) The course requires that you use the latest version of R and also using the RStudio integrated development environment to run R. R and RStudio have to be downloaded and installed separately. RStudio simplifies reading, writing and running scripts in R. R itself is a collection of packages. Individual packages have the functions that you’ll need. Once a package is installed on your machine, you’ll need to load its library for those functions to be available for your work. After installing R you’ll have a library of system packages. These are known colloquilly as the “base” packages. The libraries for these base packages are automagically loaded each time you run R. Thus, you won’t need to “call” base package librarly prior to running functions within them. From time to time you’ll need to run other functions that are only found in other packages. Over time, you’ll create a user library of these packages. Any time you need to get its functions to run you’ll need to load the package into your environment using the library() function. At the beginning of each chapter I list the libraries that will be necessary to run the scripts in that chapter. Those will be packages you’ll need to install if you don’t have them already. 3.1 My code is your code I want you to read everything in these chapters, including the R code. Especially the R code. If you’re able to see the code that leads to some output, that means I think it is important for you to understand that code, how it works. You’ll need it as a basis to write your own custom scripts…for class work, home work, and beyond. If you are new to R, at first it will seem…unintelligible. But it will make sense to you with a bit of time. The other stuff I’ve written in the chapters is about statistics. Understanding the code is no less important than understanding the statistics. 3.2 Install R and RStudio Installation of R and RStudio is pretty straight forward but not idiot proof. One sign you are doing it wrong is when you find yourself about to purchase something. Step away if that’s the case. These are free. A second sign you are doing it wrong (unless you really know what you are doing) is if you are compiling source code. Note for Windows users: If you have a 64bit processor, it is a very good idea to install and run the 64bit R version. To get started do the following in this sequence: Step 1. Install version 3.5.2 (Eggshell Igloo) of R. Go here. Select the appropriate precompiled binary distribution for your machine. Unless you know what you’re doing, don’t select the source code. Step 2. Run the installer function after downloading so that you have a running version of R on your machine Step 3. Install version 1.1.x of RStudio only after you have completed installing R. Go here. Select the free option and choose the appropriate installer for your platform from the list. In all likelihood, you DO NOT want to choose a zip/tarball or source code. Step 4. Run the RStudio installer so that you have a running version of it. If you installed R prior to RStudio as instructed, launching RStudio on your machine should automagically configure R to work within the RStudio. Poke around with the RStudio menu bar and the RStudio panes. Under Tools select Global Options and experiment with an appearance configuration to your liking. 3.3 Getting started with R Once you have R and RStudio up and running a class assignment will be to wrok on the R Programming module in the interactive swirl package. Install and load the swirl library by typing the following into the R console within R studio. install.packages(&quot;swirl&quot;) library(swirl) Swirl does a quick overview of the basics. But there are a lot of basics, most of which you’ll forget! But that’s ok. As you go through swirl I suggest you go to the source pane to open up a fresh R script or R markdown file to take notes (eg, copy/paste code snippets) for a custom cheatsheet. 3.4 Other resources There are many, many other resources with which to get started. Try some, if you wish. If you are ready to roll up your sleaves and play with R go here. As you work with R you’ll make mistakes. Of these, &gt;90% will be typos, especially missing commas and unclosed parentheses. If you didn’t mistype a command, just copy and paste error messages into your browser search bar. Most of the errors you will make have happened before. Most of what you want to do someone has done before. Almost always, the best documented solutions are usually found in StackOverflow. Code camps are nice, but by far the best way to learn R is to just start using it to solve the problems you need to solve. I’ll provide you the problems you need to solve in this course. They are about learning statistics, but you’ll end up learning how to use R as a side benefit. By the way, after you install the swirl package, go ahead and install the tidyverse package. "],
["bigpic.html", "Chapter 4 The Big Picture 4.1 What are experimental statistics?", " Chapter 4 The Big Picture To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher library(tidyverse) library(Hmisc) Let’s start by listing out some key characteristics that most biomedical experiments share in common. They… tend to involve the use of relatively small sample sizes. are usually highly exploratory in nature. generate data that are either discrete counts or measurements of continuous scalars. are structured by a small group of fairly common experimental designs. are usually interpreted in a binary way; as having “worked”, or not. test hypotheses (though too often these are unstated). aspire for rigor, replicability and reproducibility. aspire to be unbiased. The stakes of our work can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge, and opening the door to create impactful solutions, particularly in the realm of human diseases and suffering. But no less motivating are the issues more related to the professional practice of science. These include ego, the completely natural impulse to seek out validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. The point is that the process of scientific discovery is driven both by ideals and by biases. This is nothing new. The one big concept that I hope you embrace is that the statistical design and analysis of experiments serves as a working framework within which the biomedical researcher can conduct reasonably unbiased work. The statistical approaches covered in this course, it turns out, were invented long ago with all of these drivers in mind. 4.1 What are experimental statistics? Experimental statistics are used to summarize data into simpler descriptive models. as procedures to draw inferences from samples. as procedures that guide the design of experiments. to serve as framework for conducting unbiased research. Chances are you thought biostats was just one or two of those bullets, and probably not the latter two. 4.1.1 Descriptive modeling Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics. Thus, we use parameters like the sample mean and standard deviation, or median and ranges or percentiles. The alternative is unthinkable today (but common long ago), which is to discuss each replicate individually. To emphasize that sample parameters differ from population parameters, the standard in statistical notation is to use roman characters to indicate samples and greek characters to indicate the population. For example, parameter sample population mean \\(\\bar y\\) \\(\\mu\\) standard deviation \\(s\\) \\(\\sigma\\) variance \\(s^2\\) \\(\\sigma^2\\) Thus, the sample mean, \\(\\bar y\\) is an estimate of the population mean, \\(\\mu\\). Statistical tests also have a descriptive element in that they convey information about the experimental design. If you say, “I’m working up a two-tailed paired t-test,” say no more. From that alone I know something about your hypothesis, how your replicates are handled, the number of predictor groups, and the type of data you’re measuring. Regression models also describe data. For example, here is the well-known Michaelis-Menten model that describes product formation as a function of substrate concentration. \\[[P]=\\frac{[S][Vmax]}{[S]+Km}\\] That’s a model we might fit to certain kinds of enzyme kinetic data, because we use it to estimate scientifically meaningful parameters, like \\(V_{max}\\) and \\(K_m\\). In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. Take a simple example of two groups shown here. Each group has been fit to a simple model: that for the mean and standard deviation. Clearly, that model fits the control group much better than it fits the treatment group. Figure 4.1: Is the mean for each these groups a good descriptive model? Why do I say that? The treatment group data are much more skewed. Most of the data values are greater than the mean of the group. Sure, a mean can be calculated for that group, but it serves as a fairly crappy summary. Perhaps some other model (or group of statistical parameters) would better convey how these data behave? This is to point out that learning statistics is about learning to make judgments about which models are best for describing a given data set. 4.1.2 Statistical inference There are two main types of inference researchers make. One type is to infer whether an experiment “worked” or not…the so-called “significance test”. This familiar process involves calculating a test statistic from the data (eg, t-test, F-tests, etc) and then applying a threshold rule to its value. If the test passes the rule, we conclude the experiment worked. I cover this type of inference in much more detail in the p-value chapter 10, and we’ll talk about it over and again throughout the course. A second type of inference is to extrapolate from a sample some estimate for the values of the variables within the population that was sampled. Both descriptive and statistical inference are subject to error. By random chance alone our sample could be way off the mark, even with perfectly calibrated instrumentation. The real difficulty with inference is we can never know for certain whether we are right or wrong. They are called random variables for a reason. It pays to have a very healthy respect for the role played by random chance in determining the values of our parameter estimates. If we were to completely redo a fully replicated experiment once more, we would almost certainly arrive at different numbers. In a well behaved system, they’d likely be in the same ballpark as those of the first experiment. But they would still differ. To illustrate, copy and paste the code chunk below. It replicates a random triplicate sample six times, taking six means. Unlike in real life, the population parameters are known (because I coded them in): \\(\\mu=2\\) and \\(\\sigma=0.4\\). You can run that chunk tens of thousands of times and never get a “sample” with one mean that has a value of exactly 2, even though that’s the true mean of the population that was sampled. x &lt;- replicate(6, rnorm(3, 2, 0.4)) apply(x, 2, mean) ## [1] 1.765582 2.351564 2.138198 2.356915 2.167006 2.123929 4.1.3 Experimental design Experimental planning that involves dealing with statistical issues is referred here as experimental design. This involves stating a testable statistical hypothesis and establishing a series of decision rules in advance of data collection. These rules range from subject selection and arrangement, predetermination of sample size using a priori power analysis, setting some data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, declaring a primary outcome, on up to what statistical analysis will be performed on the data. Experimental design is very common in prospective clinical research. Unfortunately, very few basic biomedical scientists practice anything remotely like this. Most biomedical researchers begin experiments with only vague ideas about the statistical analysis, which is usually settled on after the fact. Much of the published work today is therefore retrospective, rather than prospective. Yet, most researchers tend to use statistics that are largely intended for prospective designs. That’s a problem. 4.1.4 Statistics as an anti-bias framework If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct. The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn’t provide us a magic woo-woo cloak to protect us from our biases. Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias. Here are some of the more important biases. 4.1.4.1 Cognitive biases From a statistical point of view biases can be classified into two major groupings. The first are Cognitive biases. These are how we think (or fail to think) about our experiments and our data. These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, “how could this not work!” you are in the throes of a pretty deep cognitive bias. In bench research, cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or recognizing telling glitches in the data as meaningful. 4.1.4.2 Systematic biases The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed, and how data is handled or transformed. Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line. 4.1.4.3 Scientific misconduct More malicious biases exist, too. These include forbidden practices such as data fabrication and falsification. This is obviously a problem of integrity. Very few scientists working today are immune from the high stakes issues that pose threats to our sense of integrity. In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them. The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite livable. If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you’ve given it your best shot. That is hard to conclude when the latter occurs. "],
["sampling.html", "Chapter 5 Statistical Sampling 5.1 Experimental units 5.2 Independent Replicates 5.3 Random process 5.4 Statistically valid samples 5.5 Independence of replicates", " Chapter 5 Statistical Sampling An experiment is no more reliable than is its sample. -TJ Murphy A statistically valid sample is comprised of independent replicates of the experimental unit, which are generated using some random process. To unpack this let’s think about each of the following terms: What are experimental units? What do we mean by independent replicates? What is a random process? When is statistical validity even important? 5.1 Experimental units The experimental unit is the source of the measurement. An experimental unit can generate one or many measurement values. I prefer the concept of an experimental unit to the concept of subject, though they often mean the same thing. I’ve found the word subject carries more ambiguity, especially for people first learning sampling and sample size concepts. In some experimental designs (eg, unpaired or completely randomized) each experimental unit generates a single measurement value. Here there is a one-to-one correspondence exists between the number of experimental units and the number of measurement values within a data set. In other designs (eg, paired or matched or repeated/related measure), a single experimental unit can generate more than one measurement values for the same variable. Such data sets have more values than experimental units. Here are some guidelines for deciding what is the experimental unit in an experiment, with full recognition that sometimes there are gray areas. Ultimately the researcher has to use scientific judgment to recognize or define the experimental unit. 5.1.1 A simple test to define the experimental unit When defining an experimental unit I recommend using a simple test: Are these measurements intrinsically-linked? If two or more measurement values are intrinsically-linked then they would comprise paired or matched or related measures from a single experimental unit. So how could you judge whether two or more measurements are intrinsically-linked? For the most part, this happens when the source of those measurements doesn’t differ. Here are a few examples: A before and after design. A mouse is scored on how well it performs a behavioral test at baseline, before a treatment. After that same mouse receives a treatment it is run through the behavioral test once more to get a second score. Those two scores are intrinsically-linked because they were taken from the same mouse. All that differs between the scores is the absence or presence of the treatment, the effect of which the researcher is trying to measure. We would also say those two scores are matched, paired or related/repeated measures. A single mouse from which two scores are derived is an independent replicate of the experimental unit. Twinning. Take for example a study involving human identical twins. In these studies identical twin pairs are modeled as a single experimental unit due to their high level of instrinsic relatedness. There are two human subjects but they are modeled statistically as a single experimental unit. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. One of the pair receives a control condition while the other receives a treatment condition. A measurement is taken from each person. There are two measurements in total, and two people, but only a single experimental unit. Given that the twins are so identical we could reasonably conclude these two measurements are intrinsically-linked. We can model the pair as one. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. Unpaired or completely randomized In contrast, imagine a study using the same control and treatment conditions using unrelated humans (or some other outbred animal species) as subjects. Each subject is assigned either a treatment or a control, and only a single measurement is taken from them. Since the subjects are each very different from each other, we could not conclude that measurements taken from them are intrinsically-linked. Each person stands alone as an experimental unit. The data would be analyzed using an unpaired, unmatched or completely randomized test. Intrinsically-linked measurements are very common in bench work. In fact they are too often overlooked for what they are and mistakenly analyzed as unmatched. Experiments involving batches of biological material, cultured cells and/or littermates of inbred animal strains routinely involve intrinsically-linked measurements. As a general rule, these should always be designed and analyzed using matched/paired/related measures procedures. Cell cultures Cell cultures are remarkably homogeneous. The typical continuous cell line is a monoculture passaged across many doubling generations. Imagine a test conducted on a 6 well multi-well cell culture plate. Each well receives a different level of some treatment condition, such as a dosing or time-course study. All of the wells were laid down at the same time from a common batch of cells. Each well is very highly related to all of the other wells. The intrinsic differences between wells would be relatively minor and mostly due to technical variation. There’s no real inherent biological variation from well-to-well other than that attributable to the level of treatment the well receives. As a result, all of the measurements taken from a plate of wells are intrinsically-linked to each other. The experimental unit is the plate. They should be designed and analyzed using matched/paired/related measure statistical procedures. Furthermore, any other plates laid down at the same time from the same source of cells are virtually identical clones of each other. If we were to expose the wells in all of those plates to various treatments followed by taking some measurement, then it is pretty easy to argue that all of those measurements taken on that passage of cells are intrinsically-linked. None of the wells are independent of any of the other wells, irrespective of the plate. Together, all of the plates represent a single experimental unit. Inbred mice In many regards, the high level of relatedness within inbred mouse strains doesn’t differ from human identical twins, or from cultured cells, for that matter. A given strain of these animals are inbred to genetic homogeneity across several generations. For all intents and purposes all mice derived from a given strain are immortalized clones of each other. Two mice from the same litter are identical twins. Indeed, two mice from different litters from the same strain are identical twins. Due to their clonal identity all measurements taken from any of these highly related subjects are intrinsically-linked. Just as for cell culture, protocols must be contrived to break up the homogeneity. A common approach is to treat the litter as the experimental unit and take measures from littermates as intrinsically-linked. Split tissue Imagine two slices of an organ (or two drops of blood) taken from a single animal. Although the two slices (or drops of blood) are obviously different from each other, any measurements derived from each are intrinsically-linked. The experimental unit would be the animal from which that biological material is derived. Batches Finally, imagine a batch of a purified protein or other biochemical material. The batch was isolated from a single source and prepared through a single process. The material in the batch is highly homogeneous, irrespective of whether it is stored away in aliquots. Any measurement taken from that batch are highly related to any other measurement. They are intrinsically-linked. The batch would be the experimental unit. 5.1.2 Blocking We have to contrive protocols to break up experimental units that have high inherent homogeneity. The statistical jargon used for this is blocking, such that blocks are essentially grouping factors that are not scientifically interesting. Going back to culture plates. Let’s say we prepared three plates on Friday. An assay performed on one plate on Monday would represent one experimental unit of intrinsically-linked measures. An assay repeated on Tuesday on a second plate would represent a second experimental unit. Wednesday’s assay on the third plate is also its own experimental unit. Here the blocking factor is the day of the week. Assuming we created fresh batches of reagents each day, there would be some day-to-day variation that wouldn’t exist if we assayed all threee plates at once on a single day. But we’re not particularly interested in that daily variation, either. More conservatively, cell line passage number can be used as a blocking factor to delineate experimental units. Each passage number would represent an experimental unit and the overall replicated experiment would be said to be blocked on passage number. Defining the experimental unit and any blocking factors requires scientific judgement. That can be difficult to do when dealing with highly homogenous material. What should be avoided is creating a design that limits random chance too severely. To measure on Monday all three plates that were laid down on Friday will probably yield tighter results than if they were blocked over the course of the week. This has to be thought through carefully by the researcher in each and every case. Reasonable people can disagree what whether one approach is superior to some other. Therefore, what is important is to make defensible decisions. To do that, you need to think through this problem carefully. When in doubt, I suggest leaning towards giving random chance a fair shot at explaining the result you’re observing. For example, you can make the case that measurements from two cell culture plates that were laid down on the same day but are collected on different days are not intrinsically-linked. That’s a harder case to make if they are collected on the same day. You will almost certainly have to make the case that measurements taken from two mice on different days or if they are from different litters are not intrinsically-linked. Before going there, we need to chat about what we mean by independent replication. 5.2 Independent Replicates That we should strive for biological observations that are repeatable seems self evident. An experiment is comprised of independent replicates of treatment conditions on experimental units. The total number of independent replicates comprises an experiment’s sample size. A primary goal in designing an experiment is to assess independent replicates that are not biased to the biological response of a more narrowly defined group of experimental units. A replicate is therefore independent when a repeat is on an experimental unit that differs materially from a previous experimental unit. A material difference could involve a true biological replicate. Measurements taken from two unrelated human subjects have a material difference. In bench biological work with fairly homogenous systems (eg, cell lines and inbred animals) a material difference will usually need to be some separation among replicates in time and space in applying the experimental treatments. 5.2.1 A simple test for independence How willing am I to certify this is a truly repeatable phenomenon when replicated in this way? A new scientific discovery would be some kind of repeatable phenomenon. 5.2.2 Some replication examples If we are performing an experiment using pairs of human twins, each pair that is studied stands as an independent replicate. Because the pair is the experimental unit, a study involving 5 pairs will have five, rather than ten, independent replicates. If we conduct an experiment using unrelated human volunteers, or someother out bred animals, each person or animal from whom a measurement is recorded is considered an independent replicate. Their biological uniqueness defines their independence. We wander into gray areas pretty quickly when thinking about the independence of experimental units in studies involving cultured cells, batches of biological material, and inbred mice. Working with these systems it is difficult to achieve the gold standard of true biological independence. The focus instead should be on repeatability….“Working with new batches of reagents and different days do I get the same response?” Imagine a 6 well plate of cultured cells. No well differs biologically from any other. If each well received a repeat of the same treatment at the same time we shouldn’t consider any measurements from that plate independent from others. Otherwise, the sample would be biased to that plate of cells measured at that particular time with a given set of reagents under those particular conditions. It is too biased to that moment. What if we screwed up the reagents and don’t know it? Rather than being independent, it is best to consider the 6 measurements drawn from the plate as technical replicates or pseudo replicates. The data from the 6 wells should be averaged or totaled somehow to improve the estimate of what happened on that plate that day. A better approach with cultured cells is to use passage numbers to delineate independence. Thus, a 6 well plates from any one passage are independent experimental units relative to all other passages. Obviously, given the homogeneity of cells in culture, it’s unlikely there is much biological variation even by these criteria. But to achieve true biological independence would require re-establishing the cell line each time an independent replicate was needed. That’s rarely feasible. Inbred mice pose much the same problem. Scientific judgment is needed to decide when 2 mice from the same strain are independent of each other. One mark of delineation is the litter. Each litter would be independent of other litters. Outcomes of two (or more) littermates could be considered matched or related-measures and thus one experimental unit. 5.3 Random process You can probably sense intuitively how randomization can guard against a number of biases, both systematic and cognitive. Systematic artifacts become randomly distributed amongst the sample replicates, whereas you are less tempted to treated a replicate as preferred if you don’t know what is its treatment level. Mathematical statistics offers another important reason for randomization. In classical statistics the effect size of some treatment is assumed to be fixed. Our estimate of that real value is the problem. Thus, when we measure a value for some replicate, that value is comprised of a combination of these fixed effects and unexplained effects. The variation we observe in our outcome variables, the reason it is a random variable, arises from these unexplained effects. These can be particularly prominent in biological systems. Randomization procedures assures those random effects are truly random. Otherwise we might mistake them for the fixed effects that are of more interest us! This concept will be discussed more formally in the section on general linear models. Suffice to say for pragmatic purposes that random sampling is crucial for limiting intentional and unintentional researcher biases. Either the experimental units should be selected at random, or the experimental units should be assigned treatments at random, and/or the outcome data should be evaluated at random (eg, blind). Sometimes, doing a combination of these would be even better. Usually, the researcher supervises this randomization using some kind of random number generator. R’s sample() function gets that job done for most situations. Let’s design an experiment that involves two treatments and a total of 12 independent experimental units. Thus, 6 experimental units will each receive either of the two treatments. Let’s say that my experimental units each have an ID, in this case, a unique letter from the alphabet. Using sample(1:12) we randomly assign a numeric value to each ID. This numeric value will be the order by which the experimental unit, relative to the other experimental units, is subjected to the experimental treatment. ID’s that are assigned even random numbers get one of the two treatments, and odd numbered ID’s get the other treatment. What we’ve done here is randomize both the order of replication and the assignment of treatment. That’s a well-shuffled deck. You can see how this approach can be readily adapted to different numbers of treatment levels and sample sizes. set.seed(1234) ID &lt;- letters[1:12] order &lt;- sample(1:12, replace=F) plan &lt;- data.frame(ID, order) plan ## ID order ## 1 a 2 ## 2 b 7 ## 3 c 11 ## 4 d 6 ## 5 e 10 ## 6 f 5 ## 7 g 1 ## 8 h 12 ## 9 i 3 ## 10 j 8 ## 11 k 4 ## 12 l 9 5.4 Statistically valid samples For any statistical test to be valid, each replicate within a sample must satisfy the following two criteria: The replicate should be generated by some random process. The replicate must be independent of all other replicates. Why? Statistical tests are one of the last stages of a hypothesis testing process. All of these tests operate, formally, on the premise that at least these two conditions are true. When these conditions have not been met the researcher is collecting data without testing a hypothesis. To run a statsitical test is to pretend a hypothesis has been tested, when it has not. 5.4.1 Select random subjects Let’s say we want to do an experiment on graduate students and need to generate a representative sample. There are 5 million people in the US who are in graduate school at an given time. Let’s imagine they each have a unique ID number, ranging from 1 to 5,000,000. We can use R’s sample() function to randomly select three individuals with numbers corresponding to that range. Sampling with replacement involves throwing a selection back into a population, where it can potentially be selected again. In that way, the probability of any selection stays the same throughout the random sampling process. Here, the replace = FALSE argument is there to ensure I don’t select the same individual twice. sample(x=1:5000000, size=3, replace = FALSE) ## [1] 1413668 4617167 1461579 All that needs to be done is to notify the three people corresponding to those IDs and schedule a convenient time for them to visit so we can do our experiment. You can imagine several variations to randomly select graduate students for measurements. You just need a way to find graduate students, then devise a way(s) to ensure the sampling is as representative as possible. Selecting subjects from a real population is pretty straight forward, a bit like picking 8 lotto balls from a spinning container. A lot of times in experimental work the number of subjects available to the researcher is fixed and smaller. The size of the population to be sampled can be much closer to the number of replicates needed for the experiment rather than a sample from a large pool. In these cases we have to come up with other ways to randomize. 5.4.2 Randomize to sequence For example, let’s say we want to compare condition A to condition B. We have 6 subjects to work with, each of which will serve as an independent replicate. We want a balanced design so will have 3 replicates for each of the 2 conditions. Let’s imagine we can only perform an experiment on one subject, one day at a time. In that case, it makes sense to randomize treatment to sequence. We can randomly generate a sequence of 6 even and odd numbers, and assign them to the daily sequence (MTWTFM) based on which random number is first on its list. We can make a rule that subjects assigned even numbers will receive condition A, whereas condition B is meted out to subjects associated with odd numbers. sample(x=11:16, size=6, replace = FALSE) ## [1] 16 12 15 11 13 14 5.4.3 Randomize to location Let’s imagine 3 treatments (negative control, positive control, experimental), that we will code 1,1,2,2,3,3. These will be applied in duplicate to cells on 6-well cell culture plate. We’ll code the plate wells with letters, a, b, c, d, e, f from top left to bottom right (ie, a and b are wells in the top row). Now we’ll generate a random sequence of those six letters. sample(letters[1:6], replace=F) ## [1] &quot;b&quot; &quot;a&quot; &quot;e&quot; &quot;d&quot; &quot;f&quot; &quot;c&quot; Next, we’ll map the sequence 1,1,2,2,3,3 to those letters. Thus, negative control goes to the wells corresponding to the first two letters in that sequence, positive control to the 3rd and 4th letters, and so forth. 5.4.4 Randomize to block In statistical lingo, a block is a subgroup within a sample. A blocked subject shares some feature(s) in common with other members of its block compared to other subjects in the overall sample. But usually, we’re not interested in block as a variable, per se. Here are some common blocks at the bench are One purified enzyme preparation vs a second preparation of the same enzyme, nominally purified the same way. The two enzyme preps represent two different blocks. A bunch of cell culture dishes plated on Friday from passage number 15 vs ones plated on Tuesday from passage number 16. The two passages represent 2 different blocks. A litter of mouse pups born in January vs a litter born in February. The two different litters represent two different blocks. An experiment run with freshly prepared reagents on Monday vs one run on Tuesday, with a new set of freshly prepared reagents. Each experimental day represents a block. Frequently, each block is taken as an independent replicate. 5.5 Independence of replicates In biomedical research the standard is for biological independence; when we speak of “biological replicates” we mean that each independent replicate represents a distinct biological entities. That standard is difficult to meet when working with many common biological model systems, particularly cell lines and inbred animals. The definition of statistical independence is grounded in the mathematics of probability: Two events are statistically independent when they convey no information about the other, or \\[p(A \\cap B)=p(A)p(B)\\]. Here the mathematics is not particularly helpful. Imagine two test tubes on the bench, each receives an aliquot of biological material from a common prep (eg, a purified protein). One tube then receives treatment A and the other treatment B. As best we know, the two tubes aren’t capable of influencing each other. But we can reasonably assume their responses to the treatments will at least be correlated, given the common source of biological material. Should each tube be treated as if it were statistically independent? Replicate independence that meets statistical validity therefore has to take on a more pragmatic and nuanced definition. My preference is to define a replicate as the independent experimental unit receiving treatment. I like this because it allows for defining the experimental unit differently depending upon the experimental design. "],
["data.html", "Chapter 6 Data Classification 6.1 Dependent and independent variables 6.2 Discrete or continuous variables", " Chapter 6 Data Classification library(datapasta) library(tidyverse) The starting point in any statistical design is to understand the types of data that are involved. Ask yourself whether the variables are discrete or continuous. Then ask if they measured, ordered or sorted? If you don’t understand those two questions, just read on. Because the answers will point you in the proper analytical direction. This is one of the most important things to learn in this course. If you don’t get the concept that not all data types are equivalent, you won’t get statistics. In this section data classification will be discussed. In all likelihood this material will sound simplistic or even obvious to you, but I cannot emphasize enough the importance of data classification in mastering a statistical framework. If for no other reason, understanding how data are classified is crucial in selecting the most appropriate statistical analysis. If you were to approach me to ask, “here’s my stuff, what statistical test should I do?” I would ask, “tell me more about your data.” And we would probably spend a lot of time with you answering my questions until I was sure I understood your data classification. Therefore, a major learning objective for you is, given a data set, to know which variables are dependent and which are independent, and whether the variables involved are continuous (measured) or discrete (ordered or sorted). 6.1 Dependent and independent variables For the experimental researcher there are two basic types of variables. An independent variable is the predictor or explanatory variable imposed by the researcher upon a system. Independent variables have values, the levels of which are determined by the researcher. For example, in a blood glucose drug study, the independent variable “Treatment” would come in two levels, “Placebo” and “Drug”. In R, we’d call treatment a factor variable with two levels. Conventionally, the independent variable is plotted on the abcissa, or x-axis, scale of some graph. A dependent variable is the response or outcome variable collected in an experiment. The values that dependent variables take on are determined by, or dependent upon, the level of the independent variables. For example, the dependent variable in the blood glucose drug study would be a measurement called “blood_glucose”. Most of the time the dependent variable is plotted on the ordinate, or y-axis, scale. In statistical notation the dependent variable is usually depicted by the uppercase symbol \\(Y\\). The values that variable can assume are symbolically represented as lowercase symbol \\(y_i\\), where \\(i\\) is the sample size, ranging from 1 to \\(n\\) independent replicates. Similarly, the indepedent variable is usually depicted by uppercase \\(X\\) (or some other letter) and its values are lowercase \\(x_i\\). I’m going to use that convention but with a twist. Independent variables denoted using \\(X\\) will represent continuous scaled variables, whereas independent variables denoted using \\(A\\) or \\(B\\), or \\(C\\), will represent discrete, factoral variables. These will take on values denoted by lowercases, eg, \\(a_i\\), \\(b_i\\), \\(c_i\\)) . To illustrate dependent and independent variables think about a linear relationship between two continuous variables, \\(X\\) and \\(Y\\) . This relationship can be expressed using the model \\(Y=\\beta_0 + \\beta_1 X\\). \\(X\\) would be a variable the researcher manipulates, such as time or the concentration of a substance. \\(Y\\) would be a variable that the researcher measures, such as absorption or binding or fluorescence. The parameters \\(\\beta_0\\) and \\(\\beta_1\\) are constants that modify the relationship between the two variables, which I’m sure you recognize as representing the y-intercept and slope, respectively, of the regression line between the two variables. Thus, \\(Y\\) takes on different values as the researcher manipulates the levels of \\(X\\). Which explains why \\(Y\\) depends on \\(X\\). For example, here’s how the data for a protein standard curve experiment would be depicted. In the R script below the variable \\(X\\) represents known concentrations of an immunoglobulin protein standard in \\(\\mu g/ml\\). The researcher builds this dilution series from a known stock, thus it is the independent variable. The variable \\(Y\\) represents \\(A_{595}\\), light absorption in a spectrophotometer for each of the values of the standard protein. The \\(A_{595}\\) values depend upon the immmunoglobulin concentration. Estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are derived from running a linear regression on the data with the lm(Y~X) script. Thus, for every one unit increment in the value of \\(X\\), there is a 0.02497 increment in the value of \\(Y\\). Again, \\(Y\\) depends upon \\(X\\). #Protein assay data, X units ug/ml, Y units A595. X &lt;- c(0, 1.25, 2.5, 5, 10, 15, 20, 25) Y &lt;- c(0.000, 0.029, 0.060, 0.129, 0.250, 0.371, 0.491, 0.630) #derive the slope and intercept by linear regression lm(Y~X) ## ## Call: ## lm(formula = Y ~ X) ## ## Coefficients: ## (Intercept) X ## -0.0008033 0.0249705 6.1.1 When there is no independent variable This is a course for experimental biologists. In other types of research, particularly in the public health, behavioral and social science fields, studies are often not strictly experimental. Researchers in these fields generally work with data sets lacking true, experimentally-manipulated independent variables as defined above. Yet these researchers are still very interested learning whether certain phenomena cause other phenomena. The problem of drawing causal inference from studies in which all of the variables are observed is beyond the scope of this course. Pearl offers an excellent primer on considerations that must be applied to extract causality from observational data here. 6.2 Discrete or continuous variables At their most fundamental level, the dependent and independent variables of experiments can each be subclassified further into two categories. They are either discrete or continuous. Discrete variables can only take on discrete values, while continuous variables can take on values over a continuous range. If that’s not clear just yet, it should become more clear by reading below. Variables can be subclassified further as either measured, ordered, or sorted. This subdivision fulfills a few purposes. First, it’s alliterative so hopefully easier to remember. It reminds me of Waffle House hashbrowns, which can be either scattered, smothered or covered, and that is just something you’ll never forget once you’ve visited a Waffle House. Second, it covers all types of data and statistical testing, and thus forms the basis for drawing a pragmatic statistical modeling heuristic. knitr::include_graphics(&quot;images/testing_heuristic.jpg&quot;) Figure 6.1: The type of data dictates how it should be modeled. Third, the “measured, ordered, sorted” scheme classifies variables on the basis of their information density, where measured &gt;&gt; ordered &gt;&gt; sorted. Different authors/softwares give these three types of variables different names, which creates some confusion. In SPSS, for example, when setting up variables you can choose to classify it as scalar, ordinal, or nominal, which correspond to measured, ordered and sorted. Another fairly common descriptive set for the three types is interval, ordinal, and categorical. These correspond to measured, ordered, and sorted, too. Though they are named differently, for the most part everybody seems to agree that all variables can be reduced to 3 subtypes, even if they can’t agree on what to name them. 6.2.1 Measured variables Because everything is measured in some sense of the word “measured”&quot; is probably not an ideal choice to describe what is meant when refering to a continuous variable. My rationale for the choice is that it almost always requires some kind of measuring instrument to grab this type of data. For the present purposes let’s considered the terms measured variables and continuous variables as synonymous. Measured variables are fairly easy to spot. Any derivative of one of the seven base SI units will be a measured variable. knitr::include_graphics(&quot;images/si_units.jpg&quot;) Figure 6.2: The seven SI units Take mass as an example. The masses of physical objects can be measured on a continuous scale of sizes ranging from super-galaxian to subatomic. Variables that are in units of mass take on a smooth continuum of values over this entire range because mass scales are infinitesimily divisible. Here’s a thought experiment for what infinitesimily divisible means. Take an object that weighs a kilogram, cut it in half and measure what’s left. You have two objects that are each one half a kilogram. Now repeat that process again and again. After each split something always remains whose mass can be measured. Even though it gets smaller and smaller. Even when you arrive at the point where only a single atom remains it can be smashed into yet even smaller pieces in a supercollider, yielding trails of subatomic particles….most of which have observable masses. But here’s what’s important about continuous variables: That continuity between gradations means that continuous variables can carry more information than other types of variables. That’s what I meant by information density, in the comment above. On a scale of micrograms, an object weighing one kilogram would have one billion subdivisions. If you have an instrument that can accurately weigh the mass of kilogram-sized objects to the microgram level, and each microgam would be informative, you would say that one kilogram is comprised of a billion bits of information. All of that possible information explains why the distinction between continuous and discrete variables is so important. You’ll see that discrete variables lack this kind of information density between their units. As you read on below as discrete variables are discussed, think about how continuous variables carry more information than discrete variables. More pragmatically, this difference is the basis for why discrete and continuous data behave so differently. And because of this inherent basis for why they behave differently, statisticians have devised statistical models that are more appropriate for one kind of data vs some other. 6.2.2 Discrete categorical and ordinal variables Discrete variables are discontinuous. The units of discrete variables are indivisible. Unlike continuous variables, discrete variables offer no information between their unit scale boundaries. There are two types of discrete variables. These are called ordinal and categorical. I like to call these ordered and sorted, respectively, again for alliterative purposes. 6.2.2.1 Sorted data Categorical variables are a bit easier to understand so let’s start with those. These variables represent objects that are counted. Because they have certain features they are sorted into categories or, as I like to say, buckets. For example, a biostats class might be comprised of 50 students, 15 of whom are men and the rest are women. The name of the variable is sex. The values that the “sex” variable can take on is either male or female. The variable sex is a factoral variable at two levels. If we count all of the men and woman in a class we arrive at another variable called count which represents the discrete counts of people who are sorted into either of the two sex categories. The variable count is an integer varible. It cannot take on any values other than integer values. There cannot be a case that has less than a whole student. A partial biostats student would be absurd! So our data set has two variables. One is sex, a factoral variable that has two levels. The other is count, an integer variable that has $$50 levels. Of course, the categorization of sex is sometimes ambiguous. If it is important to accomodate more, we would add additional categories to account for all possible outcomes. For example, the sex variable could be set to take on values of man, woman, and other. Take a moment to also think about the values of that sex variable. This is to emphasize that man, woman and other are not numeric values. Variables can have non-numeric values. R reads those levels as character values and “coerces” to classify sex as a factoral variable with three levels, man, woman and other. Let’s use R to create a summary table for the composition by sex of the biostats class. Inspection of the code shows the table has two variables, sex as described, and count. The function str(ibs538) reveals that the former variable is a “Factor w/ 3 levels”&quot; and the later is a integer variable. We used the is.integer function in the code to ensure that count would be an integer variable. Had we not, R wanted to coerce it as a numeric variable. Finally, notice how the variable count is only comprised of discrete integer values. These discrete counts are why sorted data is classified as discrete. sex &lt;- c(&quot;man&quot;, &quot;woman&quot;, &quot;other&quot;) count &lt;- as.integer(c(15, 35, 0)) ibs538 &lt;- data.frame(sex, count); ibs538 ## sex count ## 1 man 15 ## 2 woman 35 ## 3 other 0 str(ibs538) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ sex : Factor w/ 3 levels &quot;man&quot;,&quot;other&quot;,..: 1 3 2 ## $ count: int 15 35 0 Obviously, there’s nothing experimental about counting the sex of biostats students. However, many biomedical experiments generate discrete categorical data, too. Imagine the following: Neurons are poked with an electrode. Counts are recorded of the number of times they depolarize over a certain time period, in response to an depolarizing agent and its control. Cells are stained for expression of a marker protein. The number of cells in which the protein is detected are counted. Counts of a knockdown condition are compared to a control. By some criteria, cells are judged to be either alive or dead and counted as such. The number of alive cells are counted after manipulating expression of a tumor suppressor gene, and compared to a control. By some criteria, mice are judged to either show a disease phenotype or not, and counted as such. Disease incidence is counted in response to levels of a therapeutic agent, or a background genotype, or in response to some stressor. There are an infinite number of examples for experiments that can be performed in which a dependent variable is categorized and each of the replicates are sorted into one or some other level of that category. In the end, sometimes even after some fairly sophisticated instrumentation or biochemical analysis, all the researcher does is count objects that have a characteristic or some other. Unlike continuous variables, discrete variables don’t possess any information between their units. In each of the cases above the replicate either possesses a level of the variable or it does not. It belongs in one bucket or some other. 6.2.2.2 Ordered data Ordered data is, in one sense, a hybrid cross of sorted and measured data. If you’ve ever taken a poll in which you’ve been asked to evaluate something on a scale ranging from something akin to “don’t like at all” to “couldn’t live without”…then you’ve experienced ordinal scaling (such scales are called Likert scales). The precourse survey for this course is chock full of questions that generate data on an ordered scale. Ordered variables are structured to have levels which are quantitatively related to each other. Each experimental replicate is evaluated and then categorized to one of the values of the ordinal variable. It is true there is an element of sorting, but the key difference is these aren’t nominal categories as in sorted data. There are underlying gradations of the variable’s scale. This is not just the absence or presence of an attribute, but rather some amount of the attribute relative to other possible amounts the scale allows for. These gradations within the levels of the ordered variable make them somewhat like measured data. The data strucure has intervals. But ordinal data are discrete because only certain values for the measurement are allowed, depending upon the structure of the scale or scoring system for a given attribute. There is no information between the intervals. Disability status scales represent classic ordinal scales. These are used to assess neurologic abnormalities, for example, those associated with experimental multiple sclerosis. Each replicate in a study is evaluated by trained researchers and assigned the most appropriate value given its condition: 0 for no disease, 1 for limp tail, 2 for mild paraparesis, 3 for moderate paraparesis, 4 for complete hindlimb paralysis, and 5 for moribound. Obviously, in this ordinal scale, as the numeric value increases so to does the severity of the subject’s condition. Here’s what a very small set of ordinal data might look like: genotype &lt;- c(rep(&quot;wt&quot;, 3), rep(&quot;ND4&quot;, 3)) DSS_score &lt;- as.integer(c(0,1,1,5,3,5)) results &lt;- data.frame(genotype, DSS_score); results ## genotype DSS_score ## 1 wt 0 ## 2 wt 1 ## 3 wt 1 ## 4 ND4 5 ## 5 ND4 3 ## 6 ND4 5 str(results) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ genotype : Factor w/ 2 levels &quot;ND4&quot;,&quot;wt&quot;: 2 2 2 1 1 1 ## $ DSS_score: int 0 1 1 5 3 5 genotype is an indepedent, factoral variable that comes in two levels, wt or ND4. DSS_score is a dependent variable that comes in 6 levels, the integer values ranging from 0 to 5. We need to force R to read DSS_score for what it is, an integer rather than as a numeric. One of the key issues of ordinal scales is that they are not necessarily guassian. As a general rule they tend to be skewed (though there is no inherent reason for this to be the case). For example, in my precourse survey I ask students how excited they are, on a scale of 1 to 10, to take a biostats class. The result is decidedly mixed. There’s a bit of a lean towards good enthusiasm, but a fairly pronounced unenthusiastic tail. The data on this ordinal scale are not normally distributed. Any analysis would need to be with a statistical procedure that does not assume normally distributed dependent variables. ggplot(data.frame(score = c(1, 25)), aes(score)) + stat_function(fun = dnorm, args=list(mean=15, sd=1.5)) df &lt;- data.frame(score=c(1:25), y=dpois(1:25, 15)) ggplot(df, aes(score, y))+ geom_col() df &lt;- data.frame(score=c(1:25), y=dbinom(1:25, 25, 0.6)) ggplot(df, aes(score, y))+ geom_col() "],
["dispersion.html", "Chapter 7 Variability, Accuracy and Precision 7.1 Variance: Quantifying variation by least squares 7.2 Standard deviation 7.3 Other ways of describing variability 7.4 Precision and Accuracy 7.5 Standard error 7.6 Confidence intervals 7.7 Key take aways", " Chapter 7 Variability, Accuracy and Precision library(tidyverse) library(ggformula) One of my favorite sayings is “models are perfect, data are not”, because it’s a simple way to express statistics. We sample a population. In statistics, the population parameters that interest us are held to have true, fixed values. The models we conjure up to estimate these parameters are also perfect. The parameters are important because they have biological meaning: affinities, maximum responses, activity rates, depolarization frequencies,…, whatever. What’s imperfect, unfortunately, are the data that we generate. Stood up against a perfect model, they are usually come off looking pretty crappy. The models aren’t always good fits for the data, and sometimes it’s ambiguous whether one model fits best, or some other. Therefore, a major focus of statistics is to evaluate how well our perfect models fit these messy data. Quantifying the wellness of that fit is one basis for asserting how reliable are our parameter estimates. Which in turn means how well we think we understand a system. All of this points to the fact that it is not possible to understand statistical analysis without wrapping our heads around the concepts related to data dispersion: variability, accuracy and precision. The reason data are messy is that sample data possess inherent variability. This means there is always some ‘error’ between the true values of the population parameters and their estimated values within the model. That error is held to be random. In statistics, random error is defined as the variation that we can’t account for in our model. In running experiments on biological systems we think about two main contributors to this error. The first is biological variation. There are confounding factors inherent within the sampled specimens that are driving the observed variation, but we don’t understand them and are not controlling for them. The second source is so-called systematic error, having to do with our technique and equipment, the way we measure the sample. As you recall from middle school, a straight line drawn between two variables is described by the linear model \\(Y=\\beta_0+\\beta_1 X\\). This model has two parameters, \\(\\beta_0\\) represents the y-intercept while \\(\\beta_1\\) represents the slope of the line. The predictor variable is \\(X\\). Thus, passing values of \\(X\\) into this model generates values for the response variable, \\(Y\\). I mention this so I can illustrate randomness, perfect models, and all of that. Here’s a simulation of response data from two linear models, \\(Y1\\) and \\(Y2\\). Reading the code, you notice that one of the models, \\(Y1\\) has no error term, so it is perfect, which is to say the values of \\(Y1\\) are perfectly predicted by the model’s intercept and slope parameters. A linear regression goes through every data point and the output of that regression yields the exact same parameter values that were input (a bit of a circular proof, if you will). The model perfectly fits its data. The other model (\\(Y2\\)) has an error term tacked onto it. As a result it yields imperfect values of \\(Y2\\) even when given the exact same model parameters as for \\(Y1\\). Applying a linear regression to this data yields an imperfect fit…the line doesn’t go through all of the data points. Error is associated with the estimate (the gray shade), and the intercept and slope parameters estimated through linear regression differ somewhat from the parameter input values. #intital model parameter values b0 &lt;- 0 b1 &lt;- 5 X &lt;- seq(1,10,1) error &lt;- rnorm(length(X), mean=0, sd=10) #models Y1 &lt;- b0+b1*X Y2 &lt;- b0+b1*X + error #put the simulations into a data frame so it can be plotted df &lt;- data.frame(X, Y1, Y2) %&gt;% gather(model, response, -X) #plotting function ggplot(df, aes(X, response))+ geom_point()+ geom_smooth(aes(X, response), method=lm)+ facet_grid(cols=vars(model)) Figure 7.1: Models are perfect, data are not. You can appreciate how the second model, with the random error term, is more real life. What’s even more interesting is that every time you run the second model you’ll generate a completely different result. Copy and paste that code chunk into R and see for yourself. That error term is just a simulation of a random process. And that’s just how random processes behave! (Play with that code if you don’t understand how it works. Run it on your machine. Change the initializers) In real life, unlike in this simulation, we wouldn’t know the true values of the parameters (as we know them in this simulation). In real life we sample and fit a perfect model to imperfect data. The resulting output provides estimates of the true parameter values. But the output also includes several bits of information that summarizes all of the variability in the data. Interpreting that tells us whether or not the model is a good way to explain the data. At this point in the course it’s not important to interpret what all of this error analysis means. The point I’m making right now, and that hopefully you’ll grow to appreciate, is that to a large extent, statistics is mostly about residual error analysis. It’s actually sketchy if there is no residual error. I mean, look at the automagic warning message R barks out below for the summary of the first of these linear regressions…the perfect model: summary(lm(Y1~X)) ## Warning in summary.lm(lm(Y1 ~ X)): essentially perfect fit: summary may be ## unreliable ## ## Call: ## lm(formula = Y1 ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.756e-15 -1.193e-15 -2.021e-16 1.623e-15 3.315e-15 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.494e-15 1.736e-15 -2.588e+00 0.0322 * ## X 5.000e+00 2.798e-16 1.787e+16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.541e-15 on 8 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 3.193e+32 on 1 and 8 DF, p-value: &lt; 2.2e-16 summary(lm(Y2~X)) ## ## Call: ## lm(formula = Y2 ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.8861 -5.0245 0.4726 5.4177 15.1667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.946 6.292 -0.309 0.765020 ## X 5.479 1.014 5.403 0.000644 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.211 on 8 degrees of freedom ## Multiple R-squared: 0.7849, Adjusted R-squared: 0.758 ## F-statistic: 29.19 on 1 and 8 DF, p-value: 0.0006436 In some respects, you can say that what we do as researchers is iterate through discovery problems. Each time we improve our understanding of a system, in essence, we’re explaining just a little bit more of what we previously classified as residual error. Is there a biological cause for the variation I see in some assay? Can I explain it by testing a new predictor variable? If I use better technique or different equipment can I “clean up” some of the variation in the results, so that I can get a better handle on some ideas I have about what causes that biological variation? In big picture words, the signals that we are looking for as biological researchers are actually within all of that noise. Irrespective of the statistical variables and tests that you will use, every null hypothesis that you will ever test is, basically, stating “This data can only be explained by random noise.” 7.1 Variance: Quantifying variation by least squares The two primary methods to quantify variation are termed “ordinary least squares” and “maximum likelihood estimation”. Mathematically they are distinct, and that difference is beyond the scope of this course. It would require teaching math. What is important is that, in most cases, either method will arrive at roughly the same solution. Ordinary least squares is the method used for quantifying variation for most of the statistics discussed in this course. The only exception is when we get to generalized linear models. So we’ll omit consideration of maximum likelihood estimation until then. Ordinary least squares arises from the theoretical basis of variance, and is a deceptively simple concept. It can be shown (by mathematicians) that the variance of a random variable, \\(Y\\), with an expected value equivalent to its mean, \\(E(Y) = \\mu\\), is the difference between it’s squared expected value and its expected value squared. \\[Var(Y) = E(Y^2)-E(Y)^2 \\] Imagine a sample of size \\(n\\) replicates drawn from a population of the continuous random variable, \\(Y\\). The replicate sample values for \\(Y\\) are \\(y_1, y_2, ...y_n\\). The mean of these replicate values provides an estimate for the mean of the sampled population, \\(\\mu\\), and is \\[\\bar y = \\frac{{\\sum_{i=1}^n}y_i}{n}\\] The value by which each replicate within the sample varies from the mean is described alternately as deviate or as a residual. Each represents the same thing: \\(y_i-\\bar y\\). A vector of sample replicates will have values that are smaller than and larger than the mean. Thus, some variate values are positive, while others are negative. In a random sample we would expect that the values for replicates are roughly equally dispersed above and below a mean. Thus, if we were to sum up the deviates (or residuals) we’d expect that that sum’s value to approach zero. \\[E(\\sum_{i=1}^n(yi-\\bar y))=0\\] By squaring the deviates, negative values are removed, providing a parameter that paves the way to more capably describe the variation within the sample than can the sum of the deviates. This parameter is called the “sum of squares”: \\[SS=\\sum_{i=1}^n(yi-\\bar y)^2\\] A sample’s variance is the sum of the squared deviates divided by the sample degrees of freedom, \\(df=n-1\\). \\[s^2=\\frac{\\sum_{i=1}^n(yi-\\bar y)^2}{n-1}\\] A sample’s variance, \\(s^2\\) is an estimate of the variance, \\(\\sigma^2\\), of the population that was sampled, in the same way \\(y_bar\\) estimates \\(\\mu\\). You can think of variance as an approximate average of \\(SS\\). The reason \\(s^2\\) is arrived at through dividing by degrees of freedom, \\(n-1\\), rather than by \\(n\\) is because doing so produces a better estimate of the population variance, \\(\\sigma^2\\). Mathematical proofs of that assertion can be found all over the internet. Variance is a hard parameter to wrap the brain around. It describes the variability within a data set, but geometrically: The units of \\(s^2\\) are squared. For example, if your response variable is measured by the \\(gram\\) mass of objects, then the variance units are in \\(grams^2\\). That’s weird. Later in the course, we’ll discuss statistical testing using analysis of variance (ANOVA) procedures. The fundamental idea of ANOVA is to test for group effects by “partitioning the error”. That’s done with \\(SS\\). When a factor causes some effect, the \\(SS\\) associated with that factor get larger…by the square of the variation. Statistical testing is then done on the variance in groups, which in ANOVA jargon is called the “mean square”, or \\(MS\\). \\(MS\\) is just another way for saying \\(s^2\\). 7.2 Standard deviation The sample standard deviation solves the problem of working with an unintuitive squared parameter. The standard deviation is a more pragmatic descriptive statistic than is variance. The standard deviation is the square root of the sample variance: \\[sd=\\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{n-1}}\\] 7.2.1 What does the standard deviation tell us The sample standard deviation is two things at once. It is * a statistical parameter that expresses the variability within the sample. * an estimate of the variability within the population that was sampled. There aren’t many factoids in statistics worth committing to memory, but this one on the standard deviation is one of them: A bit over two thirds of the values for a normally-distributed variable will lie between one standard deviation below and above the mean of that variable. Here’s one way to calculate that using R’s pnorm function, the cumulative distribution function: pnorm(-1, lower.tail = T) #Calculates the AUC below a zscore of -1. ## [1] 0.1586553 pnorm(1, lower.tail = F) #Calculates the AUC above a zscore of 1 ## [1] 0.1586553 1-pnorm(-1)-pnorm(1, lower.tail = F) #The middle range ## [1] 0.6826895 Explore: Use pnorm to calculate the AUC between z scores of -2 and 2. Figure 7.2: About 86% of the values for a normally distributed variable are within +/- one standard deviation from the mean. No matter the scale for the variable, the relative proportion of values within 1 standard deviation for normally distributed variables will always behave this way. Here’s the distribution of serum glucose concentration values, where the average is 100 mg/dl and the standard deviation is 10 mg/dl: Figure 7.3: Modeling the distribution of a blood glucose variable. 7.3 Other ways of describing variability *Just show all of the data as scatter plots! There’s no need to hide the variability in bar plots with “error bars”. *Violin plots are pretty ways to illustrate the spread and density of variation graphically. *The coefficient of variation, \\(cv=\\frac{sd}{mean}\\), is a dimensionless index. If you tell me, “My cv is 60%,” then my response would be, “Mmmm, that sounds pretty noisy.” That is, the more cv’s you pay attention to, the better you appreciate the physical implications of a given value. *Percentiles and ranges. In particular, the innerquartile range. This is usually reserved for non-normal data, particularly discrete data. The IQR illustrates the spread of the middle 50% of data values, from the 25th to the 75th percentiles, and is usually accompanied by the use of the median as the centrality parameter. 7.4 Precision and Accuracy Even with well-behaved subjects, state-of-the-art equipment, and the best of technique and intentions, samples can yield wildly inaccurate estimates, even while measuring something precisely. My favorite illustration of this is how estimates for the masses of subatomic particles have evolved over time. We can probably assume that the real masses of these particles have remained constant. Yet, note all the quantum jumps, pun intended, in their estimated values. Ways of measuring things change. What seems very accurate today could prove to be wildly inaccurate tomorrow. And just because you’ve measured something precisely doesn’t mean you know its true value. If there is one clear take away from this it’s that all such statistical estimates are provisional. 7.5 Standard error This will sound counter-intuitive, but we can actually know how precise our estimate of some parameter is without knowing the true value. That’s because precision is the repeatability of a measurement, and it is possible to repeat something very, very reliably but inaccurately. The standard error is the statistical parameter used to express sample precision. Standard error is calculated from standard deviation \\[precision:\\ SE = \\frac{sd}{\\sqrt n}\\] In contrast, as the history of subatomic particle estimates illustrates, we can never know for sure whether an estimate derived from a sample accurately estimates the sampled population. We will always be uncertain. 7.5.1 What exactly does the standard error represent? The central limit theorem predicts a few important things: 1) A distribution of many sample means will be normally distributed, even if a non-normal distribution is sampled. 2) A distribution of sample means will have less dispersion with larger sample sizes. 3) If a sample population has a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means of sample size \\(n\\) drawn from that population will have a mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt n}\\). These features are illustrated below, as a prelude to further defining standard error. The first graph showing a big black box simply plots out a uniform distribution of the random variable \\(Y\\) ranging in values from 0 to 1, using the duniffunction. It just means that any value, \\(y\\), plucked from that distribution is as equally likely as all other values. ggplot( data.frame( x=seq(0, 1, 0.01)), aes(x))+ stat_function( fun=dunif, args=list(min=0, max=1), xlim=c(0, 1), geom=&quot;area&quot;, fill=&quot;black&quot;)+ labs(y=&quot;p(y)&quot;, x=&quot;Y&quot;) Figure 7.4: A variable with a uniform distribution. The probability of sampling any one of these values is equivalent to that for all other values. Uniform distributions arise from time-to-time. For example, important actually, the distribution of p-values from null hypothesis tests is uniform. As you might imagine, the mean of this particular uniform distribution is 0.5 (because it is halfway between the value limits 0 and 1). Probably less obvious is the standard deviation for a uniform distribution with limits \\(a\\) and \\(b\\) is \\(\\frac{b-a}{\\sqrt{12}}\\). So the standard deviation for this particular uniform distribution \\(\\sigma\\) = 0.2887. Just so you trust me, these assertions pass the following simulation test: mean(runif(n=100000, min=0, max=1)) ## [1] 0.5004781 sd(runif(n=100000, min=0, max=1)) ## [1] 0.2888716 The next graph illustrates the behavior of the central limit theorem. In the following script, random samples are taken from this uniform distribution many times. It takes either 1000 small random samples (n=3) or 1000 large random samples (n=30). It then calculates the means for each one of those samples before finally plotting out a histogram showing the distribution of all of those sample means. Notice how the distributions of sample means are normally distributed, even though they comes from a uniform distribution! That validates the first prediction of the CLT, made above. Note also that the distribution of means corresponding to the larger sample sizes has much less dispersion than that for the smaller sample sizes. That validates the second CLT prediction made above. meanMaker &lt;- function(n){ mean(runif(n)) } small.sample &lt;- replicate(1000,meanMaker(3)) large.sample &lt;- replicate(1000, meanMaker(30)) ggplot(data.frame(small.sample, large.sample))+ geom_histogram(aes(small.sample), fill=&quot;red&quot;)+ geom_histogram(aes(large.sample), fill=&quot;blue&quot;, alpha=0.5 )+ xlab(&quot;n=3 (red) or n=30 (blue&quot;) Figure 7.5: The central limit theorem in action, distributions of sample means of small (red) and large (blue) sample sizes. As for the third point, the code below calculates a mean of all these means, and the SD of all these means, for each of the groups. mean(small.sample) ## [1] 0.5085439 sd(small.sample) ## [1] 0.1585738 mean(large.sample) ## [1] 0.5000129 sd(large.sample) ## [1] 0.05231721 Irrespective of sample size, the mean of the means is a great estimator of the mean of the uniform distribution. But passing the sample means into the sd function shows that neither provides a good estimate of the \\(\\sigma\\) for the population sampled, which we know for a fact has a value of 0.2887. Obviously, the standard deviation of a group of means is not an estimator of the population standard deviation. So what is it? The standard deviation of the distribution of sample means is what is known as the standard error of the mean. This goes a long way to illustrate what the standard error of the mean of a sample, SEM, actually represents. It is an estimator of the theoretical standard deviation of the distribution of sample means,\\(\\sigma_{\\bar y}\\). Now what are the implications of THAT? There are a few. First, SEM is not a measure of dispersion. It is a measure that describes the precision by which a mean has been estimated. The SEM for the large sample size group above is much lower than that for the small sample size group. Meaning, \\(\\mu\\) is estimated more precisely by using large sample sizes. Second, statistically naive researchers use SEM to illustrate dispersion in their data (eg, using SEM as “error bars”). Ugh. They do this because, invariably, the SEM will be lower than the SD. And that looks cleaner. But they shouldn’t do this, because SEM is not an estimator of \\(\\sigma\\). Rather, SEM estimates \\(\\sigma_{\\bar y}\\). Those are two very different things. I suggest you use SEM if it is important to illustrate the precision by which something is measured. That’s usually only important for parameters that are physical constants. Third, when we do experiments we sample only once (with \\(n\\) independent replicates) and draw inferences about the population under study on that bases of that one sample. We don’t have the luxury or resources to re-sample again and again, as we can in simulations. However, these simulations illustrate that, due to the central limit theorem, a long run of sampling is predictably well-behaved. This predictability is actually the foundation of statistical sampling methodology. The SEM estimates a theoretical parameter (\\(\\sigma_{\\bar y}\\) that we would rarely, if ever, attempt to validate. Yet, on the basis of one sample it serves a purpose by providing an estimator of precision. 7.6 Confidence intervals Confidence intervals are the range of values within which we predict is the true value of a parameter, with some confidence. We just need to pick a level within which to be confident. 90%? 95%? 99%? Once that is decided, we can calculate the range within which that true value likely lies. The confidence interval serves as the statistical parameter used to express sample accuracy. Standard errors and confidence intervals are calculated from the same sample data. So there is a bit of a circular argument at play here. We use the variability within the sample to generate two related parameters, standard error and confidence intervals. The former tells us about the precision and the latter tells us about the accuracy. 7.6.1 Simulations The random sampler is a script that generates a random sample from a known distribution before calculating several sample parameters, including the confidence interval. It’s particularly useful to illustrate the relationships between confidence intervals, sample sizes, standard deviations, and confidence levels. When the confidence interval is accurate, a blue-colored bar appears. When the confidence interval is not accurate, you get a red bar. The confidence interval for a sample mean is calculated as follows: \\[CI=\\bar x \\pm t_{df(n-1)}\\cdot\\frac{sd}{\\sqrt{n}}\\] set.seed(1234) n &lt;- 3 pop.mean &lt;- 100 pop.sd &lt;- 25 sig.dig &lt;- 2 conf.level &lt;- 0.99 x &lt;- c(seq(1, 200, 0.1)) #draw sample and calculate its descriptive stats mysample &lt;- rnorm(n, pop.mean, pop.sd) mean &lt;- round(mean(mysample), sig.dig) sd &lt;- round(sd(mysample), sig.dig) sem &lt;- round(sd/sqrt(n), sig.dig) ll &lt;- round(mean-qt((1+conf.level)/2, n-1)*sem, sig.dig) ul &lt;- round(mean+qt((1+conf.level)/2, n-1)*sem, sig.dig) #print to console print(c(round(mysample, sig.dig), paste( &quot;mean=&quot;, mean, &quot;sd=&quot;, sd, &quot;sem=&quot;, sem, &quot;CIll=&quot;, ll, &quot;CIul=&quot;, ul))) ## [1] &quot;69.82&quot; ## [2] &quot;106.94&quot; ## [3] &quot;127.11&quot; ## [4] &quot;mean= 101.29 sd= 29.06 sem= 16.78 CIll= -65.25 CIul= 267.83&quot; # graph the sample confidence interval on the population pretty &lt;- ifelse(ll &gt; pop.mean | ul &lt; pop.mean, &quot;red&quot;, &quot;blue&quot;) #graph (note: using ggformula package) gf_line(dnorm(x, pop.mean, pop.sd)~x)%&gt;% gf_segment(0 + 0 ~ ll + ul, size = 2, color = pretty)%&gt;% gf_labs(subtitle = paste(100*conf.level, &quot;% CI =&quot;,ll, &quot;to&quot;,ul)) Figure 7.6: Confidence interval illustrator Here’s a script that compares the confidence intervals of two means, from random samples of known distributions. When two CI’s don’t overlap, they remain colored blue and green, and the two means they correspond to are unequal. It’s the same as a test of significance without the formal hypothesis. If the two CI’s overlap, one of them turns red. It’s a sign the two samples are indistinguishable, since the 95% CI of one sample includes values in the 95% CI of the other. It’s also the same as a test of significance, for a null outcome. What’s should be done with this simulator is to use it to gain an intuitive understanding about how confidence intervals operate. Practice changing the sample size (n), the population means and standard deviations, and even the confidence level. Under what conditions are overlapping intervals diminished? What factors influence narrower intervals? n &lt;- 5 m &lt;- 100 conf.level=0.95 t &lt;- qt((1+ conf.level)/2, n-1) pop.mean.A &lt;- 125 pop.mean.B &lt;- 200 pop.sd.A &lt;- 25 pop.sd.B &lt;- 25 x &lt;- c(seq(1, 300, 0.1)) y &lt;- seq(0.0005, m*0.0005, 0.0005) #simulate mydat.A &lt;- replicate( m, rnorm( n, pop.mean.A, pop.sd.A ) ) ldat.A &lt;- apply( mydat.A, 2, function(x) mean(x)-t*sd(x)/sqrt(n) ) udat.A &lt;- apply( mydat.A, 2, function(x) mean(x)+t*sd(x)/sqrt(n) ) mydat.B &lt;- replicate( m, rnorm( n, pop.mean.B, pop.sd.B ) ) ldat.B &lt;- apply( mydat.B, 2, function(x) mean(x)-t*sd(x)/sqrt(n) ) udat.B &lt;- apply( mydat.B, 2, function(x) mean(x)+t*sd(x)/sqrt(n) ) ci &lt;- data.frame( y, ldat.A, udat.A, ldat.B, udat.B ) alt &lt;- ifelse( udat.A &gt;= ldat.B, &quot;red&quot;, &quot;blue&quot; ) #plots made with ggformula package gf_line(dnorm( x, pop.mean.A, pop.sd.A)~x, color = &quot;dark green&quot; )%&gt;% gf_line(dnorm( x, pop.mean.B, pop.sd.B)~x, color = &quot;blue&quot; )%&gt;% gf_segment( y+y ~ ldat.A + udat.A, data = ci, color = &quot;dark green&quot; )%&gt;% gf_segment( y+y ~ ldat.B + udat.B, data = ci, color = alt )%&gt;% gf_labs( y = &quot;dnorm(x, pop.mean, sd.mean&quot; ) Figure 7.7: Comparing two samples using confidence intervals. Red-colored indicates the two CI overlap, meaning the two groups for that sample test out as no different. 7.7 Key take aways Variability is inherent in biological data. The two main sources are intrinsic biological variation–which has so many causes, and variability associated with our measurements. Statistics operates on the presumption that the values of parameters in the populations we sample are fixed. Residual error is unexplained deviation from those fixed values. About two-thirds of the values of a normally distributed variable will lay, symmetrically, within one standard deviation on either side of the variable’s mean. Variance expresses the variability within a sample, but geometrically, in squared units. The standard deviation, the square root of variance, estimates the variability of a sampled population. The standard error of the mean estimates the precision by which a sample mean estimates a population mean. The standard error of the mean grows smaller as sample size gets larger, by the square root of n. The standard error of the mean is the standard deviation of a theoretical distribution of sample means. A confidence interval estimates the accuracy by which a parameter, such as a mean, has been estimated. If the confidence intervals of two samples do not overlap, the sampled distributions likely differ. For a 95% CI, there is a 95% chance the true mean is within the interval. A 99% CI will be wider than a 95% CI, given the same data. The central limit theorem saves the day for small sample science, because even if the variable you are measuring is not normally-distributed, the means of your samples are. "],
["hypotheses.html", "Chapter 8 Framing statistical hypotheses 8.1 The decision process 8.2 Popper and falsification 8.3 Statistical hypothesis rubric", " Chapter 8 Framing statistical hypotheses “There is no more to science than its method, and there is no more to its method than Popper has said.”-Hermann Bondi Hypothesis-driven research tests predictions about the nature of the world. Testing hypotheses statistically provides a pragmatic framework for making decisions about the validity of those predictions. When planning an experiment the primary goal should be to bring hyper-focused clarity to the hypothesis. This is the time to distill your thinking down to the exact question you want answered. What are you studying? What is not known? What is your prediction? How will you measure it? What are your variables? Are they discrete or continuous? How will you test it? How will you decide whether what you predicted happened or not? Will this actually answer the question you’re asking? The statistics taught in this course are for assessing the validity of experimental outcomes in a somewhat odd way: Formally, we test the null hypothesis. The expectation is to generate observations of such extreme magnitude that we can reject the null, the hypothesis that nothing happened. At first blush that might come off as absurd. Like a Seinfeld episode, where nothing is what is most important. Hopefully this won’t seem so odd after I describe what this accomplishes and explain why it is done this way. 8.1 The decision process Everybody knows something about the p-value. When it’s low enough, the experiment “worked”. Before diving into the nitty gritty of p-values, let’s jump into a wider angle format to flesh out how they are used. The framework can be broken down into 5 key steps: We begin with a null hypothesis–yes, the boring one about nothing. Experiments generate data. The data are transformed into test statistics. P-values are calculated from the experiment’s test statistic value. Based upon a priori thresholds, a decision is made to reject a null hypothesis, or not, depending upon the extremeness of the result. knitr::include_graphics(&quot;images/hypothesis.jpg&quot;) Figure 8.1: Statistical hypotheses test the null in a multistep process Low p-values are associated with extreme values of a test statistic. Extreme values of test statistics happen when the effect sizes of the results are high. Rejecting a null on the basis of a p-value means our test statistic value is too extreme to belong in the distribution of null test statistic values. Thus, a low p-value means the effect size is improbably high if it were, in fact, not truly effective. If you learn nothing more in this course, learn that the statistics discussed here are tests of the null hypothesis. Learn that every p-value you see in R output is coupled to a test statistic value. These p-values represent the probability your evidence belongs in the null test statistic distribution. 8.2 Popper and falsification Using data to falsify an hypothesis, even if that hypothesis is the null, is a decision framework that plays well with philosopher Karl Popper’s assertion that scientific theories are probative and that unscientific theories are not. To Popper, the grandest scientific theories are those that can be falsified. In Popperian logic, the truth of nature is unknowable and unproveable….even though it is testable. Thus, the scientific method advocated by Popper doesn’t allow for proving an hypothesis, but at the same time it doesn’t forbid us from rejecting hypotheses that are inconsistent with observations. Thus enters the null hypothesis, which predicts, of course, that nothing happens. The null is an incredibly handy device because if we make observations that are extremely inconsistent with the null, meaning we have observed that something happens, we are obligated to reject the null. Thus, the null is falsifiable when we have positive results! Imagine an experiment to test whether a drug lowers blood glucose in people who have diabetes. When the glucose-lowering effect size for the drug in the sample is large enough, we can reject the hypothesis that the drug didn’t have any effect. In other words, we will accept an observation as evidence for a positive result by formally concluding that same evidence is inconsistent with a negative result. Some argue that this logic forces the researcher to test the “wrong” hypothesis and to also accept an alternate hypothesis that itself may not be true. For example, although blood glucose may be lower in the drug treatment arm of the sample, that may have occured by random chance. An unknown confounder variable could be responsible for the observation that drug-treatment is associated with lower blood glucose. In that case we would make an error by rejecting the null when it is actually true. Of course, rejecting the null is provisional. All gained knowledge is provisional. Drawing a conclusion from one experiment doesn’t preclude testing the experiment some other way. If the problem is important enough (and “real”), it will be tested from multiple angles. It will need to survive the preponderance of the evidence. I’m convinced the alternative approach, which is to seek out evidence that affirms an hypothesis, is not better. This lies at the heart of what Popper stood against. There is an inherent confirmation bias in seeking out affirmation of ideas. In the proper light, any evidence can be made to look attractive. Furthermore, what if, in seeking affirmation, nothing happens? Negative results are very difficult to interpret because the absence of evidence cannot be interpreted as the evidence of absence. So I’d hope the researcher who gives this some thought will find null falsification more pragmatic, if not ingenious. It allows us to move forward on the basis of positive evidence (granted, which may be wrong and we don’t know it), while at the same time practicing a more sound, more unbiased scientific methodology (hypothesis falsification rather than affirmation). Meanwhile, this statistical framework does allow for the possibility of designing experimental conditions to minimize false positive (type1) and false negative (type2) errors. We can operationalize our tolerance for those kinds of mistakes in meaningful ways such that we are less likely to become victims of bad luck. Finally, the decision to reject a null hypothesis can stand alone. It need not be the same as a decision to accept the alternate. Rejecting the null only asserts that the experimental evidence is inconsistent with the null. In no way does that “prove”&quot; the alternative hypothesis. For most, some of these concerns should become even less of a problem when the null and alternate hypotheses are explicitly framed in terms of population parameters and their mutually exclusive and collectively exhaustive outcomes. This approach doesn’t leave much room for ambiguity about what is being declared at the decision step. For example, the null hypothesis for the diabetes case is very explicit: \\(null, H_0: \\mu_{placebo} = \\mu_{drug}\\), Here \\(\\mu\\), since it is greek notation, represents the mean blood glucose in concentration units in the populations corresponding to the two sampled groups. Now that we have a bona fide null hypothesis, we can state the alternate hypothesis as everything the null can’t be: \\(alternate, H_1: \\mu_{placebo}\\ \\ne \\mu_{drug}\\) In other words, the inference operates on the basis of straightforward mathematical principles. Two parameters that are compared either meet our prescribed expectations, or they do not. In this case, if we reject the hypothesis that the means of the two groups are equal, then they can only be not equal. Are they truly not equal? We can never know for sure, but we are operating within a framework of known error tolerances. 8.3 Statistical hypothesis rubric Researchers have to grapple with two types of hypotheses. One type is the grand, paradigm-driving assertion of some key insight, which is designed to express the big picture in forward thinking terms. It is also designed to wow study sections and seminar audiences. The other type is the null hypothesis, which is designed to be tested statistically. The null predicts nothing will happen. The null is as boring as it gets. You’d never propose the null in a specific aims page, but you should get in the habit of thinking in terms of testing the null with your statistics. Only the null hypothesis has any statistical utility, whereas the grand hypothesis has no statistical utility. This is a conceptual hurdle that most students struggle with. The grand hypothesis is for marketing, the null hypothesis is for mattering. For that reason I’ve created a rubric for forming a statistically testable hypothesis. The rubric begins with a conceptual overview of a problem, and it ends with how the results will be interpreted. At some point during the semester you’ll have a major assignment that asks you to go through this rubric for a problem of your own choosing. That assignment is a major test for whether you “get” statistical design of experiments. Step 1: Lay out the big picture of the problem in a way that leads to a “What is not yet known” assertion. Type 2 diabetes is associated with high blood glucose levels and obesity, which each have long term effects associated with high morbidity. Exenatide is GLP-1 receptor agonist that can control blood glucose levels. When delivered as an osmotic minipump exenatide lowers blood glucose. A standard of care for type2 diabetics is to put them on a weight loss program while giving them drugs that manage blood glucose. It is not known if continuous administration via osmotic minipump can lead to greater weight loss while on this standard of care. Step 2: Transform the “What is not known” statement into a bold and simple scientific prediction, as if “what is not known” were answered: Long-term administration of exenatide via osmotic minipump to type-2 diabetics will cause weight loss. Step 3: Now frame the experimental plan in terms of the independent and dependent variables, written as an if/then statement. In narrative format, if you manipulate what predictor variables, then what outcome do you expect to observe? If an exenatide osmotic minipump is implanted into type-2 diabetics, then their weight loss will differ compared to placebo. Step 4: Define the dependent and the independent variables of the experiment. What type of variables are these? What are the experimental units? Are the measurements intrinsically-linked, or not? The dependent variable will be weight loss, calculated as the weight difference between pre-study to post-study for each human subject. Each subject is the experimental unit. The independent variable is treatment. Treatment is a discrete, factoral variable that will be at two levels, placebo and exenatide. Although pre- and post-study weights will be measured for each subject and are themselves intrinsically-linked, they are used to derive the dependent variable (weight loss), which are not instrinsically-linked. Step 5: Write the null and alternate hypothesis on the basis of the statistical parameters to be tested. Note here that greek notation is used to symbolize that the hypothesis is about the sampled population parameters, rather than the sample. Where \\(\\mu\\) represents the mean weight loss of the populations corresponding to the sampled groups, the null and alternate hypotheses are \\[H_0:\\mu_{exenatide}=\\mu_{placebo}\\] and \\[H_1: \\mu_{exenatide}\\ne\\mu_{placebo}\\] Step 6: What statistical test will be used to test the null hypothesis? What are the decision rules? A two-sided, unpaired t-test for comparing group means. The sample size will be based upon a power of 90%, which means that the tolerance level for type2 error will be 10%. The decision threshold for type1 error will be 5%. Thus, the null hypothesis will be rejected at a p-value of less than 0.05. 8.3.0.1 Two-sided vs one-sided hypothesis The above is an example for a two-sided hypothesis. In a two-sided hypothesis \\(\\ne\\) is mutually exclusive and collectively exhaustive of \\(=\\). By rejecting the null that two things are equal, we implicitly (and provisionally) accept the alternative hypothesis that they are not equal. Notice how this hypothesis doesn’t predict the direction of an effect. It only predicts there will be a difference between the two groups. If you’re willing to predict the direction of an effect, you would choose to make a one-sided hypothesis. One-sided hypotheses can happen in either of two ways. In one case we can predict one mean will be greater (\\(&gt;\\))than another mean. In the other case, we can predict one mean will be less than (\\(&lt;\\)) another mean. The mutually exclusive and collectively exhaustive alternatives to these one sided hypotheses are therefore \\(\\ge\\) and \\(\\le\\), respectively. In other words, if one mean is not greater than another mean, then the only alternative possibilities are that it is less than or equal to it. The decision to test a one- or two-sided hypothesis should be based upon scientific reasoning. In the example above, I’m unwilling to test a one-sided hypothesis that exenatide will cause a greater weight loss than placebo, even though that is the expectation (and hope!). Were I willing to test the direction of the effect, the one-sided hypothesis test would be written like this: \\[H_0:\\mu_{exenatide}&lt;\\mu_{placebo}\\] and \\[H_1: \\mu_{exenatide}\\ge\\mu_{placebo}\\] If the data show that mean weight loss is greater in the exenatide group, as expected, that null hypothesis can be rejected. But what if, unexpectedly, weight loss is greater in the placebo group? It would generate a high p-value. According to the pre-planned hypothesis, the null could not be rejected. Worse, given they are already enrolled in a standard of care weight loss program, to know the drug actually impairs weight loss would be an important finding. But in choosing the incorrect one-sided hypothesis, there is nothing to do with the result. It is a negative result. I can’t flip the tail to the other direction to get a significant result that I wasn’t planning upon. That would be extremely biased! In practice, some researchers caught in this conundrum create a whole new can of worms by simply changing the pre-planned hypothesis after the fact. It’s done flippantly but is actually a fairly serious violation of scientific integrity. Changing the hypothesis so that it is consistent with the results is not what anybody would consider sound scientific method. 8.3.0.2 Stick to two-sided hypotheses Unlike the case above, when being wrong about the direction of an effect is not a big deal, then one-sided tests are not a bad option. The example above serves to illustrate how a two-sided hypothesis would have been a better choice than a one-sided hypothesis. There are a few other reasons why it is probably better to get in the habit of always testing two-sided nulls: the two-sided test is more conservative because the p-value threshold is a bit lower. Furthermore, multiple tests and confidence intervals easier perform and to interpret, respectively. "],
["error.html", "Chapter 9 Error 9.1 Setting type 1 and type 2 error thresholds 9.2 Striking the right balance 9.3 False discovery rate", " Chapter 9 Error library(tidyverse) library(treemapify) library(pwr) In a jury trial under the American system of justice the defendant stands accused of a crime by a prosecutor. Both sides present evidence before a jury. The jury’s duty is to weigh the evidence then vote in favor of or against a conviction. The jury doesn’t know the truth. A jury is at risk of making two types of mistakes: An innocent person might be convicted, or a guilty person might be acquitted. They can also make two correct calls: Convict a guilty person or acquit someone who is innocent. Without ever knowing for sure what is actually true, they are instructed by the judge to record their decision on the basis of a threshold rule. In a trial the rule is vote to convict only when you believe “it is beyond a reasonable doubt” the accused is guilty. In science the researcher is like a jury. The experiment is like a trial. At the end, the researcher has the same problem that jurors face. There is a need to conclude whether the experiment worked or not. And there’s no way to know with absolute certainty. Mistaken judgments are possible. Whereas the jury works within the “beyond a reasonable doubt” framework, researchers operate within a framework that establishes tolerance limits for error. Every hypothesis tested risks two types of error. A type 1 error is committed when the researcher rejects the null when in fact there is no effect. This is also known as a false positive. A type 2 error is not rejecting the null when it should be rejected, which is known as a false negative. Or the researcher might not make an error at all. The sensitivity of an experiment is conclude correctly there is no effect, and power (also known as specificity) is concluding correctly there is an effect. Sensitivity and power are the complements of type 1 and type 2 error, respectively 9.1 Setting type 1 and type 2 error thresholds In the planning stages of an experiment the researcher establishes tolerance for these errors. A balance has to be struck between aversion for each error type, the ability to make the right call, and the costs involved for being either wrong or right. 9.1.1 Setting alpha-the type 1 error In the biological sciences the standard for type 1 error is 5%, meaning in any given experiment (no matter the number of comparisons to be made), the chance of generating a false positive should be limited to 5%. The acceptable type 1 error limit is labeled alpha, or \\(\\alpha\\). In several R statistical functions, it is controlled by adjusting its complement, the confidence level. Why is \\(\\alpha\\) 5% and not some other value? Credit for that is owed largely to R.A. Fisher who offered that a 1 in 20 chance of making such a mistake seemed reasonable. That number seems to have stuck, at least in the biological sciences. The researcher is always free to establish, and defend, some other level of \\(\\alpha\\). In the field of psychology, for example, \\(\\alpha\\) is historically 10%. There is nothing to stop a researcher from selecting a threshold below or above 5%. She just needs to be prepared to defend the choice. 9.1.1.1 The decision rule The \\(\\alpha\\) is stated before an experiment begins, but operationalized during the final statistical analysis on the basis of p-values generated from statistical tests. The null hypothesis is rejected when a p-value is less than this preset \\(\\alpha\\). 9.1.1.2 Experimentwise error An experiment that just compares two groups (eg, placebo vs drug) generates only one hypothesis. An experiment comparing \\(k\\) groups (eg, placebo vs drug1, vs drug2…drugk-1) generates \\(m=\\frac{k(k-1)}{2}\\) hypotheses. For experiments that generate multiple hypotheses it is important to maintain the overall \\(\\alpha\\) for the experiment at 5%. If not checked, the experiment-wise error would inflate with each hypothesis tested. Several methods have been devised to maintain experiment-wise \\(\\alpha\\) for multiple comparisons. The most conservative of these is the Bonferroni correction \\(\\alpha_m=\\frac{\\alpha}{m}\\). Thus, if \\(m = 10\\) hypotheses are tested, the adjusted threshold for each, \\(\\alpha_m\\), is 0.5%, or a p-value of 0.005. If 1000 hypotheses are tested, such as in a mini-gene screen, the p-value threshold for each would be 0.00005. 9.1.2 Power: Setting beta-the type 2 error In the biological sciences the tolerance for type 2 error, otherwise symbolized as \\(\\beta\\), is generally in the neighborhood of 20%. It’s a bit easier to discuss \\(\\beta\\) through its complement, \\(1-\\beta\\) or power. Thus, experiments run at 80% power, which are generally regarded as well-designed, run at 20% risk of type 2 error. Operationally, an experiment is designed to hit a specific level of power via planning of the sample size. “Power calculations” return sample size by integrating intended power, \\(\\alpha\\), and an estimated effect size. Students tend to fret over effect size estimates. They are nothing more than a best guess of what to expect. A crude estimate. The researcher should use values representing a minimum for a scientific meaningful effect size. The effect size is estimated on the basis of scientific judgment and preliminary data or published information. If the effect size estimate turns out to be accurate, an experiment run at that sample size should be close to the intended power. In a perfect world, we might consider powering up every experiment to 99%, completely minimizing the risk of \\(\\beta\\). As you’ll see in the simulation below, the incremental gain in power beyond ~80% diminishes with sample size. In other words, perfect power and very low \\(\\beta\\) comes at a high cost. The choice of what power to run an experiment should strike the right balance between the risk of missing out on a real effect against the cost burden of additional resources and time. R’s pwr package has a handful of functions to run power calculations for given statistical tests. These, unfortunately, do not cover all of the statistical tests, particularly for the most common experimental designs (eg, ANOVA). In this course, we’ll emphasize performing power calculations using custom Monte Carlo functions, which can be custom adapted for any type of experiment involving a statistical test. Here’s a custom Monte Carlo-based power function for a t-test. To illustrate the diminishing returns argument, the function calculates power comparing samples drawn from \\(N(0,1)\\) to samples drawn from \\(N(1,1)\\). The graph is generated by passing a range of sample sizes into the function. Note how the gain in power plateaus. t.pwr &lt;- function(n){ #Intitializers. Means and SD&#39;s of populations compared. m1=1; sd1=1; m2= 0; sd2=1 # the monte carlo ssims=1000 p.values &lt;- c() i &lt;- 1 repeat{ x=rnorm(n, m1, sd1); y=rnorm(n, m2, sd2); p &lt;- t.test(x, y, paired=F, alternative=&quot;two.sided&quot;, var.equal=F, conf.level=0.95)$p.value p.values[i] &lt;- p if (i==ssims) break i = i+1 pwr &lt;- length(which(p.values&lt;0.05))/ssims } return(pwr) } #Run t.pwr over a range of sample sizes and plot results frame &lt;- data.frame(n=2:50) data &lt;- bind_cols(frame, power=apply(frame, 1, t.pwr)) #plot ggplot(data, aes(n, power))+ geom_point() + scale_y_continuous(breaks=c(seq(0, 1, 0.1)))+ scale_x_continuous(breaks=c(seq(0,50,2)))+ labs(x=&quot;n per group&quot;) ## Validation by comparisonC to pwr package results pwr.t.test(d=1, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 16.71472 ## d = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 9.2 Striking the right balance The script below provides a way to visualize how the relationship between correct (green) and incorrect (red) decisions varies with error thresholds. The idea is to run experiments under conditions by which green is the dominant color. Unfortunately, most published biomedical research appears to be severely underpowered findings. alpha &lt;- 0.05 beta &lt;- 0.20 panel &lt;- data.frame(alpha, sensitivity=1-alpha, power=1-beta, beta) panel &lt;- gather(panel, key=&quot;threshold&quot;, value=&quot;percent&quot;) panel &lt;- bind_cols(panel, truth=c(&quot;no effect&quot;, &quot;no effect&quot;, &quot;effective&quot;, &quot;effective&quot;), decision=c(&quot;effective&quot;, &quot;no effect&quot;, &quot;effective&quot;, &quot;no effect&quot;), choice=c(&quot;error&quot;, &quot;correct&quot;, &quot;correct&quot;, &quot;error&quot;)) panel ## threshold percent truth decision choice ## 1 alpha 0.05 no effect effective error ## 2 sensitivity 0.95 no effect no effect correct ## 3 power 0.80 effective effective correct ## 4 beta 0.20 effective no effect error ggplot(panel, aes(area=percent, fill=choice, label=threshold))+ geom_treemap(color=&quot;white&quot;)+ geom_treemap_text( fontface = &quot;italic&quot;, colour = &quot;white&quot;, place = &quot;centre&quot;, grow = F )+ scale_fill_manual(values = alpha(c(&quot;green3&quot;, &quot;red&quot;), .3)) 9.3 False discovery rate The false discover rate, or FDR is another way to estimate experimental error. \\[FDR=\\frac{false\\ positives}{false\\ positives + false\\ negatives}\\] FDR varies given \\(\\alpha\\), \\(\\beta\\) and the probability of the effect. The probability of the effect bears some comment. Think of it as a prior probability, or the likelihood that an effect being studied is “real”. It takes some scientific judgment to estimate these probability values. The graph below illustrates how FDR inflates, particularly when running experiments for low probability effects when tested at low power, even at a standard \\(\\alpha\\). These relationships clearly show that the lower the likelihood of some effect that you would like to test in an experiment, the higher the stringency by which it should be tested. px &lt;- seq(0.1, 1.0, 0.1) #a range of prior probabilities tests &lt;- 10000 fdr_gen &lt;- function(beta, alpha){ real_effect &lt;- px*tests true_pos &lt;- real_effect*(1-beta) false_neg &lt;- real_effect*beta no_effect &lt;- tests*(1-px) true_neg &lt;- tests*(1-alpha) false_pos &lt;- no_effect*alpha FDR &lt;- false_pos/(true_pos + false_pos) return(FDR) } upss &lt;- fdr_gen(0.6, 0.05)#under-powered, standard specificity wpss &lt;- fdr_gen(0.2, 0.05)#well-powered, standard specificity uphs &lt;- fdr_gen(0.6, 0.01)#under-powered, high specificity wphs &lt;- fdr_gen(0.2, 0.01)#well-powered, high specificity fdrates &lt;- data.frame(px,upss, wpss, uphs, wphs) colnames(fdrates) &lt;- c(&quot;Probability&quot;, &quot;5% alpha, 60% beta&quot;, &quot;5% alpha, 20% beta&quot;, &quot;1% alpha, 60% beta&quot;, &quot;1% alpha, 20% beta&quot;) #convert to long format fdrates &lt;- gather(fdrates, tests, FDR, -Probability) ggplot(fdrates, aes(Probability,FDR, group=tests))+ geom_point(aes(color=factor(tests)))+ geom_line(aes(color=factor(tests))) "],
["pvalues.html", "Chapter 10 P Values 10.1 How p-values are calculated 10.2 How p-values should be interpreted 10.3 Interpretation 10.4 Criticisms of p-values", " Chapter 10 P Values library(tidyverse) You’ll see soon enough that when you run a statistical test function in R, it generates list objects that are chock full of useful information. Invariably, the researcher’s eyes will go right to the p-value. This is understandable, since most researchers have been trained to associate their own success with a p-value falling below some pre-set \\(\\alpha\\). Who could really blame them for peeking at the p-values first? The p-value is an instrument by which an important decision will be made. As such, it is worth understanding how that instrument works. Hypothesis-driven experiments are designed to test the null hypothesis. That null will be rejected if the effect size is large enough. Extreme effect sizes correspond to extreme values of test statistics. A p-value is the probability that a given test statistic value could be as large as it is, or even more extreme, if the null hypothesis were actually true. In other words, the p-value is an error probability. It is also a random variable. Which means that it is always possible for an experiment to generate an extreme test statistic by simple random chance. The p-value asserts the probability that this is the case. One of the reasons I like using R for experimental statistics is that R makes it easy to simulate p-values. Because of that you build an intuitive sense for how they operate. R makes it easy to understand p-values. 10.1 How p-values are calculated You can think of test statistics as a transformation of sample data. There are many test statistics. The one to use for a given data set depends on the experimental design. Each test statistic has a probability distribution. P-values are derived from the probability distributions of these test statistics and serve as a way to standardize the decision making process irrespective of the experimental design and test statistic. Probably the simplest test statistic to understand is the z-score. The z-score is a transformation of data from whatever scale it is on, to a standard normal scale. It’s usually appropriate for continuous data. \\[z_i=\\frac{y_i-\\mu}{\\sigma}\\] Let’s say we have single blood glucose value of 122 mg/dl. What is its p-value? Is the z-score corresponding to that glucose value too extreme to belong in the null distribution of z-scores? First, the blood glucose values is transformed into a z-score. We’ll say the mean and standard deviation of blood glucose in the sampled population is 100 and 10 mg/dl, respectively. The z-score for a value of 122 is therefore: z &lt;- (122-100)/10; z ## [1] 2.2 z-score units are in standard deviations. Thus, a z-score value of 2.2 indicates it is 2.2 standard deviation units greater than the standard normal mean (which is zero, of course). Next, we’ll pass that z-score value of 2.2 into the standard normal density function, pnorm. We cause the function to produce a p-value for that z-score by using a lower.tail=FALSE argument: pnorm(2.2, mean=0, sd=1, lower.tail=FALSE) ## [1] 0.01390345 In the z probability distribution below, the blue shaded region illustrates what this p-value looks like. The p-value covers the probabilities for z-score values of 2.2 and higher. The p-value is thus the area under the curve for the z probability distribution for that value of z and for more extreme values. ggplot(data.frame(zscore = c(-5, 5)), aes(zscore)) + stat_function(fun = dnorm) + stat_function(fun = dnorm, xlim= c(2.2, 5), geom = &quot;area&quot;, fill=&quot;blue&quot;)+ ylab(&quot;p(z)&quot;)+ scale_x_continuous(breaks=seq(-5,5,1)) 10.2 How p-values should be interpreted The question that’s ringing in your ears right now is, “Is a z-score value of 2.2 so extreme we can reject that it belongs to the null distribution of z-scores?” The answer to that question depends upon what threshold you deem is too extreme. Remember, a threshold is our tolerance for error; in this case, for type 1 error. If the threshold for an acceptable risk of type 1 error is 5% (\\(p &lt; 0.05\\)), then let’s see how those look on the z-distribution. First, let’s calculate z-scores corresponding the area outside 95% of the z-scores. Since extreme z-scores can lay on both the right and the left sides of the z-distribution, which is symmetrical. Therefore we split the 5% in half and use the quantile function qnorm to calculate z-scores for each: qnorm(0.025, lower.tail = F) ## [1] 1.959964 qnorm(0.025, lower.tail = T) ## [1] -1.959964 Thus, the 95% confidence limits for the z-scores are ~ +/- 1.96, almost 2 standard deviations from the mean. We plug those values as limits into our plot: ggplot(data.frame(zscore = c(-5, 5)), aes(zscore)) + stat_function(fun = dnorm) + stat_function(fun = dnorm, xlim= c(1.96, 5), geom = &quot;area&quot;, fill=&quot;red&quot;)+ stat_function(fun = dnorm, xlim= c(-1.96, -5), geom = &quot;area&quot;, fill=&quot;red&quot;)+ ylab(&quot;p(z)&quot;)+ scale_x_continuous(breaks=seq(-5,5,1)) Any z-score values corresponding to the red-shaded areas would be deemed too extreme to belong to the null. The limit on the right side is 1.96. Therefore, yes, a z-score of 2.2 (\\(p=0.0139\\)) is too extreme to belong to the standard null distribution. 10.3 Interpretation Every time we do an experiment we operate on the assumption that our data represent the null. This is analogous to considering a defendant innocent until proven guilty. So we think of test statistic values we calculate from our data, unless proven otherwise, as belonging to the null distribution of test statistic values. The interpretation of \\(p=0.0139\\) is the probability that z-score (and its corresponding glucose value of 122 mg/dl) are that large by chance is 0.0139. There’s about a 1.4% chance we are making an error by rejecting the null that it belongs to the \\(N(100, 10)\\). 10.4 Criticisms of p-values There are several criticisms of p-values, many of which are legitimate. I’ll address a few key ones here. They are too confusing, nobody understands them. I get that. I confess that p-values are a struggle to teach in a way that’s simple and memorable. Especially for students who only consider statistics episodically, perhaps a few times a year. This year I’m teaching this with a bit more emphasis upon Popper and the merits of null hypothesis falsification as the cornerstone of the scientific method and how p-values fit into that tradition. Here it is: All statistical tests (the ones I teach in this course) are tests of the null hypothesis. When the test result is extreme, we reject the null. The p-value is the probability we’re rejecting the null in error. Despite the merit of this particular criticism, p-values are not going away. They are an important inferential tool used by most biological scientists, even if poorly understood and implemented. Like any tool in the lab, it is incumbant upon the researcher to learn how it works. I think a great way to get a better intuitive understanding for p-values is to play around with the various test statistic probability and quantile distributions in R (pnorm, qnorm, pt, qt, pf, pf, pchisq, qchisq, psignrank, qsignrank etc). Use them to run various scenarios, plot them out…get a sense for how the tools work by using them. p-Values poorly protect from false discovery This is undoubtedly true. Since David Colquhoun goes over this in blistering detail I won’t repeat his thorough analysis here. The researcher MUST operate with skepticism about p-values. Since Colquhoun’s argument is entirely based in simulation it also inspires an approach for dealing with this problem. Through simulation a priori, a researcher can design and run experiments in silico that strikes the right balance between the threshold levels she can control (eg, \\(\\alpha\\) and \\(\\beta\\)) and feasibility in a way that best minimizes the risk of false discovery. Before ever lifting a finger in the lab. This criticism explains why I am such a strong advocate of Monte Carlo simulation in experimental design. With software like R, there really is no excuse anymore for the researcher being a victim of this problem. p-Values aren’t the probability I’m interested in Researchers who raise this criticism generally are interested in something the p-value was never designed to deliver: the probability that their experiment worked. A p-value doesn’t provide that information because it is an error probability. Specifically, it is the probability of making a type 1 error. For these researchers, embracing Bayesian statistics is probably a better option. I don’t teach Bayesian statistics in this course for a couple of reasons, but mostly because I don’t understand it well enough to teach it, and I don’t see how it offers a superior approach for experimental research. People use p-values as evidence for the magnitude of an effect. Sure, but they are wrong. This is more a criticism of the people who use p-values, and not the p-value. But the criticism raises the point that it is a mistake to rely solely on a p-value to interpret the outcome of an experiment. A p-value&lt;0.05 only means that there is less than 1 out of 20 chance of having detected an extreme effect when the null is true. A low p-value doesn’t provide evidence that the treatment effect is real. As a result, a p-value can’t provide any information about the magnitude of the treatment effect. Neither is a low p-value synonymous with scientific significance. A simple example of this comes from 2 way ANOVA F test analysis. When the test suggests a postive result for an interaction effect, that finding supercedes the main effects. Thus, should any main effects also have low p-values they are not scientifically meaningful. Researchers should therefore analyze p-values in conjunction with other parameters, such as effect sizes and the confidence intervals. "],
["jaxwest7.html", "Chapter 11 Reproducible Data Munging in R 11.1 Jaxwest7 glucose data 11.2 Explore the Jaxwest7 data", " Chapter 11 Reproducible Data Munging in R Data munging is the process of taking data from one source(s), working it into a condition where it can be analyzed. Because every data set will differ every data munge will be custom. Data munging is a bit like organic chemistry. You know the chemical you want to create. You know the starting materials that you have on hand. You know the reactions and the intermediates that you’ll need to produce to get that final product. Here’s one example of the munging process. What we want to achieve in this exercise, the final product, is to get some graphical views in order to visualize the data structure along with some summary statistics. We begin with some experimental data that’s a bit unstructured. Our tools include an data import function (datapasta) and a handful of functions in the tidyverse package. Not especially how every transaction with the data is recorded. If you took each of these code chuncks and ran them on your machine, you should get identical results. That’s reproducibility. 11.1 Jaxwest7 glucose data library(datapasta) library(tidyverse) The Jaxwest7 data set is a Jackson Labs experiment conducted in a mouse strain serving as a model for type 2 diabetes. Animals fed a glucose-rich diet develop a type 2 diabetes syndrome. The experiment tests whether the antidiabetic drug rosglitazone suppresses disease development. Half the subjects receive the antidiabetic drug, the other receive vehicle as placebo.The syndrome is assessed by measuring two response variables: body weight and blood glucose concentrations. There are two explanatory variables: day of study and drug treatment. The experimental design is therefore multivariate (weight, blood glucose) two-factor (drug treatment, day) ANOVA with repeated measures from the replicates. The purpose of this chapter is to illustrate how to retrieve and process data in R, focusing only on the blood glucose response variable within the Jaxwest7 data set. Going through this exercise will illustrate how to prepare data for statistical analysis. 11.1.1 Inspect the Jaxwest7 data Download the Jaxwest7.xls file from the mouse phenome database to your machine and open it with Excel (or some other spreadsheet software). Go to the BloodGlucoseGroups tab. This is readable, but what hits you is the sheet’s complexity. In fact, this sheet illustrates what unstructured data looks like. Almost every column has cells containing multiple types of values. The first 8 rows have various descriptor text, including a logo! Rows 9-14 have some other definitions. Scroll way over to the right and some graphs pop up. The data we are interested in are in rows 15 to 42, and in columns F to S. Each of those columns has two column names, a date and a day. Thus, if a column in this array is a group of variable values, then each variable has two names! There should be only 1. Columns T and U have several missing values, because those animals were used for autopsy. We’re going to have to ignore their response values. Columns 43 to 146 are missing entirely! Below the array are some summary statistics on the columns above, which represent different variables. This is not a spreadsheet that can or should be imported whole scale directly into R. Instead, we need to grab only the data we need. Then we’ll use R to structure it properly. 11.1.2 Munge the glucose concentration data into R Let’s get the glucose concentration data into R, and create a “long”&quot; data frame format, where every column represents a variable and every row is a case (or a subset of a case). We’ll have to create some row and some column variable names. What do we have to work with? What do we need to create? Glucose concentrations were measured twice per day on odd-numbered days plus day 12. Each column represents a blood draw session. This was done on each of 16 animals. Half were in a placebo group, half were in a drug group. I’m going to omit day 15 due to the NA values (those mice were harvested for autopsy, and so day 15 breaks the time series). Let’s make a proper R data frame out of this data. Open up an R script file and follow along. Step 1: Deal with cell F21. It’s value in the excel spreadsheet is “Hi”, a character value rather than a numeric (it must have tested out-of-range for the assay). We have two options: Assign it an NA value, or impute. Since this is a related-measures time series with multiple other glucose measurements for that specific replicate, we’ll impute by using the average of all these other measurements. Calculate the value that will be imputed: #Use datapasta to paste in vector values. Calculate their mean. Then impute value for cell F21 in original data set by exchanging the value &quot;Hi&quot; with the mean produced here. F21 &lt;- mean(c(449L, 525L, 419L, 437L, 476L, 525L, 499L, 516L, 485L, 472L, 535L, 500L, 497L) ); F21 ## [1] 487.3077 Ideally, you’d import the data with the “Hi” value and fix it in R, to have reproducible record. Doing so would involve walking several confusing munging steps into the weeds, which are beyond the scope of this chapter. Step 2: Fix the F21 cell in the spreadsheet, then copy the array F15:S42 to the clipboard. This gives 14 columns and 16 rows of glucose data. All values are numeric and represent the same variable: glucose concentration. Use the datapasta package addin for this procedure. Create an object name, pasting in as a tribble provides the cleanest route. gluc &lt;- tibble::tribble( ~V1, ~V2, ~V3, ~V4, ~V5, ~V6, ~V7, ~V8, ~V9, ~V10, ~V11, ~V12, ~V13, ~V14, 136L, 270L, 162L, 165L, 192L, 397L, 172L, 148L, 291L, 239L, 192L, 172L, 235L, 153L, 345L, 518L, 429L, 413L, 456L, 487L, 468L, 419L, 507L, 559L, 420L, 415L, 511L, 464L, 190L, 301L, 311L, 361L, 398L, 465L, 388L, 392L, 453L, 421L, 355L, 381L, 394L, 444L, 434L, 504L, 453L, 392L, 350L, 400L, 458L, 387L, 342L, 368L, 355L, 429L, 373L, 501L, 424L, 486L, 447L, 417L, 496L, 484L, 468L, 423L, 472L, 507L, 458L, 456L, 519L, 570L, 170L, 208L, 134L, 129L, 147L, 141L, 241L, 128L, 162L, 163L, 222L, 438L, 307L, 252L, 487L, 449L, 525L, 419L, 437L, 476L, 525L, 499L, 516L, 485L, 472L, 535L, 500L, 497L, 218L, 273L, 254L, 265L, 338L, 386L, 287L, 236L, 347L, 235L, 432L, 450L, 509L, 326L, 179L, 184L, 124L, 107L, 108L, 149L, 142L, 143L, 112L, 233L, 113L, 137L, 106L, 150L, 260L, 381L, 174L, 140L, 132L, 138L, 164L, 137L, 122L, 140L, 102L, 174L, 120L, 135L, 115L, 191L, 132L, 132L, 169L, 158L, 129L, 120L, 122L, 157L, 94L, 141L, 120L, 166L, 526L, 517L, 465L, 394L, 310L, 269L, 213L, 185L, 145L, 201L, 131L, 258L, 114L, 160L, 325L, 252L, 203L, 158L, 135L, 162L, 164L, 181L, 150L, 177L, 162L, 192L, 170L, 162L, 329L, 296L, 212L, 159L, 156L, 200L, 139L, 143L, 164L, 150L, 119L, 193L, 148L, 188L, 230L, 414L, 408L, 179L, 432L, 288L, 163L, 240L, 185L, 208L, 138L, 208L, 153L, 140L, 204L, 120L, 138L, 139L, 157L, 122L, 163L, 168L, 164L, 128L, 129L, 218L, 135L, 182L ) Notice how R coerces unique variable names for each column. At this point, they are much cleaner than the column names available in the spreadsheet. Step 3: Create a column for the ID variable. There are 16 replicate mice. We’ll given them each a unique ID name. ID &lt;- LETTERS[1:16] Step 4: Create a column for the treatment variable. The first 8 replicates received vehicle (placebo). The second 8 rosiglitazone. treat &lt;- c(rep(&quot;placebo&quot;, 8), rep(&quot;rosiglit&quot;, 8)) Step 5: Add these columns to the gluc tribble gluc &lt;- add_column(gluc, ID, treat, .before=T) Step 6: Now would be a good time to convert from a wide to a long table format. We use the gather function in the tidyverse package for that. gluc &lt;- gather(gluc, V, glucose, -ID, -treat) Step 7: Replace the variable V with two variables. Note how we created a new variable, V, in this move. As you can see, the gather function successively “gathers” all the glucose values from columns V1, V2,…, V14 into a single column under the variable V. It’s a very, very slick and useful function that’s analogous to an excel pivot table (but much easier to execute). The V variable, however, is ambiguous. The 14 levels of V actually represent two variables. The odd numbered represent the early day blood draw, while the even numbered represent the late day draw. Meanwhile, V1 and V2 represent day one, V3 and V4 represent day three,…, and V13 and V14 represent day eleven. We need a single variable column to represent early or late draw, and another variable column to represent the seven days of the study. There are two levels for the draw variable, and seven levels for the day variable. These repeat in a 16 and 32 unit pattern, respectively. draw &lt;- rep( c(rep(&quot;early&quot;, 16), rep(&quot;late&quot;, 16) ), 7 ) day &lt;- rep( c(1, 3, 5, 7, 9, 11, 12), each =32) Step 8: Update the gluc table to incorporate these new variables. gluc &lt;- add_column(gluc, draw, day, .before=T) gluc ## # A tibble: 224 x 6 ## draw day ID treat V glucose ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 early 1 A placebo V1 136 ## 2 early 1 B placebo V1 345 ## 3 early 1 C placebo V1 190 ## 4 early 1 D placebo V1 434 ## 5 early 1 E placebo V1 424 ## 6 early 1 F placebo V1 170 ## 7 early 1 G placebo V1 487 ## 8 early 1 H placebo V1 218 ## 9 early 1 I rosiglit V1 179 ## 10 early 1 J rosiglit V1 260 ## # ... with 214 more rows Step 9: Remove the V variable column from the tribble. First verify that the correct levels for draws and days correspond to the correct levels of V. After that, the V column has no use so it can be removed. gluc &lt;- select(gluc, -one_of(&quot;V&quot;)) gluc ## # A tibble: 224 x 5 ## draw day ID treat glucose ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 early 1 A placebo 136 ## 2 early 1 B placebo 345 ## 3 early 1 C placebo 190 ## 4 early 1 D placebo 434 ## 5 early 1 E placebo 424 ## 6 early 1 F placebo 170 ## 7 early 1 G placebo 487 ## 8 early 1 H placebo 218 ## 9 early 1 I rosiglit 179 ## 10 early 1 J rosiglit 260 ## # ... with 214 more rows Let’s fix one subtle issue. Each of the variables draw, day, ID and treat should all be interpreted as factor objects. But the data table interprets them as characters (or dbl in the case of day). Some statistical tests need to read these as factors. cols &lt;- c(&quot;draw&quot;, &quot;day&quot;, &quot;ID&quot;, &quot;treat&quot;) gluc[cols] &lt;- lapply(gluc[cols], factor) 11.2 Explore the Jaxwest7 data Histograms are a great way to get a first look at data sets with a reasonably healthy number of values, such as this one. We expect glucose to be a normally-distributed variable. Here it’s clearly bi-modal, perhaps representing two normally-distributed treatment groups? ggplot(gluc)+ geom_histogram(aes(glucose)) By coloring the two groups, the story grows a bit more complex. ggplot(gluc, aes(fill=treat) ) + geom_histogram(aes(glucose)) We can also look at the data as scatter plots, by draw, treatment and time. This view shows the time series for each replicate. Looking at the data atomically, in this way, gives tremendous insights! ggplot(gluc, aes(day, glucose, group=ID, color = treat) ) + facet_wrap(~draw) + geom_point() + geom_line() Now create some summaries. Here’s a plot of the means and standard deviations of the groups. The stat_summary function is a bit quirky to work with, but worth learning. ggplot(gluc, aes(day, glucose, color=treat ) ) + facet_wrap(~draw) + stat_summary(fun.data = &quot;mean_sdl&quot;, fun.args = list(mult = 1), geom =&quot;pointrange&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group=treat) ) ## Warning: Computation failed in `stat_summary()`: ## &#39;what&#39; must be a function or character string ## Warning: Computation failed in `stat_summary()`: ## &#39;what&#39; must be a function or character string Now here’s a summary of all the replicate values in tabular form. gluc %&gt;% group_by(day, treat, draw) %&gt;% dplyr::summarise(mean=mean(glucose), sd=sd(glucose), n=length(glucose)) ## # A tibble: 28 x 6 ## # Groups: day, treat [?] ## day treat draw mean sd n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 placebo early 300. 138. 8 ## 2 1 placebo late 376. 125. 8 ## 3 1 rosiglit early 271 126. 8 ## 4 1 rosiglit late 294. 134. 8 ## 5 3 placebo early 339. 146. 8 ## 6 3 placebo late 320. 118. 8 ## 7 3 rosiglit early 232 131. 8 ## 8 3 rosiglit late 176 90.6 8 ## 9 5 placebo early 352. 125. 8 ## 10 5 placebo late 404. 114. 8 ## # ... with 18 more rows The more you look at the early v late draws, both graphically and in the table above, the more you don’t have a problem treating those as technical or pseudo replicates. Perhaps it was a decision made ahead of time at Jackson? We’ll declare the early and late draws on the same day as not independent. Averaging them leaves will leave us with a tighter estimate of daily blood glucose concentrations, which is nice. The code below averages the early and late draws to produce a single mean glucose value per replicate per time point. gluc %&gt;% group_by(day, treat, ID) %&gt;% dplyr::summarise(mean=mean(glucose), sd=sd(glucose), n=length(glucose) ) ## # A tibble: 112 x 6 ## # Groups: day, treat [?] ## day treat ID mean sd n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 placebo A 203 94.8 2 ## 2 1 placebo B 432. 122. 2 ## 3 1 placebo C 246. 78.5 2 ## 4 1 placebo D 469 49.5 2 ## 5 1 placebo E 455 43.8 2 ## 6 1 placebo F 189 26.9 2 ## 7 1 placebo G 468 26.9 2 ## 8 1 placebo H 246. 38.9 2 ## 9 1 rosiglit I 182. 3.54 2 ## 10 1 rosiglit J 320. 85.6 2 ## # ... with 102 more rows The ‘mean’ column should be used as the final value for the glucose response variable, in statistical analysis. This is the right call. To leave the technical duplicates would be to flood the data set with undeserved degrees of freedom. We’ll create a final, working table of the data now. It catalogs how each replicate behaved on each day, and which treatment it received. You’ll note we’ve : glucFinal &lt;- gluc %&gt;% group_by(day, treat, ID) %&gt;% dplyr::summarise(glucose=mean(glucose) ) glucFinal ## # A tibble: 112 x 4 ## # Groups: day, treat [?] ## day treat ID glucose ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 placebo A 203 ## 2 1 placebo B 432. ## 3 1 placebo C 246. ## 4 1 placebo D 469 ## 5 1 placebo E 455 ## 6 1 placebo F 189 ## 7 1 placebo G 468 ## 8 1 placebo H 246. ## 9 1 rosiglit I 182. ## 10 1 rosiglit J 320. ## # ... with 102 more rows Because it has all of the replicate data, the glucFinal data table is what we would use for statistical testing. If we published a figure from all of this, we’d create it from the glucFinal data table. The following would be the most appropriate because it is based on average technical replicates: ggplot(glucFinal, aes(x = day, y = glucose, color=treat ) ) + stat_summary(fun.data=&quot;mean_sdl&quot;, fun.args = list(mult = 1), geom =&quot;pointrange&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group=treat)) ## Warning: Computation failed in `stat_summary()`: ## &#39;what&#39; must be a function or character string And finally, here’s some summary stats on that final data set: glucFinal %&gt;% group_by(day, treat) %&gt;% dplyr::summarise(n = length(glucose), mean = mean(glucose), median = median(glucose), sd = sd(glucose), sem = sd/sqrt(n), min = min(glucose), max = max(glucose)) ## # A tibble: 14 x 9 ## # Groups: day [?] ## day treat n mean median sd sem min max ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 placebo 8 338. 338. 128. 45.1 189 469 ## 2 1 rosiglit 8 283. 300. 121. 42.8 153 522. ## 3 3 placebo 8 330. 378. 131. 46.2 132. 472 ## 4 3 rosiglit 8 204 169. 106. 37.6 116. 430. ## 5 5 placebo 8 378. 403. 115. 40.6 144 490 ## 6 5 rosiglit 8 193. 156 85.1 30.1 128. 360 ## 7 7 placebo 8 352. 406. 132. 46.7 160 512 ## 8 7 rosiglit 8 162. 158 27.8 9.83 124. 202. ## 9 9 placebo 8 379. 396 132. 46.7 162. 533 ## 10 9 rosiglit 8 160. 160. 21.2 7.48 131 196. ## 11 11 placebo 8 386. 405. 98.5 34.8 182 504. ## 12 11 rosiglit 8 157. 164. 27.5 9.71 118. 194. ## 13 12 placebo 8 410. 428 117. 41.5 194 544. ## 14 12 rosiglit 8 147. 145. 16.0 5.66 128. 168 Phew! "],
["binomial.html", "Chapter 12 The Binomial Distribution 12.1 dbinom 12.2 pbinom 12.3 qbinom 12.4 rbinom", " Chapter 12 The Binomial Distribution library(ggplot2) The binomial probability distribution models the discrete outcomes of dichotomous processes. In other words, events that can be categorized as either a successes or as a failure. 12.1 dbinom The binomial probability mass function in R is dbinom. R’s dbinom function returns p(x), a probability value produced by the binomial probability mass function: \\[p(x)={n\\choose x}(p)^x(1-p)^{(n-x)}\\] Where \\(n\\) is the number of trials, \\(x\\) is the value of the number of successes, \\(p\\) is the probability of a single success, and therefore \\(1-p\\) is the probability of a single failure. The coin toss serves as the classic explainer for binomial events. A fair coin can land as either heads, or tails with equal probabilities. Unless you’re Patriot’s quarterback Tom Brady, for whom it always lands as heads. If Matt Ryan tossed a coin 10 times, what’s the probability of him getting EXACTLY 7 heads? dbinom(x=7, size=10, prob=0.5) ## [1] 0.1171875 That’s almost a 12% chance! Eight heads would be even more unlikely. And so on. Female mice enter estrus one out of five days. This implies that if a female mouse is mated on any random day, the probability of any single mating resulting in pregnancy is 0.2. On a given day, if you set up 12 female mize for mating, what’s the probability that exactly half of them would become pregnant? dbinom(x=6, size=12, prob=0.2) ## [1] 0.01550215 There’s only a 1.55% chance of getting EXACTLY 6 dams out of that mating set up. The script below illustrates the probabilities over a full range of possible pregnancy outcomes, for a trial of size 12 (ie, 12 matings set up) x &lt;- 1:10 size &lt;- 12 prob &lt;- 0.2 df &lt;- data.frame(x, px=dbinom(x, size, prob)) ggplot(df, aes(x, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) It’s evident that binomial distributions where the probabilities of successes and failures are uneven are skewed. The only way to make these appear more normally distributed is to have equal probabilities for successes and failures. 12.2 pbinom R’s pbinom is the cumulative probability distribution function for the binomial. \\[p(x)={\\sum_{i=0}^{x}}{n\\choose i}(p)^i(1-p)^{(n-i)}\\] This function returns the cumulative probability value for a number of successes in n trials. This can be a very useful value to model. For example, if you set up 12 matings of mice, where each had a 0.2 probability of pregnancy, what is the probability that you would have up to 6 pregnant dams? pbinom(6, 12, 0.2, lower.tail=T) ## [1] 0.9960969 There’s is a very high probability of getting UP TO 6 pregnancies from 12 matings! If we turn the lower.tail argument from TRUE to FALSE the pbinom returns a p-value like probability. What’s the probability of getting 6 or more pregnancies from 12 matings where the probability of a single pregnancy is 0.2? pbinom(6, 12, 0.2, lower.tail=F) ## [1] 0.003903132 That’s about 0.39%! Which would be a very rare outcome from the mating trial, indeed! Maybe even scientifically significant were it to occur. Perhaps it’s useful to visualize both the upper and lower tails of this cumulative function: q &lt;- 1:10 size &lt;- 12 prob &lt;- 0.2 df &lt;- data.frame(q, px=pbinom(q, size, prob)) ggplot(df, aes(q, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;pbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;lower.tail=TRUE&quot;,&quot;)&quot;)) df &lt;- data.frame(q, px=pbinom(q, size, prob, lower.tail=F)) ggplot(df, aes(q, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;pbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;lower.tail=FALSE&quot;,&quot;)&quot;)) 12.3 qbinom The quantile binomial distribution function in R is qbinom. qbinom is the inverse of the pbinom function. This predicts the number of successes that might occur given a percentile of the distribution. Assuming 12 matings are set up, where the probability of any one pregnancy success is 0.2, what number of pregnancies would be expected if the group performed at the 90th percentile? qbinom(p=0.90, size=12, prob=0.2, lower.tail=T) ## [1] 4 That’s only 4 litters. That should make sense, since only 1 in 5 would be pregnant on average. To out perform this expectation at the 90th percentile is still not a very large numer The graph below illustrates this. Notice the step-wise distribution, which is diagnostic of discrete functions. #define variables p &lt;- seq(0, .99, 0.03) #cumulative probability quantiles size &lt;- 12 #number of trials prob &lt;- 0.2 #probability of success of one trial df &lt;- data.frame(p, q=qbinom(p, size, prob)) ggplot(df, aes(p, q)) + geom_col(fill=&quot;blue&quot;) + xlab(&quot;p(q)&quot;) + ylab(&quot;q&quot;) + labs(title = paste(&quot;qbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) 12.4 rbinom The rbinom function is for random simulation of n binomial trials of a given size and event probability. The output is the number of successful events per trial. Let’s simulate 12 matings 12 times, as if we do one a mating involving 12 females, once per month. How many successes will we see per month? The output below represents the number of litters we would produce on each of those months. The point is, we don’t get the average every month. Some times its more successes, others its fewer. Models are perfect, data are not. rbinom(n=12, size=12, prob=0.2) ## [1] 3 2 1 2 2 7 1 2 0 4 3 2 Here’s a histogram from a very large number of simulations of the same scenario. You can clearly see the binomial distribution for this trial size and probability of success is skewed. You can also clearly see the average of the distribution…which is somewhere between 2 and 3. n &lt;- 10000 #number of simulations size &lt;- 12 #number of trials prob &lt;- 0.2 #probability of success of one trial df &lt;- data.frame(x=rbinom(n, size, prob)) #x=number of successful trials ggplot(data=df, aes(df$x)) + stat_count(fill=&quot;blue&quot;) + xlab(&quot;x&quot;)+ ylab(&quot;count&quot;)+ labs(title = paste(&quot;rbinom&quot;,&quot;(&quot;,&quot;number=&quot;,n,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) "],
["poisson.html", "Chapter 13 The Poisson Distribution 13.1 Poisson Events 13.2 dpois 13.3 ppois 13.4 rpois 13.5 Overdispersion", " Chapter 13 The Poisson Distribution library(tidyverse) 13.1 Poisson Events Counts of random, discrete events that occur in blocks of time or space are said to have the property of frequency. They can be modeled by the Poisson distribution. The values of these events are always integers. For example, the number of times a neuron depolarizes over a fixed period of time would be frequency. The number of cells on which an antigen can be detected that are in a fixed volume of fluid would also be a frequency. The number of facebook friends a random biostats student has would be a frequency. 13.2 dpois dpois is the Poisson probability mass function in R: \\(p(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) dpois takes as arguments i) the scalar \\(x\\), and ii) lambda, an average or expectation of the distribution, and returns the value of the probability, or otherwise known as the probability mass, for that scalar. \\(x\\) can be either a single value, or a vector comprised of many values. We use the latter, conveniently, to produce nice graphs. For example, assume a neuron, on average, can be expected to depolarize spontaneously 8 times per second. What is the probability it would only depolarize half that number of times? Use dpois to calculate the probability that a random number of events would occur in a time or space, given some expected average frequency. dpois(x=4, lambda=8) ## [1] 0.05725229 Therefore, the probability that a randomly selected neuron would depolarize exactly 4 times per second is 5.72%. The probability of some frequency we might expect to see is sometimes useful to calculate. What’s most notable about the the dpois is how it loses symmetry and becomes more skewed as its average (lambda) gets lower. x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=dpois(x, lambda)) ggplot(df, aes(x=x,y=px)) + geom_col(fill = &quot;red&quot;) + xlab(&quot;x&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dpois&quot;,&quot;(&quot;,&quot;lambda=&quot;, lambda,&quot;)&quot;)) 13.3 ppois R’s ppois function is the Poisson cumulative mass function \\[p(x)=\\sum_{i=0}^{x} \\frac{e^{-\\lambda}\\lambda^i}{i!}\\] This calculates a cumulative probability value for a certain frequency, given the average frequency of the distribution. Let’s say, for example, that a neuron depolarizes on average 8 times per second. If you took a random measure of depolarization activity, what is the probability that you’d observe a frequency as high as 4 depolarizations per second? ppois(4, 8, lower.tail=T) ## [1] 0.0996324 The value of almost 10% is higher than what we determined using the dpois because the ppois is a cumulative function! What is the probability that you’d observe a frequency of 16 or more depolarizations per second? To answer this question we have to reverse the function’s default lower.tail argument. As you might suspect, when set with the lower.tail=FALSE argument, as below, the ppois function returns a p-value. ppois(16, 8, lower.tail=F) ## [1] 0.003718021 Thus, the probability of observing a frequency twice as high or higher than the average for this distribution is quite low, at about 0.37%! Here’s the cumulative Poisson distribution for a phenomenon that has an average of 8, over a range of frequency values: x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=ppois(x, lambda, lower.tail=T)) ggplot(df, aes(x, px)) + geom_col(fill=&quot;red&quot;) It’s probably worth reversing the lower.tail argument to visualize the distribution of p-values for a Poisson x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=ppois(x, lambda, lower.tail=F)) ggplot(df, aes(x, px)) + geom_col(fill=&quot;red&quot;) ## qpois The qpoisfunction is the inverse of the cumulative Poisson mass function. It takes a probability as an argument and returns a frequency value. What depolarization frequency can we expect as the 90th percentile for a neuron that has an average frequency of 8 depolarizations per second? qpois(0.9, 8) ## [1] 12 Visualized, notice the stair-step pattern, which is diagnostic of discrete probability distributions p &lt;- seq(0, 1, 0.05) lambda &lt;- 8 df &lt;- data.frame(p, frequency=qpois(p, lambda, lower.tail=T)) ggplot(df, aes(p, frequency)) + geom_col(fill=&quot;red&quot;) 13.4 rpois The rpois function in R is used to generate random Poisson data. It takes arguments of lambda, the average frequency of a population, and the number of random counts to generate. To simulate a sample of 10 measurements of a neuron that on average depolarize 8 times per sec: rpois(10, 8) ## [1] 8 2 4 5 9 7 6 7 6 13 Here’s a histogram of a larger sample. Notice how it isn’t as perfect as the Poisson distribution would suggest. That’s because models are perfect, but samples are not. Even computer-generated samples! Also notice the low but detectable frequency of extreme values..frequencies higher than 20, by random chance. df &lt;- data.frame(s=rpois(10000, 8)) ggplot(df, aes(s)) + geom_histogram(binwidth=1, color=&quot;red&quot;) 13.5 Overdispersion It’s very common to observe systems that disobey the Poisson. In other words, these are systems that are discrete counts in time or space, and thus whose very nature is frequency. But yet they are poorly fit by the Poisson distribution model. For example, the two graphs below. The first is a distribution of the counts of facebook friends of biostats students. Their average number of friends is 610. The second is a based upon a Poisson distribution whose lambda is 610. The model obviously is a poor fit for the data. Models are perfect, data are not. The friends distribution is said to be over-dispersed. The reason for this overdispersion is likely that it is a more complex system than what a Poisson would assume. A key assumption for a Poisson model is that the counts occur randomly. It seems quite likely that biostats students probably don’t choose friends randomly. survey &lt;- read.csv( &quot;datasets/precourse.csv&quot;) ggplot(survey, aes(x=friends)) + geom_histogram(aes(y=..density..), color=&quot;red&quot;, binwidth=50) x &lt;- c(0:5000) lambda &lt;- 610 df &lt;- data.frame(x, px=dpois(x, lambda)) ggplot(df, aes(x=x,y=px)) + geom_col(fill = &quot;red&quot;) + xlab(&quot;x&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dpois&quot;,&quot;(&quot;,&quot;lambda=&quot;, lambda,&quot;)&quot;)) "],
["normal.html", "Chapter 14 The Normal Distribution 14.1 dnorm 14.2 pnorm 14.3 qnorm 14.4 rnorm", " Chapter 14 The Normal Distribution 14.0.1 The Standard Normal If the variable \\(X\\) represents a population of normally distributed values with a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the variable \\(Z\\) \\[Z=\\frac{X-\\mu}{\\sigma}\\] has a standard normal distribution with a mean of 0 and a standard deviation of 1. Converting experimental sample data to standard normal z-scores is very common. Assume we have a sample of 210 peoples heights, whose sample mean = 169.9 and sd = 10.5. The tallest height in the sample is 205. The z-score of the tallest individual is derived using the sample parameters and is (205-169.9)/10.5 = 3.34. The z-score could be interpreted this way: the tallest individual is said to be 3.34 standard deviations taller than the mean height of the students in the sample. 14.1 dnorm dnorm is the normal probability density function in R: \\(p(x)=\\frac{e^\\frac{-(x-\\mu)^2}{2\\sigma^2}}{\\sigma\\sqrt{2\\pi}}\\) dnorm takes i) the scalar \\(x\\), ii) a mean and iii) sd as arguments and returns the value of the probability, or otherwise known as the probability density, for that scalar. \\(x\\) can be either a single value, or a vector comprised of many values. For example, assume the average height of an adult in the US is 168.8 cm with a standard deviation of 7.1 cm. Use dnorm to calculate the probability that a randomly selected US adult will be 176 cm tall: dnorm(x=176, mean=168.8, sd=7.1 ) ## [1] 0.03360041 Therefore, the probability that a randomly selected US adult would be 176 cm tall is 3.36%. The probable height of one specfic individual is usually not very useful or interesting to calculate. What’s a bit more useful is to see how the function operates over a range of values. For example, we can model the distribution of adult heights using dnorm. We might be interested in the fraction of adults whose heights are 176cm and taller compared to the rest of the population. To illustrate the answer with a plot, we pass a range of x values range = c(130, 210) into the dnorm function, rather than a single value. dnorm calculates probabilities over that full range. We can make a plot to visualize the answer, shading anybody as tall or taller than 176 in blue: range &lt;- c(130,210) pop &lt;- list(mean=168.8, sd=7.1) auc &lt;- c(176,210) ggplot(data.frame(x=range), aes(x)) + stat_function(fun=dnorm, args = pop, color = &quot;red&quot;) + stat_function(fun=dnorm, args = pop, xlim = auc, geom=&quot;area&quot;, fill=&quot;blue&quot;) + xlab(&quot;heights, cm&quot;) + ylab(&quot;p(heights)&quot;) Later on, we’ll use other functions to quantify the blue shaded area. Here’s plot of the standard normal distribution over a range of z-values that are 4 SD’s below and above the mean: z &lt;- seq(-4,4,0.1) mean &lt;- 0 sd &lt;- 1 df &lt;- data.frame(z, pz=dnorm(z, mean, sd)) ggplot(df, aes(x=z,y=pz)) + geom_line(color = &quot;red&quot;) + xlab(&quot;z&quot;) + ylab(&quot;p(z)&quot;) + labs(title = paste(&quot;dnorm&quot;,&quot;(&quot;,&quot;mean=&quot;, mean,&quot;,&quot;,&quot;sd=&quot;,sd,&quot;)&quot;)) 14.2 pnorm When a height value is given, calculating the probabilities for heights up to or greater than than that limit can be of considerable interest. pnorm is the R function for that…give it the value of a normally-distributed variable, such as height, and it returns a cumulative probability for the distribution on either side of that value. Thus, pnorm is called the normal cumulative distribution function in R: \\[p(x)=\\int^x_{-\\inf}\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi}}\\] By default pnorm will return the cumulative value of the normal pdf up to the value of the input scalar. In otherwords, given the value of a variable, pnorm returns the probability of that value or less. However, this can be reversed by changing to the lower.tail=FALSE argument. In this case, pnorm returns something akin to a p-value. Given some value of a variable, pnorm returns the probability of that value or greater. The first line in the script below calculates the probability that a US adult will be less than or equal to 175.9 tall, which in this instance turns out to be 1 sd taller than the mean height. It returns a value of about 84%. The second line calculates the probability that a US adult will be greater than or equal to 175.9 cm tall. It returns a value of about 16%. pnorm(175.9, 168.8, 7.1, lower.tail = T) ## [1] 0.8413447 pnorm(175.9, 168.8, 7.1, lower.tail = F) ## [1] 0.1586553 Subtracting the latter from the form illustrates that about 68% of US adults will be within 1 standard deviation of the average US adult height: pnorm(175.9, 168.8, 7.1, lower.tail = T) - pnorm(175.9, 168.8, 7.1, lower.tail = F) ## [1] 0.6826895 About 95% of the values for any continuous, normally-distributed variable will be between 2 standard deviations of the mean: pnorm(q=2, mean=0, sd=1, lower.tail=T) ## [1] 0.9772499 pnorm(2, 0, 1, lower.tail=F) ## [1] 0.02275013 pnorm(2) - pnorm(2, lower.tail=F) ## [1] 0.9544997 (The script above illustrates a few of the different shorthands that can be taken working with R’s probability functions). Calculating “p-values”&quot; using pnorm Let’s go back to human heights. What’s the probability of an US adult being as tall or taller than 205 cm? Notice the lower.tail=F argument: pnorm(205, 168.8, 7.1, lower.tail=F) ## [1] 1.71095e-07 That’s a very low probability value, because the height is so extreme. Calculating percentiles using pnorm What is the height percentile of a 205 cm tall US adult? pnorm(205, 168.8, 7.1)*100 ## [1] 99.99998 14.3 qnorm qnorm is the inverse of the cumulative distribution function of a continuous normally-distributed variable. By default, qnorm takes a cumulative probability value (eg, a percentile) as an argument (along with the mean and sd of the variable) and returns a limit value for that continuous random variable. Here’s what the qnorm distribution looks like: p &lt;- seq(0.0, 1.0, 0.01) mean &lt;- 0 sd &lt;- 1 df &lt;- data.frame(p, z=qnorm(p, mean, sd)) ggplot(df, aes(p, z)) + geom_line(color = &quot;red&quot;) + xlab(&quot;p(z)&quot;) + ylab(&quot;z&quot;) + labs(title = paste(&quot;qnorm&quot;,&quot;(&quot;,&quot;mean=&quot;, mean,&quot;,&quot;,&quot;sd=&quot;,sd,&quot;)&quot;)) You’re probably very familiar with quantiles since percentiles are a class of quantiles. For example, if your standardized exam score is in the 90th percentile, then you did as well or better than 90% of test takers. If you forgot your score, but remember your percentile, you could use qnorm to return your specific test score…so long as you also know the mean and sd values of the test scores. Back to heights. What’s the height in cm of a US adult who is at the 99th percentile? What about the 5th percentile? qnorm(0.99, 168.8, 7.1) ## [1] 185.3171 qnorm(0.05, 168.8, 7.1) ## [1] 157.1215 The first script below returns the height of the 84th% quantile of US adults. 84% are about 175.9 cm tall or less. The 84th is the upper 1 SD quantile. The second script returns the height of the complement of the 84th quantile of US adults, by switching the default lower.tail argument. This complement is the lower 1 SD quantile. qnorm(0.84, 168.8, 7.1, lower.tail = T) ## [1] 175.8607 qnorm(0.84, 168.8, 7.1, lower.tail = F) ## [1] 161.7393 About two thirds of US adults are between 161.7 and 175.9 cm tall. 14.3.0.0.1 Confidence interval limits and qnorm The qnorm distributon has pragmatic utility for finding the limits for confidence intervals when using the normal distribution as a model for the data. In a standard normal distribution, the limits for the lower and upper 2.5% of the distribution are about \\(\\pm\\) 1.96 standard deviation units. Thus, the 95% confidence interval for standard normal z-values is -1.96 to 1.96. qnorm(0.025) ## [1] -1.959964 qnorm(0.025, lower.tail=F) ## [1] 1.959964 Based upon the average height and sd of US adults, the 95% confidence interval for US adult height is: paste(round(qnorm(0.025, 168.8, 7.1),1), &quot;to&quot;, round(qnorm(0.025, 168.8, 7.1, lower.tail=F),1)) ## [1] &quot;154.9 to 182.7&quot; The confidence interval means there is a 95% chance the true average US adult is within that range. Rember that height sample from near the very start of this document? It had 210 replicates, a mean = 169.9, sd = 10.5. What is the 95% confidence interval of US adult heights, based upon that sample? paste(round(qnorm(0.025, 169.9, 10.5),1), &quot;to&quot;, round(qnorm(0.025, 169.9, 10.5, lower.tail=F),1)) ## [1] &quot;149.3 to 190.5&quot; Drawing inference to the whole population on the basis of this sample, and assuming a normal model, there is a 95% chance the true US adult height is within this slightly wider range. The truth is, a better model to derive confidence intervals for normally-distributed samples is the t-distribution. 14.4 rnorm R’s rnorm is the random number generator function for the normal distribution. This has a lot of utlity in synthesizing data sets, for example, when running simulations. It returns random normally distributed values given size, mean and standard deviation arguments. For example, here is a randomly generated sample of 10 US adult heights, rounded to 1 significant digit. We’ll use this function a LOT in this course to generate normally-distributed data for various purposes. Here’s 10 random, simulated heights of US adults, rounded to the first digit to make it cleaner looking: round(rnorm(10, 168.8, 7.1), 1) ## [1] 164.2 178.6 164.5 178.1 163.1 172.8 156.8 164.8 183.9 172.9 14.4.1 Plotting histograms of some rnorm samples Histograms are a powerful way to explore the underlying structure in a dataset. They are a descriptive tool. Adjusting the bin parameters provides a way to evaluate the data. A histogram takes one variable, plotting its values on the x-axis while displaying the counts or the density of those values on the y-axis. The distribution of the data in a histogram is controlled by adjusting the number of bins, or by adjusting the binwidth. 14.4.2 Bins and Binwidth Think of bins as compartments. If using, for example 100 bins as a plotting argument, the range of the sample values are split, from the shortest to the tallest heights, into 100 evenly spaced…compartments. For normally distributed variables, the central bins are more dense or have more counts then those on the tails of the histogram, because values near the average are more common. A binwidth argument can be used instead of bin. Setting the binwidth to 10 on the graph below would look the same as setting the number of bins to 6. Adjusting bins or binwidths has the effect of rescaling the y-axis. 14.4.2.1 Histograms of counts vs density Because histograms are so commonggplot2 has a geom_histogram function to simplify creating the plots. The default argument creates a histogram of values of a random variable of interest. To create a histogram of densities, use a density aes argument in the function, instead as you’ll see below. Counts are the number of cases within a given bin. Density is the fraction of all counts within a given bin. Here are 1000 random, simulated heights, plotted as a histogram of counts. set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=168.8, sd=7.1)) ggplot(df, aes(s)) + geom_histogram(bins=100) Same thing, plotted as a probability density: set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=168.8, sd=7.1)) ggplot(df, aes(s)) + geom_histogram(aes(y = ..density..), bins=100) 14.4.2.2 Histogram with Distribution For an example of plotting a model along with the raw data, here’s a density histogram of the rnorm sample, plotted along with a dnorm distribution model, given the population mean and sd. By eyeball, the model seems to be a decent fit for the sample. But it is not a perfect fit. It’s not the model’s fault. Models are perfect, data are not. How does the sample size affect how well the model fits the data? set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=0, sd=1)) ggplot(df, aes(s)) + geom_histogram(aes(y = ..density..), bins=100) + stat_function(fun = dnorm, args=list(mean=0, sd=1), color = &quot;red&quot;, size = 2) "],
["categorical.html", "Chapter 15 Statistics for Categorical Data 15.1 Types of categorical data 15.2 Exact v Asymptotic Calculations of p-values 15.3 Overview of the types of hypothesis testing 15.4 Comparing proportions 15.5 Exact tests for two proportions 15.6 Goodness of fit Tests 15.7 Contingency Testing 15.8 Doing a priori power analysis for proportion tests 15.9 Power analysis functions for proportion tests 15.10 Graphing Proportions", " Chapter 15 Statistics for Categorical Data library(PropCIs) library(tidyverse) library(binom) library(pwr) library(statmod) library(EMT) Biomedical research is full of studies that count discrete events. A common mistake made by many researchers is to use statistics designed for measured variables on discrete count variables. For example, they transform count data into scaler measures (eg, percents, folds etc) and then apply statistics designed for continuous variables to events that are fundamentally discrete by nature. The problem with that is there are inherent differences in the behaviors of continuous and discrete variables. Therefore, it is important to recognize that there are statistical tests that can be used to analayze discrete data directly, without resorting to these transformations. 15.1 Types of categorical data What proportion of cells express a specific antigen and does an experimental treatment cause that proportion to change? What proportion of rats treated with an anxiolytic drug choose one chamber over others in a maze test? How many people who express a certain marker go on to have cancer? In these three scenarios the primary data are counts. All of the study results have integer values. The counts are categorized with variable attributes, thus they are called categorical data. In fact, the three scenarios above are very different experimental designs. The first represent experiments that compare simple proportions. The second compare freqeuncies, and the third is an association study. The analysis of these require using a common suite of statistical tools in slightly different ways. Broadly, all of these tools boil down to dealing with proportions. A few types of proportions (eg, odds ratio, relative risk) that can be calculated from these datasets are sometimes used as effect size parameters. Other times we’d use a confidence interval as a way of conveying an effect size. Statistical tests are then used to evaluate whether these effect sizes, or frequencies, or simple proportions, are extreme enough relative to null counterparts so that we can conclude the experimental variable had some sort of effect. 15.1.1 Proportions We might… Inactivate a gene hypothesized to function in the reproductive pathway. To test it, mating triangles would be set up to count the number of female mice that become pregnant or not. Implant a tumor in mice, before counting the number of survivors and non-survivors at a given point later. Mutate a protein that we hypothesize moves in and out of an intracellular compartment, before staining cells to count the number of cells where it is and is not located in a compartment of interest. Each of the examples above have binomial outcomes…pregnant vs not, dead vs alive, or inside vs outside. In each case above, both the succesful and the failed events are counted in the experiment. A proportion is a simple ratio of counts of success to counts of failures. 15.1.2 Frequencies Other kinds of counted data occur randomly in space and time. The examples below illustrate this. Note how only the number of events are recorded, rather than categorizing them as successes or failures. These counts therefore have the statistical property of frequency, such as counts per time or counts per volume or counts per area. We can… Expose a neuron to an excitatory neurotransmitter, then count the number of times it depolarizes over a given period of time. In a model of asthma, count the number of immune cells that are washed out in a pulmonary lavage protocol after an immunosuppressive agent. The key difference for these compared to binomial events is that their non-event counterparts are meaningless. For example, it is not possible to measure the number of depolarizations that don’t occur, or know the number of immune cells that don’t wash out in the lavage. 15.1.3 Associations Lastly, the examples below illustrate the design of association studies, which are based upon, according to the null hypothesis, independent predictors and outcomes. Here are some examples of association study designs: You might wish to identify causal alleles associated with a specific disease phenotype by counting the number of people with and without the disease, who have or don’t have a particular allelic variant. determine if a history of exposure to certain carcinogens is associated with a higher risk of cancer by counting people with cancer who have been exposed. know if a drug treatment causes a higher than expected frequency of a side-effect by counting the people on the drug with the side-effect. In the simplest (and most general) case, association studies are 2X2 in design: A predictor is either present or absent as the row factor, and an outcome was either a success for a failure as the column factor. Subjects are categorized into groups on the basis of where they fall in the 4 possible combinations that such 2X2’s allow for. It should be noted that higher order association studies are also possible, which can be either symmetric (eg, 3x3) or non-symmetric (eg,) 9X2, 2x3, and so on. 15.1.4 Statistics Covered Here Confidence intervals of proportions One-sample proportions test Two-sample proportions test Goodness of fit tests Tests of associations Power analysis of proportions (including Monte Carlo simulation) Plotting proportions with ggplot2 15.2 Exact v Asymptotic Calculations of p-values The statistical tests for hypotheses on categorical data fall into two broad categories: exact tests (binom.test, fisher.test, multinomial.test) and asymptotic tests (prop.test, chisq.test). Exact tests calculate exact p-values. That’s made possible using factorial math. The prop.test and chisq.test generate asymptotic (aka, approximate) p-values. They calculate a \\(\\chi^2\\) test statistic from the data before mapping it to a \\(\\chi^2\\) probability density function. Because that function is continuous, the p-values it generates are asymptotically-estimated, rather than exactly calculated. 15.2.1 Choosing exact or asymptotic As a general rule, given the same datasets and arguments, exact and approximate hypothesis test functions will almost always give you p-values that differ, but only slightly. That’s usually not a problem unless you’re near a threshold value. Typically, an integrity crisis is evoked when that happens: “Which is”right???&quot; Do you p-hack and choose the favorable one or not? You should use the test you said you’d use when you first designed the experiment. And if you didn’t pre-plan…or at least have some idea about where you are going…recognize that the exact tests are more accurate. Another issue that arises is how well the tests perform with low count numbers. For example, as a rule of thumb, avoid using the chisq.test when the data have counts less than 5 in more than 20% of the cells because the accuracy of the chisq.test is less at low cell counts. Use an exact test instead. 15.3 Overview of the types of hypothesis testing We’ll go through each below in more detail, emphasizing practical experimental design and interpretation principles. 15.3.1 Proportion analysis You can learn a lot about experimental statistics by thinking about proportions. So a lot of time is spent on it. Proportions are derived from events that can be classified as either successes or failures. Sometimes we want to compare simple proportions to decide if they are the same or not. 15.3.2 Goodness of fit testing We do this when we want to compare the frequency distribution we observe in an experiment to the null expectation for that frequency distribution. 15.3.3 Contingency Analysis Contingency analysis, otherwise known as tests of independence, are very different from goodness-of-fit test and simple proportion tests, in design and in purpose. They allow us to ask if two (or more) variables are associated with each other. Unlike a lot of the statistics we’ll deal with, there is a hint of a predictive element associated with these types of studies because the effect sizes we use to explain their results are related to odds and risk and likelihood. Which is not to say that we couldn’t use the same predictive concepts in proportions and goodness of fit testing. Contingency tests are very common in epidemiology and in clinical science. You recognize by their names as cohort studies, case control studies, and so forth. 15.4 Comparing proportions In their simplest use, the tests here can be used to compare one proportion to another. Is the proportion of successes to failures that results from a treatment different from the proportion that results from control? We’ll dive into this further below. 15.4.1 A Mouse T Cell Pilot Experiment: The Cytokine-inducible antigen gradstudin Let’s imagine a small pilot experiment to see how a cytokine affects T cells. This is a very crude experiment designed mostly to illustrate some principles. A cytokine is injected into a single mouse. There is no control injection, just one mouse/one cytokine injection. A time later, blood is harvested from the mouse to measure an antigen on T cells. Let’s call the antigen gradstudin. Assume a method exists to detect T cells in the sample that express gradstudin and don’t express gradstudin. That method implies some nominal criteria are established to categorize T cells as either expressing gradstudin or not. FACS machines are very useful for this. The machine typically produces continuous fluorescent data, where intensity is proportional to gradstudin levels. But we don’t care about the magnitude of the expression level, we just care whether it is there or not. Based upon our scientific expertise, we establish cutoff gating criteria above which fluoresence == gradstudin is present. The machine therefore returns simple counts of both gradstudin-positive and gradstudin-negative cells. 15.4.2 Calculating Proportions Here’s the data, counts of cells expressing and not expressing gradstudin. It is a very simple dataset: pos &lt;- 5042 neg &lt;- 18492 A proportion is the count of a particular outcome relative to the total number of events. It’s customary to use the number of successes as the numerator. Whereas its customary to refer to the total number of events, n, as the trial number rather than as sample size, but they mean the same thing. n &lt;- pos+neg #trial size prop &lt;- pos/n prop ## [1] 0.2142432 15.4.3 What A Proportion Estimates This sample proportion is descriptive statistic. It serves as a point estimate of the true proportion of the population we sampled. The only way to know the true proportion would be to count every T cell in every drop of the subjects blood! This point estimate is statistically valid if our sample meets two conditions. First, that this is a random sample of the T cells in the subject’s blood. Second, if we consider every T cell in the sample as statistically independent of every other T cell. We can safely assume those conditions are met. Strictly, as an estimate this proportion only infers the population of blood borne T cells in that one subject. We really can’t generalize much further than that, including the composition of T cells in sequesterd compartments (thymus, nodes, etc). Which is fine for our purposes now because we’re trying to keep this simple. 15.4.4 Confidence Intervals of Proportions Confidence intervals (CI) have features of both descriptive and inferential statistics. 15.4.4.1 Definition of a 95% CI The 95% CI for a sample proportion represents a range of proportions within which 95% of the time we would expect the true population proportion. There’s a lot going on there. The value of the proportion we measured in the sample is a mathematical fact that is not in dispute. It is what it is. The question is, what does it represent? Although there might be some error associated with measuring it, our single sample offers no real information about what that error might be. As an n=1 sample, there is no variation! What is unknown is the true proportion of gradstudin+ T cells in the population we sampled. CI’s are designed to give us some insights into that unknown. 95% CI’s are a range estimate of what that true population proportion might be. CI’s are calculated in part upon the quality of the point estimate. In the case of proportions, the quality of the point estimate is driven by the size of the sample, the number of counts that are involved in calculating the proportion. As you might imagine intuitively, the more counts we have in the sample, the more confidence we should have that our proportion provides a good estimate of the population’s proportion. 15.4.4.2 Calculating CI with R The PropCIs package offers several ways to calculate a CI. Because of that, when publishing it is important is to state which CI method is used. Is one better than the other? Sometimes, yes. For now, let’s not worry about that. Wilson’s score interval with continuity correction] is suggested as the most accurate for proportions. Other methods are more commonly used than Wilson’s because they gained traction as being easier to compute by hand, and old habits die slowly. Taking the data on cytokine induced gradstudin+ T cells, the chunk below illustrates how to use PropCIs to derive a Wilscon score interval-based 95% CI: scoreci(pos, n, conf.level=0.95) ## ## ## ## data: ## ## 95 percent confidence interval: ## 0.2090 0.2195 15.4.4.3 Interpretation of a CI The value of our sample proportion, 0.214, falls within this 95% CI. That’s not a big surprise, given the 95% CI was calculated from our proportion! On the basis of the sample proportion, we can conclude from this CI that there is a 95% chance the true proportion of gradstudin positive T cells falls within this very narrow range. 15.4.4.4 Using the CI as a quick test of statistical significance. Let’s say, for example, that we have tremendous experience and great scientific reason to expect to see under normal conditions that only 15% of T cells would be gradstudin-positive normally. Does our sample proportion differ from that expectation? Since a proportion of 0.15 is not within the 95% CI calculated above, we can conclude that the cytokine-induced sample proportion differs from this expectation at the 5% level of statistical significance. We just did a statistical test, without running any software (sorta) or generating any p-values!! And it is perfectly valid inference. 15.4.5 A One-Sample Proportion Test We’ll use prop.test to run a test that generates a p-value to decide if the sample proportion we have above differs from 0.15. 15.4.5.1 Hypothesis Tested in a One-Sample Proportion Test In this test the sample antigen-positive proportion is compared to a theoretical proportion. If the typical proportion of antigen-positive T cells within a blood sample is 15%, is the result after cytokine treatment different from this proportion? Let’s say that our scientific hypothesis going into all this is that the cytokine induces the antigen on T cells. Since we are predicting an increase, we should establish a one-sided alternative (thus using greater as an argument in prop.test below) as our statistical hypothesis. Our statistical hypothesis is the null. We’ll decide whether or not to reject the null on the basis of the test results. Philosophically, we’re using a falsification method. The statistical alternate hypothesis: \\(\\pi&gt;15\\%\\) The statistical null hypothesis: \\(\\pi\\le15\\%\\) We use Greek notation to represent the ‘true’ population proportion. This reminds us that a statistical hypothesis is an inferential test about the population proportion. Again, there is no question that the sample proportion differs from a proportion of 15%. 21% != 15%. That’s a simple numerical fact. Statistical tests are not necessary to make that assertion. On the basis of the sample proportion p, we’d like to draw inference on the composition of all of the T cells in the blood of the subject. Thus, the sample p is only an estimate of a true \\(\\pi\\) (which we notate using Greek letters). Statistical testing allows us to generate some insight into the reliability of our estimate. The chunk below lays out these arguments using R’s prop.test function: #pos and n in the test arguments are objects that were defined above! prop.test( pos, n, p=0.15, alternative = &quot;greater&quot;, conf.level = 0.95, correct = TRUE ) ## ## 1-sample proportions test with continuity correction ## ## data: pos out of n, null probability 0.15 ## X-squared = 761.29, df = 1, p-value &lt; 2.2e-16 ## alternative hypothesis: true p is greater than 0.15 ## 95 percent confidence interval: ## 0.2098559 1.0000000 ## sample estimates: ## p ## 0.2142432 15.4.5.2 Interpreting one-sample prop.test output Like all statistical tests, this one is evaluated under the assumption that the null hypothesis is true. We use the test outcome to decide whether the null hypothesis should be rejected. The prop.test conducts a chi-square analysis. The value of \\(\\chi^2\\) for this sample is very large. The p-value represents the probability of obtaining a \\(\\chi^2\\) value as larger or larger then what is calculated from our sample. If the null hypothesis is true in this case, the probability of a \\(\\chi^2\\) value as large or larger than we obtained is 2.2e-16, which is very, very low. The 95% CI is 0.2098 to 1.0. There is a 95% chance the population proportion is greater than 0.2098. The reason it differs from the Wilson’s CI calculated above is that we used greateras a one-sided hypothesis argument in the prop.test. These CI’s from one-sided hypothesis tests are not particularly useful when attempting to use the CI as an index of accuracy. 15.4.5.3 How to write this up The proportion of gradstudin positive T cells after cytokine treatment in the subject differs from an expected value of 0.15 (one-sided one-sample proportions test, p-value=2.2e-16, 95% CI = 0.209 to 1.0) Notice how I didn’t say “significantly” or “statistically significantly” or some such. Whether an outcome is signficant or not should be a scientific assertion, rather than statistical. 15.4.5.4 An exact test for one proportion R’s binom.test function is an exact test for whether a proportion differs from a theoretical expectation. It compares proportions using an entirely different procedure. As a one-proportion test the binom.test gives an exact p-value derived from the binomial distribution, whereas the prop.test gives approximate p-values because it uses the chi-square distribution. That distinction is hard to see with our examples here, but the differences will become more noticable when analyzing samples with far fewer events. Here’s the binomial test run on two different proportions. In each, the test is comparing the experimental proportion to the proportion value of 0.15. binom.test(pos, n, p=0.15) ## ## Exact binomial test ## ## data: pos and n ## number of successes = 5042, number of trials = 23534, p-value &lt; ## 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.15 ## 95 percent confidence interval: ## 0.2090155 0.2195416 ## sample estimates: ## probability of success ## 0.2142432 binom.test(x=567, n=1778, p=0.15) ## ## Exact binomial test ## ## data: 567 and 1778 ## number of successes = 567, number of trials = 1778, p-value &lt; ## 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.15 ## 95 percent confidence interval: ## 0.2972673 0.3411272 ## sample estimates: ## probability of success ## 0.3188976 15.4.6 Comparing Two Proportions We can stick with the T cell-cytokine-gradstudin scenario, but let’s change up the experiment a tad. Let’s imagine we’ve withdrawn a sample of blood from a subject and enriched for T cells. Half of the sample is exposed in vitro to a cytokine for a few hours. The other half is exposed to vehicle as a control. We count the gradstudin-positive and gradstudin-negative T cells in both groups. We now have a predictor group at two levels (treatment = vehicle or cytokine) and an outcome group at two levels (antigen = positive or negative) 15.4.6.1 Hypothesis Tested Let’s test the hypothesis that the proportion of postive T cells in the two samples differ. The choice is not to test whether one proportion is greater than the other. We just want to know if they differ. The statistical hypotheses here differs from the one sample hypotheses in two ways. First, notice how we’re comparing the population proportion of cytokine- to that for vehicle-treatment. Second, we’re making this a two-tailed (two.sided) test instead of one-tailed (greater). The statistical alterate hypothesis: \\(\\pi_c\\ne\\pi_v\\) The statistical null hypothesis: \\(\\pi_c=\\pi_v\\) A second way of writing these hypotheses to say the same thing: Alternate: \\(\\pi_c-\\pi_v\\ne0\\) Null: \\(\\pi_c-\\pi_v=0\\) 15.4.6.2 Running the test Let’s say that here are the outcome data: gradstudin-positive, gradstudin-negative, total Cytokine-treated: 567, 1211, 1778 Vehicle-treated: 412, 1485, 1897 The data can be dumped directly into prop.test. Normally we’d run this at 0.95 confidence level (type1 error of 5%). Let’s say we have good scientific reasons to run this test at a much more stringet threshold level for type1 error, because we’re really hoping to avoid a false positive: prop.test( x=c(567, 412), n=c(1778, 1897), conf.level=0.9999999 ) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(567, 412) out of c(1778, 1897) ## X-squared = 48.066, df = 1, p-value = 4.121e-12 ## alternative hypothesis: two.sided ## 99.99999 percent confidence interval: ## 0.02364902 0.17977620 ## sample estimates: ## prop 1 prop 2 ## 0.3188976 0.2171850 15.4.6.3 Interpretion of Two-Sample proportions test output This test is evaluated under the assumption that the null hypothesis is true. The test results helps us decide whether the null hypothesis should be rejected. We’ll do that if the p-value is less than our type1 error threshold. The prop.test conducts a chi-square analysis using the Yates continuity correction. This \\(\\chi^2\\) value is very large; extremely large. The p-value represents the probability of obtaining a \\(\\chi^2\\) value as larger or larger than that by chance. If the null hypothesis is true in this case, the probability of that sized \\(\\chi^2\\) value is 4.12e-12, which is very low, and still much lower than our type1 error threshold. We can reject the null. If we subtract prop2 from prop1 we get a value of about 0.1017, as a point estimate for the difference between the two proportions. The 99.99999% CI for the difference between two proportions does not include the value of 0. Since the 99.99999% CI does not overlap with 0, we can conclude from it alone that there is a difference between the two proportions. Even at this extremely high confidence level!! 15.4.6.4 Write Up The proportion of gradstudin positive T cells after cytokine differs from that in vehicle treated cells (two-sided two-sample proportions test, p-value=4.12e-12, 99.99999% CI for proportion difference = 0.023 to 0.179) Note how we don’t say “statistically significantly different”. Notice also how the 99.99999% CI is very wide. It would be narrower if run at a 95% confidence level. Why? 15.5 Exact tests for two proportions An alternative to prop.test to compare two proportions is the fisher.test, which like the binom.test calculates exact p-values. The fisher.test requires that data be input as a matrix or table of the successes and failures, so involves a bit more munging. We need to make a matrix of the data first, then perform the fisher.test on that matrix: gradstudin-positive, gradstudin-negative, total Cytokine-treated: 567, 1211, 1778 Vehicle-treated: 412, 1485, 1897 Notice below that here we’re entering the successes and the failures in the matrix. We’re tagging the matrix with some dimension names so it doesn’t get confusing. M &lt;- matrix( c(567, 412, 1211, 1485), nrow=2, dimnames = list( Treat=c(&quot;Cytokine&quot;, &quot;Vehicle&quot;), Antigen=c(&quot;Positive&quot;, &quot;Negative&quot;) ) ) M ## Antigen ## Treat Positive Negative ## Cytokine 567 1211 ## Vehicle 412 1485 fisher.test(M) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: M ## p-value = 3.433e-12 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.451760 1.962188 ## sample estimates: ## odds ratio ## 1.687312 Notice the output differs from the prop.test. In adddition to a p-value, the Fisher test produces an odds ratio and its confidence interval. The p-value leads to the same result and write-up as for the two proportion test. The odds ratio could be interpreted like this: The cytokine increases the odds of gradstudin+ T cells by 1.687 compared to vehicle treatment (Fisher’s Exact Tet for Count Data, p = 3.433e-12, OR 95% CI = 1.45 to 1.96). 15.6 Goodness of fit Tests Goodness-of-fit tests are useful for testing hypotheses about patterns of counts in time or space…whether the distribution of their observed frequencies differs from expectations of a null case. In other words, do they occur in a non-random pattern? There is no independent variable in these tests. The shape of these datasets is either as 1 row or 1 column, where every cell is a time or space and the cell value is the the number of counts that occured in that time or space. Either the multinomial.test (for exact p-values) or the chisq.test (for approximate p-values) can be used for Goodness-of-fit testing. The latter is most commonly used. These designs compare the disribution of events to a hypothetical (or expected) model null distribution of those events. These expected counts are entered in the test script as a prop or p argument. This can be confusing. It’s important to recognize you should enter a vector of null probabilities in p or prop! Don’t enter the counts you hope to see if the test were positive!!! Say we had a spatial memory test in which 28 independent subjects are placed into a maze for testing (one at a time) and we count which of 4 chambers they enter first. They do so at the following frequencies: A=14, B=3, C=7, D=4. Does this frequency distribution differ from the null expectation, A=7, B=7, C=7, D=7, where no chamber is more likely to be entered than another? Failure is not an option in this design! Only successes are counted. Given enough time, a subject will always choose a chamber (iIf one fails the task, it should be censured). Let’s test this at the 5% type1 error threshold: 15.6.1 An Exact Goodness of Fit test NB:The multinomial.test function requires you to assert the null frequency distribution explicitly and as fractions whose sum is 1. x &lt;- c(A=14, B=3, C=7, D=4) prob &lt;- c(A=.25, B=.25, C=.25, D=.25) multinomial.test( x, prob=prob ) ## ## Exact Multinomial Test, distance measure: p ## ## Events pObs p.value ## 4495 1e-04 0.0235 Note on the multinomial.testoutput: Please see this site for further information on what is represented by events (its a combination result–a metric of the computation it took to do this) and pObs (its a multinomial probability). We’re only intersted in the p-value, since we’re using this function as an exact goodness-of-fit hypothesis test for the null hypothesis. Why use an exact test rather than a chisq.test? Because we have two cells in the dataset with counts &lt; 5! An exact p-value test will be more accurate than chisq.test. The test compares our sample frequency distribution to that in the null model. We actually wrote the latter explicitly in the function argument: null is uniform distribution–the subjects are equally likely to enter each chamber. H0: The probability of choice is equal for each chamber. \\(\\pi_A=\\pi_B=\\pi_C=\\pi_D\\) H1: The probability of choice is not equal for each chamber. \\(\\pi_A\\ne\\pi_B\\ne\\pi_C\\ne\\pi_D\\) (Note: this is an omnibus test. It doesn’t explicity tell us which chambers are preferred by the subjects.) Because the p-value is less than 0.05, we reject the null hypothesis and conclude that the chamber choice is not equitable across the four options. 15.6.1.1 Write up In a 4 chamber maze test, the subjects displayed a clear, non-uniform chamber preference (Exact Multinomial Test, n=28, p=0.0235) Note how this implies the null hypothesis. 15.6.2 An Approximate Goodness of Fit test The \\(\\chi^2\\) test of the same data is really simple to execute. It offers the same conclusion, but note how the p-value is very different. Note also that we didn’t enter the null frequency argument. The chisq.testfunction will coerce the null distribution if it is not entered as an argument explicitly, as you can see from the output for the second line. If for some reason to test against a non-uniform null distribution, you’ll need to write that in your argument explicitly (eg, p = c(A=0.5, B=0.25, C &amp; D = 0.125). chisq.test(x) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 10.571, df = 3, p-value = 0.01428 15.6.2.1 Write up The interpretation is no different than for the exact test. The write up is: In a 4 chamber maze test, the subjects displayed a clear chamber preference (Chi square test for uniform probabilities, \\(\\chi^2\\)=10.571, df=3, p=0.01428) 15.7 Contingency Testing Contingency testing is for deciding whether two or more variables are associated or not. These either explicitly (ie, when using fisher.text) or implicity (ie, when using chisq.test) use ratio’s of proportions–the odds ratio, or relative risk, or the likelihood ratio, or sometimes other proportions–as parameters that express the magnitude of these associations. In other words, the hypothesis test asks whether these ratio’s of proportions are more extreme than the null (which would be 1). Let’s take the cancer marker data from the contingency analysis lecture. As you recall, a marker has been discovered that is hoped to be strongly associated with cancer. 100 people were tested for whether or not they have the marker, and whether or not they go on to have cancer. We’ll create a simple matrix then pass it through the fisher.test function to illustrate the procedure and interpretation. x &lt;- matrix( c(14, 16, 6, 64), ncol=2, dimnames = list( Marker=c(&quot;Present&quot;, &quot;Absent&quot;), Cancer = c(&quot;Present&quot;, &quot;Absent&quot;) ) ) x ## Cancer ## Marker Present Absent ## Present 14 6 ## Absent 16 64 fisher.test(x) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x ## p-value = 3.934e-05 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.762514 33.678765 ## sample estimates: ## odds ratio ## 9.061278 #more argument customization than this is possible 15.7.1 Intepretation of Contingency Results The odds of a person with the marker having cancer are 9.06 times greater than that for those who don’t have the marker. There is a 95% chance the true odds ratio in the population is between 2.76 and 33.68. There is an association between the presence of this marker and the probability that cancer occurs. 15.7.2 Write Up The large OR indicates the presence of this marker is strongly associated with cancer (n=100, OR = 9.06, 95% CI = 2.76 to 33.68, Fisher’s Exact Test for Count Data, p = 3.934e-05). The word “strongly” is used to emphasize the effect size, which is OR, not the smallness of the p-value. Here are the other tests you might use to conduct for a contingency analysis, to illustrate how they differ: ` x &lt;- matrix( c(14, 16, 6, 64), ncol=2, dimnames = list( Marker=c(&quot;Present&quot;, &quot;Absent&quot;), Cancer = c(&quot;Present&quot;, &quot;Absent&quot;) ) ) x ## Cancer ## Marker Present Absent ## Present 14 6 ## Absent 16 64 prop.test(x) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: x ## X-squared = 16.741, df = 1, p-value = 4.284e-05 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.2496194 0.7503806 ## sample estimates: ## prop 1 prop 2 ## 0.7 0.2 chisq.test(x) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x ## X-squared = 16.741, df = 1, p-value = 4.284e-05 chisq.test(x, correct=F) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 19.048, df = 1, p-value = 1.275e-05 First, note that the prop.test is just the chisq.test. You get the same \\(\\chi^2\\) value and p-value for each. They just differ in parameter output and input options. If you’d like a confidence interval (you should), use prop.test. Second, note how turning off the Yates continuity correction changes the \\(\\chi^2\\) value and the p-value. That’s to be expected, it changes the calculation! Both the prop.test and chisq.test use Yates by default. The best way to think about Yate’s is that it acts as a smoothing function to take off some of the jigger in the calculation of the \\(\\chi^2\\) value. 15.7.3 Interpretation of chi-square output There is an association between the presence of this marker and the probability that cancer occurs. We could take the prop test’s calculation of the proportions and their difference, along with the 95% CI of their difference and make some hay out of that (the probability of getting cancer with the marker is 70%, and without the marker is 20%). But it’s more customary to use the odds ratio or relative risk rather than differences between probabilities to make effect size assertions. 15.7.4 Write Up You would want to derive the odds ratio and its 95% CI, even though the \\(\\chi^2\\) test doesn’t produce it for you. The easiest way to do that is with fisher.test. Having that: The large OR indicates the presence of this marker is strongly associated with cancer (n=100, OR = 9.06, 95% CI = 2.76 to 33.68, Pearson’s Chi-square test with Yate’s continuity correction, p = 4.284e-05). As before, the word “strongly” is used to emphasize the effect size, which is the OR, rather than the extremeness of the p-value. 15.7.5 Which contingency test is best? With so many options, the question that always arises is which is best to use for contingency analysis? The answer is, * make this decision in advance and use what you said you would use before you started the experiment. * when it comes to p-values, are you an exactophile or an exactophobe? * for datasets with low cell numbers (eg, counts less than 5 in a cell), exact tests tend to provide more accurate p-values. * the fact that fisher.test generates the OR and its CI is very, very convenient. I prefer the fisher.exact test. However, in R you’ll need to understand how to configure its arguments to get it to work on higher dimension contingency tables (eg, 2x3, 3x2, 3x3, etc). 15.7.6 Higher dimension contingency analysis Not infrequently we have studies with many levels of a predictor variable and two outcomes (eg, 7x2), or two predictors and more than two outcomes (eg, 2x3). For these, you’ll find the chisq.test works fairly automagically. In contrast, you’ll need to customize arguments in fisher.test for it to pass in anything other than a 2x2 matrix. Furthermore, with dimensions higher than 2x2 there is more than a single odds ratio or relative risk or likelihood ratio in higher dimensions to be computed. A step-wise approach for these more complex analyses is to first run an omnibus chisq.test on the intact dimension. If the test is positive (low p-value), analyze 2x2 segments of the grid post hoc using the fisher.test to derive odds ratios and to see which of the proportion ratio’s explain the significance. Such post hoc analyses must include correction for multiple comparisons (eg, the Bonferroni correction) when drawing inference. To do this, pass a vector of p-values into the p.adjust function. 15.7.7 Other experimental designs involving categorical data Imagine an experiment to compare two or more conditions (eg, placebo v drug, wildtype vs mutant) and the outcome variable is discrete (eg, frequency counts or success and failure counts). The experiment involves several independent replications. For example, cell depolarizations are counted both in the absence or presence of a condition on several different occassions. Table 15.1: Hypothetical replicates comparing depolarizations in mutant and wildtype cells should be analyzed using poisson regression. replicate predictor counts one wildtype 87 two wildtype 102 three wildtype 105 one mutant 125 two mutant 126 three mutant 139 Alternately, the fraction of cells in a culture dish that have died in the absence or presence of a condition is repeated a handful of times. Table 15.2: Hypothetical replicates comparing in mutant and wildtype cells should be analyzed using poisson regression. replicate predictor alive dead one wildtype 30 67 two wildtype 33 73 three wildtype 37 76 one mutant 65 38 two mutant 56 36 three mutant 62 42 The key distinction here, compared to what’s been discussed in this chapter up to this point, is that within each replicate, all of the events are intrinsically-linked. Through replication we’re establishing whether the events are repeatable. Logistic (for dichotomous data) or poisson (for frequency data) regression are the appropriate analytical tools for these designs. These involve using the generalized linear model, conducted with the function glm or the function glmer (for so-called mixed models). These are discussed in the logistic regression chapter. 15.8 Doing a priori power analysis for proportion tests Power analysis should be done before starting an experiment. The purpose of a conducting power calculations a priori is to determine the number of trials, or subjects or sample size, to use for the study. This is a two step process. Step 1: Using scientific judgement, decide what is the value of a null proportion and an alternate that you think would be a scientifically meaningful proportion to observe. You need to have some insight into the system you’re studying to make these calls. What’s important is to establish an expectation of what a minimally scientifically significant outcome would look like. Step 2: Calculate the number of subjects (or trials) you’ll need to study, given these proportions (and also given some type1 and type2 error tolerances). There are several options in R for the second step. In the examples below, we’re declaring a 5% difference between the null (0.15) and alternate (0.20) proportions would be a scientifically meaningful. We’re also using 5% for type1 error and 20% for type2 error (80% power) as tolerance thresholds. 15.9 Power analysis functions for proportion tests The function pwr.p.test is for one-sample proportion tests. The calculations below return a sample size n that should be used in the study, given a null and an alternate proportions, in addition to error rates. NB: Since takes a Cohen’s effect size as an argument, you 1st must calculate a Cohen effect size, h, given the alternate and null proportions you expect, using Es.h(). Then plug that effect size into the power calculator. h &lt;- ES.h(0.2, 0.15) h ## [1] 0.1318964 pwr.p.test( h, sig.level=0.05, power=0.8, alternative=&quot;two.sided&quot; ) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.1318964 ## n = 451.1706 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided binom.power is a function from the binom package. Instead of returning sample size, this function returns power, given sample size. You iterate through (by hand) entering sample sizes until it returns an acceptable power. Then run the experiment at that sample size. Note that it doesn’t give exactly the same result as pwr.p.test. The calculation differs, but the result is close. binom.power( 0.2, n=451, p=0.15, alpha=0.05, alternative = &quot;two.sided&quot;, method=&quot;exact&quot; ) ## [1] 0.7908632 To estimate sample size needed for a two-sample proportion test design, use the power.prop.test function. power.prop.test( p1=0.15, p2=0.2, sig.level = 0.05, power=0.8 ) ## ## Two-sample comparison of proportions power calculation ## ## n = 905.3658 ## p1 = 0.15 ## p2 = 0.2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Finally, the statmod package has the power.fisher.test, which returns the power for a Fisher’s exact test, given arguments of proportion, trial size and type1 error. Note how it is in close but not exact agreement with power.prop.test. power.fisher.test( 0.15, 0.2, 905, 905, 0.05, alternative = &quot;two.sided&quot; ) ## [1] 0.81 15.9.0.1 Monte Carlo power simulations Monte Carlo’s are very simple. The basic gist is to simulate and test a very large number of experiments. Each of these experiments is comprised of a random sample of some size, corresponding to your minimal effect size you define as scientifically meritorious. These are run through the test of significance, to calculate a p-value. The fraction of simulations that are “hits”–for example, that have p-values &lt; 0.05, is the power! Simulations are re-run by adjusting the sample size until a desired power is achieved. That’s the sample size you’d use in a real experiment! The question the script addresses is this: What is the power of an experiment, given this trial size n, the null and alternate proportions evaluated, and the type1 error threshold? If n is too low, the test will return a power below 0.8 meaning it is not adequetely powered to test the difference between the null and alternate proportions. Iterate through some a few sample sizes (n) until you arrive at an acceptable value for power. 15.9.0.1.1 monte carlo simulation for prop.test power #these are the initializers #number of experiments to simulate, each of trial size n sims &lt;- 1000 #expected null proportion null &lt;- 0.15 #expected minimal effect proportion alternate &lt;- 0.20 #binomial trial size, just a guess n &lt;- 450 #type 1 error threshold alpha &lt;- 0.05 #s1 is a random sample vector #each value is the number of #successes observed in a trial #of size n, given the alternate proportion. #it simulates the outcome of one experiment &quot;sims&quot; times s1 &lt;- rbinom(sims, n, alternate) #t1 is a vector of p-values, #derived from a one sample proportion test #on each of the values in s1. #read from inside the function to see the logic t1 &lt;- unlist( lapply( s1, function(s1){ prop.test( s1, n, null, alternative=&quot;two.sided&quot;, conf.level=1-alpha, correct=T)$p.value } ) ) power &lt;- length(which(t1 &lt; alpha))/sims power ## [1] 0.789 15.10 Graphing Proportions (needs improvement…add mosaic plots) Here’s a few ggplot2-based ways of visualizing proportion data. First thing is to create a dataframe of the proportion data since data fed into ggplot2 must be in dataframe format. prop.df &lt;- data.frame( group=c(&quot;positive&quot;, &quot;negative&quot;), value=c(pos, neg) ) prop.df ## group value ## 1 positive 5042 ## 2 negative 18492 15.10.0.0.1 Simple stacked bar chart ggplot( prop.df, (aes(x=&quot;&quot;, y=value, fill=group) ) ) + geom_bar(stat=&quot;identity&quot;) 15.10.0.0.2 Side-by-side bar chart Note: There is no error to report. There’s no variation. Cells were classified as either having or not having the antigen. ggplot( prop.df, (aes( x=group, y=value, fill=group) ) ) + geom_bar(stat=&quot;identity&quot;) .. "],
["chisquare.html", "Chapter 16 The Chi-square Distribution 16.1 Background 16.2 dchisq 16.3 pchisq 16.4 qchisq 16.5 rchisq", " Chapter 16 The Chi-square Distribution library(tidyverse) 16.1 Background The chi-square permeates biostatistics in several important ways. For example, the \\(\\chi^2\\) distribution is used to generate p-values for Pearson’s chi-square test statistic, \\[\\chi^2=\\sum_{i=1}^{n}\\frac{(O_i-E_i)^2}{E_i}\\] which is used in goodness-of-fit and independence tests. The \\(\\chi^2\\) distribution is used to generate p-values for tests of homogeneity and also to calculate the confidence intervals of standard deviations. A good way to think of the chi-square distribution more generally is as a probability model for the sums of squared variables. As such, the \\(\\chi^2\\) test statistic only takes on positive values. If \\[X_1,..,X_k\\] is a set of standard normal variables, then the sum of their squared values is a positive, random variable \\(Q\\), \\[Q=\\sum_{i=1}^{k}{X^2_i}\\] that will take on a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom. The mean of any \\(\\chi^2\\) distribution is \\(k\\) while it’s variance is \\(2k\\). The distributions are skewed (mode = \\(k-2\\)) except that those having very large degrees of freedom, which are approximately normal. 16.2 dchisq dchisq is the \\(\\chi^2\\) probability density function in R. The simplest way to think of dchisq is as the function that gives you the probability distribution of the \\(\\chi^2\\) test statistic. The \\(\\chi^2\\) probability density function is: \\[p(x)=\\frac{e^\\frac{-x}{2}x^{\\frac{k}{2}-1}}{2^\\frac{k}{2}\\Gamma(\\frac{k}{2})}\\] Given a chi-square value and the degrees of freedom of the dataset as input, dchisq returns the probability for a given chi-square value. For example, if a 2X2 test of independence (df=1) returns a \\(\\chi^2\\) value of 4, the probability of obtaining that value (its “density”) is equal to 0.02699: dchisq(x=4, df=1) ## [1] 0.02699548 For continuous distributions like the \\(\\chi^2\\) such point density values are usually not particularly useful. In contrast, inspecting how the function behaves over a range of \\(\\chi^2\\) values and at different df’s is informative. That’s plotted below: df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- dchisq(x, i) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;p(chi-square)&quot;) 16.3 pchisq pchisq is the \\(\\chi^2\\) cumulative distribution function in R. The simplest way to think about the pchisq function is as the probabilities under the \\(\\chi^2\\) curve. The \\(\\chi^2\\) cumulative distribution function is: \\[p(x)=\\frac{\\gamma(\\frac{k}{2}\\times\\frac{x}{2})}{\\Gamma(\\frac{k}{2})}\\] It returns the cumulative probability for an area under the curve up to a given \\(\\chi^2\\) value. For example, the probability that a \\(\\chi^2\\) value with 1 degree of freedom is less than 4 is 0.9544997: pchisq(q=4, df=1, lower.tail = T) ## [1] 0.9544997 df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- pchisq(x, i, lower.tail = T) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;cumulative p(chi-square)&quot;) 16.3.1 Calculating p-values from pchisq The pchisq function is used to calculate a p-value. A p-value is a probability that a \\(\\chi^2\\) value, x, is as large or larger. \\[p-value = P(\\chi^2 \\ge x)\\] This can be calculated using the pchisq function simply by changing lower.tail argument in the function tolower.tail = F pchisq(q=4, df=1, lower.tail = F) ## [1] 0.04550026 Here p-values for the \\(\\chi^2\\) at various df’s: df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- pchisq(x, i, lower.tail = F) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;p-value&quot;) 16.4 qchisq qchisq is the inverse of the \\(\\chi^2\\) cumulative distribution function. This function takes a probability value as an argument, along with degrees of feedom, and returns a \\(\\chi^2\\) value corresponding to that probability. Here’s the \\(\\chi^2\\) value corresponding to the 95th percentile of a distribution with 3 degrees of freedom: qchisq(0.95, 3) ## [1] 7.814728 Here is what the quantile \\(\\chi^2\\) distribution looks like for several different df’s: df = 9 p = seq(0.01, .99, 0.01) xpd &lt;- matrix(ncol=df, nrow=length(p)) for(i in 1:df){ xpd[,i] &lt;- qchisq(p, i) } xpd &lt;- data.frame(xpd) colnames(xpd) &lt;- c(1:df) xpd &lt;- cbind(p, xpd) xpd &lt;- gather(xpd, df, qchisq, -p) ggplot(xpd, aes(p, qchisq, color=df)) + geom_line() + xlab(&quot;probability&quot;)+ylab(&quot;chi-square&quot;) 16.5 rchisq R’s rchisq function is used to generate random values corresponding to \\(\\chi^2\\)-distributed data. This has utility in simulating data sets with skewed values, for example, to mimic overdispersed Poisson distributions. Here are 10 random values from a \\(\\chi^2\\) distribution with 3 degrees of freedom: rchisq(10, 3) ## [1] 1.7793487 1.6128441 1.9412011 8.1186947 2.5548657 2.8908581 4.7170045 ## [8] 5.9611732 3.7333294 0.7858192 "],
["nonparametrics.html", "Chapter 17 Nonparametric Statistical Tests 17.1 Experiments involving discrete data 17.2 Deviant Data 17.3 Sign Test 17.4 Wilcoxon Sign Rank Test for One Group 17.5 Wilcoxon Mann Whitney Rank Sum Test for 2 independent groups 17.6 Wilcoxon Sign Rank Test for paired groups 17.7 Kruskal-Wallis 17.8 Friedman test 17.9 Nonparametric power calculations 17.10 Summary", " Chapter 17 Nonparametric Statistical Tests Non-parametric statistical tests are versatile with respect to the dependent variables they tolerate. They are typically applied to data sets involving ordered data. One of the nonparametric tests, the sign test, is used to assess simple proportions. They can also be for data on measured, equal interval scales, for which the normality and equal variance assumptions of parametric statistical testing are not satisfied or cannot be assumed. Nonparametric statistics are parameter-less. They don’t compare means, or medians. Though people frequently treat nonparametrics as tests of medians, that is not strictly true. They don’t involve least squares calculations, or standard deviations, or variance, or SEMs. They do compare distributions of data. This happens by transforming the data into a standardized measure of ranks–either signs, sign ranks or rank sums. The tests, essentially, evaluate whether the distribution of ranks in an experimental outcome differs from a null distribution of ranks, given a sample size. That can seem pretty abstract. But it’s actually a simple and elegant way to think about these tests. With the exception of the Sign Test, which has a probability as an effect size, strictly speaking there really isn’t an effect size that describes non-parametric outcomes other than the value of the test statistic. However, it is possible to use confidence interval arguments in R’s tests to coerce them into providing effect size output as estimates of medians. This can be sometimes useful. Non-parametric analogs exist for each of the major parametric statistical tests (t-tests and one-way anova and two-way. Which analog to use for a given data set analysis depends entirely upon the experimental design. Sign Test -&gt; analog to the binomial Test -&gt; when events are categorized as either successes or failures. Wilcoxon Sign Rank Test for one group -&gt; analog to the one sample t-test -&gt; compare a one group data set to a standard value. Mann Whitney Rank Sum Test for 2 independent groups -&gt; analog to the unpaired t test -&gt; for comparing two groups in a data set. Wilcoxon Sign Rank Test for paired groups -&gt; analog to the paired t-test -&gt; comparing a group of paired outcomes in a data set to no effect null. Kruskal-Wallis Test -&gt; analog to one way completely randomized ANOVA -&gt; comparing 3 or more groups Friedman Test -&gt; analog to one way related measures ANOVA -&gt; comparing 3 or more groups. In R, the wilcox.testfunction is a work horse for non-parametric analysis. By simply changing the function’s arguments it can do either a WSRT, or MW, or a WSRT for paired groups analysis. 17.1 Experiments involving discrete data Discrete data can be either sorted or ordered. Discrete data arises from counting objects or events. They also occur when the meaurements taken from the experimental units are assigned discrete values. Counted objects are easy to spot—they are indivisible. They belong in one bucket or some other bucket(s). Dependent variables that have discrete values are also easy to spot. On scatter plots they exist as discrete rows. There is no continuum of values between the rows. When planning an experiment ask whether the data will be sorted into categories on the basis of nominal characteristics (eg, dead vs alive, in vs out). Or will the data be categorized on some ordered basis. For example, a score of 1 = the attribute, a score of 2 = more of the attribute, a score of 3= even more of the attribute, …and so on. The discrete counts within one category of an ordered scale mean that they have more or less of some feature than do the counts in another category in the ordered group. Thus, compared to nominal data, ordered data have more information. Whereas nominal events are just sorted into one bucket or another, ordered events are inherently categorized by rank. Ordered data are common in survey instruments and polling. Certain experimental designs generate inherently ordered data as well. For example, imagine a test that scores dermal inflammatory responses. Given a subject, * Score 1 if we don’t see any signs of inflammation. * Score 2 if there was a faint red spot. * Score 3 for a raised pustule. * Score 4 for a large swollen area that feels hot to the touch. * Score 5 for anything worse than that, if it is possible! Using that ordered scale system, we’d run experiments, for example, to compare a steroid treatment that might reduce inflammation compared to a vehicle control. Or we’d look at a gene knockout, or CRISP-R fix, or whatever, and score an outcome response compared to a control group. After an evaluation by a trained observer, each experimental unit receives a score from the scale. In quantifying effect sizes for such studies, a mistake you often see is parametric analysis. The researcher uses parameters such as means, standard deviations, performs t-tests, and so forth on the score rank values. This isn’t always bad, but it assumes a couple of things. First, that the distribution of the data is approximately normal, as is the population that was sampled. Second, the scoring scale is equal interval. That is to say, “the difference between inflammation scores of 1 and 2 is the same as the difference between scores 2 and 3, and so on…”. Suffice to say that researchers should validate whether these assumptions are true before resorting to parametric tests. Or they can just use nonparametric tests and save themselves from all that validation work! It happens the other way, too. Sometimes we take measurements of some variable on a perfectly good measurement scale, one that satisfies these assumptions, but then break the data out to some ordered scale. Take blood pressure, for example, which is a continuous variable, usually in standardized units of mmHg. We might measure it’s value for each subject, but on the basis of that measurement sort the subjects into ordered categories of low, medium and high. Our scientific expertise drives what blood pressure values match those categories. And we should have good reasons to resort to a categorization because doing so tends to throw away perfect good scalar information. It is on this ordered scale, of discrete events, rather than the original measurements on a continuous scale, that we might then run statistical tests. My point for this latter example is, of course, that not all ordered scales are based upon subjective assessments. 17.2 Deviant Data Any scale, whether discrete or continuous, can yield deviant data. What I mean by deviant data is non-normal, skewed, has unequal variances among groups, has outliers, and is just plain ugly. When data are deviant there are two options: Use reciprocal or log transform functions to transform the data distribution into something more normal-like. Run the statistical tests intended for normal data on the transformed values. Run non-parametric statistical tests on the raw, untransformed data. These tests transform the data into a rank-based distribution. These (the sign rank and the rank sum distributions), are discrete normal-like, and can be coerced to cough up p-values. Tossing outliers is almost always a bad and unnecessary option. Outlier tossing introduces bias! Because they are based on ranks, the nonparametric tests condense outliers back with the rest of the variables, providing a very slick way to deal with deviant data. 17.3 Sign Test The Sign Test is a non-parametric way of saying a binomial test. An experiment is conducted on a group of subjects, who are graded in some way for either passing (+) or failing (- ) some test. Did a cell depolarize, or not? Is a stain in the cell nucleus, or not? Did the animal move fast enough, or not? Did the subject meet some other threshold you’ve established as a success, or not? Simply count the number that passed. Given them a “+” sign. The number that failed receive a “-” sign. Using scientific judgement, assume a probability for the frequency of successes under the null hypothesis. For example, the null might be to expect 50% successes. If after analyzing the data the proportion of successes differs from this null proportion, you may have a winner! Here’s an analysis of a behavioral test, the latency to exit a dark chamber into a brief field, as an index of anxiety. Let’s say that exiting a chamber in less than 60 seconds is a threshold for what we’d consider “non-anxious” behavior. Scientific judgement sets that threshold value. Fifteen subjects are given an anti-anxiety drug. The null probability of exiting the chamber is 0.5. Which is to say there is a 50/50 chance a mouse will, at random, exit the chamber at any given time before or after 60 sec. Or put another way, under the null, neither exiting nor remaining in the chamber by 60 seconds is favored. Let’s imagine we have an alarm set to go off 60 seconds after placing the subject in the chamber. When the alarm sounds, we score the subject as either (+) or (-). The results are that twelve exited the chamber in less than 60 seconds, and 5 did not. We have not recorded times. Scientifically, we predict that experimental units on an anti-anxiety drug are more likely to exit before this mark. This experiment tests the null hypothesis that the probability of successes are less than or equal to 50%. If something is not less than or equal to another, it can only be greater. Thus, we choose the “greater” for the alternative hypothesis argument in the binomial test function. We think on an anti-anxiety drug the probability is greater that the subjects will successfully exit the chamber! binom.test(x=12, n=15, p=0.5, alternative =&quot;greater&quot;, conf.level=0.95 ) ## ## Exact binomial test ## ## data: 12 and 15 ## number of successes = 12, number of trials = 15, p-value = 0.01758 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.5602156 1.0000000 ## sample estimates: ## probability of success ## 0.8 scoreci(x=12, n=15, conf.level = 0.95) ## ## ## ## data: ## ## 95 percent confidence interval: ## 0.5481 0.9295 17.3.1 Interpretation The effect size is 0.8, which represents the fraction of subjects that left the chamber prior to the 60 second threshold we set. The p-value is the probability of observing an effect size this large, if the null hypothesis is actually true. There is a 95% chance the true effect size is within the range of 0.56 to 1. To get a clear sense of what’s going on, here is the distribution of the binomial function for the null hypothesis. # I&#39;ll use the rbinom function to simulate data &lt;- data.frame(x = c(0:15),y=dbinom(c(0:15), 15, prob=0.5)) ggplot(data, aes(x, y))+ geom_col(fill=&quot;blue&quot;) + xlab(&quot;exits before 60s&quot;) + ylab(&quot;prob of that many exits&quot;) + geom_text(aes(x=1, y=.20, label=&quot;H0 distribution&quot;)) And here is the distribution for the alternate hypothesis: data &lt;- data.frame(x = c(0:15),y=dbinom(c(0:15), 15, prob=0.8)) ggplot(data, aes(x, y))+ geom_col(fill=&quot;green&quot;) + xlab(&quot;exits before 60s&quot;) + ylab(&quot;prob of that many exits&quot;) + geom_text(aes(x=1, y=.20, label=&quot;H1 distribution&quot;)) This is to emphasize that the binomial distribution is used here as a model of the experimental effect. Thus, we might also conclude that our data is consistent with a binomial distribution of 15 trials wherein the probability of event success is 80%. In effect, our p-value allows us to conclude this alternate distribution is a better model for the population than is the null distribution. This is subject to a 1.758% chance that this might be a false positive conclusion. Use your scientific judgement to decide whether that’s an acceptable risk of being wrong. This also is a way to visualize the confidence interval, which says we should expect more than 8 successes 95% of the time…an assertion that covers all but two of the lower bins in this distribution! 17.3.2 Write Up Drug treatment increases fearlessness (one-sided binomial test, p = 0.01759). The fraction exiting the chamber (0.8) is greater than expected for the null of 0.5 (95% CI = 0.55 to 1.0, Wilson’s CI) 17.4 Wilcoxon Sign Rank Test for One Group The test statistic for the Wilcoxon Sign Rank is determined as follows. 1. Calculate the difference between the theoretical median or threshold value and the values recorded for each independent replicate. 2. Rank those differences from lowest (rank = 1) to highest (rank = n). 3. Assign a negative value to the replicate values that are less than the median. 4. The test statistic V is the sum of the positive values. (software other than wilcox.test in R may calculate W, the sum of the positive and negative values). The test statistic V has an approximately normal discrete distribution, whose cumulative function psignrank can be used to compute p-values. 17.4.1 Wilcoxon Sign Rank Experimental Designs This experimental design is similar to the Sign Rank test except in one important detail: We actually measure the time it takes for the subjects to exit the chamber. No alarm sounds to end the game at 60 sec. If subjects dawdle about and take longer than 60 sec to exit, we wait and record that time! Thus, because the data set is comprised of the actual values for the latency variable, rather than counts of a simple (+) or (-) score, the Wilcoxon Sign Rank design collects more information than does the Sign Rank Test. Let’s say we have a chamber test on 7 subjects who’ve all been given an anti-anxiety drug. After placement in the chamber, their exit times (in seconds) are 3, 5, 8, 15, 19, 21 and 108. Based upon scientific judgement, we think exiting sooner than 60 would represent fearlessness (less anxiety). This test ranks each subject’s performance relative to that reference time and then “signs” it as negative or positive based on whether it’s original value was below or above the 60 second threshold. In our data, only one subject exceeded that value…108 sec. Our prediction is that less anxious subjects should exit the comfort of the dark chamber sooner than would be expected. The null hypothesis is that the “location”&quot; of the null distribution is greater than or equal to 60 seconds. The alternate is the location is “less” than 60 seconds, since less is everything that greater than or equal to cannot be. We run the Wilcoxon Sign Rank test to test this hypothesis using the arguments below. wilcox.test(x=c(3,5,8,15,19,21,108), mu=60, alternative = &quot;less&quot;, conf.level = 0.95, conf.int = 0.95) ## ## Wilcoxon signed rank test ## ## data: c(3, 5, 8, 15, 19, 21, 108) ## V = 4, p-value = 0.05469 ## alternative hypothesis: true location is less than 60 ## 95 percent confidence interval: ## -Inf 61.5 ## sample estimates: ## (pseudo)median ## 14 17.4.2 Interpretation The value of the test statistic, V is four. How extreme is that? It is pretty far to the left on the test statistic distribution (see below) for this sample size. The p-value is above the threshold of 5%. The evidence is not enough to reject the null hypothesis. Otherwise, the probability of making an error doing so would be 0.05469. That V = 4 means it is the value corresponding to the sum of the positively signed ranks in the sample. The pseudo-median of the latency time is 14 seconds. The one-sided 95% confidence ranges from -infinity to 61.5. Here’s a null signrank distribution for a sample size of 7. The values of the x scale are V, the test statistic. These are all the possible values that V can take on, given the sample size. For example, if all the signed ranks were positive…if every subject took longer than 60 sec to exit)…then V would equal 28. If all subjects exited before 60 sec, then V would equal zero. Which is to say the location of this distribution is, by coincidence, also centered on 14. The value of 4 is less than this location, but not extremely-enough lower to be considered as belonging to some other distribution with a different location! The 95% confidence interval of the location on the V test statistic ranges from -infinity to 62.5. psignrank(q=4, n=7) ## [1] 0.0546875 psignrank(q=1, n=7) ## [1] 0.015625 upper &lt;- 28 n &lt;- 7 df &lt;- data.frame(x=0:upper, y=dsignrank(0:upper, n)) ggplot(df, (aes(x,y)))+ geom_col() + xlab(&quot;V&quot;) + scale_x_continuous(breaks=(seq(0,upper,1))) 17.4.3 Write Up Analysis of the chamber test results indicates the anti-anxiety drug has no effect (Wilcoxon Signed Rank test, V = 4, n = 7, p= 0.0547) 17.5 Wilcoxon Mann Whitney Rank Sum Test for 2 independent groups This nonparametric test, often referred to simply as the Mann-Whitney test, is analogous to the parametric unpaired t-test. It is for comparing two groups that receive either of 2 levels of a predictor variable. For example, in an experiment where one group of m independent replicates is exposed to some control or null condition, while a second group with n independent replicates is exposed to some treatment. More generally, the two groups represent two levels of a predictor variable given to m+nindependent replicates. The rank sum is calculated as follows: The data are collected from any scale, combined into a single list, whose values are ranked from lowest (rank 1) to highest (rank m+n), irrespective of the level of the predictor variable. Let \\(R_1\\) represent the sum of the ranks for the one level of the predictor variable (eg, group2). Let \\(U_1\\) represent the number of times a data value from group2 is less than a data point from group1. \\(U_1=m*n+\\frac{m(m+1)}{2}-R_1\\) And \\(U_2=m*n-U_1\\) The rank sum test computes two test statistics, \\(U_1\\) and \\(U_2\\) that are complementary to each other. Here’s another in the line of the mighty mouse experiments. 55 independent subjects were split into two groups. One group received an anti-anxiety drug and the second a vehicle as control. The subjects were run through the dark chamber test. The scientific prediction is the drug will reduce anxiety levels and so the drug treated mice will exit the chamber more quickly compared to the control mice. Since this is a parameter-less test, the null hypothesis is that location of the distribution of the drug-treated population is greater than or equal to the location of the vehicle distribution. The alternative hypothesis is that the location of the distribution of the drug-treated population is less than that of the vehicle distribution. The alternative is consistent with our scientific prediction and represents an outcome that is exclusive and comprehensive of the null! We choose the “less” option for the alternative argument in the test. mightymouse &lt;- read.csv(&quot;datasets/mightymouse.csv&quot;) wilcox.test(Time ~ Group, data = mightymouse, alternative =&quot;less&quot;, conf.level=0.95, conf.int=T) ## ## Wilcoxon rank sum test ## ## data: Time by Group ## W = 55, p-value = 0.1804 ## alternative hypothesis: true location shift is less than 0 ## 95 percent confidence interval: ## -Inf 6 ## sample estimates: ## difference in location ## -9.8 17.5.1 Interpretation The test statistic you see in the output, W, warrants some discussion. W is equal to \\(U_2\\) as defined above. By default, R produces \\(U_2\\) (labeled W!) as the test statistic. Most other software packages use \\(U_1\\), which in this case would be 88 (easy to compute in the console given \\(U_2\\)). Think of W as a value on the x axis of a rank sum distribution for a sample size of m+n. The rank sum distribution has a function in R called dwilcox. Here it is (note the large value this distribution can take on is m*n and the smallest is zero): df &lt;- data.frame(x=0:143, y=dwilcox(0:143, 11, 13)) ggplot(df, (aes(x,y)))+ geom_col()+ xlab(&quot;W&quot;)+ scale_x_continuous(breaks=c(55, 88)) All this can seem confusing, but it is very elegant. First, the rank sums of samples, like the rank signs of samples, take on symmetrical, normal-like distributions. The greater the sample sizes, the more normal-like they become. Second, the bottom line is the same as for all other statistical tests: test statistic values at either extreme of these null distributions are associated with large effect sizes. The non-extreme-ness of the test statistic value for our sample is illustrated in that plot. Clearly, W=55, it is well within the null distribution. I calculated it’s symmetrical counterpart, \\(U_1\\) = 88, from the relationship above. As you can see, the value of the test statistic and 88 frame the central location of this null ranksum distribution null quite nicely: The p-value for W=55 indicates that the probability of creating a false positive by rejecting the null is 18.04%, well above the 5% type1 error threshold. So we should not reject the null given we’d have a 1 in 5 chance of being wrong if we did! The “effect size” is in the output is the magnitude of the difference between the location parameters (pseudo-medians) of the two groups, on the scale of the original data. The 95% confidence interval indicates there is a 95% chance the difference in locations is between negative infinity and 6. Since the 95% confidence interval includes zero, the possibility exists that there is zero difference between the two locations. That provides additional statistical reasoning not to reject the null. 17.5.2 Write Up There is no difference in performance using the closed chamber test between subjects randomized to anti-anxiety drug (n=11) or to vehicle (n=13) (Mann-Whitney test, W = 55, p = 0.1804). 17.6 Wilcoxon Sign Rank Test for paired groups The classic paired experimental design happens when two measurements are taken from a single independent subject. For example, we take a mouse, give it a sham treatment, and measure it’s latency in the chamber test. Later on we take the same mouse, give it an anti-anxiety drug treatment, and then measure its latency once again. This kind of design can control for confounding factors, like inter-subject variability. But it can also introduce other confounds. For example, what if the mouse “remembers” that there is no real risk of leaving the dark chamber? Pairing can happen in many other ways. A classic pairing paradigm is the use of identical twins. Individuals of inbred mouse strains are all immortal clones. Two litter mates would be identical twins and would also be, essentially, clones of their parents and their brothers and sisters from prior litters! Two dishes of cultured cells, passed together and now side-by-side on a bench are intrinsically-linked. All of these can be treated, statistically, as pairs. In this example, we take a pair of mice from each of 6 independent litters produced by mating two heterozygotes of a nogo receptor knockout. One of the pair is nogo(-/-). The other is nogo(+/+). We think the nogo receptor causes the animals to be fearful, and predict animals in which the receptor is knocked out will be more fearless. The independent experimental unit in this design is a pair. We have six pairs, Therefore, the sample size is 6 (even though 12 animals will be used!) We’ll measure latency in the dark chamber test. Our random variable will be the difference in latency time between the knockout and the wild type, for each pair. Here’s the data, latency times are in sec units: mmko &lt;- data.frame(knockout=c(19, 24, 4, 22, 15, 18), wildtype=c(99, 81, 70, 62, 120, 55)) #create a long data fram to do formula arguments in wilcox test mmkotidy &lt;- gather(mmko, genotype, latency ) Scientifically, we predict there will be a difference in latency times within the pairs. Specifically, the knockout will have lower times than their paired wild-type. The null hypothesis is that the difference within pairs will be greater than or equal to zero. The alternative hypothesis is the difference will be less than zero. wilcox.test(latency ~ genotype, data=mmkotidy, paired=T, conf.level=0.95, conf.int=T, alternative=&quot;less&quot;) ## ## Wilcoxon signed rank test ## ## data: latency by genotype ## V = 0, p-value = 0.01563 ## alternative hypothesis: true location shift is less than 0 ## 95 percent confidence interval: ## -Inf -40 ## sample estimates: ## (pseudo)median ## -61.5 17.6.1 Interpretation Note that this is not a rank sum test as for the Mann-Whitney, but a signed rank test. So we have seen the V test statistic before. It’s value of 0 is as extreme as can be had on the null distribution, as is evident in the distribution below! That happened because in each of the six pairs, the knockout had a lower latency time than its paired wildtype. All of the signed ranks were negative! In terms of position differences, it is as strong of an effect size as possible. upper &lt;- 21 n &lt;- 6 df &lt;- data.frame(x=0:upper, y=dsignrank(0:upper, n)) ggplot(df, (aes(x,y)))+ geom_col() + xlab(&quot;V&quot;) + scale_x_continuous(breaks=(seq(0,upper,1))) The p-value is exact…and it can never be lower, given this sample size. We can reject the null since it is below our 5% threshold and it says the probably that we are accepting a type1 error is 1.563%. The pseudo-median is in units of latency time. It represents the median for the differences in latency within the pairs. In other words, there are six values of differences, one difference value for each pair. -61.5 is the median of those 6 differences. There is a 95% chance the true median of the differences lies between negative infinity and -40. Note that the 95% CI does not include the value of zero. 17.6.2 Write up Dark chamber test latency differs markedly within pairs of knockout and wildtype subjects (Wilcoxon Signed Rank Test for pairs, n=6, V = 0, p=0.01563) 17.7 Kruskal-Wallis The kruskal.test is a non-parametric method for comparing 3 or more treatment groups. It serves as an omnibus test for the null hypothesis that each of the treatment groups belong to the same population. If the null is rejected, post hoc comparison tests are then used to determine which groups differ from each other. A post hoc test for this purpose in base R is pairwise.wilcox.test. The PMCMRplus package has others. Documentation within the PMCMRpackage vignette provides excellent background and instructions for these tests. The Kruskal-Wallis test statistic is computed as follows. Values of the outcome variables across the groups are first converted into ranks, from high to low. Tied values are rank-averaged. The test can be corrected for large numbers of tied values. The Kruskal-Wallis rank sum test statistic is: \\[H=\\frac{12}{n(n+1)}\\sum_{i=1}^k\\frac{R_{i}^2}{n_i}-3(n+1)\\] \\(n\\) is the total sample size, \\(k\\) is the number of treatment groups, \\(n_i\\) is the sample size in the \\(ith\\) group and \\(R_i^2\\) is the squared rank sum of the \\(ith\\) group. Under the null, \\(\\bar{R_i} = (n+1)/2\\). The H statistic is approximated using the \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom to produce p-values. Let’s analyze the InsectSprays data set, it comes with the PMCMRplus package. This is a multifactorial experiment in which insects were counted in agricultural field plots that had been sprayed with 1 of 6 different insecticides. Each row in the data set represents an independent field plot. Do the insecticides differ? ggplot(InsectSprays, aes(spray, count))+ geom_violin() The violin plots (modern day versions of box plots) illustrate how the groups have unequal variance. Such data are appropriate for non-parametric analysis. #insectsprays &lt;- read.csv(&quot;insectsprays.csv&quot;) kruskal.test(count ~ spray, data=InsectSprays) ## ## Kruskal-Wallis rank sum test ## ## data: count by spray ## Kruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10 data(insectsprays) pairwise.wilcox.test(InsectSprays$count, InsectSprays$spray, p.adjust.method=&quot;bonferroni&quot;, alternative =&quot;two.sided&quot;) ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: InsectSprays$count and InsectSprays$spray ## ## A B C D E ## B 1.00000 - - - - ## C 0.00058 0.00058 - - - ## D 0.00117 0.00104 0.03977 - - ## E 0.00051 0.00051 0.78860 1.00000 - ## F 1.00000 1.00000 0.00052 0.00105 0.00052 ## ## P value adjustment method: bonferroni We first do a Kruskal-Wallis rank sum omnibus test to test the null hypothesis that the locations of the groups within the data set are the same. The null is rejected given the large \\(\\chi^2\\) test statistic, which has a p-value well below the threshold. That’s followed by a pairwise Wilcoxon rank sum test…think of it as running a Mann-Whitney test on all possible pairs in the group. The number of pairwise tests for 6 groups is choose(6, 2)= 15. Each pairwise test is a single hypothesis test associated with 5% type1 error risk. If we don’t make a correction for doing the multiple comparisons, the family-wise type1 error would inflate to \\(15 x 5% = 75%\\)! The Bonferroni adjustment is the most conservative and simple to understand. It multiples every unadjusted p-value by 15, the number of comparisons made. Thus, each of the p-values in the grid is 15X larger than had the adjustment not been made. Every p-value less than 0.05 in the grid is therefore cause for rejecting the null hypothesis that the pair does not differ. The highest among these is the comparisons between sprays C and D, which has a p-value of 0.03977. More generally, the Bonferroni correction is \\(adjusted\\ type1\\ threshold = 0.05/C\\) where C is the number of comparisons to make. 17.7.1 Write Up A non-parametric omnibus test establishes that the locations of the insecticide effects of the six sprays differ (Kruskal-Wallis, \\(\\Chi^2\\) = 54.69, df=5, p=1.511e-10). Posthoc pairwise multiple comparisons by the Mann-Whitney test (Bonferroni adjusted p-values) indicate the following sprays differ from each other: A v(0.00058), D(0.00117), E(0.0051), …and so on 17.8 Friedman test 17.9 Nonparametric power calculations To my knowledge, there are no simple power functions in R designed for nonparametric testing. I assume the reason for that is that nonparametric tests can be performed on a wide variety of data types. As such, it’s hard to create one-size-fits all, plug and play functions. The function below, nonpara.pwr is configured to simulate the power of nonparametric one- and two-group comparisons using the Wilcoxon test function of R. The intended use of this function is to establish a priori the sample size needed to yield a level of power that you deem acceptable. What the function does is simulate a long run of experimental results at a given sample size, from which it calculates the power. This is a Monte Carlo method. Thus, it simulate random values that mimic experimental results based upon “known” population parameters. The function strictly takes the argument of a sample size value and returns an experimental power that sample size generates. But it also requires that you customize your initialization. Do this by entering estimates for parameters of the population you expect to sample. All you need to do this is a pretty good guess of the values for the outcome variable you expect your experiment will produce. 17.9.1 How it works Use the initializer to define values for the parameters of the population you’re sampling. These are passed into a function that randomly generate values for the outcome variable. A statistical test is performed on that outcome. A secondary ‘hit’ test asks whether the test result crosses a threshold. Here a ‘hit’ is counted if the p-value for the simulated test is less than 0.05. Power is the franction of p-values that fall below the confidence level that you set. For example, if you set a confidence level of 95%, every p-value less than 0.05 will be scored as a hit. Power = hits/number of simulations. The function is configured to repeat this simulation/testing/hit count process many times, hundreds or even thousands of times. The more times it simulates a result, the more accurate the power estimate. 17.9.2 Initialization with population parameters Let’s focus for now on the thought process involved in how to intialize it. What’s most important is to initialize the function using a random value generator that best simulates the type of data you expect to generate in your experiment. Using the random number generator functions of the various probability distributions is suitable for many cases. For example, use rnorm for measured variables for which you believe you can predict means and standard deviations. # simulate a sample size of 5 for a normally-distributed variable with mean of 100 units and standard deviation of 25 units control.1 &lt;- rnorm(n=5, mean=100, sd=25) treatment.1 &lt;- rnorm(n=5, mean=150, sd=25) sim.set &lt;- data.frame(control.1, treatment.1); sim.set ## control.1 treatment.1 ## 1 123.78070 144.2373 ## 2 54.59601 144.8087 ## 3 88.29962 178.2538 ## 4 97.86539 165.6107 ## 5 109.42299 141.8250 Use rpois to simulate variables that represent discrete events, such as frequencies. #simulate 5 events, where each occurs at an average frequency of 7. control.2 &lt;- rpois(n=5, lambda=7) treatment.2 &lt;- rpois(n=5, lambda=10) sim.set2 &lt;- data.frame(control.2, treatment.2); sim.set2 ## control.2 treatment.2 ## 1 7 11 ## 2 8 9 ## 3 9 10 ## 4 8 12 ## 5 3 9 It’s even possible to simulate ordinal data, such as the outcome of likert tests. The sample() function can be configured for this purpose. Note how probability vector argument is used to cast expectd distributions of the values #simulate 6 replicates scored on a 5-unit ordinal scale, where 1 is the lowest level of an observed outcome and 5 is the highest. control &lt;- sample(x=c(1:5), size=6, replace=T, prob=c(.80, .15, .05, 0, 0)) disease &lt;- sample(x=c(1:5), size=6, replace=T, prob=c(0, 0, 0.05, 0.80, 0.15)) results &lt;- data.frame(control, disease); results ## control disease ## 1 2 5 ## 2 1 4 ## 3 1 4 ## 4 1 4 ## 5 1 4 ## 6 1 4 If a more sophisticated/less-biased simulation of ordinal data is needed, you’ll find it is doable but suprisingly not trivial. Please recognize that simulating data for these functions mostly depends upon your scientific understanding of the variables. You have to make scientific judgements about the values your variable will take on under control and treatment conditions. To get started statistically, go back to the basics. Is the variable discrete or continuous. Is it measured or ordered or sorted? What means and standard deviations, or frequencies, or rank sizes should you estimate? Those are scientific judgements. Either select the values you predict will occur, OR enter values that you believe would be a minimal, scientifically meaningful effect size. 17.9.3 An example Let’s say we study mouse models of diabetes. In non-diabetic control subjects, mean plasma glucose concentration is typically ~100 mg/dL and the variability tends to be quite low (15 mg/dL). Most everybody in the field agrees that average blood glucose concentration of 300 mg/dL represents successful induction of diabetes in these models. However, experience shows these higher levels of glucose are associated with considerably greater variability (sd=150 mg/dL) than under normal states. Large differences in variability between two groups are called heteroscedaticity. The presence of heteroscedaticity can preclude the use of parametric statistical tests since it raises type1 error risk. Thus, nonparametric testing would be used when expected such outcomes. Let’s say you want to test a new idea for diabetes induction. What sample size would be necessary, assuming an nonparametric testing and an unpaired design, to reliably detect a difference between these two groups at 80% power or better? The function below calculates the power of experiments that might be expected to generate such results, at given sample sizes. 17.9.4 nonpara.pwr Running this first code chunk reads the function into the environment. There’s no output, yet. But read it line-by-line to see if you can follow the logic. nonpara.pwr &lt;- function(n){ #Intitializers. Place expected values for the means and standard deviations of two groups to be compared here. m1= 100 #mean of group 1 sd1= 10 #standard deviation of group 1 m2= 300 #mean of group 2 sd2= 150 #standard deviation of group 2 # the monte carlo just repeats the random sampling i times. It runs a t-test on each sample, i, grabs the p-value and places it in a growing vector, p.values[i] ssims=1000 #The function below produced p-values. This empty vector will be filled with p-values as they are generated p.values &lt;- c() #This function inside a function repeats the simulation ssims times, collecting a p-value each time. Importantly, change the arguments for the statitical test to suit your experimental design. It i &lt;- 1 repeat{ x=rnorm(n, m1, sd1); y=rnorm(n, m2, sd2); p &lt;- wilcox.test(x, y, paired=F, alternative=&quot;two.sided&quot;, var.equal=F, conf.level=0.95)$p.value p.values[i] &lt;- p if (i==ssims) break i = i+1 #This calculates the power from the values in the p.value vector. pwr &lt;- length(which(p.values&lt;0.05))/ssims } return(pwr) } To get some output, pass a sample size of 5 per group into the function. set.seed(1234) nonpara.pwr(5) ## [1] 0.626 At this seed, the power is 62.6% for a sample size of 5 per group. A slightly higher power would be desirable. Change the value of n in the code chunk to see what’s necessary to get 80% power. Or, more easily, just run nonpara.pwr over a range of sample sizes, plotting a power vs sample size curve: #frame is a data frame with only one variable, sample size frame &lt;- data.frame(n=2:50) #data is a data frame with two variables, sample size and a power value for each data &lt;- bind_cols(frame, power=apply(frame, 1, nonpara.pwr)) #plot ggplot(data, aes(n, power))+ geom_point() + scale_y_continuous(breaks=c(seq(0, 1, 0.1)))+ scale_x_continuous(breaks=c(seq(0,50,2))) This result shows that a sample size of 7, 8, or 9 per group would give ~85% power. Thus, a sample size of 7 per group would be the minimal size necessary to test the two-sided null that there is no difference between the groups, at a 95% confidence level. The experimental design, and thus these scripts, can be reconfigured in many ways. 17.10 Summary If you’re used to comparing means of groups, nonparametrics can be somewhat disorienting. There are no parameters to compare! And the concept of location shifts or differences seems rather abstract. The tests transform the values of experimental outcome variables into either sign rank or into rank sum units. That abstraction can be disorientating, too. But it is important to recognize that sign ranks and rank sum distributions are approximately normal. Therefore, perhaps its best to think of nonparametrics as a way to transform non-normal data into more normal data. The nonparametrics are powerful statistical tests that should be used more widely than they are. "],
["signrank.html", "Chapter 18 Signed Rank Distribution 18.1 Transformation of data into sign ranks 18.2 R’s Four Sign Rank Distribution Functions", " Chapter 18 Signed Rank Distribution The signed rank per se represents a type of data transformation. Experimental data are first transformed into signed ranks (see below). When signed ranks for a group of data are summed that sum serves as a test statistic. The sign rank test has a symmetrical normal-like distribution, which is discrete rather than continuous. In R, when using the wilcox.test to evaluate data from one sample or for paired sample nonparametric designs, the sums of only the postive signed ranks are calculated as the test statistic \\(V\\). The distribution of \\(V\\) is the focus of this document. There is a unique sign rank sum distribution for sample size \\(n\\). These represent the distributions that would be expected under the null hypothesis. In practice, values of \\(V\\) that are on the extreme ends of these distributions can have p-values that are so low that we would reject the null \\(V\\) distribution as a model for our dataset and accept an alternative. The use of \\(V\\) as a test statistic and its null distributions can be a bit confusing if you’ve worked with other software to do nonparametric hypothesis testing. For example, Prism reports \\(W\\) as a test statistic for sign rank-based nonparametric experimental designs. On that platform, \\(W\\) is the sum of all signed ranks, not just the positives. The center location of \\(W\\) as a sign rank test statistic is at or near zero on the \\(W\\) distributions, whereas zero is the lowest possible value for \\(V\\). Although calculated slightly differently, \\(W\\) and \\(V\\) are equivalent ways to express the same relationship between the values within the original dataset. 18.1 Transformation of data into sign ranks 18.1.1 For a one group sample A one group sample represents a single vector of data values. Let a sample of size \\(n\\) for a variable \\(X\\) take on the values \\(x_1, x_2, ... x_n\\). The location of this vector will be compared to a location value \\(x = \\mu\\), such that \\(z_i=x_i-\\mu\\) for \\(i=1\\ to\\ n\\). To transform the sample to a vector of signed ranks, first rank \\(|z_i|\\) from lowest to highest. \\(R_i\\) is the rank value for each value of \\(|z_i|\\). \\(\\psi = 0\\) if \\(z_i &lt; \\mu\\), or \\(\\psi= 1\\) if \\(z_i &gt; \\mu\\). The signed ranks of the original vector values are therefore \\(R_1\\psi, R_2\\psi,...R_n\\psi\\). 18.1.2 For a paired sample The differences between paired groups in an experimental sample also represent a single vector of data values, which explains why a sign rank-based test are used, rather than a rank sum test. A sample of \\(n\\) pairs of the variables \\(X\\) and \\(Y\\) has the values \\((x_1,y_1); (x_2,y_2); ... (x_n,y_n)\\). The difference between the values of \\(X\\) and \\(Y\\) will be compared to zero. For \\(i=1\\ to\\ n\\), the difference is \\(z_i=x_i-y_i\\). The sign rank transformation of \\(z_i\\) for each pair is performed as above. 18.1.3 The sign rank test statistic in R The sign rank test statistic for both the one- and two-sample cases is \\(V = \\sum_i^{n}R_i\\psi_{&gt;0}\\), which has a symmetric distribution ranging from a minimum of \\(V = 0\\) to a maximum of \\(V = \\frac{n(n+1)}{2}\\). This test statistic is produced by the wilcox.test for one-sample and paired-sample expriments. Again, \\(V\\) is the sum of the positive signed-ranks only. Other softer produces a test statistic \\(W\\), which is the sum of all the signed ranks 18.1.3.1 More about the test statistic The expectation of \\(V\\) (ie, its median) when the null hypothesis is true is \\(E_0(V)=\\frac{n(n+1)}{4}\\) and its variance is \\(var_0(V)=\\frac{n(n+1)(2n+1)}{24}\\) The standardized version of \\(V\\) is \\(V^*=\\frac{V-E_0(V)}{var_0(V)}\\). As \\(n\\) gets large, \\(V^*\\) asymptotically approaches a normal distribution \\(N(0,1)\\). Zero and tied values of \\(|Z_i|\\) occur in datasets for both the one group and paired group transformations of raw data. When values of \\(|Z|=0\\) occur, they are discarded in the calculation of \\(V\\) and the \\(n\\) is readjusted. The integer value of ranks for tied \\(|Z&#39;s|\\) are averaged. Although these ties don’t change \\(E_0(V)\\), they do reduce the variance of \\(V\\). \\(var_0(V)=\\frac{n(n+1)(2n+1)-\\frac{1}{2}\\sum_{j=1}^{g}t_j(t_j-1)(t_j+1)}{24}\\) 18.2 R’s Four Sign Rank Distribution Functions 18.2.1 dsignrank The function dsinerank returns a probability value given two arguments: \\(x\\) is an integer value, meant to represent \\(V\\), which is the value of the sign rank test statistic; and \\(n\\) is the sample size of either a one-sample or a paired experiment. The probabilities for individual values of \\(V\\) are sometimes useful to calculate. For example, the probability of obtaining \\(V=3\\) for an \\(n\\)=10 experiment is: dsignrank(3, 10) ## [1] 0.001953125 More ofen it is useful to visualize the null distribution of the sign rank test statistic over a range of values. Here’s a distribution of \\(V\\) for an experiment of sample size 10. n &lt;- 10 #number of independent replicates in experiment max &lt;- n*(n+1)/2 df &lt;- data.frame(pv=dsignrank(c(0:max), n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p(V)&quot;) 18.2.2 psignrank This is the cumulative distribution function for the sign rank test statistic. \\(q\\) is an integer to represent the expected value of \\(V\\), and \\(n\\) is the sample size. Given these arguments, psignrank will return a p-value for a given test statistic value. psignrank returns the sum of the probabilities of \\(V\\) over a range, and is reversible using the lower.tail argument. Here we use psignrank to generate the p-value when \\(V\\)=3 and \\(n\\)=1. Then we show the sum of the dsignrank output for \\(V\\)=0:3 is equal to psignrank. Finally, the symmetry of the distribution is illustrated: psignrank(3, 10, lower.tail=T) ## [1] 0.004882812 sum(dsignrank(c(0:3), 10)) == psignrank(3, 10, lower.tail=T) ## [1] TRUE psignrank(51, 10, lower.tail=F) ## [1] 0.004882812 psignrank(51, 10, lower.tail=F)==psignrank(3, 10, lower.tail=T) ## [1] TRUE The distributions of psignrankfor its lower and upper tails: n &lt;- 10 max &lt;- n*(n+1)/2 df &lt;- data.frame(pv=psignrank(c(0:max), n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p-value for V&quot;) df &lt;- data.frame(pv=psignrank(c(0:max), n, lower.tail=F)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p-value for V&quot;) 18.2.3 qsignrank This is the quantile signrank function in R. If given a quantile value (eg, 2.5%) and sample size \\(n\\), qsignrank will return the value of the test statistic V. For example, here is how to find the critical limits for a two-sided hypothesis test for an experiment with 10 independent replicates, when the type1 error of 5% is evenly distributed to both sides: qsignrank(0.025, 10, lower.tail=T) ## [1] 9 qsignrank(0.025, 10, lower.tail=F) ## [1] 46 Interpretation of critical limits output: When the null two-sided hypothesis is true, \\(9\\le V\\le46\\). In other words, the null hypothesis would not be rejected if an experiment generated a \\(V\\) between 9 and 46. Alternative interpretation: \\(p&lt;0.05\\) when \\(9&gt;V&gt;46\\). In other words, a two sided null hypothesis would be rejected if it generated a \\(V\\) below 9 or greater than 46. And here are the critical limits for one-sided hypothesis tests for an experiment with 10 independent replicates, when the type1 error is 5%. Notice how the critical limits differ between one-sided and two-sided tests: qsignrank(0.05, 10, lower.tail=T) ## [1] 11 qsignrank(0.05, 10, lower.tail=F) ## [1] 44 Standard interpretion: For one-sided type1 error of 5%, When the null hypothesis is true, \\(V\\ge11\\), or \\(V\\le44\\). Alternative interpretation: \\(p &lt; 0.05\\) when \\(V&lt; 11\\) or \\(V&gt;44\\) Here is a graphical representation of the qsignrank function. Note the stairstep pattern, characteristic of discrete distributions: n &lt;- 10 x &lt;- seq(0,1,0.01) df &lt;- data.frame(v=qsignrank(x, n, lower.tail=T)) ggplot(df, aes(x=x, v)) + geom_point() + xlab(&quot;p-value&quot;) + ylab(&quot;V&quot;) 18.2.4 rsignrank This function is used to simulate random values of the test statistic for null distributions for \\(V\\). Here’s a group of 5 random values from a distribution for a sample size of 10. The output values represent the sum of the positive sign ranks, aka the \\(V\\) test statistic values that would be generated from doing 10 different random experiments of this sample size…if the null hypothesis were true: rsignrank(5, 10) ## [1] 35 30 35 43 24 Let’s simulate 1,000,000 such experiments. And then let’s count the number of extreme values of \\(V\\) that would randomly occur in that number of experiments. We know from the qsignrank function that 11 is the critical value for a one-sided test at \\(\\alpha=0.05\\) for a sample size of 10. Thus,our “significance test” for each of the 1,000,000 random experiments is to ask whether \\(V\\) is below the critical value of 11 for that sample size. Given we’ve set a 5% false positive rate, we’d expect to see around 50000 false positives (5% of 1,000,000): sim &lt;- rsignrank(1000000, 10) test &lt;- sim&lt;11 length(which(test==TRUE)) ## [1] 42334 If you ran that chunk repeatedly you’d come up with something around 4.2% each time. Why is it not exactly 5%? What do you think that means? Are there experimental sample-sizes where the type1 error would be even further or closer to 5%? "],
["ranksum.html", "Chapter 19 Rank Sum Distribution 19.1 R’s Four Sign Rank Distribution Functions", " Chapter 19 Rank Sum Distribution The Wilcoxon rank sum per se represents a type of data transformation. The transformation is then used to calculate the nonparametric test statistic \\(W\\) for the Wilcoxon Test for independent two group samples, which is equivalent to the Mann-Whitney test. The distribution of \\(W\\) is discrete but normal-like. Given two groups for comparison, such as a control population, \\(X\\) vs a treatment population, \\(Y\\). Under the null hypothesis, the distributions of the values of \\(X\\) and \\(Y\\) are equal. Any effect due to treatment can be defined as \\(\\Delta = E(Y)-E(X)\\), where \\(E(Y)\\) and \\(E(X)\\) are the averages for the treatment and control effects, respectively. Under the null hypothesis, \\(\\Delta = 0\\). 19.0.1 Transformation of data into rank summs \\(X\\) has a sample size \\(m\\) and \\(Y\\) has a sample size \\(n\\) and the total sample size is \\(N=m+n\\). Combine all values of \\(X\\) and \\(Y\\) and rank them from smallest to largest. If \\(S_1\\) is the rank of \\(y_1,..,S_n\\) then W is the sum of the ranks assigned to the \\(Y\\) values: \\(W=\\sum_{j=1}^nS_j\\) 19.0.2 The sign rank test statistic in R When the null is true, the average of W is \\(E_0(W)=\\frac{n(m+n+1)}{2}\\) and the variance is \\(var_0(W)=\\frac{mn(m+n+1)}{12}\\). \\(W\\) can take on the values from zero to \\(mn\\). 19.1 R’s Four Sign Rank Distribution Functions 19.1.1 dwilcox Given a value of \\(W\\) and group sample sizes \\(m\\) and \\(n\\), dwilcox will return the value of the probability for that \\(W\\). For example, here is the probability of getting a \\(W\\) of exactly 10 with group sizes of 5 and 6: dwilcox(10, 5, 6) ## [1] 0.04978355 That probability is NOT a p-value. The p-value would be the sum of the probabilities returned from the dwilcox function for the range of \\(W\\) from 0 to 10. Given the range of possible values for \\(W\\) its distribution can be plotted for any combination of group sizes. We can see that a value of 10 for \\(W\\) is left-shifted, but not too extreme. m &lt;- 5 #number of independent of group 1 n &lt;- 6 #number of independent replicates of group 2 max &lt;- m*n df &lt;- data.frame(pv=dwilcox(c(0:max), m, n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p(W)&quot;) 19.1.2 pwilcox The pwilcox function returns a p-value when given \\(W\\) along with the sample sizes \\(m\\) and \\(n\\) corresponding to the two groups. Thus, the probability of obtaining a \\(W\\) value of 10 or less with group sample sizes of 5 and 6 is: pwilcox(10, 5, 6, lower.tail=T) ## [1] 0.2142857 The probability of obtaining a \\(W\\) value of 10 or more with group sample sizes of 5 and 6 is: pwilcox(10, 5, 6, lower.tail=F) ## [1] 0.7857143 The relationship of pwilcox to dwilcox is as depicted here there is a slight inequality between the two functions somewhere beyond 10 significant digits, thus the rounding: round(sum(dwilcox(c(0:10), 5, 6)), 10) == round(pwilcox(10, 5, 6), 10) ## [1] TRUE Here are the lower and upper tailed cumulative distributions of the Wilcoxon distribution: m &lt;- 5 n &lt;- 6 max &lt;- m*n df &lt;- data.frame(pv=pwilcox(c(0:max), m, n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p-value for W&quot;) df &lt;- data.frame(pv=pwilcox(c(0:max), m, n, lower.tail=F)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p-value for W&quot;) 19.1.3 qwilcox This is the quantile signrank function in R. If given a quantile value (eg, 2.5%) and sample sizes \\(m\\) and \\(n\\), qwilcox will return the value of the corresponding test statistic \\(W\\). For example, this can be used to find the critical limits for a two-sided hypothesis test for an experiment with 10 independent replicates, when the type1 error of 5% is evenly distributed to both sides: qwilcox(0.025, 5, 6, lower.tail=T) ## [1] 4 qwilcox(0.025, 5, 6, lower.tail=F) ## [1] 26 Interpretation of critical limits output: When the null two-sided hypothesis is true, the values of \\(W\\) are \\(4\\le V\\le26\\). In other words, the null hypothesis would not be rejected if an experiment generated a \\(W\\) between 4 and 26. Alternative interpretation: \\(p&lt;0.05\\) when \\(4&gt;V&gt;26\\). In other words, a two sided null hypothesis would be rejected if it generated a \\(W\\) below 4 or greater than 26. And here are the critical limits for one-sided hypothesis tests for an experiment with 10 independent replicates, when the type1 error is 5%. Notice how the critical limits differ between one-sided and two-sided tests: qwilcox(0.05, 5, 6, lower.tail =T) ## [1] 6 qwilcox(0.05, 5, 6, lower.tail = F) ## [1] 24 Thus, having obtained a \\(W\\) of less than 6 or 24 and greater we would reject the null hypothesis. Do notice the distribution for these two sample sizes lacks perfect symmetry: pwilcox(6, 5, 6) ## [1] 0.06277056 pwilcox(24, 5, 6, lower.tail=F) ## [1] 0.04112554 The quantile distribution of \\(W\\) is depicted below: m &lt;- 5 n &lt;- 6 x &lt;- seq(0,1,0.01) df &lt;- data.frame(w=qwilcox(x, m, n, lower.tail=T)) ggplot(df, aes(x=x, w)) + geom_point() + xlab(&quot;p-value&quot;) + ylab(&quot;W&quot;) 19.1.4 rwilcox We would use rwilcox to generated random values of \\(W\\) from null distributions. Here are 7 random values of \\(W\\) for experiments involving sample sizes of \\(m=5\\) and \\(n=6\\) rwilcox(7, 5, 6) ## [1] 17 19 10 22 6 15 17 Let’s simulate 1,000,000 such experiments. And then let’s count the number of extreme values of \\(W\\) that would randomly occur in that number of experiments. We know from the qwilcox function that 24 is the critical value for a one-sided test at \\(\\alpha=0.05\\). Thus, our “significance test” for each of the 1,000,000 random experiments is to ask whether \\(W\\) is equal to or greater than 24. Recall, the cumulative distribution function indicates the p-value for a one-sided test of a \\(W=24\\) is: pwilcox(24, 5, 6, lower.tail=F) ## [1] 0.04112554 Given we’ve set a 5% false positive rate, we’d expect to see around 50000 false positives (5% of 1,000,000) by simulation of \\(W=24\\) under that scenario: sim &lt;- rwilcox(1000000, 5, 6) test &lt;- sim&gt;=24 length(which(test==TRUE)) ## [1] 62199 In this instance, we see a much higher number of false positive 6.2% than the expected number of 5%! Why? "],
["ttests.html", "Chapter 20 The t-tests 20.1 Data assumptions for t-tests 20.2 The t Statistic 20.3 t Test Hypotheses 20.4 Confidence Intervals of Means 20.5 t Tests: Running the analysis 20.6 Plotting t Tests 20.7 t Test Power", " Chapter 20 The t-tests The t-tests, otherwise known as “Student’s t tests” are for experimental designs that involve testing hypotheses comparing one group to a standard or two groups two each other, and where the outcome variable is continuous and on an equal interval scale. In other words, t-tests are for measured data in two or fewer groups. To recall, continuous data are on scales that have values between their intervals. For example, we might use a mass balance to measure the mass of individual mice, and then record how their weight change in response to some factor at two different levels. Here, mass would be an equal interval measurement. Mass is continuous, in so far as a theoretically infite number of values can exist between two units on the scale, depending upon the sensitivity of the measurements. Derivative continuous variables are common in t-testing. For example, growth rate, in grams/week, is a derivative of the mass and time measurement scales. Like mass and time, growth rate is also continuous. Just as importantly, the statistics derived from continuous, equal interval data also have the properties of continuous, equal interval data. For example, the means and standard deviations, the standard errors of the mean, and the t-test statistic itself are also scaled continuous. 20.1 Data assumptions for t-tests The following assumptions should be checked to use t-tests properly. 20.1.0.1 Strong assumptions, t tests are invalid if these not met: The replicate data arise from a randomized process. Each sample replicate is independent of every other sample replicate. 20.1.0.2 Weak assumptions: t-tests are less robust when these are not met: The sampled population for each group is normally distributed. The variances of the compared groups are equivalent. The smaller the sample size, the more difficult it is to validate these latter two assumptions. Unless you’re dealing with sample sizes of 30 or greater, it is probably futile to try to make these assessments. There are two equally valid ways to approach this. First, a property of continuous data is that it will be normally distributed when it is measured within a linear range on its scale. The same holds for the variance assumption. There are a normality tests such as the Shapiro-Wilk (shapiro.test), which tests the null hypothesis that the values of a variable are normally-distributed. Levene’s test library(car), leveneTest is useful to test homogeneity of variance. The use of these tests would be proscribed in some preset protocol. If their nulls are rejected, either the data can be normalized through log or reciprocal transformation, or nonparametric testing can be used instead. Alternately, the researcher can just conduct hypothesis testing using nonparametric procedures. The performance of t-tests and their nonparametric counterparts are virtually identical when the normality and homoscedasticity assumptions are met. Whereas nonparametrics statistics are an appropriate option when these conditions are unmet. On non-normal or heteroscedastic samples false positive detection is reduced slightly by using nonparametric testing compared to t-tests. 20.2 The t Statistic The t statistic is a ratio signifying the signal to noise ratio in a sample. The numerator would be a measure for the magnitude of the average effect, whereas the denominator would represent the precision by which those mean differences are determined. There are three t-tests, reflecting 3 very different experimental designs. The decision to use one or the other t-test is entirely scientific and based upon how the experiment is conducted. If you understand your experimental design, you’ll know which t test is applicable. One sample tests inspect the difference between a sample mean and a hypothetical mean. Unpaired tests inspect the difference between the means of two groups. *The paired t test inspects the mean difference between paired observations and the value zero. A common mistake is to default to an unpaired t-test if an uneven number of measurements are in the data set. That is only correct when the measurements are not intrinsically-linked. 20.2.1 One sample t tests For comparing the mean response \\(\\bar y\\) to a single level of a predictor variable against a hypothetical population mean, \\(\\mu\\). A group comprised of \\(n\\) independent observations, whose \\(\\bar y\\) The one sample t test has \\(n-1\\) degrees of freedom. \\[t=\\frac{\\bar y-\\mu}{\\frac{sd}{\\sqrt n}}\\] 20.2.2 Unpaired t tests This test is used for experiments in which the measurements are all independent from each other. Th unpaired t-test compares the difference between the mean responses to each of two levels, A and B, of a predictor variable. The design is comprised of a total of \\(n_A + n_B\\) observations and has \\((n_A + n_B)-2\\) degrees of freedom. \\(s^2_p\\) is the pooled variance of the sample. \\[t=\\frac{\\bar y_A-\\bar y_B}{\\sqrt{\\frac{s^2_p}{n_A}+\\frac{s^2_p}{n_B}}}\\] The pooled variance is calculated using the sum of the squared deviates \\(SS_A\\ and\\ SS_B\\) from each group as follows: \\[s^2_p=\\frac{SS_A+SS_B}{df_A+df_B}\\], where \\(df_A=n_A-1\\) and \\(df_B=n_B-1\\). The denominator of the \\(t\\) ratio is the standard error of the test. The standard error represents the test’s precision. Inspection of the pooled variance equation and how it factors into the calculation of the test statistic should give you some indication for why its not a good idea to have unequal sample sizes in unpaired experimental designs. If the sample size for one group is much larger than the other, it’s variance will dominate the \\(t\\) calculation. That becomes a more significant problem the more unequal are the group variances. 20.2.3 Paired t tests This test is used when measurements are intrinsically-linked. Each of \\(n\\) replicates is exposed to both levels, A and B, of a predictor variable. There are \\(n\\) pairs of measurements in the design. The mean difference \\(\\bar d\\) of the paired responses is compared to the value of zero. \\(sd_d\\) is the standard deviation of the \\(n\\) differences. The experiment has \\(n-1\\) degrees of freedom. \\[t=\\frac{\\bar d}{\\frac{sd_d}{\\sqrt n}}\\] 20.3 t Test Hypotheses As for whether to choose a paired or unpaired analysis, the choice of t-test hypotheses depends upon the experimental design, and the scientific question at hand. Resist the temptation to o toggle between two-sided and one-sided options. That’s p-hacking, which is a bias. Since t-tests are parametric, hypotheses are stated on the basis of the statistical parameters. In this case, the means of the samples are meant to infer the sampled population, so we revert to greek notation. To put this another way, that the two samples differ is a mathematical fact. We don’t need a statistical test to tell us that. The test helps us infer whether the populations that were sampled differ. One-sided hypotheses predict the direction of an effect. For example, “the response to treatment will be greater than control.” Or, “the response to treatment will be less than control.” Two-sided hypothesis do not predict the direction of an effect: “The response to treatment will differ from control, either higher or lower.” Therefore, use a one-sided hypothesis if you think your treatment will go in a specific direction. Choose a two-sided test when you’re not willing to bet on an effect’s direction. This matters because at the 95% confidence level the threshold value of the t statistic will be lower for a one-sided test ( eg,qt(0.05, 2) than for a two-sided test (qt(0.025, 2)) given the same data. Put another way, the “significance” threshold will always be a higher bar to cross for a two-sided hypothesis. For a one-sided test, all of the 5% cumulative probability is on one side of the distribution. For a two-sided test, that 5% is evenly split to both sides. Therefore, two-sided tests are slightly more stringent. 20.3.1 One sample hypotheses Two sided: Use when, going into an experiment, you are not sure which direction the predictor variable will change the outcome variable relative to the population mean. H_0: \\(\\bar x = \\mu\\) H_1: \\(\\bar \\ne \\mu\\) One sided: Use when, going into an experiment, you are confident the predictor variable will cause the outcome response to be higher than the population mean. H_0: \\(\\bar x \\le \\mu\\) H_1: \\(\\bar x&gt;\\mu\\) Or you are confident the predictor variable will cause the outcome response to be lower than the population mean. H_0: \\(\\bar x \\ge \\mu\\) H_1: \\(\\bar x&lt;\\mu\\) 20.3.2 Unpaired hypotheses Two sided: Use when, going into an experiment, you are not sure whether level A or B of the predictor will cause a higher outcome response. H_0: \\(\\bar x_A = \\bar x_B\\) H_1: \\(\\bar x_A \\ne \\bar x_B\\) One sided: Use when, going into an experiment, you are confident level A of the predictor variable will cause the outcome response to be higher than that for level B. H_0: \\(\\bar x_A \\le \\bar x_B\\) H_1: \\(\\bar x_A &gt; \\bar x_B\\) Or when you are confident the level A of the predictor variable will cause the outcome response to be lower than that for level B. H_0: \\(\\bar x_A \\ge \\bar x_B\\) H_1: \\(\\bar x_A &lt; \\bar x_B\\) 20.3.3 Paired hypotheses Two sided: Use when, going into an experiment, you are not sure whether the mean difference between levels of the predictor variable will be less than or greater than zero. H_0: \\(\\bar x = 0\\) H_1: \\(\\bar x \\ne 0\\) One sided: Use when, going into an experiment, you are confident the mean difference between levels of the predictor will be greater than zero. H_0: \\(\\bar x \\le 0\\) H_1: \\(\\bar x&gt;0\\) Or when, going into an experiment, you are confident the mean difference between levels of the predictor will be less than zero. H_0: \\(\\bar x \\ge 0\\) H_1: \\(\\bar x&lt;0\\) 20.4 Confidence Intervals of Means A 95% confidence interval is an inferential statistic that allows for estimating the accuracy of a sample. The 95% CI is the range of values in which there is a 95% chance the true mean exists. A 95% CI can be calculated as follows: \\(mean\\pm qt(0.975, df)*\\frac{sd}{\\sqrt n}\\) For example, given a sample size of n=3, that has a mean of 100. and a standard deviation of 25 upper.limit &lt;- 100+qt(0.975, 2)*(25/sqrt(3)) lower.limit &lt;- 100-qt(0.975, 2)*(25/sqrt(3)) upper.limit ## [1] 162.1034 lower.limit ## [1] 37.89656 20.5 t Tests: Running the analysis In R t.test represents a single function by which each of the three t test experimental designs can be analyzed. 20.5.1 One sample t test Let’s say a standard to measure against is the value of 100. We can ask if a random sample that is 2-fold greater is different than 100, less than 100, or greater than 100: set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25) t.test(a, mu=100, alternative=&quot;two.sided&quot; ) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.02635 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 129.1056 273.4744 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.9868 ## alternative hypothesis: true mean is less than 100 ## 95 percent confidence interval: ## -Inf 250.2778 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative =&quot;greater&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.01318 ## alternative hypothesis: true mean is greater than 100 ## 95 percent confidence interval: ## 152.3023 Inf ## sample estimates: ## mean of x ## 201.29 20.5.1.1 Interpretation t is a descriptive statistic. Notice how the t-value doesn’t change. The same sample will give the same signal to noise ratio, irrespective of the hypothesis tested. The p-value is the probability of making a type1 error by rejecting the null hypothesis. The p-value differs, given the hypothesis argument. The p-value is an inferential statistic. Notice how the p-value for a two sided test is larger than for a one sided test. Notice how the alternative output corresponds to the alternative input. Notice how the 95% confidence level is the default output. If you want a differnt confidence level, enter an argument for it. The 95% Confidence Interval differs for each hypothesis. Like the p-value, that indicates the 95% CI is an inferential statistic. The simplest way to get a typical 95%CI of a mean in R is to run a two.sided t.test and pull it out of that. 20.5.2 Unpaired t test Now will pull two random samples: one from a normal distribution that has a mean of 200, and the second from a distribution that has a mean of 100. Both have standard deviations of 25. Will these random samples differ? set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25) b &lt;- rnorm(3, mean=100, sd=25) t.test(a, mu=100, alternative=&quot;two.sided&quot; ) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.02635 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 129.1056 273.4744 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.9868 ## alternative hypothesis: true mean is less than 100 ## 95 percent confidence interval: ## -Inf 250.2778 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative =&quot;greater&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.01318 ## alternative hypothesis: true mean is greater than 100 ## 95 percent confidence interval: ## 152.3023 Inf ## sample estimates: ## mean of x ## 201.29 20.5.2.1 Interpretation The interpretation is is similar to the one sample case. The “mean of x” corresponds to the a sample, or the first group in the argument. In the unpaired t test, good practice is to put the ‘treatment’ as the first data argument, and the ‘control’ as the second argument. 20.5.3 Paired t Test The structure of the data input matters. If entered as two vectors, the first value of the first vector will be paired with the first value of the second vector, and so on. set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25);a ## [1] 169.8234 206.9357 227.1110 b &lt;- rnorm(3, mean=100, sd=25);b ## [1] 41.35756 110.72812 112.65140 t.test(a, b, alternative=&quot;two.sided&quot;, paired=T ) ## ## Paired t-test ## ## data: a and b ## t = 12.105, df = 2, p-value = 0.006756 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 72.86194 153.22676 ## sample estimates: ## mean of the differences ## 113.0443 t.test(a, b, alternative=&quot;less&quot;, paired=T) ## ## Paired t-test ## ## data: a and b ## t = 12.105, df = 2, p-value = 0.9966 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 140.314 ## sample estimates: ## mean of the differences ## 113.0443 t.test(a, b, alternative =&quot;greater&quot;, paired=T) ## ## Paired t-test ## ## data: a and b ## t = 12.105, df = 2, p-value = 0.003378 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 85.77465 Inf ## sample estimates: ## mean of the differences ## 113.0443 20.5.3.1 Intepretation Notice how, given the same data, the paired computes a different value for t compaired to the unpaired. The sample estimate for the paired analysis is the mean of the differences. The 95% CI is for the mean of the differences. There is a 95% chance the true mean of the differences in the population sampled is within the CI’s range of values. 20.6 Plotting t Tests 20.6.1 Unpaired A t test compares two means, so the plot should illustrate that comparison. There’s a growing movement to show all the data as points. Tidy the data, create a descriptive statistics summary table #data munge data.u &lt;- data.frame(control=b, treatment=a) %&gt;% gather(key=Predictor, value=Response) %&gt;% add_column(id=LETTERS[1:6]) #summary statistics table to have data.y &lt;- data.u%&gt;% group_by(Predictor) %&gt;% summarise( mean=mean(Response), sd=sd(Response), n=length(Response), sem=mean(Response)/sqrt(n) ) Plot the data points with mean +/- standard error of the mean. Since the dotplot produces the actual data, the variability of the sample can be ascertained. In this case, showing the SEM is appropriate. One of the more straight forward ways to add error bars is by use of the stat_summary function: ggplot(data.u, aes(Predictor, Response)) + geom_dotplot(binaxis=&#39;y&#39;, dotsize=1, stackdir=&quot;center&quot;) + stat_summary(fun.data=&quot;mean_se&quot;, fun.args=list(mult=1), geom=&quot;errorbar&quot;, color=&quot;red&quot;, width=0.2) + stat_summary(fun.y=mean, geom=&quot;point&quot;, color=&quot;red&quot;) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Computation failed in `stat_summary()`: ## &#39;what&#39; must be a function or character string 20.6.2 Paired Paired t test data should always be plotted with point-to-point lines to illustrate the paired experimental design. In the paired design, each replicate is a pair of observations. The means of the observations for the two groups are irrelevant. The design tests for the difference the treatment causes within each pair, which the slope of the line illustrates. For example, a horizontal line connect a replicate would indicate no effect of the predictor variable! Munge the data into a long data frame format. Adding an ID for each replicate allows using it as a grouping variable for the plot data.w &lt;- data.frame(control=b, treatment=a, id=LETTERS[1:3]) %&gt;% gather(key=Predictor, value=Response, -id) ggplot(data.w, aes(Predictor, Response, group=id)) + geom_point(size=2) + geom_line(color=&quot;red&quot;) 20.7 t Test Power A power analysis should precede every planned experiment to decide upon an optimal sample size. The principle output of a power function is the sample size that would be necessary to conduct a well-powered experiment. The pwr library has the pwr.t.test function that can be used for this purpose and is very simple to execute. This function requires that several decisions be made in advance. First, it takes arguments for acceptable type1 error and for intended power. The standard for these is 5% and 80%, respectively, but you can use any level you deem appropriate. Two other arguments are type, which is the experimental design, and alternative, which is the hypothesis. Finally, there is the d argument, which is Cohen’s \\(\\delta\\). This is less obvious, but very important and simple to understand. Cohen’s \\(\\delta\\) is the signal to noise for the effect size you anticipate. For example, imagine you anticipate measurements that will have a standard deviation of 25 units. You estimate this value on the basis of your own preliminary data or published information. You also believe the average response to a control level of the predictor varible will be around 50 units. In your mind, a minimally scientifically valid treatment effect will have a 100 unit response. The signal will be 100-50=50, andn the noise will be 25. Cohen’s \\(\\delta\\) will therefore be 50/25=2. A power calculation for a one-sided hypothesis is illustrated for a unpaired design: pwr.t.test(d=2, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;, alternative=&quot;greater&quot;) ## ## Two-sample t test power calculation ## ## n = 3.986998 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater ## ## NOTE: n is number in *each* group 20.7.1 Interpretation The experiment should have a total sample size total of 8. Randomly allocate each of two levels of the predictor variable to 4 replicates each. Notice how this function produces a bizarre, fractional sample size. Obviously, there is no such critter as a partial replicate! So always round up. People seem to have difficulty with Cohen’s delta. It’s nothing more complicated than a simple signal-to-noise estimate. The t-statistic. You need some idea about what value to expect for a mean (whether its difference between means or mean of the differences). You need some idea of the standard deviation. Don’t make the mistake of putting too fine of a point on Cohen’s delta. Scientific judgement is required to calculate the values to use to calculate Cohen’s delta. Use your best scientific guess for standard deviation and the response magnitude, and err on the conservative side. I strongly caution against using Cohen’s delta values recommended for “small”, “medium”, and “large” effects. There’s a good chance his idea of small effects is a lot smaller than your idea of small effects. "],
["ttestmc.html", "Chapter 21 Statistical design of t-tests 21.1 About this chapter 21.2 One sample t-test Monte Carlo 21.3 Unpaired t-test Monte Carlo 21.4 Paired t-test Monte Carlo", " Chapter 21 Statistical design of t-tests library(datapasta) library(tidyverse) 21.1 About this chapter This chapter walks through a statistical design for three different types of t-test experiments; the one-sample, the unpaired and the paired t-tests. Each of these examples use Monte Carlo simulation to assess experimental power at given sample and effect sizes, so you’d know many replicates are needed for a real life experiment. For preliminary data, these all use information in a mouse diabetes study conducted by the Jackson Labs. This is done to illustrate how existing data can be used to refine predictions about the values we might expect in our own experiment. In a single cycle of a Monte Carlo t-test simulation, a random distribution function is used to simulate a random sample. This mimics how the values for replicates might be generated in a real life experiment. The key distinction between real life and simulated samples is that we know the true parameters of the sampled population for the latter…because we code them in! In these t-test cases, we’ll use the rnorm function to generate sample replicates. That’s because, ideally, t-tests are only used on dependent variables that are continuous and normally distributed. In this case, we’ll be simulating experiments that involve measuring glucose concentration, which is a continuous, normally distributed variable. Having a simulated sample of a proper sample size, a t-test is next configured to test that sample, and then the p-value for it is collected. A p-value less than 0.05 would be counted as a “hit”. A cycle of random sampling, t-testing, and p-value evaluation is repeated anywhere from 100 to 10000 times, depending upon how accurate you’d like to be. The fraction of ‘hits’ relative to the number of simulations is the power of the test. The higher the number of simulations, the more accurate the Monte Carlo in terms of predicting “hits” and power. If the result is not an acceptable power, change the sample size n in the rnorm function until a Monte Carlo simulation run gives you an acceptable fraction of ‘hits’. There’s your sample size! Furthermore, any other assumptions or conditions can be changed, too. Need to re-evaluate the predicted standard deviation? Change it! Will the effect size be larger or smaller than you think? Simulate that! Want to compare a one-tailed to a two-tailed hypothesis? Switch it up! The time to do p-hacking and HARKing is during a Monte Carlo, before running the real life experiment. 21.1.1 Scenario Let’s imagine we have developed a new drug we hope will be useful for treating type II diabetes. Our role is to generate pre-clinical data in support of an FDA application. The planning is based on some mouse phenome data in a diet-induced obesity study in a mouse strain, which is a common model for type II diabetes. On the basis of expertise in the field, we make the judgment that a 50% reduction in blood glucose caused by the drug in this diet-induced obesity model would be a scientifically meaningful outcome. We need to design an experiment capable of detecting that 50% effect size. We’ve already compiled the phenome data into summary format. We’ll use that to guide our estimates in these Monte Carlo. These data show that under diabetogenic conditions the animals have an average blood glucose of 368 and SD = 119 (mg glucose/dl). Since this is an exercise in prediction and estimation, we’ll round those values to 370 and 120, if only to make the point that highly precise numbers misses the point of what we’re doing here. A 50% reduction would yield a target value of about 185 mg/dl. Finally, we’d like to run the real life experiments at 90% power. Why? Let’s imagine that these are a pretty important test: a “go” vs “no go” inflection point for a novel drug candidate. When the stakes are high so too should be the power of the test. 21.2 One sample t-test Monte Carlo In this single arm design, all C57Bl/6J mice are enrolled in an diabetogenic protocol would receive the drug treatment. Blood glucose levels are taken at the end of a proscribed period. The statistical test evaluates the null hypothesis, that mean blood glucose with drug treatment does not differ from the mean blood glucose that would be expected in animals that undergo the diabetogenic protocol. There is no formal placebo control group. Step 1: Enter mean and sd parameter estimates for the expected effect of the new drug. The sd estimate is a judgment call to think through and to model out. The entry below is conservative. It assumes the drug-treated group has the same sd as an untreated group. The Jaxwest7 data suggest a lower sd might happen with drug (the sd was 80 for the rosiglitazone group). Also enter an estimate for the theoretical mean of the population it will be evaluated against. Finally, enter a value for the sample size of the drug-treated group. meanDrug &lt;- 185 sdDrug &lt;- 120 muCon &lt;- 370 nDrug &lt;- 5 Step 2: Declare relevant arguments for the t-test function: alt =&quot;two.sided&quot; pairing = FALSE var = FALSE alpha=0.05 Step 3: Declare the number of simulations for the experiment and set up an empty vector to collect p-values generated by the function. nSims &lt;- 1000 p &lt;- c() Step 4: Run the simulation function. Notice how with each loop it simulates a new random sample then runs a on-sample t-test on that sample, then stores the p-value in a vector that grows with each loop. for(i in 1:nSims){ x&lt;-rnorm(n = nDrug, mean = meanDrug, sd = sdDrug) z&lt;-t.test(x, alternative = alt, paired = pairing, mu=muCon, var.equal = var, conf.level = 1-alpha) p[i]&lt;-z$p.value #get the p-value and store it } Step 5: Calculate and show “hits” and power. Remember, a “hit” is a simulation with a p-value &lt; 0.05. Power is the fraction of all simulations that meet this hit critria. # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 748 power &lt;- hits/nSims; power ## [1] 0.748 Step 6: Visualize the p-value output with a histogram. Because it’s pretty. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) Next steps: The returned power for the estimates above is about 75%. That’s a bit lower than a power of 90%, which we’d like here. Change the value of the nDrug term to a higher sample size, before re-running the simulation loops, until a power of ~90% is achieved. You can also change the estimate for sdDrug, too. How does lowering that influence sample size for 90% power? Why 90%? That’s both a scientific and strategic call. In this instance a positive result will have important implications for committing further to a costly drug development process. For that reason, the study should be run at a higher power than what might be chosen for a test that is more exploratory in nature. 21.3 Unpaired t-test Monte Carlo This is an alternative experimental design to the one above. A group is used to directly measure glucose concentration in placebo control for comparison to the drug effect, rather than assume what the glucose concentration would be in the placebo control This design therefore involves two groups of animals. All animals would be subjected to the diabetes-inducing diet. In the control arm, the group would receive a placebo. In the experimental arm, the group would receive the new drug. Each animal would be assumed as statistically independent of every other animal. The objective is to test the null hypothesis that the means of the blood glucose concentrations do not differ between the two groups. Step 1: Let’s call the “A” group the placebo, and the “B” group the drug treated. We’ll use standard deviation and the mean estimates for blood glucose levels as described above. We’ll design for equal sample sizes, though this test can tolerate differences. #Sampled population paramemters # sample A placebo meanA &lt;- 380 sdA &lt;- 120 nA &lt;- 5 # sample B new drug meanB &lt;- 190 sdB &lt;- 120 nB &lt;- 5 Step 2: Set the t-test function arguments as initializers, rather than down in the loop function, so they are easy to read and to modify. #t-test function arguments alt&lt;- &quot;two.sided&quot; pairing &lt;- FALSE var &lt;- TRUE alpha &lt;- 0.05 Step 3: Declare the number of simulations. The larger the number of simulations, the more accurate will be the power calculation. Also set up an empty vector for the following function to fill as it cycles through simulations and generates p-values. nSims &lt;- 10000 #number of simulated experiments p &lt;- c() Step 4: Run the simulation function. # the monte carlo function for(i in 1:nSims){ #for each simulated experiment x&lt;-rnorm(n = nA, mean = meanA, sd = sdA) #produce n simulated participants #with mean and SD y&lt;-rnorm(n = nB, mean = meanB, sd = sdB) #produce n simulated participants #with mean and SD z&lt;-t.test(x,y, alternative = alt, paired = pairing, var.equal = var, conf.level = 1-alpha) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } Step 5: Print out the power, which is the number of “significant” results divided by the total number of simulations. # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 5973 power &lt;- hits/nSims; power ## [1] 0.5973 Step 6: Plot out the distribution of p-values. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) Next steps: This configuration with a sample size of 5 in each group is a bit underpowered. Adjust these sample sizes to dervied a power of about 90%. Also experiment with adjusting other features of the test. What happens if the SD for the drug-treated group is lower? What about a one-tailed hypothesis instead of a two-sided? Monte Carlo is the time for p-hacking and harking. 21.4 Paired t-test Monte Carlo Another way to test whether the new drug can reduce blood sugar concentrations is by using a paired design. Paired designs offer some level of control over random variability by accounting for some of it to the variation within subjects. For example, individuals may differ wildly in their absolute blood glucose concentrations, but the proportional change due to drug from subject to another will be fairly consistent. The paired t-test challenges the null hypothesis that the average change in blood glucose caused by the drug is zero. Step 1: When simulating data for a paired design it is important to account for the expected correlation between variables. That’s best accomplished on the basis of some data. We can use the serial glucose measurements from individual subjects in the Jaxwest7 data set to extract this information. There are two daily blood glucose measurements taken on days 1, 3, 5, 7, 9, 11 and 12 of a study, from each of 16 different subjects. We can think of each blood collection as a variable, for which 16 independent replicate measurements are taken. Across the blood collections we expect to see high correlation within the replicates. In other words, animals with high values should be consistently high across the study period, and animals with low values should be consistently low across the same time frame. #Copying cells F14:S32 of the Jaxwest7 table (the value at F21 was imputed as the average of its row before pasting) using the datapasta package. bloodGlucose &lt;- data.frame( day01 = c(136L, 345L, 190L, 434L, 424L, 170L, 487L, 218L, 179L, 260L, 115L, 526L, 325L, 329L, 230L, 204L), day01 = c(270L, 518L, 301L, 504L, 486L, 208L, 449L, 273L, 184L, 381L, 191L, 517L, 252L, 296L, 414L, 120L), day03 = c(162L, 429L, 311L, 453L, 447L, 134L, 525L, 254L, 124L, 174L, 132L, 465L, 203L, 212L, 408L, 138L), day03 = c(165L, 413L, 361L, 392L, 417L, 129L, 419L, 265L, 107L, 140L, 132L, 394L, 158L, 159L, 179L, 139L), day05 = c(192L, 456L, 398L, 350L, 496L, 147L, 437L, 338L, 108L, 132L, 169L, 310L, 135L, 156L, 432L, 157L), day05 = c(397L, 487L, 465L, 400L, 484L, 141L, 476L, 386L, 149L, 138L, 158L, 269L, 162L, 200L, 288L, 122L), day07 = c(172L, 468L, 388L, 458L, 468L, 241L, 525L, 287L, 142L, 164L, 129L, 213L, 164L, 139L, 163L, 163L), day07 = c(148L, 419L, 392L, 387L, 423L, 128L, 499L, 236L, 143L, 137L, 120L, 185L, 181L, 143L, 240L, 168L), day09 = c(291L, 507L, 453L, 342L, 472L, 162L, 516L, 347L, 112L, 122L, 122L, 145L, 150L, 164L, 185L, 164L), day09 = c(239L, 559L, 421L, 368L, 507L, 163L, 485L, 235L, 233L, 140L, 157L, 201L, 177L, 150L, 208L, 128L), day11 = c(192L, 420L, 355L, 355L, 458L, 222L, 472L, 432L, 113L, 102L, 94L, 131L, 162L, 119L, 138L, 129L), day11 = c(172L, 415L, 381L, 429L, 456L, 438L, 535L, 450L, 137L, 174L, 141L, 258L, 192L, 193L, 208L, 218L), day12 = c(235L, 511L, 394L, 373L, 519L, 307L, 500L, 509L, 106L, 120L, 120L, 114L, 170L, 148L, 153L, 135L), day12 = c(153L, 464L, 444L, 501L, 570L, 252L, 497L, 326L, 150L, 135L, 166L, 160L, 162L, 188L, 140L, 182L) ) We calculate the correlation between any two daily sets of values. In fact, we can calculate the correlation between all possible pairs of daily values. This leaves us with a large number of unique correlation coefficients. We then derive from these an overall average correlation coefficient for use in Monte Carlo function. #create a full correlation matrix cormat &lt;- cor(bloodGlucose) #remove lower half of matrix cormat[lower.tri(cormat)] &lt;- NA #remove matrix diagonal cormat[cormat==1.0000000] &lt;- NA How correlated are the glucose levels in the Jaxwest7 data set? #calculate the average correlation coefficient among all the correlations in the Jaxwest7 glucose level data set mean(cormat, na.rm=T) ## [1] 0.7665732 #phew! What does this value mean? First, it can be show that when the value of the correlation coefficient between the variables \\(X,Y\\) is \\(r\\), then the relationship between each pair of \\(x_i, y_i\\) values in the set is \\[y_i=x_i\\times r+y_i\\sqrt{1-r^2}\\] Intuitively, this should make sense. You can see, when \\(r=0\\), then \\(y_i=y_i\\). When \\(r=1\\), then, \\(y_i=x_i\\)) So the correlation coefficeint from the Jaxwest7 data set means that within each subject in our experiment the expected correlation between pre-drug glucose concentrations and post-drug glucose concentrations is 0.7666. Step 2: Initialize the Monte Carlo with estimates for the measurement values. We start with the mean and sd values for the pre-drug blood glucose measurements. Their estimates are derived from the placebo group in the Jaxwest7 data set, rounded to 380 and 120, respectively. A scientifically-meaningful effect of the drug would be a 50% reduction in glucose. We want to set up an experiment that can detect that effect. The expected correlation between pairs of measures is 0.7666, rounded to 0.75. #Sampled population paramemters # pre-drug measurements mean1 &lt;- 185 sd1 &lt;- 120 # post-drug response mean2 &lt;- 380 sd2 &lt;- 120 r &lt;- 0.75 k &lt;- sqrt(1-r^2) # number of paired measures pairs &lt;- 3 Step 3: This step sets the arguments in the t-test function. Even though we predict a reduction in glucose, we’ll test this as a two-tailed hypothesis. It’s a little more stringent. The t.test function needs to be set for paired=TRUE so that it runs the appropriate test. #t-test function arguments alt&lt;- &quot;two.sided&quot; pairing &lt;- TRUE var &lt;- TRUE alpha &lt;- 0.05 Step 4: Declare the number of simulations. The larger the number of simulations, the more accurate will be the power calculation. Also set up an empty vector to fill with p-values, as they are generated each cycle. nSims &lt;- 10000 #number of simulated experiments p &lt;- c() Step 5: Re-simulate and re-run the t-test nSims times. The y1 and y2 vectors are each a set of randomly generated values for the post- and pre-drug measurements, respectively. Both measures need to be simulated as random variables. But the y2 vector needs to be corrected for its correlation with y1. NOTE The correlation only works by always simulating the lower of two mean values as y1. for(i in 1:nSims){ #for each simulated experiment y1&lt;-rnorm(n = pairs, mean = mean1, sd = sd1) #produce n simulated participants #with mean and SD y2&lt;-rnorm(n = pairs, mean = mean2, sd = sd2) #produce n simulated participants #with mean and SD #correlated y2 &lt;- r*y1+k*y2 z&lt;-t.test(y1,y2, alternative=alt, paired=pairing, var.equal=var, conf.level=1-alpha) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } Step 6: Calculate power as the fraction of p-values less than 0.05. # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 5937 power &lt;- hits/nSims; power ## [1] 0.5937 Step 7: Visualize the p-value distribution. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) library(pwr) pwr.t.test(n=6, d=1.58, sig.level=0.05, type=&quot;paired&quot;, alternative=&quot;two.sided&quot;) ## ## Paired t test power calculation ## ## n = 6 ## d = 1.58 ## sig.level = 0.05 ## power = 0.867253 ## alternative = two.sided ## ## NOTE: n is number of *pairs* pwr.t.test(n=8, d=1.58, sig.level=0.05, type=&quot;two.sample&quot;, alternative=&quot;two.sided&quot;) ## ## Two-sample t test power calculation ## ## n = 8 ## d = 1.58 ## sig.level = 0.05 ## power = 0.8358049 ## alternative = two.sided ## ## NOTE: n is number in *each* group "],
["tdist.html", "Chapter 22 t Distributions 22.1 dt 22.2 pt 22.3 qt 22.4 rt", " Chapter 22 t Distributions Sample means are a statistical model most appropriate when applied to groups of measured continuous data. Student’s t statistic is a transformation of measured data as a ratio of a sample mean to its standard error. Therefore, Student’s t distributions are continuous probability models used for comparing the signal to noise ratios of sample means. The t-distribution is used widely in experimental statistics, a) for experiments that compare one or two variables with t-tests, b) for post hoc tests following ANOVA, c) for confidence intervals and d) for testing regression coefficients. A sample for the variable \\(Y\\) with values \\(y_1, y_2,...,y_n\\), has a sample mean: \\(\\bar y =\\sum_{i=1}^ny_i\\). The degrees of freedom for the sample mean is \\(df=n-1\\). The sample standard deviation is \\(s=\\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{df}}\\) and the standard error of the mean for a sample is \\(sem=\\frac{s}{\\sqrt n}\\) The t-distribution can be scaled in three different ways, depending upon the experimental design: The t scale units are \\(sem\\) for a one sample t test: \\(t=\\frac{(\\bar y-\\mu)}{sem}\\) where \\(\\mu\\) is a hypothetical or population mean for comparison. \\(sedm\\) for a two sample unpaired t test: \\(t=\\frac{\\bar y_A-\\bar y_B}{sedm}\\) where \\(\\bar y_A\\) and \\(\\bar y_B\\) are the means the uncorrelated groups A and B comparison, \\(s_p^{2}\\) is the pooled variance and \\(sedm=\\sqrt{\\frac{s_p{^2}}{n_A}+\\frac{s_p{^2}}{n_B}}\\) is the standard error for the difference between the two means. \\(sem_d\\) for a two sample paired t test: \\(t=\\frac{\\bar d}{sem_d}\\), where \\(\\bar d\\) is the mean of the treatment differences between correlatd pairs whose variance is \\(s_d^{2}\\), and \\(sem_d=\\sqrt\\frac{s_p^{2}}{n}\\). 22.1 dt dt is a continous probability density function of the \\(t\\) test statistic. \\[p(t)=\\frac{\\Gamma(\\frac{df+1}{2})}{\\sqrt{df\\pi}\\Gamma(\\frac{df}{2})}(1+\\frac{t^2}{df})^{-(\\frac{df+1}{2})}\\] Thedtfunction takes two arguments, a value of \\(t\\) derived from an experimental dataset, and also a value for the \\(df\\) of the sample. Let’s assume a simple one-sample t test was performed. The sample had 3 independent replicates, and thus 2 degrees of freedom. The value for \\(t\\) calculated from the test is 3.3. The exact probability for that value of \\(t\\) is: dt(3.3, 2) ## [1] 0.0216083 That is not a p-value. Alone, a single probability value from a continuous distribution such as this is not particularly useful. But a range of \\(t\\) values can be interesting to model. Note how this is a continuous function, thus we draw a line graph rather than columns. df &lt;- 2 t &lt;- seq(-5, 5, 0.001) data &lt;- data.frame(dt=dt(t, df)) g &lt;- ggplot(data, aes(x=t, y=dt))+ geom_line()+ scale_x_continuous(breaks=seq(-5,5,1)) + xlab(&quot;t&quot;) +ylab(&quot;p(t)&quot;); g A couple of important features of the \\(t\\) probability density function: 1) there is a unique \\(t\\) distribution for every sample size, 2) the t distribution approaches the normal distribution with larger sample sizes. Here’s a plot comparing a sample size of 3 (\\(df=2\\)), 6 (\\(df=5\\)), 51 (\\(df=50\\)) and the normal distribution. Relative to the normal distribution, the \\(t\\) distributions at these \\(df\\) are “heavy” shouldered. It’s as if a finger is pressing down from the top, spreading the distribution on the sides. This has the effect of increasing the area under the curves, relative to the normal distribution, at more extreme values on the x-axis. Increase the \\(df\\) for the blue-colored plot. At what values do you think it best approximates the normal distribution? g + stat_function(fun=dnorm, args=list(mean=0, sd=1), color=&quot;red&quot;) + stat_function(fun=dt, args=list(df=5), color=&quot;blue&quot;)+ stat_function(fun=dt, args=list(df=50), color=&quot;green&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.35, label=&quot;N(0,1)&quot;, color=&quot;red&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.3, label=&quot;t(df=50)&quot;, color=&quot;green&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.25, label=&quot;t(df=5)&quot;, color=&quot;blue&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.2, label=&quot;t(df=2)&quot;, color=&quot;black&quot;)+ labs(x=&quot;t or z&quot;)+ theme_bw() One additional feature of \\(t\\) distributions is the \\(ncp\\) argument, the non-centrality parameter. Full treatment of non-centrality is quite involved and beyond the scope here. Suffice to say that a distribution with \\(ncp&gt;0\\) would differ from a null distribution. Thus, \\(ncp\\) is used when simulating alternative distributions, for example, for the expectation of skewed data in power analysis. df &lt;- 2 ncp &lt;- 1 t &lt;- seq(-5, 5, 0.001) data &lt;- data.frame(dt=dt(t, df, ncp)) g &lt;- ggplot(data, aes(x=t, y=dt))+ geom_line()+ scale_x_continuous(breaks=seq(-5,5,1)) + xlab(&quot;t&quot;) +ylab(&quot;p(t)&quot;); g 22.2 pt If given a \\(t\\) ratio from a comparison and also the \\(df\\) for the test, pt can be used to generate a p-value. As the cumulative probability function for the \\(t\\) distribution dt returns the area under the curve when given these arguments. Thus, about 96% of the area under the curve is to the left of a \\(t\\) value of 3.3 at \\(df\\)=2, and about 4% of the AUC is to the right of that value. pt(q=3.3, df=2, lower.tail =T) ## [1] 0.9595762 pt(q=3.3, df=2, lower.tail=F) ## [1] 0.04042385 That’s precisely what is depicted graphically here, with navy representing the lower tail and green the upper tail of the cumulative function on either side of \\(t_{df2}=3.3\\): ggplot() + stat_function(aes(x=-5:5), n=150, geom=&quot;line&quot;, fun=dt, args = list(df=2), color=&quot;black&quot;, size=2) + stat_function(aes(x=-5:5), n=150, fun=dt, args=list(df=2), xlim = c(-5, 3.3), geom = &quot;area&quot;, fill= &quot;navy&quot;) + stat_function(aes(x=-5:5), n=150, fun=dt, args=list(df=2), xlim = c(3.3, 5), geom = &quot;area&quot;, fill= &quot;green&quot;) + scale_x_continuous(breaks=seq(-5,5,1)) + xlab(&quot;t&quot;) +ylab(&quot;p(t)&quot;) 22.3 qt The inverse cumulative function qt is most useful as a tool to generate critical value limits. This is a particularly important function given it’s use in constructing confidence intervals. For example, the two sided, 95% critical limits for \\(t_{df2}\\) are: qt(.025, 2) ## [1] -4.302653 qt(0.025, 2, lower.tail=F) ## [1] 4.302653 Whereas each of the one-sided 95% critical limits \\(t_{df2}\\) are: qt(0.05, 2) ## [1] -2.919986 qt(0.05, 2, lower.tail=F) ## [1] 2.919986 The inverse cumulative distribution is shown here: df &lt;- 2 x &lt;- seq(0, 1, 0.01) data &lt;- data.frame(qt=qt(x, df)) g &lt;- ggplot(data, aes(x=x, y=qt))+ geom_line()+ scale_x_continuous(breaks=seq(0,1,.1)) + xlab(&quot;p&quot;) +ylab(&quot;t&quot;); g 22.4 rt Finally, the rt function can be used to simulate a random sample of t values for a distribution with \\(df\\) degrees of freedom. For example, here are 5 t values from a \\(df2\\) and another 4 from \\(df20\\). You’re more likely to get crazy outliers from the former rather than the latter. set.seed(12345) rt(5, 2) ## [1] 0.5514286 -0.2275454 4.1377301 -1.1483718 -0.4579998 rt(5, 20) ## [1] -0.28197205 -0.06705986 -0.28785784 0.27207442 1.66794394 "],
["simcorrelation.html", "Chapter 23 Simulating correlated variables 23.1 Estimating correlation between two variables 23.2 Simulating correlated variables 23.3 Monte Carlo simulation", " Chapter 23 Simulating correlated variables library(pwr) library(tidyverse) Experimental designs involving paired (or related/repeated) measures are executed when two or more groups of measurements are expected to be intrinsically-linked. Take for example, a before and after design. A measure is taken before the imposition of some level of a predictor variable. Then the measure is taken afterwards. The difference between those two measures is the size of the effect. Those two measures are intrinsically-linked because they arise from a common subject. Subjects are tuned differently. You can imagine a subject who displays a low basal level of the measure will generate a low-end response after some inducer, whereas one with a high basal level will generate a high-end response. Statistically, these intrinsically-linked measurements within such designs are said to be correlated. Monte Carlo simulations of experimental power afford the opportunity to account for the level of correlation within a variable. Building in an expectation for correlation can dramatically impact the expected power, and thus the sample size to plan for. 23.1 Estimating correlation between two variables How to estimate correlation? Inevitably you’ll run an experiment where the actual values of the dependent variables, at first blush, differ wildly from replicate to replicate. But on closer inspection, a more consistent pattern emerges. For example, an inducer seems to always elicits close to a 2-fold response relative to a control, and this response is consistently inhibited by about a half by a suppressor. That consistency in the fold-response, irrespective of the absolute values of the variable, is the mark of high correlation! Here are some data to illustrate this problem. Four independent replicates of the same experiment that measures NFAT-driven luciferase reporter gene output, on each of 4 different passages of a cultured cell line. The data have several other treatment levels, but those corresponding to vehicle and drug represent negative and positive responses, respectively. Luciferase reacts with luciferin to produce light. The values here are in arbitrary light units on a continuous scale beginning at zero and linear for up to at least 5 orders of magnitude higher. Thus, values of the variable can be assumed to be normally-distributed. Here’s the experimental data: Table 23.1: NFAT-Luciferase reporter values of different replicates. P11, P12… represent different passages of a cell line. id vehicle drug P11 20.2 38.3 P12 5.7 9.1 P13 2.1 3.6 P14 9.9 15.5 The data show that the luciferase values in response to vehicle wanders substantially across passages over a 10-fold range. Yet the drug response as a ratio to the vehicle is more consistent from passage to passage. In fact, the two variables, vehicle and drug, are actually very highly correlated: cor(df$vehicle, df$drug) ## [1] 0.9955158 ggplot(df, aes(vehicle, drug))+ geom_point(size=4, color=&quot;#012169&quot;) This example points to how you can derive an estimate for the correlation coefficient between two variables. Simply plot out their replicates as \\(XY\\) pairs and calculate their correlation coefficient using R’s cor function. Where do you find values for these variables? They can come from pilot or from published data. 23.2 Simulating correlated variables It can be shown that when the correlation coefficient between a pair of random variables \\(X, Y\\) is \\(r\\), then for each \\(x_i, y_i\\) pair, a correlatd value of \\(y_i\\) can be calculated as \\(z_i\\) by \\[z_i=x_ir+y_i\\sqrt{1-r^2}\\] Thus, we can first simulate a random pair of \\(X,Y\\) values, then convert the values of \\(Y\\) into \\(Z\\), such that the \\(X,Z\\) values are correlated. Using the luciferase example above, here’s some code to accomplish that. Each pair is initially uncorrelated, but then becomes correlated after using the relationship above. There is a slight twist in this. When using an rnorm function with the means and sd estimates from the table above, negative values will be produced. However, the luciferase values are a ratio scale, with an absolute 0 value. The code below uses the absto simulate only positive values. This generates a skewed normal distribution #first simulate and view uncorrelated random variables set.seed(1234) x &lt;- abs(rnorm(10000, 10, 8)) y &lt;- abs(rnorm(10000, 17, 15)) cor(x,y) ## [1] 0.004357573 #scatter plot the simulated vectors ggplot(data.frame(x,y), aes(x,y))+ geom_point() #now convert y to z, so that it correlates to x r=0.99 k&lt;- sqrt(1-r^2) z &lt;- r*x+k*y #confirm the correlation ggplot(data.frame(x,z), aes(x,z))+ geom_point() cor(x, z) ## [1] 0.9673284 #explore the distribution of z ggplot(data.frame(x,z))+ geom_histogram(aes(z))+ geom_histogram(aes(x)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 23.3 Monte Carlo simulation Here’s a Monte Carlo simulation of a paired t-test between an A and a B group. The “true” effect size programmed to be very modest. The code also factors in a fairly strong correlation between the two measures of the variable. #Initial Parameters # sample A intial true parameters nA &lt;- 3 meanA &lt;- 1.5 sdA &lt;- 0.5 # sample B intial true parameters nB &lt;- 3 meanB &lt;- 2.0 sdB &lt;- 0.5 alpha &lt;- 0.05 nSims &lt;- 10000 #number of simulated experiments p &lt;-numeric(nSims) #set up empty container for all simulated p-values # correlation coefficient r &lt;- 0.8 # the monte carlo function for(i in 1:nSims){ #for each simulated experiment x&lt;-rnorm(n = nA, mean = meanA, sd = sdA) #produce n simulated participants #with mean and SD y&lt;-rnorm(n = nB, mean = meanB, sd = sdB) #produce n simulated participants #with mean and SD #correlated w &lt;- r*x+sqrt(1-r^2)*y z&lt;-t.test(x,w, paired=T) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } # the output hits &lt;- length(which(p &lt; alpha));hits ## [1] 7027 power &lt;- hits/nSims;power ## [1] 0.7027 #now plot the histogram #main=&quot;Histogram of p-values under the null&quot;, hist(p, col = &quot;blue&quot;, ylim = c(0, 10000), xlim = c(0.0, 1.0), main =&quot;Histogram of simulated p-values&quot;, xlab=(&quot;Observed p-value&quot;)) The result as written above is a bit underpowered, but not too shabby. Now run the code by dialing down the correlation to an r = 0. How much more underpowered is the planned experiment? Factoring in the correlation between variables makes a huge difference. "],
["introanova.html", "Chapter 24 Introduction to ANOVA 24.1 Factors and levels 24.2 ANOVA models: One-, Two-, and Three-way 24.3 ANOVA inference protocol 24.4 ANOVA calculations 24.5 Completely randomized or related measures 24.6 Two-way ANOVA 24.7 Other ANOVA models 24.8 Alternatives to ANOVA", " Chapter 24 Introduction to ANOVA library(tidyverse) library(RColorBrewer) The choice of any statistical design and analysis is always driven by the type of outcome and predictor variables involved. Recall that all variables are either continuous or discrete. Furthermore, it’s helpful to think of outcome variables further classified as either measured, ordered or sorted, where measured variables are continuous, and ordered and sorted are discrete. Once the dependent variable is deemed to be continuous measured, the experimental design model is determined by the number of explanatory groups involved. If the grouping factor is discrete, use t-tests with two or fewer. Figure 24.1: ANOVA heuristic The analysis of variance (ANOVA) is a method to design and evaluate experiments in which the predictor variable(s) are discrete factors for three or more groups and when the outcome variable is on some continuous measured scale. ANOVA is also univariate, in so far as the analysis involves only a single outcome variable. The validity of an ANOVA depends upon fulfilling the following assumptions: * Every replicate is independent of all others. * Some random process is used when generating measurements, * The distribution from which the outcome variable is derived is continuous random normal. * The variances of the groups are approximately equal. When the first two of these assumptions cannot be met, it’s a scientific experience, not an experiment designed for unbiased hypotheses testing. There is no need pretend otherwise by performing statistical testing. In that case, just report descriptive statistics while omitting inference (ie, don’t do p-values). On the basis of small samples it usually difficult to conclude that the 3rd and 4th assumptions are met. Both affirmative and negative results of tests of normality, tests for homogeneity of variance and outlier tests should be taken with a grain of salt. The smaller the sample size, the larger the grain of salt. Use your judgment. Do you have any reason to believe the variable is not normally distributed? Are you confident the variable being measured in a linear range? If a truly normally distributed variable is measured in the linear range, you can be reasonably confident that you are satisfying these assumptions. Data that appears skewed can be transformed using log or reciprocal functions, followed by ANOVA testing on those transformed values. There are alternative analytic options as will be detailed below. In particular, there is no need to use ANOVA for discrete types of outcome data. ANOVA is not designed to analyze such data. Additionally, regression, either linear or nonlinear, is often a preferred method of analysis when the predictor variable is continuous rather than discrete. ANOVA represents a family of about a dozen or so statistical tests. These differ by how many predictor factors are involved and whether or not the measurements are intrinsically-related such that replicates are completely randomized or repeated/related. ANOVA experiments tend to design themselves since they are fairly intuitive way of asking questions. In fact, ANOVA is probably the most widely used experimental design in the biomedical sciences. If you go to any article in your favorite journal randomly, chances are it will have a figure or table depicting an ANOVA design, which would look something like these. Figure 24.2: Typical graphs depicting some of the different ANOVA designs. The reasons why ANOVA is so popular are very simple. First, ANOVA is versatile. ANOVA allows for testing many groups simultaneously, for one or more factors, each at several levels. You can test for the main effect of each factor, or for interactions between factors. You can also test for differences between specific individual groups, using post hoc analysis. Repeated/related designs are readily accommodated, including mixed designs where one factor is completely randomized and another is related measure within a single multi-factor experiments. Second, ANOVA is efficient. Fewer experimental units are needed to make the same number of pairwise group comparisons than would otherwise be necessary using a t-test-based experimental design. That efficiency can improve modestly as the number of groups increases. As you know, each individual hypothesis test carries a risk of type1 error. In one sense, ANOVA serves as a protocol to detect differences between many groups while ensuring that the overall type1 error, the so-called experimentwise error, remains fixed at the same tolerable threshold we’d set for a single t-test between two groups. 24.1 Factors and levels In ANOVA jargon, predictor variables are classified as “factors”. ANOVA designs are said to be factorial. They are multifactorial if more than a single factor is involved. In other corners, ANOVA is referred to as factorial analysis (which should not be confused with factor analysis). Where some people describe an ANOVA experiment as a “one-way ANOVA” others might describe it as “one-factor ANOVA”. It’s all the same. The factors of ANOVA represent categorical, discrete variables that are each applied at two or more levels. For example, a factor at three levels is a predictor variable that has three discrete values and those three groups in the experiment. Imagine an experiment to explore how a particular gene influences blood glucose levels. Blood glucose levels, a continuous response variable, are measured in experimental units comprising a total of three different genotypes: wild-type, heterozygous knockouts of that gene, and homozygous knockouts of the gene. Here, genotype is a discrete predictor variable, a factor, which has three levels. To run functions related to ANOVA, R requires that your predictor variables are classified as factors in data sets. The following script creates a vector object called genotype. The object is a representation of the genotype variable. The data class for that vector is character because it is comprised of character strings. But look what happens when it is packaged into a data frame called my.factors. R coerces genotype into a factor variable with 3 levels. Which is nice. genotype &lt;- c(&quot;wild-type&quot;, &quot;heterozygote&quot;, &quot;homozygote&quot;) class(genotype) ## [1] &quot;character&quot; my.factors &lt;- data.frame(genotype) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 1 variable: ## $ genotype: Factor w/ 3 levels &quot;heterozygote&quot;,..: 3 1 2 In contrast, this coercion won’t occur if a factor in an experiment represents a variable with continuous scale values. To illustrate what I mean by this, imagine adding a factor to the genotype experiment. We would test for the effect of an antidiabetic drug on blood glucose at 0, 10 and 30 microgram/kg. These effects would be measured at each level of the genotype factor. We would create the vector drug as an object representing the drug variable and its three levels as follows. Note however that here, R does not coerce numeric values as factors: drug &lt;- c(0, 10, 30) my.factors &lt;- data.frame(genotype, drug) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ genotype: Factor w/ 3 levels &quot;heterozygote&quot;,..: 3 1 2 ## $ drug : num 0 10 30 class(drug) ## [1] &quot;numeric&quot; That’s easily fixed using the as.factor function drug &lt;- as.factor(c(0, 10, 30)) my.factors &lt;- data.frame(genotype, drug) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ genotype: Factor w/ 3 levels &quot;heterozygote&quot;,..: 3 1 2 ## $ drug : Factor w/ 3 levels &quot;0&quot;,&quot;10&quot;,&quot;30&quot;: 1 2 3 class(drug) ## [1] &quot;factor&quot; Alternately, we would enter the drug levels as character strings, which would be coerced into a factor at two levels when added to a data frame, like for the first example above: drug &lt;- c(&quot;zero&quot;, &quot;ten&quot;, &quot;thirty&quot;) I bring this up to point out that you can do ANOVA with continuous predictor variables. You just have to treat them as factors. When a continuous predictor variable is used at many levels (eg, a time series, a dose series, etc), regression per se may can be a better alternative to ANOVA. Regression allows for capturing that additional information encoded within continuous variables. But that decision is more scientific than it is statistical. For example, you would use regression rather than ANOVA if the goal is to derive regression parameter estimates, such as slopes for rates or affinity constants. Only use ANOVA when the goal is to determine whether the factor has any effect at all, or if it interacts with some other factor. Be aware that R will bark out error messages at you ff you use continuous variables as factors without actually converting them to factor class objects. 24.2 ANOVA models: One-, Two-, and Three-way If an experiment has only one factor, it a one-way ANOVA design (alternately, “one-factor ANOVA”). If an experiment has two factors, it is a two-way ANOVA or two factor design. Three factors? Three-way ANOVA. These models can also be either completely randomized, repeated/related measures, or mixed. In a completely randomized structure, every level of the factor(s) is randomly assigned to replicates. Every replicate measurement is independent from all others. In a related measures structure, measurements within an experimental unit are intrinsically-linked. Every replicate receives all levels of a factor (eg, before-after, stepped dosing, or when subjects are highly homogeneous, such as cultured cells or inbred animals). Thus, the possible ANOVA models are quite diverse: One-way ANOVA -completely randomized One-way ANOVA -related measures Two-way ANOVA -completely randomized on both factors Two-way ANOVA -related measures on both factors Two-way ANOVA -mixed, one factor completely randomized, the other factor related measures Three-way ANOVA -can be CR, RM or mixed, I should mention that ANOVA for even more than three factors is conceptually possible. However, you are strongly cautioned such large, complex designs have considerable downside. Three-way ANOVA, for example, allows for such a large number of hypotheses to be tested (three different main effects and four possible interaction effects, not to mention the large number of post hoc group comparisons) that it can be difficult to conclude what is responsible for any observed effects. These larger designs also tend to break the efficiency rule, it’s fair to say that three way ANOVA designs tend to be over-ambitious experiments…over-designed and usually under powered for the large number of hypotheses they can test. Designing an experiment as completely randomized or related measures is largely a scientific, not a statistical, decision. What that means is this: measurements are either intrinsically-linked or they are not. Making that determination is a scientific call, based upon the nature of the biological material that you are working with. When you’ve concluded measurements are intrinsically-linked, then the choice must be a related measures design and analysis. Similarly, choosing to run experiments as one-way or two-way ANOVA designs is also scientific. In fact, you would choose the latter mostly to test whether two factors interact. If you are uninterested in whether two factors interact, don’t combine them in an experiment. You might wish to ask your questions using separate one-way ANOVAs, instead. We’ll discuss interaction hypotheses and effects in more detail later. 24.3 ANOVA inference protocol ANOVA can be used inferentially as stand alone test, or as an omnibus test. The test statistic for ANOVA is the F-test, which will be described below. A positive F-test result can be used to infer whether a factor, or an interaction between factors, is effective. Given the example above, I can use positive F-test to conclude that genotype at a given locus influences blood glucose. And just leave it at that, without demonstrating which conditions differ from each other. Alternately, a positive F-test result can be used as an omnibus. Here, a positive F-test implies that at least two group means differ from each other. The positive F-test grants access to explore which group means differ from each other. This is done by making pairwise group comparisons. The decision to use the F-test as a stand alone or as an omnibus is driven by your scientific objectives. Figure 24.3: ANOVA work flow These “post hoc” pairwise comparisons are, essentially, any of several variations on the t-test designed to adjust p-values on the basis of the multiple comparisons. The choices of groups to compare after the F-test are driven by scientific, rather than statistical, reasoning. You can compare all groups to each other, or you can compare a much more limited subset of groups. What is important, statistically, is to make adjustments to the p-value threshold given all the comparisons made, so that the experimentwise type1 error does not exceed your declared threshold (usually 5%). In an experiment whose number of groups equals \\(k\\), there are a total of \\(\\frac{k(k-1)}{2}\\) possible comparisons to make. Let’s use the simplest case of a \\(k=3\\) groups ANOVA as an example. There are \\(C=\\frac{3(3-1)}{2}=3\\) comparisons that can be made. Using the Bonferroni correction (\\(p_{adjust}=\\frac{0.05}{C}=0.01667\\)), only comparisons between 2 groups whose p &lt; 0.0167 would test as different from each other. 24.4 ANOVA calculations The simplest way to think about ANOVA is that it operates like a variance budgeting tool. In the final analysis, the higher the ratio of the variance associated with the grouping factor(s) compared to the residual variance, the more likely that some group means will differ. ANOVA uses the least squares method to derive and account for sources of variation within a data set. Recall that the variance of a random variable \\(Y\\) is estimated through sampling, and calculated by dividing the sum of its squared deviates, \\(SS\\), by the sample degrees of freedom (df). \\[var(Y)=\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{n-1}=\\frac{SS}{df}=MS\\] In ANOVA jargon the variance is also commonly referred to as the mean square \\(MS\\), illustrating that variance can be thought of as an averaged deviate. Now, that formula only illustrates how variance is calculated for a single sample group. What about multiple groups? As you might imagine, we have to incorporate information from all of the groups. To begin to understand that, recognize that all ANOVAs, irrespective of the specific design have two fundamental sources of variation: Variation due to the experimental model, which is specified by the nature of the predictor variables. Residual variation, which is variation that cannot be explained by predictor variables. A useful property of sums of squared deviates is that the total variation within an experiment, whether due to known or known sources, can be accounted for. That total variation within an experiment can be expressed as the sum of the squared deviates, and it is the sum of the squared deviates for the model and residual components: \\[SS_{total}=SS_{model}+SS_{residual} \\] Perhaps it helps if you first think about this visually. Let’s imagine a simple one-way completely randomized ANOVA data set that looks like the graph below. There is only one factor, at three levels. Each group has five independent replicates, for a total of 15 replicates within the entire experiment. We would express the model for this experiment as \\[SS_{model}=SS_{genotype} \\] and thus \\[SS_{total}=SS_{genotype}+SS_{residual} \\] The graph illustrates each data point, the means for each group (black bars) and the grand mean of the sample (gold bars). You can readily imagine the distances from the data points to the group means and to the grand means. You can also appreciate and the distances from the group means to the grand mean. You probably have a harder time visualizing the squares of those distances. My mind sees it Euclidean (geometrically). Larger distances, squared, lead to bigger boxes! The bigger the boxes, the greater that replicate contributes to the variance. With that picture in mind, think of the variances within the experiment as follows: Total variance: average squared distances of the all the points to the grand mean Model variance: weighted average squared distances of group means to the grand mean Residual variance: average squared distances of the points to the group means Figure 24.4: A completely randomized one way ANOVA, the genotype factor has three levels. Gold bar = grand mean, black bar = group means 24.4.1 Sums of Squares partitioning The first step in an ANOVA involves partitioning the variation in a data set using sums of squares. In this experiment, there are \\(i=1, 2..n\\) independent replicates. There are also \\(j=1, 2..k\\) groups. The total sum of squares is the sum of the squared deviation from all data points to \\(\\hat y\\), which is the grand mean of the sample. \\[SS_{total}=\\sum_{j=1}^k\\sum_{i=1}^n(y_i-\\hat y)^2\\] The sum of squares for the genotype effect is sum of the weighted squared deviation between the group means, \\(\\bar y_j\\) and the grand mean. Here, \\(n_j\\) is the sample size within the \\(j^{th}\\) group. \\[SS_{genotype}=\\sum_{j=1}^kn_j(\\bar y_j-\\hat y)^2\\] Some software refers to this variation as the “treatment” sum of squares. Parenthetically, let’s pause to reflect for a moment to consider one consequence of how that equation shows the weighting of group deviation by sample size. The level of model variation can skew to one group when its sample size differs markedly from the others. This explains why you want to keep group sizes reasonably balanced, or roughly equivalent, when designing experiments. Finally, the residual sum of squares is calculated as the sum of the squared deviation between replicate values and group means, \\[SS_{residual}=\\sum_{j=1}^k\\sum_{i=1}^n(y_i-\\bar y_j)^2\\] which, because the total variation amount of is fixed, can also be calculated as follows: \\[SS_{residual}=SS_{total}-SS_{genotype}\\] In some software residual variation is referred to as “error”. The term “error” arises from ANOVA theory, which holds that the true population means represented by these sample groups are “fixed” in the population. Thus, any variation associated with our estimate must be in error. Residuals are the measurements of that “error”. Perhaps you can intuit a few things. First, the residual variation is the variation unaccounted for by the model. Meaning that whatever its causes, they are not under experimental control. Second, if the variation around each group mean remains similar, but as the group means differ from each other more, the greater the fraction of the overall variation that will be associated with the model, and the less that will be associated with the residual. Third, when the noise around those group means increases, less of the total variation will be associated with the model of group means, and the more with the residual. In other words, noisy experiments tend to hide detectable differences between means, while clean experiments favor detecting these differences. Similarly, predictors that lead to large effects on means If that seems bloody obvious to you, and it is simple, then you should not have any problem processing how ANOVA works. The following two graphs emphasize these observations. In the null graph, the group means are roughly equivalent and very nearly the same as the grand mean. There’s very little model variation. Most of the variation is in the residuals. In the effective treatment graph, where the means truly differ because I coded them to differ, the residual variation is about the same as the null. But you can see there is a lot more model variation, at least compared to the null graph. 24.4.2 Degrees of freedom Again, variance is an averaged deviation. To calculate a variance we’ll need to divide the sum of squares for a component by its degrees of freedom. Just as sum of squares are calculated differently depending on whether it is total, or model or residual, so too are degrees of freedom. The theory behind \\(df\\) is a bit more complicated than this, as a general rule, we lose a degree of freedom every time the calculation of a mean is involved in determination of a given \\(SS\\). The basic idea is this: For that mean value to be true, one of the replicates must remain fixed, while all the others are free to vary. The degrees of freedom for total variance are \\(df_{total}=N-1\\). We use all 15 replicates, \\(N\\), to calculate \\(\\hat y\\), the grand mean. We lose a degree of freedom because for that grand mean to be true, one of those replicate values must be fixed while the others are free to vary. We have \\(k\\) groups. The degrees of freedom for the genotype model variance are \\(df_{genotype}=k-1\\), because the calculation is based upon the group means, two of which can be free to vary. The residual degrees of freedom are \\(df_{residual}=N-k\\). 24.4.3 The mean squares The mean squares are ANOVA jargon to represent variances, and variance can be thought of as averaged variation. Total variance is \\[MS_{total}=\\frac{SS_{total}}{df_{total}} \\] The variance associated with the model is \\[MS_{model}=\\frac{SS_{model}}{df_{model}} \\] And the residual variance is \\[MS_{residual}=\\frac{SS_{residual}}{df_{residual}} \\] 24.4.4 The ANOVA table The typical ANOVA table lists the following: source of variation, its \\(df\\), * its \\(SS\\), * its \\(MS\\) * an F-test, where appropriate * a p-value from the F-test ANOVA functions in R vary in their output. But here’s the ANOVA table output for the data in the last previous figure: anova(lm(blood_glucose ~ genotype, data)) ## Analysis of Variance Table ## ## Response: blood_glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 2 21176.9 10588.4 22.979 7.877e-05 *** ## Residuals 12 5529.4 460.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 24.4.5 The F-test F-tests are the first step in drawing inference in ANOVA. The value of F is the ratio of two variances. For the result in the ANOVA table above \\(F=\\frac{10588.4}{460.8}=22.979\\). The variance associated with the genotype model is 22.979x greater than the residual variance. The p-value is derived from an F probability distribution with 2 and 12 degrees of freedom. The result in the table can be mimicked using R’s pf function): pf(22.979, 2, 12, lower.tail=F) ## [1] 7.87784e-05 The example above is a simple one factor completely randomized model for which there is only one F test, because it has only one component. A two-way ANOVA would have three F tests if completely randomized. An F test will pop into the ANOVA table when a factor is treated as related/repeated measures. Three-way ANOVA can have about a dozen F tests! 24.4.6 Post-hoc group comparisons An final stage of ANOVA, which is optional but done very commmonly, is to run what are essentially t-test comparisons between groups. The goal is to see which specific groups actually differ from each other. The comparisons to make should be driven by scientific judgement. For example, an experiment with \\(k\\) groups may be designed to test which of several groups differ from a negative control. The total number of comparisons that could be made are \\(\\frac{k(k-1)}{2}\\). Yet, only a specific \\(k-1\\) subset are of any interest. There are several ways to run these post-hoc group comparisons. What is most important, however, is these be done in a way that keeps the FWER below the pre-set type1 error threshold. What this means 24.5 Completely randomized or related measures Up until now we’ve discussed ANOVA in its simplest use case, the one-way completely randomized (CR) ANOVA. Related measures (RM) ANOVA is done when the measurements are not completely independent, but instead are intrinsically-linked. Examples of intrinsically-linked subjects include identical human twins, before and after on a single subject, all plates and wells from a single batch or passage of a cell culture, a protein preparation from a single batch, a single cell in culture, split tissues from one animal subject, and litter mates of isogenic animal strains. There are certainly others. Since an RM design involves taking multiple measurements from each of the same replicates, each replicate may vary randomly. That random variation can be accounted for, too. That source of variation is no longer in the residual error term, but can be taken right into the model. 24.5.1 The problem of lost data in related measures designs The CR vs RM design decision has a few important consequences. First, when within-subject correlation is high, RM are much more efficient and less costly to produce. How much more? You can run Monte Carlo simulations to establish this for virtually any set of conditions. Second, over the course of any experiment it is possible to lose specific response values here and there. For example, a data value may be lost due to a bad lane in a replicate western blot, you accidently throw away a tube from a series, or any of a number of such primitive errors. CR ANOVA is tolerant of such losses. That leads to a missing replicate, and an unbalanced data sets, but it’s only one or a few values out of many. That is not the case with related measures designs. All of the values for every level of every factor for every replicate must be included. If any values are missing for a given replicate, all of the remaining values for that replicate either have to be censored, or the missing values should be imputed. The missing data problem becomes amplified in two way and three way related measures ANOVA! Those experiments tend to have more groups, meaning more data is at risk of being censored. Researchers often ask if it is reasonable to ‘flip’ to a completely randomized analysis when they notice too many values are missing from their data set. No, it is not reasonable. The type of experimental design is scientifically-driven. Intrinsically-linked measurements are not independent, and should not be analyzed as if they are independent. To do so violates one of the two primary assumptions of the statistical analysis. There are a few options to deal with this, and both happen in planning. First, where possible, include an extra replicate or two as a hedge over what the power analysis suggests is necessary. Second, don’t make a RM design too over-ambitious. Limit the number of levels to that which is scientifically important. Third, be aware of the risk of lost values. Is the experimental protocol difficult? Are any protocol steps at high risk of failure? Are there any intrinsic barriers to efficiently collecting the data? 24.6 Two-way ANOVA This is a method to investigate the effects of two factors simultaneously. Thus, the variation associated with each factor can be partitioned. This also allows for assessing the variation associated with an interaction between the two factors. What is an interaction? Simply, it is a response that is greater (or lesser) than the sum of the two factors combined. Let’s go back to the genotype blood_glucose problem. We’ll add a factor, and simplify the study a bit. We’re interested in a gene that, when absent, raises blood glucose. We’re also interested in a drug that, when present, lowers blood glucose. We have reason to hypothesize that a genotype:drug interaction might exist. For example, the gene might encode a protein that metabolizes our drug, thus impairing the drug’s ability to lower blood glucose. We’ll simulate a \\(2\\times 2\\) experiment that has only the presence or absence of each of these factors. Here’s the data: set.seed(12345) blood_glucose &lt;- round(c(rnorm(5, 100, 20), rnorm(5, 75, 20), rnorm(5, 200, 20), rnorm(5, 100, 20)), 1) drug &lt;- as.factor(rep(rep(c(0, 30),each=5),2)) genotype &lt;- rep(c(&quot;WT&quot;, &quot;KO&quot;), each=10) test &lt;- data.frame(genotype, drug, blood_glucose) y0 &lt;- mean(subset(test, genotype==&quot;WT&quot; &amp; drug==0)$blood_glucose) y30 &lt;- mean(subset(test, genotype==&quot;WT&quot; &amp; drug==30)$blood_glucose) yend0 &lt;- mean(subset(test, genotype==&quot;KO&quot; &amp; drug==0)$blood_glucose) yend30 &lt;- mean(subset(test, genotype==&quot;KO&quot; &amp; drug==30)$blood_glucose) ggplot(test, aes(genotype, blood_glucose, color=drug))+ geom_jitter(size=6, width =0.3)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ stat_summary(fun.y=mean, geom=&quot;point&quot;, shape = 95, size= 15)+ scale_x_discrete(limits=c(&quot;WT&quot;, &quot;KO&quot;))+ labs(y=&quot;blood glucose&quot;)+ geom_segment(aes(x=&quot;WT&quot;, y=y30, xend=&quot;KO&quot;, yend=yend30))+ geom_segment(aes(x=&quot;WT&quot;, y=y0, xend=&quot;KO&quot;, yend=yend0)) ## Warning: Computation failed in `stat_summary()`: ## &#39;what&#39; must be a function or character string An interaction effect can be represented by the differing slopes of those two lines. If the lines are not parallel, it means that the effect of the drug is not the same at both levels of genotype. Or you could say the effect of the genotype is not the same at both levels of the drug. Whatever. The statistical term used to describe such phenomena is that the two factors interacted. Thus, a statistical interaction occurs when the effects of two factors are not the same across all of their levels. Here’s a quick ANOVA table for those data. It has an F test for each of the factors genotype and drug, and an F test for the genotype:drug interaction. All three F-tests are extreme. anova(lm(blood_glucose ~ genotype + drug +genotype*drug, test)) ## Analysis of Variance Table ## ## Response: blood_glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 1 25127.0 25127.0 94.333 4.116e-08 *** ## drug 1 26028.1 26028.1 97.716 3.225e-08 *** ## genotype:drug 1 4845.4 4845.4 18.191 0.0005923 *** ## Residuals 16 4261.8 266.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since the F-test for the genotype:drug interaction is extreme, we can reject the null that there is no interaction between them. Furthermore, the existence of the interaction effect complicates the interpretation of the effects of the genotype and drug factors. Generally, an interaction effect supercedes an effect of either factor, alone. Although genotype seems to have a strong effect on glucose levels, that’s really blunted in the presence of drug. And although the drug seems to reduce glucose, that effect becomes remarkably prominent when the genotype changes. When interactions occur, the effect of one factor cannot be interpreted without condition on the other factor! It might be useful to see the data for the discussion that follows. genotype drug blood_glucose WT 0 111.7 WT 0 114.2 WT 0 97.8 WT 0 90.9 WT 0 112.1 WT 30 38.6 WT 30 87.6 WT 30 69.5 WT 30 69.3 WT 30 56.6 KO 0 197.7 KO 0 236.3 KO 0 207.4 KO 0 210.4 KO 0 185.0 KO 30 116.3 KO 30 82.3 KO 30 93.4 KO 30 122.4 KO 30 106.0 Four sources of variation are accounted for: the main effect of genotype, the main effect of drug, the interaction between drug and genotype, and the residual error. Three F tests were performed. Each of these used \\(MS_{residual}\\) in the denominator and the \\(MS\\) for the respective source in the numerator. How has this variation been determined? The experimental model is as follows: \\[SS_{model}=SS_{genotype}+SS_{drug}+SS_{genotype\\times drug} \\] The grand mean of all the data is computed as before: \\[\\hat y=\\frac{1}{n}\\sum_{i=1}^ny_i \\] There are two factors, each at two levels. Thus, there are a total of \\(j=4\\) experimental groups. Each group has a sample size of \\(n_j=5\\) replicates and represent a combination of predictor variables: WT/0, WT/30, KO/0, KO/30. The mean of each group is \\(\\bar_j\\). Therefore, \\[SS_{model}= \\sum_{j=1}^kn_j(\\bar y_j-\\hat y)^2 \\] represents the total model variation. We can also artificially group these factors and levels further. Two groups correspond to the levels of the genotype factors and two correspond to the levels of the drug factor. Their means are \\(\\bar y_{wt}, \\bar y_{ko}\\) and \\(\\bar y_0, \\bar y_{30}\\), respectively. Each of these groups has a sample size of \\(2n_j\\). These contrived means are used to isolate for the variation of each of the two factors: \\[SS_{genotype}= \\sum n_{wt}(\\bar y_{wt}-\\hat y)^2+n_{ko}(\\bar y_{ko}-\\hat y)^2 \\] \\[SS_{drug}= \\sum n_{0}(\\bar y_{0}-\\hat y)^2+n_{30}(\\bar y_{30}-\\hat y)^2 \\] All that remains is to account for the variation associated with the interaction effect. That can be solved for algebraically: \\[SS_{genotype\\times drug}=SS_{model}-SS_{genotype}-SS_{drug} \\] Because two way ANOVA’s have two factors, one of the factors can be applied completely randomized, and the other can be applied as related measures. Or both factors can be completely randomized, or both can be related measures. 24.7 Other ANOVA models For all ANOVA experiments, irrespective of the design, the total amount of deviation in the data can be partitioned into model and residual terms: \\[SS_{total}=SS_{model}+ SS_{residual}\\] What’s interesting is that different ANOVA experimental designs have different models. We’re interested in two factors, factorA and factorB, and have the ability to study each at multiple levels. The interaction between factorA and factorB is \\(A\\times B\\) One way CR: \\(SS_{model}=SS_{factorA}\\) One way RM: \\(SS_model=SS_{factorA}+SS_{subj}\\) Two way CR: \\(SS_{model}=SS_{factorA}+SS_{factorB}+SS_{A\\times B}\\) Two way RM on A factor: \\(SS_{model}=SS_A+SS_B+SS_{A\\times B}+SS_{subj\\times A}\\) Two way RM on both factors: \\(SS_{model}=SS_A+SS_B+SS_{A\\times B}+SS_{subj\\times A}+SS_{subjXB}+SS_{subj\\times A\\times B}\\) The big difference between completely randomized and related measure designs is that in the latter, we’re now accounting for the deviation associated with each replicate in the model! Otherwise, that replicate deviation would have been blended into the residuals. This turns out to be a pretty big deal. When that deviation due to the subjects is pulled out of the residual, it lowers the value of the denominator of the F statistic. Thus making the F statistic larger! 24.7.1 R and ANOVA There are a handful of ways to conduct ANOVA analysis on R. These are not necessarily more right or wrong than the others. What is important to know, however, is that they do perform calculations differently under certain circumstances (eg, Type 1 v Type 2 v Type 3 SS calculations). Therefore, they produce distinct results which can be confusing, particularly when comparing R’s results to other software you might be familiar with. This again emphasizes the need to share specific details of the analysis in our publications. In this case, specify using R, specify the R function used, and even specify the type argument used in ezANOVA. Given data and a group of arguments we’ll call foo, R’s ANOVA function options are as follows: anova - A function in R’s base. eg, anova(lm(foo)) aov - A function in R’s base. eg, `aov(foo) Anova - A function in R’s car package. eg, Anova(lm(foo)) ezAnova - A function in R’s ezAnova package, ezAnova(foo) There are others. For example, since ANOVA analyses are also general linear models the same basic problem can also be solved using lm(foo)without ANOVA. Passing an `lm(foo) into an ANOVA function is generally designed to provide you an ANOVA table. For the ANOVA part of this course, we’ll use ezANOVA from the ez package. In particular, it is a bit more straightforward to use than the other options when dealing with related measures and with two-way ANOVA, which are very common in biomedical research. Key Jargon to understand to do ezANOVA in R Don’t confuse ezANOVA’s use of between and within the way it is used elsewhere in ANOVA jargon. Specify a completely random design by defining the ‘between’ variable as your factor name. The between here is meant to imply comparisons between groups. Specify a related measures design by defining the within variable as your factor name. The within here is meant to imply comparisons within replicates. 24.7.1.1 Type of calculation There are three ways ANOVA can be calculated, which are referred to as type I, type II and type III. When an experiment is balanced, which is to say it has equal sample sizes per group, the type of calculation is immaterial. In that case,type I, II and III yield the same output. Unbalanced experiments are those in which the sample sizes of groups are not the same. As long as the differences are not too large, the presence of unbalance is usually not a problem. But it can impact the precise output of different ANOVA functions, depending upon whether they perform type I, II or III calculations. This causes some confusion, particularly when comparing the output of different ANOVA functions in R (eg, Anova vs aov vs anova vs ezAnova) and/or commercial software (eg, SAS, SPSS, Prism). The researcher scratchers her head, wonders which is “correct”. In one sense, they are all correct. Type I, II and II sum of squares calculations are explained here and also here. Suffice to say this is important to not overlook. This serves to illustrate how providing good detail about the software used to analyze data is important for reproducibility. The most significant point to understand is that some commercial software uses type 3 calculations by default. As a consequence, given the same data set, the results from those packages may not coincide perfectly with those of ezANOVA unless using a type = 3 argument in the function. My recommendation is to use type = 2 when interested in testing hypotheses about the main effects of factors, and there is no interest in an interaction if working on a two- or three-way ANOVA data set. That’s because type = 2 is purported to yield consistently higher power for main effects. Use type = 3 when, instead, the experiment is designed to test whether an interaction occurs between factors. When an interaction occurs, the main effects are not interpretable. Type I sum of squares are calculated when using the anova and aov functions of base R. This is otherwise known as “sequential” sum of squares calculation. On multifactor data with those functions, the results can differ given the order by which the factors are argued. Thus, aov(lm(outcome~factorA + factorB)) might yield slightly different results compared to aov(lm(outcome~factorB + factorA)). The idea is to calculate the effect on a factor that is most interesting to you scientifically, while “controlling” for the effect of the other factor. 24.8 Alternatives to ANOVA When the outcome variable is measured and the design is completely randomized, the data can be analyzed using the general linear model with R’s lm function, rather than by ANOVA. This allows for analyzing interaction effects between factors. The results will be the same as ANOVA. If the design has a related measures component, then a linear mixed effects model should be run instead. In that case, use lmer in the lme4 package. Alternately a nonparametric analysis can be performed using either the Kruskal-Wallis (completely randomized) for the Friedman (related measures) test. Bear in mind that there is no nonparametric analog for the two-way ANOVA. Thus, hypotheses related to interaction effects are not testable using nonparametric statistics. Finally there is the generalized linear model (glm) for completely randomized designs or the generalized linear mixed model (glmer) for designs that incorporate related measures, respectively. Each of these allow for testing interactions between factors. These allow for a flexible array of outcome variables. These should be used, rather than ANOVA, when the outcome variable is non-normal or is discrete. For example, these are the tools of choice when the outcome variable is binomial or frequency data and there are 3 or more groups to compare. Additional families are possible. 24.8.1 Screw ANOVA, Just Tell Me How to t-Test Everything OK, fine. This is far from ideal because of the bias it introduces. You don’t have to do ANOVA for an experiment with 3 or more groups (or anything else for that matter–you just have to be able to defend your choices). A major purpose of ANOVA is to maintain an experiment-wise type1 error of 5%. But there are other ways to accomplish this objective. For example, you might skip the ANOVA step and simply run serial t-tests comparing all of the groups in an experiment. Or run t-tests to compare a pre-planned sublist of all possible comparisons. The emphasis here on pre-planning is important. Make decisions ahead of time about what is to be compared, then make only those comparisons. No more and no less. Otherwise, you’re snooping. Once you’re in snooping mode, you’re deeply biased towards opportunistic outcomes. For example, you may run multiple control groups within your experiment to signal that some important aspect of the protocol is working properly, but these controls are otherwise not scientifically interesting (with respect to testing new hypotheses). You may not wish to expend any of your type1 error budget doing comparisons on these controls. With those reservations noted, what follows are two ways to go about this. To begin, if we have \\(k\\) predictor variables and \\(k\\) groups in our experiment it has a total of \\(C=k(k-1)/2\\) possible comparisons that could be made. The pairwise.t.test is the function to use for this purpose. Use it to make the group comparisons that interest you. choosing the p.adjust.method that strikes your fancy. Two of the latter are listed below. 24.8.1.0.1 Bonferroni Correction If \\(C\\) is the number of comparisons to be tested, whether or not it is equal to \\(k(k-1)/2\\), and if \\(\\alpha\\) is the type1 error threshold you’ve set for the entire experiment, then the corrected type1 error threshold for each comparison is \\(\\alpha_c=\\frac{alpha}{C}\\). Thus, you would reject the null hypothesis for any comparison for which a t-test yields a p-value that is less than \\(\\alpha_c\\). 24.8.1.0.2 Holm-Sidak Correction This is a modestly more liberal alternative to the Bonferroni correction. Here, for \\(C\\) comparisons and an experimentwise type1 error threshold \\(\\alpha\\), the corrected per comparison threshold would be \\(\\alpha_c=1-(1-\\alpha)^C\\). Again, for a given comparison reject, the null if its p-value is less than \\(\\alpha_c\\). "],
["fdistr.html", "Chapter 25 The F distribution 25.1 Background 25.2 df 25.3 pf 25.4 qf 25.5 rf", " Chapter 25 The F distribution library(tidyverse) 25.1 Background George Snedecor once said that Student’s t distribution is the “distribution that revolutionized the statistics of small samples.” As the inventor of the F distribution, nobody would have faulted Snedecor for praising himself the same way. The F distribution is heavily used in parametric statistics. The most common use of the F statistic is to examine ratios for two estimators of population variance. In particular, F-tests in ANOVA are ratios of model variances, at given degrees of freedom, to residual variances, a some other given degrees of freedom. Another common use is to F-test nested regression models to determine if the more complex model provides a better fit for the data. When the value of this ratio is extreme, the variances are not equivalent, meaning the variance in an experiment is better explained by factor effects than by residual error. An F distribution represents the null probability distribution for such ratios and is therefore used to test hypotheses involving variances. F is used widely in statistics to answer a simple question: Are two samples drawn from populations that have the same variance? For example, the F statistic serves as an omnibus test for ANOVA, and is used in regression to determine which of two models best fit a dataset, and is also used for normality tests. More generally, the F statistic can be used to analyze a proportion of two random, \\(\\chi^2\\)-distributed variables. 25.1.1 Sample Variance and F’s PDF A sample of size \\(i=n\\) for a random, independent variable \\(X\\) can take on the values \\(x_1, x_2,...x_i\\). The sample mean is \\(\\bar x=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) with \\(df=n-1\\) degrees of freedom. The sum of the residual deviation from the sample mean, also known as the sample “sum of squares”, (\\(SS\\)) is: \\[SS=\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\] The sample variance is: \\[s^2=\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{df}\\] The sample variance is otherwise known as the “mean square” in jargon commonly associated with ANOVA: \\[MS=\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{df}\\] The \\(MS_{df}\\) of a normally distributed random variable \\(X\\) with \\(df\\) will have a \\(\\chi^2_{df}\\) distribution. Now let the normally distributed variables \\(X1\\) and \\(X2\\) have \\(df_1\\) and \\(df_2\\) degrees of freedom, and also variances of \\(MS_{df1}\\) and \\(MS_{df2}\\), respectively. The F statistic is: \\[F=\\frac{MS_{df1}}{MS_{df2}}\\] The probability density function for the F statistic is: \\[f(x)=\\frac{(\\frac{df_1}{df_2})^\\frac{df_1}{2}\\Gamma[\\frac{(df_1+df_2)}{2}]x^{\\frac{df_1}{2}-1}}{\\Gamma[\\frac{df_1}{2}]\\Gamma[\\frac{df_2}{2}][1+(\\frac{df_1x}{df_2})]^{\\frac{(df_1+df_2)}{2}}}\\] 25.2 df R’s function for the F PDF is df and returns a value for the probability of F, given its degrees of freedom. It takes as arguments a value for x, which represents F. x can either be unique value or represent a range of values. Other arguments include df1 and df2, which represent values for the degrees of freedom represented in the numerator and denominator, respectively. There is a unique F distribution for any combination of df1 and df2. The exact probability when F has a value of 2.5 and 2 and 10 degrees of freedom (\\(F_{df_1,df_2=2.5}\\)) is: df(2.5, df1=2, df2=10) ## [1] 0.0877915 The distribution of the F statistic can vary quite markedly depending upon the combination of df1 and df2. For example, let’s imagine the 3 curves below correspond to each of 3 different one-way ANOVA experimental designs. The red distribution represents a null distribution for F for an ANOVA experiment having 3 predictor groups with a sample size of 5 independent subjects per group. The blue distribution represents the null of F for an experiment of 10 groups with 3 replicates per group. The green distribution is the null of F for an experiment with 20 groups, each with 4 replicates. Thus, since the numerator and denominator of the F statistic represent two different populations, the F distribution is extraordinarily flexible in terms of the comparisons that can be made using it! ggplot(data.frame(x=c(0,6)), aes(x)) + stat_function(fun=&quot;df&quot;, args=list(df1=2, df2=12), color=&quot;red&quot;)+ stat_function(fun=&quot;df&quot;, args=list(df1=9, df2=20), color=&quot;blue&quot;) + stat_function(fun=&quot;df&quot;, args=list(df1=19, df2=60), color=&quot;green&quot;)+ labs(x =&quot;F&quot;, y=&quot;p(F)&quot;) 25.3 pf The cumulative distribution function for F returns the cumulative probability under the F distribution for a value of the F statistic and a given pair of degrees of freedom. pf(q=4, df1=2, df2=12) ## [1] 0.953344 A p-value is returned by using the following argument: lower.tail=F. Thus, the probability of an F statistic whose value is 4.0 or larger is: pf(q=4, df1=2, df2=12, lower.tail=F) ## [1] 0.046656 ggplot(data.frame(x=c(0,6)), aes(x)) + stat_function(fun=&quot;pf&quot;, args=list(df1=2, df2=12), color=&quot;red&quot;)+ stat_function(fun=&quot;pf&quot;, args=list(df1=9, df2=20), color=&quot;blue&quot;) + stat_function(fun=&quot;pf&quot;, args=list(df1=19, df2=40), color=&quot;green&quot;)+ labs(x =&quot;F&quot;, y=&quot;p(F)&quot;) 25.4 qf The inverse cumulative probability function for the F distribution is qf. This function will take a probability as an argument, and return the corresponding value of the F statistic for a given pair of degrees of freedom. qf(p=0.95, df1=2, df2=12) ## [1] 3.885294 An F statistic limit for a given p-value can be calcuated using the lower.tail=F argument. qf(p=0.05, df1=2, df2=12, lower.tail=F) ## [1] 3.885294 ggplot(data.frame(x=c(0,1)), aes(x)) + stat_function(fun=&quot;qf&quot;, args=list(df1=2, df2=12), color=&quot;red&quot;)+ stat_function(fun=&quot;qf&quot;, args=list(df1=9, df2=20), color=&quot;blue&quot;) + stat_function(fun=&quot;qf&quot;, args=list(df1=19, df2=40), color=&quot;green&quot;)+ labs(x =&quot;p(F)&quot;, y=&quot;F&quot;) 25.5 rf The rf function can be used to generate n random F statistic values for a given pair of degrees of freedom. rf(n=10, df1=2, df2=12) ## [1] 0.5597275 0.9402234 0.5752848 1.8002392 0.6221102 0.2859954 1.4767106 ## [8] 2.2144931 1.4721874 2.4914325 "],
["onewayanova.html", "Chapter 26 One-way ANOVA Completely Randomized 26.1 Using ezANOVA 26.2 The chickwt data set 26.3 Run the ANOVA 26.4 Post hoc pairwise comparisons 26.5 Reporting the result", " Chapter 26 One-way ANOVA Completely Randomized library(magrittr) library(tidyverse) library(ggformula) library(DescTools) library(ez) library(lsr) If you haven’t yet read the ANOVA big picture, this is worth reading) 26.1 Using ezANOVA R has several functions to run ANOVA. In this course we’re going to use ezANOVA from the ez package because I feel the syntax for defining CR vs RM is a bit more straightforward. In this chapter, we’ll run through an analysis of a one-way completely randomized ANOVA data set as ‘how to’ example. 26.2 The chickwt data set This data set in R’s base. It compares the influence of 6 different types of food sources on chick weight. There is one predictor variable, the factor ‘feed’, which is tested at 6 different levels (the various food sources). There has one continuous outcome variable (weight). We can assume the chicks are outbred, and thus not intrinsically-related, and that they have been randomly assigned to a level of feed, and have been weighed after a period of time. This is a classic one-way completely randomized ANOVA design. This chapter illustrates how to go through an analysis of the data. 26.2.1 Inspect the data The next few scripts involve inspecting the data set, which should always be done prior to running any statistical tests. data(chickwts) #take a look at the data structure, depending upon how you like to view data str(chickwts) ## &#39;data.frame&#39;: 71 obs. of 2 variables: ## $ weight: num 179 160 136 227 217 168 108 124 143 140 ... ## $ feed : Factor w/ 6 levels &quot;casein&quot;,&quot;horsebean&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... chickwts ## weight feed ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean ## 7 108 horsebean ## 8 124 horsebean ## 9 143 horsebean ## 10 140 horsebean ## 11 309 linseed ## 12 229 linseed ## 13 181 linseed ## 14 141 linseed ## 15 260 linseed ## 16 203 linseed ## 17 148 linseed ## 18 169 linseed ## 19 213 linseed ## 20 257 linseed ## 21 244 linseed ## 22 271 linseed ## 23 243 soybean ## 24 230 soybean ## 25 248 soybean ## 26 327 soybean ## 27 329 soybean ## 28 250 soybean ## 29 193 soybean ## 30 271 soybean ## 31 316 soybean ## 32 267 soybean ## 33 199 soybean ## 34 171 soybean ## 35 158 soybean ## 36 248 soybean ## 37 423 sunflower ## 38 340 sunflower ## 39 392 sunflower ## 40 339 sunflower ## 41 341 sunflower ## 42 226 sunflower ## 43 320 sunflower ## 44 295 sunflower ## 45 334 sunflower ## 46 322 sunflower ## 47 297 sunflower ## 48 318 sunflower ## 49 325 meatmeal ## 50 257 meatmeal ## 51 303 meatmeal ## 52 315 meatmeal ## 53 380 meatmeal ## 54 153 meatmeal ## 55 263 meatmeal ## 56 242 meatmeal ## 57 206 meatmeal ## 58 344 meatmeal ## 59 258 meatmeal ## 60 368 casein ## 61 390 casein ## 62 379 casein ## 63 260 casein ## 64 404 casein ## 65 318 casein ## 66 352 casein ## 67 359 casein ## 68 216 casein ## 69 222 casein ## 70 283 casein ## 71 332 casein It is also helpful to calculate some descriptive stats for inspection: cw1 &lt;- chickwts %&gt;% group_by(feed) %&gt;% summarise( mean= mean(weight), median=median(weight), sd= sd(weight), n = n(), var=var(weight) ) cw1 ## # A tibble: 6 x 6 ## feed mean median sd n var ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 casein 324. 342 64.4 12 4152. ## 2 horsebean 160. 152. 38.6 10 1492. ## 3 linseed 219. 221 52.2 12 2729. ## 4 meatmeal 277. 263 64.9 11 4212. ## 5 soybean 246. 248 54.1 14 2930. ## 6 sunflower 329. 328 48.8 12 2385. The means and medians are about equal within each group. That’s a quick and dirty way to suggest no skew. Overall sample size is 71, distributed as 10-14 replicates per group, it’s a little unbalanced. But within acceptable limits. The variances are not equivalent, but are they unequal? Levene’s test in the ANOVA will provide that answer. Plot the data for a look. Simply looking at the data graphically goes a long way to ensuring this is a one-way ANOVA design. Jitter plots are a great way to see group data like this. I like the crossbar geom to overlay some summary stats. ggplot(chickwts, aes(feed, weight))+ geom_jitter(width = 0.2, size=2) + stat_summary(fun.data = mean_sdl, fun.args = list(mult=1), geom=&quot;crossbar&quot;, width=0.2, color=&quot;red&quot; ) + theme_classic() 26.3 Run the ANOVA We can imagine designing an experiment like this with either of 3 experimental objectives in mind. Perhaps we’re interested mostly in whether any feeds are better than others for achieving weight gain? We could answer that by making all possible pairwise comparisons. Since there are 6 levels of the factor feed, that would involve \\(C=\\frac{6(6-1)}{2}=15\\) comparisons(!!) Imagine casein is the standard feed, and we wish to know if any of the other feeds differ from this standard? We would compare casein to every feed. That would involve only 5 comparisons. Perhaps we just want to know if any of the feeds differ in causing weight gain, but we aren’t interested in which specific feeds differ? We could answer that question using the F-test result, and not comparing any groups post hoc. Each of those objectives are scientifically-driven. They should be addressed before running an experiment so that an unbiased analysis is conducted after the data are in. Other than, perhaps, how we order our data in the data set, which of these objectives is true doesn’t influence how we run the ezANOVA function per se. However, the objective will influence which post hoc analysis we conduct. 26.3.1 Run the chickwts One Way ANOVA First, ezANOVA requires a ‘wid’, which is a unique ID variable for each independent replicate. We need to add one to the chickwts data set. Since all the measures are independent, we’ll just do that by row number. At the same time we’ll convert the integer to a factor so ezANOVA won’t bark at us. chickwts$ID &lt;- as.factor(1:nrow(chickwts)) You should look at R’s help for ezANOVA ?ezANOVA to understand these test arguments. The help page is pretty clear for most of these. Since we don’t use the term ‘dependent variable’ much in this course, to be clear, ‘dv’ is the outcome response variable. We have to specify it in the ezANOVA arguments. If measurements for levels of the factor are not instrinsically-linked, if they are distributed to each replicate independently, the design is completely randomized. That factor should be listed in the function using a ‘between’ argument. If measurements for levels of the factor are intrinsically-linked, it is arelated/repeated measures design. List it as a ‘within’ argument, rather than ‘between’. Here, the feed factor is between. Every chick was randomly assigned a level of feed. Notice that ezANOVA is a function. Use it to create a list object called my.ezaov, which has all of the output information. The object name could have been foo. We call specific elements from the my.ezaov object to see the results. my.ezaov &lt;- ezANOVA( data = chickwts, wid = ID, dv = weight, between = feed, type = 2, return_aov = T, detailed = T) ## Warning: Data is unbalanced (unequal N per group). Make sure you specified ## a well-considered value for the type argument to ezANOVA(). ## Coefficient covariances computed by hccm() my.ezaov ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 feed 5 65 231129.2 195556 15.3648 5.93642e-10 * 0.5416855 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 5 65 4389.241 76154.92 0.7492639 0.5896095 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## feed Residuals ## Sum of Squares 231129.2 195556.0 ## Deg. of Freedom 5 65 ## ## Residual standard error: 54.85029 ## Estimated effects may be unbalanced # my.ezaov$ANOVA # my.ezaov$Levene # my.ezaov$aov 26.3.2 Interpreting the One-Way CR ANOVA Output The ezANOVA output prints 3 list objects by default: $ANOVA (which is the first data frame) $Levene's Test for Homogeneity of Variance (which is the 2nd data frame) $aov (which is the end of the console output) In fact, there is a great deal more computed that is not printed, which you can visualize in the console by typing str(my.ezaov). 26.3.2.1 $ANOVA: The ANOVA table For a CR one way ANOVA design, the SS are partitioned as follows, in general: \\(SS_{total}=SS_{model}+SS_{residual}\\). In this example, \\(SS_{model}= SS_{feed}\\). Thus, the ANOVA table summarizes the feed model. The DFn = 5 corresponds to the 6 groups, less 1 degree of freedom (one is lost to calculate mean of groups (sort of)) for the model source of variance. The DFn = 65 corresponds to the degrees of freedom for the residuals (one df is lost per group to calculate group means). Therefore, this ANOVA tests a feed model against a null F distribution with 5 and 65 degrees of freedom. \\(F=MS_{feed}/MS_{residual}=15.3648\\), where \\(MS = SS/df\\). The SS can be found in the $aov output. ges = generalized eta-squared. ges is an effect size parameter for ANOVA. For this particular experimental design, \\(ges=\\frac{SS_n}{SS_n+SS_d}\\). In other words, ges summarizes the variation associated with the model as a fraction of the total variation in the data. Thus, 54.16% of the variation in weight is attributable to the different levels of feed in the experiment. In other words, the model explains 54.16% of the variation in the data. Think of eta-squared, partial eta-squared, and generalized eta-squared as all related to the more commonly understood \\(R^2\\) of regression. They are each calculated differently, but all related to \\(R^2\\) in so far as they serve as estimates for how much of the variation is due to the model. ges takes on values from 0 to 1. Higher values indicate a greater degree of the overall variation is due to the factor tested in the experiment. Having said that, it’s a bit of a Goldilock statistics. You only really begin to appreciate what it means after producing several of them. 26.3.2.2 $aov This table provides the accounting for the sum of squares and degrees of freedom, while calculating the residual standard error. It is somewhat redundant with the $ANOVA table, though the residual standard error can come in handy. DFn=degrees freedom for numerator. k-1, where k = levels of factor. DFd=degrees freedom for denominator. n-k, where n = number of independent replicates. SSn &amp; SSd = sum of squares for model and residual, respectively Residual standard error is a parameter that estimates the precision by which the data fit the model, and is in units of the outcome variable, weight. \\(SE\\) is the square root of the residual variance: \\(S_{y.x}=\\sqrt{\\frac{SS_{residual}}{df_{residual}}}\\) If \\(S_{y.x}\\) were zero, there would be no residuals. The data points would all rest at the value of the group means. The data would fit perfectly to a model of 6 group means at their observed values. \\(S_{y.x}\\) therefore is a descriptive statistic that declares how much error, or the degree by which the data is unexplained by the model. It has some utility for confidence intervals and power analysis. 26.3.2.3 The F test The scientific prediction for this experiment is that chick weights will vary depending upon the type of feed they are grown on. The null is that their weights will be roughly the same, irrespective of food source. ANOVA tests this hypothesis through the variance parameter. The question is whether the variance associated with the model, one of 6 different feed group means, is fractionally greater than the residual variance in the sample. The null statistical hypothesis is that the variance associated with the different levels of feed is less than or equal to the residual variance. Therefore, the alternate hypothesis is the variance associated with feed is greater than residual variance. \\(H_0: MS_{feed}\\le MS_{residual}\\), \\(H_1: MS_{feed}&gt;MS_{residual}\\) Because of the relationship of group means to variance, it is just as valid to express the null hypothesis in terms of the group means, and that can be proven mathematically by a competent statistician: \\(H_0: \\mu_a=\\mu_b=\\mu_c=\\mu_d=\\mu_e=\\mu_f\\) Though, strictly, rejecting the null doesn’t mean that all group means differ from each other, it just means that some of them differ. \\(H_1: \\mu_a\\ne\\mu_b\\ne\\mu_c\\ne\\mu_d\\ne\\mu_e\\ne\\mu_f\\) The F statistic of 15.3648 is extreme for a null F distribution of 5 and 65 degrees of freedom. The very low p-value illustrates this extremeness. The probability of erroneously rejecting the null hypothesis is about 5e-10. We can reject the null and conclude that differences in effect on chick weights exist between this group of feeds. 26.3.2.4 Levene’s test for homogeneity of variance Levene’s test determines whether there is a substantial level of differences in variance between groups. Levene’s test is run as a check to determine if the groups variance is homogeneous, as homoskedasticity is one of the validity assumptions of ANOVA. Levene’s test statistic is calculated as follows: \\[W=\\frac{(n-k)}{(k-1)}\\frac{\\sum\\limits_{i=1}^{k}n_i(\\bar Z_i-\\bar Z)^2}{\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{n_i}(Z_{ij}-\\bar Z_i)^2}\\] where \\(Z_{ij}=|x_{ij}-\\bar x_i|\\) and \\(Z_i\\) are the group means and \\(\\bar Z\\) is the overall mean of \\(Z_{ij}\\). The null hypothesis of the Levene test is rejected when \\(W&gt;F_{(\\alpha,\\ k-1,\\ n-k)}\\), where the F is the critical value. Levene’s test output is a 2nd ANOVA table, and can easily be confused with the ANOVA output. Levene’s test lacks a \\(ges\\) parameter, nor does it have a column that lists the factor name. If the Levene’s F value is low and the p-values is high, as is the case here, we can’t reject the null that the variances are the same. Thus, the variance homogeneity assumption is validated. If this were not the case, you have two options. Option 1: Simply ignore the result. The luck of the draw with small samples can explain group differences in variance, where none really exists. The impact on your inference will be very modest. What’s more important is that the population you are sampling is normally distributed. Do you have reason to think that is not the case? Option 2: Transform the data to homogenize outliers and variance, or switch the analysis to a Kruskal-Wallis nonparametric test. 26.4 Post hoc pairwise comparisons If the ANOVA F test for the factor is extreme you may be interested in knowing which treatment groups differ. That’s achieved by conducting post hoc analysis, which typically involves multiple group comparisons. In other words, post hoc involves testing several hypotheses simultaneously. Each test risks type1 error. Therefore, post hoc testing should be done in a way that keeps the cumulative, family-wise type1 error rate (FWER) below the type 1 error threshold that was set for the experiment; usually 5%. 26.4.1 Overview of options Sometimes we do these experiments without any clear plans of what groups we might want to compare. In those cases, we compare every group to all other groups. Doing so will involve \\(\\frac{k(k-1)}{k}\\) comparisons in an experiment with \\(k\\) groups. That “burns” a lot of alpha, perhaps needlessly, but that’s the price paid for not planning. The tests for that include the Bonferroni (aka Dunn’s) and its cousins, Holm, Hommel, and Hochberg. They differ slightly, and will generate slightly different adjusted p-values given a common set of unadjusted p-values. Choose one. Step-down tests operate a bit differently but do the same thing in terms of making all comparisons, these including the Tukey HSD and Newman-Keuls. When we intend to compare all groups back to one level of the factor, thus making only a subset of all possible comparisons, we use Dunnett’s or the FisherLSD. But it’s just as valid to use any other of these methods. 26.4.1.1 Two imperatives in pairwise testing Since every p-value represents a hypothesis, when simultaneously testing many hypotheses always make adjustments to keep the FWER &lt; 0.05 (or whatever overall type1 error limit you preset for the experiment). Let your scientific questions drive the comparisons you’ll make, and thus your choice of post hoc test. I suggest you worry less about the specific post hoc test, or about gamification of p-values, and think more about whether a comparison is scientifically important. But there are a couple of provisos: 1) Bonferroni (or its cousins) works just fine if you don’t have too many means to compare, 2) Dunnett’s is for comparing two or more means back to a control mean but should not be used in related measures factors, 3) Fishers LSD will give you output to make those one or few key comparison you set out to make, even though you have many other groups in the experiment (ie, several of the other groups are internal controls and not particularly scientifically important). 26.4.1.2 Examples No p-value adjustment: Sometimes it’s useful to generate p-values that are not corrected for multiple comparisons. For example, although you’ve done a simple ANOVA, the intention all along was to test only one hypothesis between two groups. The pairwise.t.test function produces a matrix of all comparisons, within which you can find the p-value for the comparison of interest. The script below will generate a matrix of p-values for t tests of all possible comparisons, none of which are adjusted for multiple comparisons. When running the other scripts below, come back to this to see how adjusted p-value methods changes these results allPairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, p.adjust= &quot;none&quot;) allPairs ## ## Pairwise comparisons using t tests with pooled SD ## ## data: chickwts$weight and chickwts$feed ## ## casein horsebean linseed meatmeal soybean ## horsebean 2.1e-09 - - - - ## linseed 1.5e-05 0.01522 - - - ## meatmeal 0.04557 7.5e-06 0.01348 - - ## soybean 0.00067 0.00032 0.20414 0.17255 - ## sunflower 0.81249 8.2e-10 6.2e-06 0.02644 0.00030 ## ## P value adjustment method: none The unadjusted p-value output from the pairwise.t.test function is a matrix, which is important to know when the need arises to pull out specific elements of the analysis. class(allPairs$p.value) ## [1] &quot;matrix&quot; allPairs$p.value ## casein horsebean linseed meatmeal soybean ## horsebean 2.067997e-09 NA NA NA NA ## linseed 1.493344e-05 1.522197e-02 NA NA NA ## meatmeal 4.556672e-02 7.478012e-06 1.347894e-02 NA NA ## soybean 6.654079e-04 3.246269e-04 2.041446e-01 0.17255391 NA ## sunflower 8.124949e-01 8.203777e-10 6.211836e-06 0.02643548 0.0002980438 To quickly scan which comparisons are below the p &lt; 0.05 threshold we can apply this custom extreme function across the matrix: extreme &lt;- function(x){ ifelse(x &lt; 0.05, TRUE, FALSE) } apply(allPairs$p.value, c(1, 2), extreme) ## casein horsebean linseed meatmeal soybean ## horsebean TRUE NA NA NA NA ## linseed TRUE TRUE NA NA NA ## meatmeal TRUE TRUE TRUE NA NA ## soybean TRUE TRUE FALSE FALSE NA ## sunflower FALSE TRUE TRUE TRUE TRUE All comparisons with Bonferroni adjustment: The Bonferroni correction is the easiest to understand, so I’ll use that to illustrate p-value adjusting. Let \\(C\\) be the number of possible comparisons for a total of \\(k\\) groups in the experiment. An adjusted type 1 error threshold for each comparison is: \\[\\alpha_k=\\frac{\\alpha}{C}\\] Thus, in an experiment with 15 possible comparisons, the adjusted type1 error threshold for each comparison would be \\(\\alpha_K\\) = 0.05/15 = 0.003333. The adjusted p-value functions, however, makes the correction and then recalibrate the p-values back to the confidence level of choice. This can be a bit confusing for the first time. But you’ll note that the net effect is to increase each p-value. Some that were below the threshold unadjusted are greater than the threshold after the adjustment. For example, you can see that the p-values adjusted using the Bonferroni correction below are all higher than they were above, when unadjusted. bonf.adjustedAllpairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, alternative = &quot;two.sided&quot;, p.adjust = &quot;bonferroni&quot;) bonf.adjustedAllpairs ## ## Pairwise comparisons using t tests with pooled SD ## ## data: chickwts$weight and chickwts$feed ## ## casein horsebean linseed meatmeal soybean ## horsebean 3.1e-08 - - - - ## linseed 0.00022 0.22833 - - - ## meatmeal 0.68350 0.00011 0.20218 - - ## soybean 0.00998 0.00487 1.00000 1.00000 - ## sunflower 1.00000 1.2e-08 9.3e-05 0.39653 0.00447 ## ## P value adjustment method: bonferroni And here’s a quick scan for which of these are now below a FWER p &lt; 0.05. Note how a handful of comparisons that were scored extreme by this simple test above, without a p-value adjustment, are no longer extreme with the p-value adjustment. extreme &lt;- function(x){ ifelse(x &lt; 0.05, TRUE, FALSE) } apply(bonf.adjustedAllpairs$p.value, c(1, 2), extreme) ## casein horsebean linseed meatmeal soybean ## horsebean TRUE NA NA NA NA ## linseed TRUE FALSE NA NA NA ## meatmeal FALSE TRUE FALSE NA NA ## soybean TRUE TRUE FALSE FALSE NA ## sunflower FALSE TRUE TRUE FALSE TRUE Optional methods in these functions for the Bonferroni correction are “holm”, “hochberg” and “hommel”, each calculates p-value adjustments in slightly different ways. They are each considered slightly more liberal versions of the “bonferroni”. Which is the best? For all practical purposes, none. Choose one early in your career and stick with it forever. What’s most important is to declare in the planning stages which will be used, and stick with it. That’s the best way to keeps it unbiased. You don’t want to ever find yourself interating through adjustment methods just to find the one that gives a borderline result an asterisk. Read Wright for more information on the adjustment calculations and performance Adjusting p-values for subsets of comparisons Often, we don’t want to burn so much type1 error making scientifically uninteresting comparisons. In such cases, we instead want to compare subsets. Here’s a three step procedure for doing just that. Step1: First, run the pairwise.t.test function, setting the argument p.adjust=&quot;none&quot;. The output includes a matrix of p-values we’ll name allPairs, providing all possible comparisons. #just repeating from above allPairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, p.adjust= &quot;none&quot;) Step2: Select from the allPairs matrix only the p-values that correspond to the comparisons you’d like to make. Name that vector of unadjusted p-values, selectPairs. This takes a bit of cleverness depending on what you want to grab from the matrix. For example, we only want to compare all of the diets to casein. The comparisons we want are all in the first column. Use your matrix indexing skillz to grab only the unadjusted p-values from that first column: selectPairs &lt;- allPairs$p.value[, 1] selectPairs ## horsebean linseed meatmeal soybean sunflower ## 2.067997e-09 1.493344e-05 4.556672e-02 6.654079e-04 8.124949e-01 Step3: Pass unadjusted p-values in the selectPairs vector into the p.adjust function. The output of this step is a vector of adjusted p-values for the selected group of comparisons. adjustedPvalues &lt;- p.adjust(selectPairs, method=&quot;bonferroni&quot;) adjustedPvalues ## horsebean linseed meatmeal soybean sunflower ## 1.033998e-08 7.466720e-05 2.278336e-01 3.327039e-03 1.000000e+00 Which of these are extreme? If it’s not clear by inspection (or too large), use a simple boolean: adjustedPvalues &lt; 0.05 ## horsebean linseed meatmeal soybean sunflower ## TRUE TRUE FALSE TRUE FALSE Fisher LSD This also doesn’t make adjustments for multiple comparisons. Notice how the p-values are no different than if using the p.adjust=&quot;none&quot;. Fisher LSD is better than ‘none’ because it provides CI’s on the differences. Note: diff = difference between diet means. #from the DescTools package PostHocTest(my.ezaov$aov, method = &quot;lsd&quot;) ## ## Posthoc multiple comparisons of means : Fisher LSD ## 95% family-wise confidence level ## ## $feed ## diff lwr.ci upr.ci pval ## horsebean-casein -163.383333 -210.287097 -116.4795699 2.1e-09 *** ## linseed-casein -104.833333 -149.554317 -60.1123496 1.5e-05 *** ## meatmeal-casein -46.674242 -92.400318 -0.9481673 0.04557 * ## soybean-casein -77.154762 -120.248980 -34.0605437 0.00067 *** ## sunflower-casein 5.333333 -39.387650 50.0543170 0.81249 ## linseed-horsebean 58.550000 11.646237 105.4537634 0.01522 * ## meatmeal-horsebean 116.709091 68.846051 164.5721307 7.5e-06 *** ## soybean-horsebean 86.228571 40.873216 131.5839270 0.00032 *** ## sunflower-horsebean 168.716667 121.812903 215.6204301 8.2e-10 *** ## meatmeal-linseed 58.159091 12.433016 103.8851660 0.01348 * ## soybean-linseed 27.678571 -15.415647 70.7727896 0.20414 ## sunflower-linseed 110.166667 65.445683 154.8876504 6.2e-06 *** ## soybean-meatmeal -30.480519 -74.616890 13.6558510 0.17255 ## sunflower-meatmeal 52.007576 6.281501 97.7336509 0.02644 * ## sunflower-soybean 82.488095 39.393877 125.5823134 0.00030 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All of the following scripts do adjust p-values for multiple comparisons. Therefore, notice how they increase the p-values we see in the output above. Frequently, “significant” unadjusted p-values become “nonsignificant” after these adjustments. 26.4.1.2.1 Other posthoc tests Dunnett’s: This post hoc method differs from above because it only does dependent t tests, on a subset of the means. For example, use this when all group means are compared to the negative control mean. The fewer comparisons don’t spread the allowed FWER as thin as the other options. The following script is configured to compare the means at each level of feed to that for the first listed feed. Dunnett’s is nice because it gives you the effect size (diff) and the confidence interval limits for the difference, as well. Note: diff = the difference between diet means for the compared groups. Here’s an example comparing the weight effect of the various food sources back to casein. Adjusted for mulitple comparisons, horsebean, linseed and soybean each differ from casein. DunnettTest(weight ~ feed, data = chickwts) ## ## Dunnett&#39;s test for comparing several treatments with a control : ## 95% family-wise confidence level ## ## $casein ## diff lwr.ci upr.ci pval ## horsebean-casein -163.383333 -223.95852 -102.80815 6.3e-09 *** ## linseed-casein -104.833333 -162.58951 -47.07716 8.6e-05 *** ## meatmeal-casein -46.674242 -105.72847 12.37999 0.1670 ## soybean-casein -77.154762 -132.81000 -21.49952 0.0032 ** ## sunflower-casein 5.333333 -52.42284 63.08951 0.9995 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Tukey HSD: A method based upon the Studentized range, doing all comparisons, and providing adjusted p-values along with confidence intervals. Used when the question is, “Are there any group differences here at all?” TukeyHSD(my.ezaov$aov, &quot;feed&quot;, ordered = T) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## factor levels have been ordered ## ## Fit: aov(formula = formula(aov_formula), data = data) ## ## $feed ## diff lwr upr p adj ## linseed-horsebean 58.550000 -10.413543 127.51354 0.1413329 ## soybean-horsebean 86.228571 19.541684 152.91546 0.0042167 ## meatmeal-horsebean 116.709091 46.335105 187.08308 0.0001062 ## casein-horsebean 163.383333 94.419790 232.34688 0.0000000 ## sunflower-horsebean 168.716667 99.753124 237.68021 0.0000000 ## soybean-linseed 27.678571 -35.683721 91.04086 0.7932853 ## meatmeal-linseed 58.159091 -9.072873 125.39106 0.1276965 ## casein-linseed 104.833333 39.079175 170.58749 0.0002100 ## sunflower-linseed 110.166667 44.412509 175.92082 0.0000884 ## meatmeal-soybean 30.480519 -34.414070 95.37511 0.7391356 ## casein-soybean 77.154762 13.792470 140.51705 0.0083653 ## sunflower-soybean 82.488095 19.125803 145.85039 0.0038845 ## casein-meatmeal 46.674242 -20.557722 113.90621 0.3324584 ## sunflower-meatmeal 52.007576 -15.224388 119.23954 0.2206962 ## sunflower-casein 5.333333 -60.420825 71.08749 0.9998902 Other: “BH” is Benjamini-Hochberg, an FDR-based tool that will gives you more positives at the risk of higher false positives. Use this when you have an excessive number of comparisons to make. For example, over-designed ANOVA, 3 way ANOVA, multivariate experiments. The BH procedure first ranks the differences between paired groups from largest to smallest, and then performs the tests, unadjusted, running down the list. It debits each p-value against a starting value of 0.05 (or whatever). Once that 0.05 is exhausted, it stops testing. In this way all of the type1 experimentwise error is spent on the biggest differences. 26.5 Reporting the result Note how the R way is to subtract the 2nd group from the 1st. For this reason, the diff values are all negative, even though the feed sources caused higher growth than casein. The effect of feed source accounted for 54% of the experiment’s observed variation in chick weights (one-way completely randomized ANOVA, type=2, F(5,65) = 15.365, p = 5.9e-10, ges=0.54. Pairwise group analysis using Dunnett’s test shows that horsebean (p=6.3e-9), linseed(p=8.6e-5) and soybean (p=0.0032) each differ from casein. (In methods: ANOVA analysis was performed using R v4.3, ez package). (Note, it is better to put CI’s and p-values in a table) "],
["onewayRM.html", "Chapter 27 One-way ANOVA Related Measures 27.1 Data prep 27.2 Run the ANOVA 27.3 Interpretation 27.4 Post-hoc analysis 27.5 Write up", " Chapter 27 One-way ANOVA Related Measures library(tidyverse) library(readxl) library(viridis) library(ez) library(lme4) The analysis, interpretation and presentation of a one-way ANOVA related/repeated measures experimental design is covered in this chapter. This design has one predictor variable that is imposed at three or more levels. But the essential feature of the RM design that distinguishes it from a CR design is that the measurements of the outcome variable are intrinsically related for all levels of the predictor. Thus, a train of measurements, one for each level of the predictor variable, are taken from a single independent replicate. An complete experiment is therefore comprised of many independent replicates, within each of which are collected related, intrinsically-linked measurements. We’ll use data the Jaxwest2 study to illustrate this. In this example, growth of human tumors in immunodeficient mice is assessed by repeated measurements of size over time within each of several subjects. The outcome or dependent variable is tumor volume, in \\(mm^3\\). Tumor volumes are calculated after the researchers measured the lengths and widths of tumors using calipers. The predictor variable is time, in units of days. Although time is usually a continuous/measured variable, we’ll treat it as a discrete factorial variable for this analysis. The model organism is an immunodeficient mouse strain. Each mouse is an experimental unit that has been implanted with HT29 human colon tumor cells. The experiment is designed to determine whether these cells will grow (rather than be rejected by the host immune system). The overall scope of the experiment is to test whether the immunodeficient mouse strain is suitable to study the properties of human cancers. A meaningful effect of the time variable in the ANOVA analysis implies that, yes, human tumors can grow in this host. 27.1 Data prep The data are in a file called Jaxwest2.xls, which can be downloaded from the Jackson Labs here. That site offers more details about the study design than are listd here. The munge has already been done conducted. For clarity, the script won’t be shown again here. However, it is used in this chapter to create a data frame object jw2vol to be used for plotting and statistical analysis. ggplot(jw2vol, aes( day, tumor_vol, group=mouse_ID, color=mouse_ID))+ geom_point(size=3)+ geom_line()+ scale_color_viridis(discrete=T)+ theme_classic() Figure 27.1: Connected points illustrate each replicate and the repeated-measures aspect of the experimental design. 27.2 Run the ANOVA Running the function is straightforward. The data have been munged previously into the data frame jw2vol. See above and here. Because this involves repeated measures for each mouse, the time variable day is argued as within. Type 2 sum of squares is chosen for calculation because the only scientific concern is the main effect of the day. Strictly, does the value of “day” have any influence on tumor growth? In other words, does the tumor grow with time? A detailed ANOVA table is called. There are additional arguments that could be made for custom situations. Consult ?ezANOVA for more information. The model for the RM design has two components, one for day and the other for the mouse_ID. The SS partitioning for the complete model is \\(SS_{total}=SS_{day} + SS_{error}\\), where \\(SS_{error}= SS_{mouse_ID} + SS_{residual}\\). Because it is a repeated measures design, we can account for the random variation associated with differences in mice, \\(SS_{mouse_ID}\\). Thereby using a reduced error term, \\(SS_{residual}\\), in the F-test for the effect of day. There are a few ways to output the analysis. Here’s the simplest: one_wayRM &lt;- ezANOVA(data = jw2vol, dv = tumor_vol, wid = mouse_ID, within = day, type = 2, detailed =T, return_aov=F) one_wayRM ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 (Intercept) 1 10 25544522 2479228 103.03417 1.385832e-06 * ## 2 day 12 120 27137472 3115620 87.10135 2.828478e-53 * ## ges ## 1 0.8203288 ## 2 0.8290727 27.3 Interpretation Line 1 of the ANOVA table, labeled (intercept) in the Effect column, shows the residual variation (SSd) partitioned out by accounting for the variation of the individual mice. Therefore, \\(SS_d=SS_{mouse_ID}\\). Frankly, it’s not clear how \\(SS_n\\) on this line is calculated. The statistical tests of these (intercept) lines are not particularly important and can be safely ignored. The most important line is #2. The null F-statistic distribution has 12 and 120 degrees of freedom, corresponding to 13 days and 143 measurements, less degrees of freedom to calculate the means for individual days and mice. The F-statistic calculated for the main effect of day is very high, and the corresponding p-value is very low. Here, \\[F=\\frac{\\frac{SS_n}{df_n}}{\\frac{SS_d}{df_d}}\\] This F-statistic tests the null hypothesis, which is that the variation associated with the time factor is equal to or less than residual variation. The p-value is the probability of obtaining an F-statistic value this high or even more extreme if the null hypothesis were true. The extreme F-statistic value means that null can be safely rejected. The ges is a regression coefficient that can take on values between 0 and 1. GES is the ratio of the variation due to the effect of the day factor to the total variation in the data set: \\[ges=\\frac{SS_{n(day)}}{SS_{n(day)}+SS_{d(day)}+SS{_{d(mouse_ID)}}}\\] The value of 0.829 can be interpreted as follows: 82.9% of the observed variation in the data is associated with the differences between days. Scientifically, you can infer from this F-test result that the HT29 tumor volumes grow with time when implanted in this mouse strain. Sometimes, that’s all you wish to conclude. If you wish to further identify specific pairs of days where tumor volume differs you could do a post-hoc analysis. 27.4 Post-hoc analysis I have a fairly extensive discussion. Go here and scroll down to the section on Post-hoc analysis. Perhaps we’d like to dig a little deeper. For example, we might want to know on which days tumor growth differs from the first day in the recorded series of measurements. The approach taken below involves two steps. First, all pairwise comparisons are made using a paired t-test to generate a matrix of all p-values. Importantly, these p-values are unadjusted. Second, a vector of select p-values will be collected from this matrix. These p-values will then passed into the p.adjust function so that they are adjusted for multiple comparisons. First, the pairwise t-test. Note the arguments. No adjustment is made (yet) and a two-sided paired t-test is called. The output of the function is stored in an object named m. m &lt;- pairwise.t.test(x = jw2vol$tumor_vol, g = jw2vol$day, p.adjust = &quot;none&quot;, paired = T, alternative = &quot;two.sided&quot; ) m ## ## Pairwise comparisons using paired t tests ## ## data: jw2vol$tumor_vol and jw2vol$day ## ## 17 18 19 22 24 26 29 31 33 ## 18 0.00182 - - - - - - - - ## 19 6.9e-05 0.02324 - - - - - - - ## 22 4.1e-05 3.7e-05 0.00026 - - - - - - ## 24 0.00042 0.00080 0.00235 0.02634 - - - - - ## 26 6.4e-05 0.00012 0.00019 0.00056 0.00984 - - - - ## 29 2.3e-05 4.8e-05 5.3e-05 0.00023 0.00488 0.06582 - - - ## 31 1.4e-05 1.5e-05 1.7e-05 3.7e-05 6.0e-05 0.00077 0.00083 - - ## 33 1.3e-05 1.7e-05 1.6e-05 2.4e-05 4.1e-05 3.6e-05 1.8e-05 0.00053 - ## 36 1.0e-06 1.1e-06 1.1e-06 1.6e-06 1.3e-06 1.3e-06 1.0e-06 8.6e-07 0.00058 ## 38 1.3e-06 1.5e-06 1.4e-06 1.6e-06 1.2e-06 1.1e-06 1.6e-06 7.1e-06 1.4e-05 ## 40 6.3e-06 6.9e-06 6.6e-06 7.3e-06 6.0e-06 8.0e-06 8.9e-06 1.4e-05 1.3e-05 ## 44 7.0e-07 7.0e-07 6.9e-07 7.6e-07 6.5e-07 7.8e-07 7.1e-07 1.1e-06 8.6e-07 ## 36 38 40 ## 18 - - - ## 19 - - - ## 22 - - - ## 24 - - - ## 26 - - - ## 29 - - - ## 31 - - - ## 33 - - - ## 36 - - - ## 38 0.00023 - - ## 40 0.00014 0.00247 - ## 44 4.5e-06 1.1e-05 0.00181 ## ## P value adjustment method: none The pairwise.t.test output m is a list of 4 elements. str(m) ## List of 4 ## $ method : chr &quot;paired t tests&quot; ## $ data.name : chr &quot;jw2vol$tumor_vol and jw2vol$day&quot; ## $ p.value : num [1:12, 1:12] 1.82e-03 6.91e-05 4.07e-05 4.20e-04 6.40e-05 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:12] &quot;18&quot; &quot;19&quot; &quot;22&quot; &quot;24&quot; ... ## .. ..$ : chr [1:12] &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;22&quot; ... ## $ p.adjust.method: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr &quot;pairwise.htest&quot; The most important of these is $p.value, which you can see is a matrix. class(m$p.value) ## [1] &quot;matrix&quot; The matrix contains p-values that represent the outcome of paired t-tests for tumor_vol between all possible combinations of days. Thus, the p-value in the cell defined by the first row and first column of the matrix (m$p.value[1,1]=0.00182) reflects that for the mean difference in tumor_vol between the 17th and 18th days. The first column of p-values, m$p.value[,1], are paired t-test comparisons of tumor_vol between the 17th day and each of the days 18 through 44. pv &lt;- m$p.value[,1] pv ## 18 19 22 24 26 ## 1.820962e-03 6.908852e-05 4.070177e-05 4.196400e-04 6.396672e-05 ## 29 31 33 36 38 ## 2.284286e-05 1.434352e-05 1.320905e-05 1.030862e-06 1.299382e-06 ## 40 44 ## 6.341556e-06 7.021455e-07 To create the adjusted p-values, we pass the vector of p-values pv selected from the p-value matrix m into the p.adjust function. Use your judgement to select an adjustment method that you deem most appropriate. p.adjust(p = pv, method = &quot;holm&quot;, n = length(pv) ) ## 18 19 22 24 26 ## 1.820962e-03 2.558669e-04 2.035088e-04 8.392799e-04 2.558669e-04 ## 29 31 33 36 38 ## 1.370571e-04 1.056724e-04 1.056724e-04 1.133948e-05 1.299382e-05 ## 40 44 ## 5.707401e-05 8.425746e-06 Since each of these adjusted p-values is less than the type 1 error threshold of 0.05, we can conclude that the mean difference in tumor volume changes on each day through the study. If we were one to to put asterisks on the figure, we would illustrate one for each of the days (other than day 17). 27.5 Write up Here’s how we might write up the statistical methods. For the jaxwest2 experiment, each of 11 mice are treated as independent replicates. Repeated tumor volume measurements were collected from each beginning on day 17 post-implantation. The tumor volume value for day = 17 for the 3rd subject was lost. This was imputed using the average tumor volume value for day 17 of all other subjects. The effect of time was assessed by one-way related measures ANOVA using the ezANOVA function in R v3.5.2. For a post hoc analysis, the mean differences in tumor vol between study days were compared using two-sided paired t-tests (pairwise.t.test, R v3.5.2), with p-values adjusted using the Holm method. In the figure legend, something like this: Tumor volume increases with days after implantation (one-way RM ANOVA, F(12,120)=87.1, p=2.8e-53). Asterisks = adjusted p &lt; 0.05. "],
["twowayCR.html", "Chapter 28 Two-way ANOVA Completely Randomized 28.1 Effect of Strain and Diet on Liver 28.2 The test 28.3 Interpretation of 2 Way CR ANOVA Output 28.4 Post Hoc Multiple Comparisons", " Chapter 28 Two-way ANOVA Completely Randomized library(tidyverse) library(ez) library(knitr) library(kableExtra) library(viridis) A two-way ANOVA experimental design is one that involves two predictor variables, where each predictor has 2 or more levels. There is only one outcome variable in a 2 way ANOVA and it is measured on an equal interval scale. The predictor variables are often referred to as factors, and so ANOVA designs are synonymous with factorial designs. The experimental designs can be as follows: Completely randomized (CR) on both factors Related measures (RM) on both factors Mixed, CR on one factor and RM on the other In one sense, a two-way ANOVA can be thought of as two one-way ANOVA’s run simultaneously. The major difference, however, is the ability to test whether an interaction exists between the two factors. In this chapter we’ll focus on the data structure and analysis of a two-way ANOVA CR experimental design. 28.1 Effect of Strain and Diet on Liver A (hypothetical) study was conducted to evaluate the influence of mouse background strain and diet on the accumulation of cholesterol in liver. Ten animals were selected randomly from each of the C57BL/6 and C57BL/10 strains. They were each split randomly onto either of two diets, normal and high fat. After about two months they were sacrificed to obtain cholesterol measurements in liver tissue. Three predictions, and the corresponding null hypotheses that each tests, can be evaluated here simultaneously: The two strains differ in liver cholesterol content. \\(H0:\\sigma^2_{strain}\\le\\sigma^2_{residual}\\) The diets differ in how they affect liver cholesterol content. \\(H0:\\sigma^2_{diet}\\le\\sigma^2_{residual}\\) The liver cholesterol content is influenced by both diet and strain. \\(H0:\\sigma^2_{strainXdiet}\\le\\sigma^2_{residual}\\) The first two of these are commonly referred to as the main effects of the factors, whereas the third is referred to as the interaction effect of the factors. Here’s some simulated data. But they are guided guided using means and standard deviations from the Jackson Labs phenome database). along with a graph of the results: ID Strain Diet Cholesterol 1 C57BL/6 Normal 49 2 C57BL/6 Normal 43 3 C57BL/6 Normal 44 4 C57BL/6 Normal 37 5 C57BL/6 Normal 47 6 C57BL/6 High Fat 91 7 C57BL/6 High Fat 86 8 C57BL/6 High Fat 120 9 C57BL/6 High Fat 111 10 C57BL/6 High Fat 101 11 C57BL/10 Normal 100 12 C57BL/10 Normal 123 13 C57BL/10 Normal 125 14 C57BL/10 Normal 115 15 C57BL/10 Normal 88 16 C57BL/10 High Fat 207 17 C57BL/10 High Fat 228 18 C57BL/10 High Fat 217 19 C57BL/10 High Fat 220 20 C57BL/10 High Fat 217 ggplot( liver, aes(Diet, Cholesterol, fill=Strain) ) + stat_summary( fun.data=&quot;mean_sdl&quot;, fun.args=list(mult=1), geom= &quot;errorbar&quot;, position=position_dodge(width=1), width=0.2, size=1 ) + stat_summary( fun.y=&quot;mean&quot;, fun.args=list(mult=1), geom= &quot;bar&quot;, position=position_dodge(width=1), width=0.5, size=2 ) + scale_fill_viridis( discrete=T ) + theme( legend.position=(c(0.8, 0.8)) ) + labs( title=&quot;Strain and Diet Effects on Liver Cholesterol, mean +/- SD&quot;, x=&quot;Diet&quot;, y=&quot;Liver Cholesterol, mg/&quot; ) Figure 28.1: Two-way completely randomized ANOVA results In particular, pay attention to the data structure. It has four columns: ID, Strain, Diet, Cholesterol. All of these are variables, including two columns for each of the predictor variables (Strain, Diet), and one for the response variable (Cholesterol). The ID can also be thought of as a variable. 28.2 The test We use ezANOVA in the ANOVA package to test the three null hypotheses. Here are the arguments: out.cr &lt;- ezANOVA(data = liver, dv = Cholesterol, wid = ID, between = c(Strain,Diet), type = 3, return_aov = F, detailed = T ) out.cr ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 (Intercept) 1 16 280608.05 2096.4 2141.63747 1.801635e-18 * ## 2 Strain 1 16 41496.05 2096.4 316.70330 5.742386e-12 * ## 3 Diet 1 16 34196.45 2096.4 260.99180 2.499051e-11 * ## 4 Strain:Diet 1 16 3100.05 2096.4 23.65999 1.722945e-04 * ## ges ## 1 0.9925845 ## 2 0.9519091 ## 3 0.9422366 ## 4 0.5965707 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 3 16 283.8 748.4 2.022448 0.1513253 28.3 Interpretation of 2 Way CR ANOVA Output We get two F test tables, Levene’s and the ANOVA. Specific statistical details about these tests are covered in the document “Completely Randomized One Way ANOVA Analysis”. The only difference in two-way CR ANOVA compared to a one-way CR ANOVA is the test of the nulls for the additional factor and for the interaction between the two factors. The model is a bit more complex. 28.3.1 Levene’s Levene’s tests the null that homogeneity of variance is equivalent across the groups. The p-value of 0.15 is higher than the 0.05 type1 error rejection threshold. Levene’s is inconclusive, offering no evidence the homogeneity of variance assumption has been violated. 28.3.2 ANOVA Table The ANOVA table shows p-values that are below the 0.05 type1 error threshold for each factor and for their interaction. We can safely reject the interaction null hypothesis (p=0.00017). Scientifically, we would conclude that the effect of diet on liver cholesterol depends upon the strain of the animal. The partial eta-square indicates this interaction between diet and strain explains about 59.6% of the observed variation. Since the interaction is positive, it’s difficult to say much about diet and strain. Sure, diet affects liver cholesterol. Strain does as well. The ANOVA per se cannot parse those out from the interaction effect (we wait for regression analysis for that!). 28.4 Post Hoc Multiple Comparisons We could leave well enough alone and draw our inferences on the basis of the interaction effect alone. However, it would not be unreasonable to compare the effect of diet within each strain. at each level of diet. Nor is it unreasonable to compare the each strain across the diets. We’ll do run a pairwise.t.test function to make all possible comparisons, and use a p-value adjustment method to keep the family-wise error rate within 5%. The following script yieds every possible comparison between levels of the two factors. pairwise.t.test(liver$Cholesterol, interaction(liver$Strain, liver$Diet), paired=F, p.adj = &quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: liver$Cholesterol and interaction(liver$Strain, liver$Diet) ## ## C57BL/10.High Fat C57BL/6.High Fat C57BL/10.Normal ## C57BL/6.High Fat 1.4e-10 - - ## C57BL/10.Normal 3.5e-10 0.26 - ## C57BL/6.Normal 3.4e-13 1.1e-06 2.8e-07 ## ## P value adjustment method: holm Diet effect within each strain: 10.High v 10.Norml: p=3.5e-10 6.High v 6.Normal: p=1.1e-06 Strain effect across diets: * 10.High v 6.High p=1.4e-10 * 10.Normal v 6.Normal p=2.8e-7 Of the six possible comparisons, only one shows no difference (liver cholesterol in C57Bl/10 on normal diet is no different from C57Bl/6 on high diet, p=0.26) 28.4.1 Write Up The interaction betwen diet and strain accounts for nearly 60% of the variation liver cholesterol levels (2 way CR ANOVA, p=0.00017, n=20). Pairwise differences in the liver cholesterol response exist between levels of diet within strains, and across strains at each level of diet (Holm’s adjusted p&lt;0.05, pairwise t tests). "],
["twowayRM.html", "Chapter 29 Two-way ANOVA Related Measures 29.1 Cell culture 29.2 The test 29.3 Interpretation of the output 29.4 Post Hoc multiple comparisons 29.5 Write Up", " Chapter 29 Two-way ANOVA Related Measures library(tidyverse) library(ez) library(knitr) library(kableExtra) library(viridis) A two-way ANOVA experimental design is one that involves two predictor variables, where each predictor has 2 or more levels. There is only one outcome variable in a 2 way ANOVA and it should be continuous, measured on an equal interval scale, and ideally sampled from a normally-distributed population. The predictor variables are often referred to as factors, and so ANOVA designs are synonymous with factorial designs. The experimental designs can be as follows: Completely randomized (CR) on both factors Related measures (RM) on both factors Mixed, CR on one factor and RM on the other In one sense, a two-way ANOVA can be thought of as two one-way ANOVA’s run simultaneously. So it is possible to assess the effects of two factors at once. The major difference, however, is the ability to also test whether an interaction exists between the two factors. In this chapter we’ll focus on the data structure and analysis of a two-way ANOVA RM experimental design. These are particularly common in cell-culture and other in vitro-based experiments, owing to the homogeneity of the biological material. For an experiment such as the one described below, all of the biological material assayed within a single replicate comes from a common source. Every well in every plate prepared from that source is identical, for all intents and purposes. Therefore, any measurements taken from these wells are intrinsically-linked. For this reason, the data should be analyzed as related-measures. Figure 29.1: A two-way ANOVA RM design viewed from the bench. Each combination of the two factors is tested in technical duplicate. All measurements are intrinsically-related. An independent replicate occurs when the same protocol is followed, from start to finish, beginning with a new batch of plates. It’s hard to argue these represent biological replicates in such designs. The cells are likely clonal, and probably aren’t changing much from batch to batch. The statistical analysis is therefore essentially focused on establishing whether the observations occur repeatedly or not. 29.1 Cell culture Cornary artery atheromas are enriched in fast-growing mesenchymal-like cells, which have prominent inflammatory characteristics. They express the transcription factor NFAT, which is regulated by mitogenic receptor signaling to control inflammatory gene expression. Cells from atheroma explants are easy to grow in culture, often indefinitely as for cancer cells. Eventually, a fast growing cell population dominates these monocultures when passaged serially. But these probably mimic the “bad” cells in an atheroma pretty well, so are useful to study. An experiment was performed on culutured human coronary smooth muscle cells derived from an atheroma to determine if the bzip suppresser ICER attenuates NFAT and whether it is mitogen-selective. One predictor variable is mitogen at 3 levels: vehicle, ADP and PDGF. A second predictor variable is an exppression vector, which is either empty or encodes a cDNA to express ICER. The output response, luminescence in cell extracts due to an NFAT-driven luciferase transcriptional reporter, is detected using a luminometer. Luminescence is linearly proportional to the amount of luciferase in the cells. The protocol involved three independent transfections of the CASM cell line with the expression vector and luciferase reporter, followed a few days later by treatment with the mitogens for 4 hrs before measuring luminescence. All treatments are peformed in technical duplicate, from which the average value is used for the response measure.. Three predictions, and the corresponding null hypotheses that each tests, can be evaluated here simultaneously: Icer and its control differ in how they each affect NFAT-mediated transcription. \\(H0:\\sigma^2_{suppressor}\\le\\sigma^2_{residual}\\) The mitogens differ in how they affect NFAT-mediated transcription. \\(H0:\\sigma^2_{mitogen}\\le\\sigma^2_{residual}\\) NFAT-mediated transcription is influence by a combination of suppressor and mitogen. \\(H0:\\sigma^2_{suppressorXmitogen}\\le\\sigma^2_{residual}\\) Here’s some data: replicate vector mitogen Nfatluc A empty Veh 23.2 B empty Veh 15.7 C empty Veh 17.2 A empty ADP 83.5 B empty ADP 95.9 C empty ADP 89.0 A empty PDGF 124.6 B empty PDGF 187.7 C empty PDGF 170.9 A ICER Veh 16.8 B ICER Veh 5.4 C ICER Veh 27.6 A ICER ADP 89.8 B ICER ADP 80.2 C ICER ADP 52.5 A ICER PDGF 57.0 B ICER PDGF 78.1 C ICER PDGF 67.4 ggplot(hcsmc, aes( mitogen, Nfatluc, group=replicate) ) + geom_line( ) + geom_point( aes( color=replicate), size=4 ) + facet_grid( ~vector, switch=&quot;both&quot; ) + scale_shape_manual( values=c(16, 16) ) + scale_color_viridis( discrete=T ) + theme( legend.position=&quot;top&quot; ) + scale_x_discrete( limits=c(&quot;Veh&quot;,&quot;ADP&quot;,&quot;PDGF&quot;) ) + labs( y=&quot;NFAT-luciferase, light units/mg&quot;, x=&quot;Mitogen&quot; ) Figure 29.2: A cell culture-based two-way ANOVA RM experiment 29.2 The test Scientific judgement dictates running this test as related measures. Each replicate was performed on a different passage of cells, approximately 1 week apart. Within each passage the cells are highly homogenous. The measurements within a replicate are intrinsically-linked. Perhaps most importantly, each measure taken on a given day is NOT independent of all other measures that day. There really is no other way to analyze these data. out.rm &lt;- ezANOVA(data = hcsmc , dv = Nfatluc , wid = replicate , within = c(vector, mitogen) , type = 3 , return_aov = F , detailed = T ); out.rm ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 (Intercept) 1 2 91378.125 388.5700 470.33031 0.002119408 * ## 2 vector 1 2 6156.801 471.0011 26.14347 0.036186976 * ## 3 mitogen 2 4 29018.893 1981.2567 29.29342 0.004084641 * ## 4 vector:mitogen 2 4 7333.031 623.0722 23.53830 0.006133042 * ## ges ## 1 0.9634772 ## 2 0.6399535 ## 3 0.8933620 ## 4 0.6791774 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 mitogen 0.6795372 0.8243404 ## 4 vector:mitogen 0.4963405 0.7045144 ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] ## 3 mitogen 0.7573102 0.01101064 * 2.620487 0.004084641 ## 4 vector:mitogen 0.6650442 0.02123339 * 1.485468 0.006133042 ## p[HF]&lt;.05 ## 3 * ## 4 * 29.3 Interpretation of the output 29.3.1 ANOVA Table Note just as for the 2-way ANOVA CR analysis, there are 3 F tests for the model (the intercept F test is inconsequential for now). The F value for the suppress:mitogen interaction is beyond the critical limit for a null F distribution with 2 and 4 degrees of freedom. The result is extreme (p = 0.00613). The interaction takes precedence, since the main effects are difficult to inteprete if an interaction occurs. It is safe to reject the interaction null and conclude that variance for its effect exceeds that for its residual. Scientifically, this means that levels of NFAT-mediated transcription are influenced by both the suppressor and the mitogen stimuli, as designed in this experiment. About 68% of the variance in the data can be explained by this interaction effect. 29.3.2 Mauchly’s Sphericity Test Sphericity is defined as uniform variance of the differences. Think of it as the RM analog to the CR homogeneity of variance assumption. Sphericity, as for homogeneity of variance, is an assumption ideally met in RM ANOVA for validity of the result. Mauchly’s tests the null hypothesis that the variances among the differences are equal. If the test statistic were extreme, the null would be rejected, meaning these variances are unequal and sphericity is violated. If that were the case, we’d conduct inference on the basis of a corrected p-value for the ANOVA..the p[GG], which is the Geisser-Greenhouse corrected p-value. In our analysis, the Mauchly test is not extreme. We have no reason to believe the sphericity assumption is not satisfied. We can use the p-value in the ANOVA table to guide our inference, without using the sphericity correction. 29.4 Post Hoc multiple comparisons The true scientific objective here is to know whether the suppressor selectively attenuate NFAT-mediated transcription by either of the mitogens. In other words, scientifically, it would be interesting to learn whether an ICER-suppressable factor participates in the pathway of one of the mitogens, but not the other. Given that objective, there are some fairly specific planned comparisons worth making. If we compare ICER to Empty for each level of mitogen, we’ll have an answer to the question posed above. In other words, we only care about 3 of the 15 possible comparisons that could be made. Therefore, we’ll run a pairwise.t.test without creating any adjusted p-values. After that, we’ll create a small vector of p-values for the 3 comparisons we wish to make and use the p.adjust function to correct them for multiple comparisons. We use a paired=T argument given the intrinsically-linked relationship of the measures in the samples. Not all of the measures are independent so the unpaired t-test is inappropriate. m &lt;- pairwise.t.test(hcsmc$Nfatluc, interaction(hcsmc$vector, hcsmc$mitogen), paired=T, p.adj = &quot;none&quot;) m ## ## Pairwise comparisons using paired t tests ## ## data: hcsmc$Nfatluc and interaction(hcsmc$vector, hcsmc$mitogen) ## ## empty.ADP ICER.ADP empty.PDGF ICER.PDGF empty.Veh ## ICER.ADP 0.3413 - - - - ## empty.PDGF 0.0439 0.0803 - - - ## ICER.PDGF 0.0129 0.6800 0.0191 - - ## empty.Veh 0.0066 0.0316 0.0214 0.0276 - ## ICER.Veh 0.0147 0.0720 0.0215 0.0429 0.7723 ## ## P value adjustment method: none pv &lt;- c(m$p.value[1,1], m$p.value[3,3], m$p.value[5,5]) pv ## [1] 0.34127356 0.01905007 0.77231675 Thus, the three comparisons and their unadjusted p-values are: ICER.ADP to empty.ADP p=0.3413 ICER.PDGF to empty.PDGF p=0.0191 ICER.Veh to empty.Veh p=0.7723 p.adjust(p=pv, method=&quot;holm&quot;, n=length(pv)) ## [1] 0.68254711 0.05715022 0.77231675 Each of these unadjusted p-values is greater than the Bonferroni adjusted type1 error threshold of 0.01666. On this basis, we cannot reject the null hypothesis that these response are the same. This illustrates the occassional case where a positive result at the level of the omnibus test is coupled to a negative finding in post hoc testing. There’s no accident in this case, it’s likely a fairly underpowered study given. 29.5 Write Up Statistical analysis of the luciferase data indicates there is an interaction between levels of the suppressor and the mitogen variables (2 way RM ANOVA, p=0.0061, n=3). However, post hoc comparisons do not support the hypothesis that ICER selectively suppresses either of the mitogens (pairwise two-sided paired t-tests, Holm adjusted p-values for multiple comparisons). The experiment may have been underpowered. "],
["twowaymixed.html", "Chapter 30 Two-way ANOVA RM/CR 30.1 ChickWeight Dataset 30.2 Munge ChickWeight data 30.3 The test 30.4 Interpreting the ANOVA output 30.5 Post hoc pairwise tests", " Chapter 30 Two-way ANOVA RM/CR #library(magrittr) library(tidyverse) library(DescTools) library(ez) library(nlme) Two-way ANOVA experiments allow for simultaneously testing the effects of two predictor variables. In a two way completely randomized ANOVA, all of the levels of both factors are randomly allocated to individual experimental units. Each experimental unit represents an independent replicate and each generates a single measurement for analysis. In a two-way related/repeated measures ANOVA, all of the levels for both factors are measured within a single experimental unit. The experimental unit represents the independent replicate, and therefore the data set contains many intrinsically-related measurements from each replicate. A hybrid of the two above is called a two-way mixed ANOVA experimental design. In a mixed ANOVA, one factor is completely randomized, whereas the other is structured as a related/repeated measures within each replicate. The ChickWeight study is a good example for visualizing how a two-way mixed ANOVA would operate. 30.1 ChickWeight Dataset The ChickWeight dataset in base R is from a study testing the effect of 4 diets on the growth of chicks. You can learn more about it by typing ?ChickWeight in the console. As you might imagine, the principal goal of a study like this is to determine if any of the 4 diets differ from each other in terms of how they impact chick weight over time. This is a two way mixed ANOVA design. The two factors are diet and time. Chicks are randomly assigned to a diet. While on the diet each chick is weighed repeatedly at various time points. Therefore, Diet is a completely randomized (CR) factor, whereas the factor time (in units of days) is a related/repeated measure variable. If that’s not entirely clear, it should become more obvious by looking at a plot of the data. 30.2 Munge ChickWeight data We have several things to do before the ANOVA can be run. First, let’s just look at the data graphically. That makes the study design a bit easier to understand. Then we’ll clean it up for what we need to do. Step 1: Plot it for a quick visualization Note how each point-to-point line represents a unique chick replicate. You can get a sense from this visual how weight correlates with the replicates by time. Heavier chicks are heavy throughout the time course, lighter chicks are lighter throughout the time course. This serves as an interesting visualization of biological variation. Visually, can you tell if any of the diets differ from the others? ggplot(ChickWeight, aes(Time, weight, group=Chick, color=Chick) ) + geom_point( ) + geom_line( ) + scale_color_viridis_d( ) + facet_grid( ~Diet ) Step 2: First, check for missing data on the within factor, chicks. We can’t have any for a RM design. ezDesign is a slick function in the ez package with which to check for missing values: ezDesign( data = ChickWeight, x = .(Time), y = .(Chick), row = NULL, col = NULL, cell_border_size = 1 ) Step 3: Remove Chicks for which full Time series is missing, while creating a repaired dataset. Here’s a hemi dplyr-based way to do that: temp &lt;- as.data.frame( ChickWeight%&gt;% group_by(Chick)%&gt;% tally() ) # show only the Chick replicates that have incomplete measures on time temp[temp$n&lt;12, ] ## Chick n ## 1 18 2 ## 2 16 7 ## 3 15 8 ## 8 8 11 ## 41 44 10 # create new dataframe for ANOVA analysis in which the incomplete replicates are removed ChickWeight1 &lt;- ChickWeight[!(ChickWeight$Chick %in% temp$Chick[temp$n&lt;12]),] Step 4: Check to make sure the new dataframe from the previous step is clean, that incomplete replicates are out ezDesign( data = ChickWeight1 , x = .(Time) , y = .(Chick) , row = NULL , col = NULL , cell_border_size = 1 ) Step 5: The ChickWeight dataframe has Time as a numeric, which needs to be converted to a factor to get stuff to work. ChickWeight1$Time &lt;- as.factor(ChickWeight1$Time) One last check of the data structure: head(ChickWeight1) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 str(ChickWeight1) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 540 obs. of 4 variables: ## $ weight: num 42 51 59 64 76 93 106 125 149 171 ... ## $ Time : Factor w/ 12 levels &quot;0&quot;,&quot;2&quot;,&quot;4&quot;,&quot;6&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Chick : Ord.factor w/ 45 levels &quot;13&quot;&lt;&quot;9&quot;&lt;&quot;20&quot;&lt;..: 11 11 11 11 11 11 11 11 11 11 ... ## $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language weight ~ Time | Chick ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time&quot; ## ..$ y: chr &quot;Body weight&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(gm)&quot; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Diet ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;FUN&quot;)=function (x) ## - attr(*, &quot;order.groups&quot;)= logi TRUE Done with the munge! 30.3 The test The main scientific objective of this study is to learn whether growth differs between the four diets. The study design calls for a two-way mixed ANOVA analysis as an omnibus test. If that passes, we’ll have access to compare the diets post-hoc. We’re not particularly interested in the effect of the Time factor, per se. We know the chicks will grow with time. Nor are we particularly interested in the interaction effect between Time and Diet. Which is to say that we don’t want to know at what times the various levels of the diets differ. We just want to know if the diets differ. Having said that, the repeated measures of weight on time seems, intuitively, a solid way for designing the experiment, compared to a few conceivable alternatives. It collects 12 different weights per replicate than a study that only weighed each critter once, say, on the 21st day. Thus, there is more thorough information on how each replicate responded to the diet. Which seems useful. But we’re not explicitly applying a rate model to the data to derive growth rate constants that might be compared. The growth rate is more implicit…it’s the information buried within the repeated measures. And we’re also not particularly interested in knowing at what specific time points the various diets might differ. We just want to know if the four diets differ from each other. Here’s how that’s done. To encode a two-way mixed ANOVA using ezANOVA, simply configure the arguments such that the repeated measure factor is within and the completely randomized factor is between. An object named two_wayMM, which could have been named foo, is used to store the ANOVA output. two_wayMM &lt;- ezANOVA(data = ChickWeight1, dv = weight, wid = Chick, between = Diet, within = Time, type = 3, detailed = T ) two_wayMM ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 (Intercept) 1 41 8404690.44 313495.0 1099.195477 3.122264e-31 * ## 2 Diet 3 41 116403.57 313495.0 5.074559 4.428259e-03 * ## 3 Time 11 451 2023644.28 295322.5 280.945086 6.411563e-194 * ## 4 Diet:Time 33 451 81375.09 295322.5 3.765802 9.341051e-11 * ## ges ## 1 0.9324550 ## 2 0.1605077 ## 3 0.7687269 ## 4 0.1179020 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 Time 2.67541e-17 1.032609e-251 * ## 4 Diet:Time 2.67541e-17 1.032609e-251 * ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] ## 3 Time 0.114145 2.005482e-24 * 0.1160483 8.633944e-25 ## 4 Diet:Time 0.114145 1.045740e-02 * 0.1160483 1.001674e-02 ## p[HF]&lt;.05 ## 3 * ## 4 * 30.4 Interpreting the ANOVA output Here ezANOVA creates Four objects: * $ANOVA * $Mauchly’s Test for Sphericity * $Sphericity Corrections` * $aov (not shown) 30.4.1 $ANOVA: The ANOVA table Before partioning for sujects, the total residual sums of squares in the sample would be the sum of the residual error terms, or \\(295322.5 + 313495.0 = 608817.5\\). Because weight measurements were repeated on each chick over time, this procedure allows us to account for some of the variation in the Time variable as that due to variation among the chicks. Pulling 313495 out from the overall residual provides a new (and lower than otherwise) residual term for the effect of Diet, while lowering the residual term for the effect of Time and the interaction. The plot above gives you a sense of what’s going on. What’s really noticable, I think, is a lot of chick-to-chick variation, irrespective of the diet. There’s also a lot of variation across the time frame. But the chick-to-chick variation grows really prominent near the end of the time course. What this does is help to isolate from the variation over the time course that which is due to the differences between the 45 chicks. It doesn’t explain why that variation between the chicks occurs, its obviously biological, but it does allow us to partition it out from the Time effect. In the ANOVA table the intercept effect is not particularly important except for the SSd, which tabulates the SS that have been partitioned to the chicks. It is the subject effect. You can think of it as the average subject effect. An extreme value means that there is substantial variation within the chick replicates. Of course there is, they’re growing up! The second effect is Diet, which is important for all the reasons above. There are four diets and thus 3 degrees of freedom for the F test numerator. There are 45 chicks serving as replicates spread out into the four diet groups. The four diet group means cost a total of 4 degrees of freedom, leaving 41 degrees of freedom for the Diet residual variance (denominator). F has been calculated as \\[F=\\frac{\\frac{SSn}{df_n}}{\\frac{SSd}{df_d}}\\] The p-value is the probability of getting an F value as or more extreme from the null F(3,41) distribution. Given the p-value is less than a type 1 error threshold of 5%, we can reject that the F test statistic obtained in the sample belongs to the null. Normally we would conclude that this means the diets effect on weight differs. But that’s hard to call to make given the interaction effect is also positive. The general eta-squared (ges) value indicates that 16% of the overall variation is attributable to the Diet effect. The second effect is that for the twelve levels of the variable Time. The interpretation of it’s table is much the same. There are seven time points per replicate, and one is lost to calculate the grand time mean, leaving 11 df for this test. The residual \\(df_d=(45-1)*(12-1)-(12-1)*(4-1)=451\\). Almost 77% of the total variation in the sample can be explained the effect of time. Although a large effect, it isn’t very interesting scientifically. There’s no surprise that the chicks grew a lot! The third effect is the interaction between the Diet and Time variables. \\(df_n=(4-1)*(12-1)=33\\). The interaction between these two variables explains about 11.8% of the total variation in the sample. This suggests the various diets have some differential influence on the growth rates. It isn’t just the diets, and it isn’t just the time; the two factors together combine to change weights in a way that neither does alone. 30.4.2 Mauchly’s Test and Corrections You’ll recall one of the assumptions to be met for valid ANOVA analysis is that the response variances of the different levels of a factor should be about equivalent (homoskedasticity). This concept extends to the related measures scenarios. Sphericity is defined as uniform variance for the differences between factor levels. Sphericity is an assumption of RM ANOVA, and if violated, should be corrected. Quick inspection of the time course shows violation. Focus on the connecting lines. They have far greater slope near the end of the time course compared to early stages. Mauchly’s test, with its extreme F-statistic value, indicates we can reject the null that sphericity is present. There are differences in variance of differences across the time scale. The Df’s for this test are DfnGGe &amp; DfdGGe. Because of that, we correct the Time result for sphericity using the Geiser-Greenhouse correction. We do this by using the value of p[GG] as a p-value for our test. The correction is dramatic, at least with respect to the p-value correction for the interaction effect. The p[GG]= 0.01045 for the interaction test is that which should be used for inference. 30.5 Post hoc pairwise tests A positive F-test opens the gate to perform post hoc analysis of any groups that interest us. The F tests for the two factors and the interaction shows that there are effects of time and diet on the growth of chicks. The effect of time is not particularly interesting since it is no surprise that chicks grow. What the F tests haven’t told us is which diets differ. Is one better than any other? Is one worse than any other? The approach that we’ll use to answer these questions is the TukeyHSD test, which is used to make a variety of comparisons between groups. The only groups that are of interest scientifically, however, are those associated with the main effects of the diet. Even so, we should make those comparisons in the context of the overall experimental design. The TukeyHSD test runs in three steps. The first step is to create a linear model of the design. The second step is to create an anova object from that model. And the third step is the reason for the first two. The TukeyHSD test function takes an aov object as its data source before running the group comparisons. Because it has a related measures component, the two-way mixed ANOVA experimental design is also known as a general linear mixed-effects model. Exactly what this means will be more clear when we get to discussing general linear models. We use the lme function from the nlme package to regress the model on the ChickWeight1 dataset, generating a large table of coefficients for all the model elements. model &lt;- lme(weight ~ Diet + Time + Diet*Time, data=ChickWeight1, random = ~ 1|Chick) model ## Linear mixed-effects model fit by REML ## Data: ChickWeight1 ## Log-restricted-likelihood: -2401.1 ## Fixed: weight ~ Diet + Time + Diet * Time ## (Intercept) Diet2 Diet3 Diet4 Time2 ## 41.5625000 -0.8625000 -0.7625000 -0.6736111 6.3125000 ## Time4 Time6 Time8 Time10 Time12 ## 15.1250000 26.1250000 40.0000000 55.6875000 72.8750000 ## Time14 Time16 Time18 Time20 Time21 ## 85.7500000 104.2500000 118.9375000 131.6875000 136.1875000 ## Diet2:Time2 Diet3:Time2 Diet4:Time2 Diet2:Time4 Diet3:Time4 ## 2.3875000 3.2875000 4.6875000 3.9750000 6.2750000 ## Diet4:Time4 Diet2:Time6 Diet3:Time6 Diet4:Time6 Diet2:Time8 ## 8.4305556 8.5750000 10.9750000 16.6527778 11.0000000 ## Diet3:Time8 Diet4:Time8 Diet2:Time10 Diet3:Time10 Diet4:Time10 ## 17.6000000 25.0000000 12.1125000 20.6125000 30.3125000 ## Diet2:Time12 Diet3:Time12 Diet4:Time12 Diet2:Time14 Diet3:Time14 ## 17.7250000 30.7250000 40.3472222 15.4500000 37.9500000 ## Diet4:Time14 Diet2:Time16 Diet3:Time16 Diet4:Time16 Diet2:Time18 ## 37.8055556 19.7500000 52.3500000 40.9722222 28.0625000 ## Diet3:Time18 Diet4:Time18 Diet2:Time20 Diet3:Time20 Diet4:Time20 ## 73.3625000 49.3958333 33.2125000 86.4125000 61.3125000 ## Diet2:Time21 Diet3:Time21 Diet4:Time21 ## 37.8125000 93.3125000 61.4791667 ## ## Random effects: ## Formula: ~1 | Chick ## (Intercept) Residual ## StdDev: 24.13746 25.5894 ## ## Number of Observations: 540 ## Number of Groups: 45 Next, that linear model is passed into the aov function, to create an aov object. Why not use ezANOVA to create this? It doesn’t create an object of the proper R class for the posthoc function. This is passed Finally, we run the TukeyHSD test. This function is in the DescTools package. We specify Diet so that it only returns all ofthe possible comparisons between Diet groups. (If you remove the list argument it will return all posssible combinations of every group in the experiment! ) bark &lt;- lme(weight ~ Diet + Time + Diet*Time, data=ChickWeight1, random = ~ 1|Chick) PostHocTest(aov(bark,data=ChickWeight1), which=&quot;Diet&quot;, method=&quot;hsd&quot;) ## ## Posthoc multiple comparisons of means : Tukey HSD ## 95% family-wise confidence level ## ## $Diet ## diff lwr.ci upr.ci pval ## 2-1 14.976042 4.423629 25.52845 0.0016 ** ## 3-1 35.309375 24.756962 45.86179 9.7e-11 *** ## 4-1 30.692708 19.785494 41.59992 1.1e-10 *** ## 3-2 20.333333 8.626482 32.04018 5.5e-05 *** ## 4-2 15.716667 3.689020 27.74431 0.0045 ** ## 4-3 -4.616667 -16.644313 7.41098 0.7555 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What I like about the TukeyHSD test run this way is that it yields effect size, confidence intervals, and adjusted p-values for each comparison. For example, the difference in mean weight between the first and the second diets is 14.9 grams (95%CI(adjusted): 4.4 to 25.5 grams, adjusted p-value=0.0016). 30.5.1 Here’s what’s been discovered Thus, in terms of how the 4 levels of the Diet factor differ in causing chick weight we can conclude: a) all diets are better than Diet 1, b) Diets 3 and 4 are better than Diet 2, c) Diets 3 and 4 do not differ. Reporting “The data were analyzed by type 3 two-way mixed ANOVA with repeated measures on Time (R 3.5.2, ezANOVA). The diet, time interaction F test was positive following correction for a positive Mauchly’s test (correction factor = 0.114145, GG-adjusted p-value = 0.01045). Post hoc comparisons using TukeyHSD adjustments for multiple comparisons indicate the mass of animals on Diet 1 is less than on all others, and that weight is greater on Diets 3 and 4 than on Diet 2.” Ideally, report the last sentence in a table, showing the results. It’s very important to understand here that we took the p-value of p[GG] from the sphericity correction table, rather than that for Time from the ANOVA table, as our inference for this experiment "],
["jaxwest2.html", "Chapter 31 Reproducible Data Munging Mostly with Tidyverse 31.1 Look at the original data carefully 31.2 Our goal 31.3 Read the data into R 31.4 Select the variables 31.5 Trim the cases 31.6 Go long 31.7 Pull out the values for the day variable 31.8 Convert day to numeric 31.9 Convert tumor_vol to numeric 31.10 Deal with that NA 31.11 Convert variables to factor 31.12 Plot the data", " Chapter 31 Reproducible Data Munging Mostly with Tidyverse library(tidyverse) library(readxl) library(viridis) library(RColorBrewer) Reproducibility is when someone who has your data can conduct the same analysis, arriving at the same parameter estimates and conclusions. The data processing steps of an analysis are perhaps the most critical determinant of reproducibility. Ideally, this is performed using a breadcrumbs process, where each step is traceable. That’s what R scripts do and why they are better than munging data in GUI software, such as excel or other stats packages. Here’s an example of what I mean by an R script munge. I thought it would be interesting to try and pull this off using mostly tidyverse functions, if possible. The Jaxwest2 data represent an experiment to establish tumor xenograft growth in \\(NOD.CB17-Prkdc^{scid}/J\\), an immunodeficient mouse strain. Jaxwest2 is a nice data set to illustrate a one-way related measures ANOVA. They also provides an opportunity to illustrate some data wrangling technique. The latter is the focus here. In particular, I’ll illustrate how a complete reproducible data munge can be accomplished using mostly just the tidyverse. The study design involved injecting HT29 human colon cancer cells into the mice. Over the next few weeks repeated daily measurements were collected from each mouse on a handful of outcome variables, including body weight and tumor lengths, widths, and heights. Tumor volume was calculated from length and width data. The multiple measures taken from individual subjects are instrinsically-linked. The day of measurement is the only factor, and it has multiple levels. This all fits a one-way repeated measures ANOVA experimental design model. In the study per se, three groups are compared: 1) Control (no vehicle), 2) Control (vehicle), and 3) Test Group (pretreatment). The latter is apparently proprietary, providing very little outcome data. The first two are similar and can be expected to generate the same outcomes. I’m not interested in comparing these groups since the comparisons aren’t particularly interesting scientifically. So we’ll take only the first of these groups to conduct an ANOVA analysis (later) and imagine the other two didn’t exist. Had we compared these groups, too, it would be a two-way ANOVA repeated measures design. Therefore, we can use this to study the effect of time on tumor cell growth. We can answer the following question: Will tumor cells grow if injected into the animals? We’ll focus only on a subset of the data, the tumor volume measurements over time in the first group. This chapter illustrates how to wrangle that subset out from the rest of the data in the excel file. 31.1 Look at the original data carefully The data are in a file called Jaxwest2.xls, which can be downloaded from the Jackson Labs here. Before starting the munge take a close look at the excel file. A few things to note. First, there are two worksheets. One has the experimental data. The second is a variable key. Now look at that first worksheet. There are two header rows, which is problematic. The first header row is incomplete since it has no values over the first 7 columns. The label in the 8th column actually refers to header values in the remainder of the columns, not the data beneath it. Those values correspond to the day data were collected in a time series. The second header row nicely defines the variables for each column. Note how beginning with the 9th column, the variable name incorporates the day number. Thus, bw_1 is the variable body weight on the first day post injection. Thus, the information about the time series is embedded within each variable name. In other words, most of the variable names are hybrids, carrying information about both the measurement and the day. We’ll need to deconvolute those names. The good news is that the first header row doesn’t provide any information we can’t get from the second header row, so when we read in the data we’ll simply omit that first header row. It would only complicate the munge. Finally, below the header, every row is a case that corresponds to a unique mouse. The values for the variable mouse_ID illustrates as much. Here’s the big picture, however. The column and row structure indicate that repeated measures of multiple outcome variables were collected for each of these mice. 31.2 Our goal Stop me if I’ve used this metaphor previously. But starting a munge is a lot like starting an organic chemistry synthesis. You have the reagents. You know the final product. The only question is how will you create the latter given the former. Let’s collect the time series only for the tumor_vol variable. We’ll ignore all the other outcome variables. The output–the goal–is to create a plot of the data. Eventually, we’ll run a one-way related measures ANOVA analysis to test whether time has an effect on tumor growth (it does, by the bloody obvious test). To get there we’ll read in all but the top row of the first sheet of the excel file, then simplify by selecting only the variables that we want from the Jaxwest2 data. We want a long format data frame where every column represents a unique variable. It will have a 1) numeric tumor volume variable, and a 2) day of measurement variable as a factor, and a 3) variable for the mouse ID also as a factor, and will have data corresponding to only one treatment group (Control (no vehicle)). 31.3 Read the data into R We’ll read in all but the first header row. The function read_excel is from the readxl package, which is part of the tidyverse but you may need to install it separately. The script below creates the object jw2, which is a data frame of 103 variables! Except for the first header row, jw2 contains all of the data in the first sheet of the source file. Note how jaxwest2.xls is otherwise untouched. No changes have been made locally to the original source file. That’s important because it is good reproducible practice. jw2 &lt;-&quot;datasets/jaxwest2.xls&quot; %&gt;% read_excel( skip=1, sheet=1 ) jw2 ## # A tibble: 35 x 103 ## strain sex mouse_ID birth arrival cage_code ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 NOD.C~ f 38 2007-04-04 00:00:00 2007-06-04 00:00:00 IMMS1-19 ## 2 NOD.C~ f 39 2007-04-04 00:00:00 2007-06-04 00:00:00 IMMS1-19 ## 3 NOD.C~ f 43 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-20 ## 4 NOD.C~ f 48 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-21 ## 5 NOD.C~ f 51 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-22 ## 6 NOD.C~ f 58 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-24 ## 7 NOD.C~ f 62 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-25 ## 8 NOD.C~ f 63 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-25 ## 9 NOD.C~ f 64 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-25 ## 10 NOD.C~ f 66 2007-04-24 00:00:00 2007-06-04 00:00:00 IMMS1-26 ## # ... with 25 more rows, and 97 more variables: HT29_inj &lt;chr&gt;, ## # test_group &lt;chr&gt;, bw_1 &lt;dbl&gt;, bw_12 &lt;dbl&gt;, bw_15 &lt;dbl&gt;, bw_17 &lt;dbl&gt;, ## # bw_19 &lt;dbl&gt;, bw_22 &lt;chr&gt;, bw_24 &lt;chr&gt;, bw_26 &lt;chr&gt;, bw_29 &lt;chr&gt;, ## # bw_31 &lt;chr&gt;, bw_33 &lt;chr&gt;, bw_36 &lt;chr&gt;, bw_38 &lt;chr&gt;, bw_40 &lt;chr&gt;, ## # bw_43 &lt;chr&gt;, bw_44 &lt;chr&gt;, bw_chg_12 &lt;dbl&gt;, bw_chg_15 &lt;dbl&gt;, ## # bw_chg_17 &lt;dbl&gt;, bw_chg_19 &lt;dbl&gt;, bw_chg_22 &lt;chr&gt;, bw_chg_24 &lt;chr&gt;, ## # bw_chg_26 &lt;chr&gt;, bw_chg_29 &lt;chr&gt;, bw_chg_31 &lt;chr&gt;, bw_chg_33 &lt;chr&gt;, ## # bw_chg_36 &lt;chr&gt;, bw_chg_38 &lt;chr&gt;, bw_chg_40 &lt;chr&gt;, bw_chg_43 &lt;chr&gt;, ## # bw_chg_44 &lt;chr&gt;, tumor_L_17 &lt;chr&gt;, tumor_W_17 &lt;chr&gt;, tumor_H_17 &lt;chr&gt;, ## # tumor_vol_17 &lt;chr&gt;, tumor_L_18 &lt;chr&gt;, tumor_W_18 &lt;chr&gt;, ## # tumor_H_18 &lt;chr&gt;, tumor_vol_18 &lt;chr&gt;, tumor_L_19 &lt;dbl&gt;, ## # tumor_W_19 &lt;dbl&gt;, tumor_H_19 &lt;dbl&gt;, tumor_vol_19 &lt;dbl&gt;, ## # tumor_L_22 &lt;chr&gt;, `tumor_W_ 22` &lt;chr&gt;, `tumor_H_ 22` &lt;chr&gt;, ## # `tumor_vol_ 22` &lt;chr&gt;, tumor_L_24 &lt;chr&gt;, tumor_W_24 &lt;chr&gt;, ## # tumor_H_24 &lt;chr&gt;, tumor_vol_24 &lt;chr&gt;, tumor_L_26 &lt;chr&gt;, ## # tumor_W_26 &lt;chr&gt;, tumor_H_26 &lt;chr&gt;, tumor_vol_26 &lt;chr&gt;, ## # tumor_L_29 &lt;chr&gt;, tumor_W_29 &lt;chr&gt;, tumor_H_29 &lt;chr&gt;, ## # tumor_vol_29 &lt;chr&gt;, tumor_L_31 &lt;chr&gt;, tumor_W_31 &lt;chr&gt;, ## # tumor_H_31 &lt;chr&gt;, tumor_vol_31 &lt;chr&gt;, tumor_L_33 &lt;chr&gt;, ## # tumor_W_33 &lt;chr&gt;, tumor_H_33 &lt;chr&gt;, tumor_vol_33 &lt;chr&gt;, ## # tumor_L_36 &lt;chr&gt;, tumor_W_36 &lt;chr&gt;, tumor_H_36 &lt;chr&gt;, ## # tumor_vol_36 &lt;chr&gt;, tumor_L_38 &lt;chr&gt;, tumor_W_38 &lt;chr&gt;, ## # tumor_H_38 &lt;chr&gt;, tumor_vol_38 &lt;chr&gt;, tumor_L_40 &lt;chr&gt;, ## # tumor_W_40 &lt;chr&gt;, tumor_H_40 &lt;chr&gt;, tumor_vol_40 &lt;chr&gt;, ## # tumor_L_44 &lt;chr&gt;, tumor_W_44 &lt;chr&gt;, tumor_H_44 &lt;chr&gt;, ## # tumor_vol_44 &lt;chr&gt;, tumor_chg_22 &lt;chr&gt;, tumor_chg_24 &lt;chr&gt;, ## # tumor_chg_26 &lt;chr&gt;, tumor_chg_29 &lt;chr&gt;, tumor_chg_31 &lt;chr&gt;, ## # tumor_chg_33 &lt;chr&gt;, tumor_chg_38 &lt;chr&gt;, tumor_chg_40 &lt;chr&gt;, ## # tumor_chg_44 &lt;chr&gt;, tumor_wt_44 &lt;chr&gt;, survival_500 &lt;chr&gt;, ## # survival_1500 &lt;chr&gt; From here forward I’m illustrating the munge in step-by-step chunks, which otherwise could be combined with the one above into a single longer script. I’m also only using, or mostly using, the tidyverse for this project. 31.4 Select the variables We slim the data set considerably using the select function. We want only the mouse_ID, the test group, and all the columns that correspond to a tumor volume measurement on a given day. We get the latter using the contains function. We want the mouse_ID because the data are repeated measures. We’ll need it as a grouping factor for both ggplot and ANOVA. The test group variable will initially serve as a check to know we grabbed the right data. We can omit it later. contains is really helpful because the tumor volume variables for each day of measurement have a slightly different name, yet each contain the characters tumor_vol_ as a common stem. We’ll create a new object, jw2vol to represent the data. Notice how in subsequent chunks jw2vol is modified as we successively munge the data into shape. jw2vol &lt;- jw2 %&gt;% select( mouse_ID, test_group, contains(&quot;tumor_vol_&quot;) ) jw2vol ## # A tibble: 35 x 15 ## mouse_ID test_group tumor_vol_17 tumor_vol_18 tumor_vol_19 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 38 Control (~ 27.19000000~ 54.75 77.6 ## 2 39 Control (~ 12.65 15.85 24.8 ## 3 43 Control (~ NA 30.03999999~ 54.9 ## 4 48 Control (~ 7.879999999~ 30.17000000~ 31.0 ## 5 51 Control (~ 16.14999999~ 25.82999999~ 28.6 ## 6 58 Control (~ 61.46000000~ 105.3 93.2 ## 7 62 Control (~ 20.07 44.07 60.1 ## 8 63 Control (~ 56.10999999~ 93.06999999~ 104. ## 9 64 Control (~ 17.12000000~ 78.14000000~ 79.8 ## 10 66 Control (~ 15.08 32.25999999~ 35.4 ## # ... with 25 more rows, and 10 more variables: `tumor_vol_ 22` &lt;chr&gt;, ## # tumor_vol_24 &lt;chr&gt;, tumor_vol_26 &lt;chr&gt;, tumor_vol_29 &lt;chr&gt;, ## # tumor_vol_31 &lt;chr&gt;, tumor_vol_33 &lt;chr&gt;, tumor_vol_36 &lt;chr&gt;, ## # tumor_vol_38 &lt;chr&gt;, tumor_vol_40 &lt;chr&gt;, tumor_vol_44 &lt;chr&gt; 31.5 Trim the cases We only want a subset of the test_groups. Looking at jw2vol we can see those happen to be the first 11 cases in the data set. We’ll slice them out, throwing away the rest. jw2vol &lt;- jw2vol %&gt;% filter( test_group == &quot;Control (no vehicle)&quot; ) jw2vol ## # A tibble: 11 x 15 ## mouse_ID test_group tumor_vol_17 tumor_vol_18 tumor_vol_19 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 38 Control (~ 27.19000000~ 54.75 77.6 ## 2 39 Control (~ 12.65 15.85 24.8 ## 3 43 Control (~ NA 30.03999999~ 54.9 ## 4 48 Control (~ 7.879999999~ 30.17000000~ 31.0 ## 5 51 Control (~ 16.14999999~ 25.82999999~ 28.6 ## 6 58 Control (~ 61.46000000~ 105.3 93.2 ## 7 62 Control (~ 20.07 44.07 60.1 ## 8 63 Control (~ 56.10999999~ 93.06999999~ 104. ## 9 64 Control (~ 17.12000000~ 78.14000000~ 79.8 ## 10 66 Control (~ 15.08 32.25999999~ 35.4 ## 11 67 Control (~ 21.68 26.96000000~ 44.6 ## # ... with 10 more variables: `tumor_vol_ 22` &lt;chr&gt;, tumor_vol_24 &lt;chr&gt;, ## # tumor_vol_26 &lt;chr&gt;, tumor_vol_29 &lt;chr&gt;, tumor_vol_31 &lt;chr&gt;, ## # tumor_vol_33 &lt;chr&gt;, tumor_vol_36 &lt;chr&gt;, tumor_vol_38 &lt;chr&gt;, ## # tumor_vol_40 &lt;chr&gt;, tumor_vol_44 &lt;chr&gt; 31.6 Go long The iteration above is 15 columns wide. We use the gather function to make it long, and only 4 columns wide. Importantly, the new variable measurement is intended to be an intermediate. It has the information about day of measurement within the character string, which we’ll pull out in a moment. First, make it long: jw2vol &lt;- jw2vol %&gt;% gather( measurement, tumor_vol, -mouse_ID, -test_group ) jw2vol ## # A tibble: 143 x 4 ## mouse_ID test_group measurement tumor_vol ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 38 Control (no vehicle) tumor_vol_17 27.190000000000001 ## 2 39 Control (no vehicle) tumor_vol_17 12.65 ## 3 43 Control (no vehicle) tumor_vol_17 NA ## 4 48 Control (no vehicle) tumor_vol_17 7.8799999999999999 ## 5 51 Control (no vehicle) tumor_vol_17 16.149999999999999 ## 6 58 Control (no vehicle) tumor_vol_17 61.460000000000001 ## 7 62 Control (no vehicle) tumor_vol_17 20.07 ## 8 63 Control (no vehicle) tumor_vol_17 56.109999999999999 ## 9 64 Control (no vehicle) tumor_vol_17 17.120000000000001 ## 10 66 Control (no vehicle) tumor_vol_17 15.08 ## # ... with 133 more rows 31.7 Pull out the values for the day variable In the preceding step, all of the values for the measurement variable begin with the same character string. Getting rid of that will leave us with the day number. str_remove_all is from the stringr package and transmute is a dplyr function. All are from the tidyverse. We want the resulting object to be a data frame with three variables. One each for the day, mouse_ID, and tumor volume. Having confirmed the cases are from the group we want, we no longer need a test_group variable. jw2vol &lt;- jw2vol %&gt;% transmute( day=str_remove_all(measurement, &quot;tumor_vol_&quot;), mouse_ID, tumor_vol ) jw2vol ## # A tibble: 143 x 3 ## day mouse_ID tumor_vol ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 17 38 27.190000000000001 ## 2 17 39 12.65 ## 3 17 43 NA ## 4 17 48 7.8799999999999999 ## 5 17 51 16.149999999999999 ## 6 17 58 61.460000000000001 ## 7 17 62 20.07 ## 8 17 63 56.109999999999999 ## 9 17 64 17.120000000000001 ## 10 17 66 15.08 ## # ... with 133 more rows 31.8 Convert day to numeric Because they are characters the values for days may be out of sequence (they were on my machine). Converting them to numeric should retain the correct order. Later, we’ll convert the day variable to a factor for plotting and ANOVA purposes. jw2vol &lt;- jw2vol %&gt;% transmute( day=(as.numeric(day)), mouse_ID, tumor_vol ) jw2vol ## # A tibble: 143 x 3 ## day mouse_ID tumor_vol ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 17 38 27.190000000000001 ## 2 17 39 12.65 ## 3 17 43 NA ## 4 17 48 7.8799999999999999 ## 5 17 51 16.149999999999999 ## 6 17 58 61.460000000000001 ## 7 17 62 20.07 ## 8 17 63 56.109999999999999 ## 9 17 64 17.120000000000001 ## 10 17 66 15.08 ## # ... with 133 more rows 31.9 Convert tumor_vol to numeric You may have noticed above that R read the tumor_vol values as character. Here we convert them to numeric, and then round their values to 1 significant digit. jw2vol &lt;- jw2vol %&gt;% transmute( tumor_vol=round(as.numeric(tumor_vol), 1 ), mouse_ID, day ) ## Warning in evalq(round(as.numeric(tumor_vol), 1), &lt;environment&gt;): NAs ## introduced by coercion jw2vol ## # A tibble: 143 x 3 ## tumor_vol mouse_ID day ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27.2 38 17 ## 2 12.7 39 17 ## 3 NA 43 17 ## 4 7.9 48 17 ## 5 16.1 51 17 ## 6 61.5 58 17 ## 7 20.1 62 17 ## 8 56.1 63 17 ## 9 17.1 64 17 ## 10 15.1 66 17 ## # ... with 133 more rows 31.10 Deal with that NA You notice a missing tumor_vol value for a mouse_ID 43 on day 17. There are two options when confronted with NA values. De-list or impute. Since this is a repeated measures design, delisting this one of a string of repeated values would mean losing all of the data on the mouse. Ugh. Since we have 10 other values for day 17, it makes sense to impute a value for 43, on their bias. We’ll take the means of all cases on day 17 and replace the NA with that value. First, check the means. Second, replace. means &lt;- group_by( jw2vol, day ) %&gt;% summarise( mean=mean(tumor_vol, na.rm=T) ) means ## # A tibble: 13 x 2 ## day mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 17 25.6 ## 2 18 48.8 ## 3 19 57.6 ## 4 22 84.5 ## 5 24 113. ## 6 26 179. ## 7 29 202. ## 8 31 318. ## 9 33 469. ## 10 36 601. ## 11 38 854. ## 12 40 1122. ## 13 44 1422. jw2vol &lt;-jw2vol %&gt;% replace_na( list(tumor_vol=means$mean[1] ) ) jw2vol ## # A tibble: 143 x 3 ## tumor_vol mouse_ID day ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27.2 38 17 ## 2 12.7 39 17 ## 3 25.6 43 17 ## 4 7.9 48 17 ## 5 16.1 51 17 ## 6 61.5 58 17 ## 7 20.1 62 17 ## 8 56.1 63 17 ## 9 17.1 64 17 ## 10 15.1 66 17 ## # ... with 133 more rows 31.11 Convert variables to factor ANOVA are called factorial analyses. That’s because the predictor variable is a factor. We need to convert the day variable into a factor. While we’re at it, we’ll convert the mouse_ID into a factor, too, so that it plays nice with ggplot and ANOVA. jw2vol &lt;- jw2vol %&gt;% mutate( mouse_ID=as.factor(mouse_ID), day=as.factor(day) ) jw2vol ## # A tibble: 143 x 3 ## tumor_vol mouse_ID day ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 27.2 38 17 ## 2 12.7 39 17 ## 3 25.6 43 17 ## 4 7.9 48 17 ## 5 16.1 51 17 ## 6 61.5 58 17 ## 7 20.1 62 17 ## 8 56.1 63 17 ## 9 17.1 64 17 ## 10 15.1 66 17 ## # ... with 133 more rows 31.12 Plot the data Repeated measures on subjects is the primary feature of this data set. Within each mouse_ID, every measurement is intrinsically-related to every other measurement. Point-to-point graphing illustrates this. Here’s all the data! It’s beautiful. ggplot(jw2vol, aes( day, tumor_vol, group=mouse_ID, color=mouse_ID))+ geom_point()+ geom_line()+ #scale_color_brewer(palette=&quot;Paired&quot;)+ scale_color_viridis(discrete=T)+ theme_classic() "]
]
