[["index.html", "JABSTB: Statistical Design and Analysis of Experiments with R Preface", " JABSTB: Statistical Design and Analysis of Experiments with R TJ Murphy PhD, Department of Pharmacology and Chemical Biology, School of Medicine, Emory University, Atlanta, GA biostats538@gmail.com 2021-01-21 Preface This book is assembled as resource for students enrolled in my biostats course at Emory University in Atlanta: IBS538 Statistical Design and Analysis. The students are in one of Emorys biomedical and biological sciences PhD programs. The class usually also includes the occasional Emory honors program undergrads, students from Emorys public health school, and a few Georgia Tech graduate students. When I finally made the switch to teaching my course in R I needed to prepare a bunch of handouts, essentially handout per statistical model. I did that using R Markdown. Then I decided to write some intro handouts. Then I found Bookdown. Before I knew it, I had a book. This is that. JABSTB. A collection of handouts. Not included in this book are additional materials for the course (eg, take home and group exercises, slide decks, data sets, my extensive collection of stats cartoons, etc). The scope of the book is to provide some background on statistical fundamentals that are most relevant to the biomedical researcher and to provide examples for running and interpreting various statistical functions. These people test ideas by generating data after manipulating some independent variable(s). They need to know principles of sampling, error, statistical hypotheses, types of data and experimental design. Each chapter has a corresponding R Markdown document. If you wish to grab those documents (and any data sets read in those Markdowns) instead of using this material as HTML, go grab it on from the Github jabstb repo. This book is a living document, subject to a lot of on-the-fly revision. Stuff will be added and eliminated over time. As I edit these words, in 2021, my main disclaimer is that it remains an MVP. And it is clearly over-written. If you find errors, have any suggestions, or would otherwise like to contribute, I welcome your input. Please submit a pull request and/or contact me by email. This work was supported by the National Institute of General Medical Sciences-NIH, T32 supplement 3T32GM008490-23S. Copyright 2018-2021 Â© TJ Murphy MIT license. "],["author.html", "Chapter 1 About the author and book", " Chapter 1 About the author and book I first learned this material as a graduate student at Mizzou. The stats courses that impacted me the most were taught by the late Gary Krause, then a professor and statistician in Mizzous agricultural college. The light turned on for me during Garys Experimental Design course. Thats when the fog of mathematical statistics cleared enough so I could finally get the pragmatic value of statistics for the researcher. Why it didnt hit me earlier, I dont know. Id been involved in plenty of research and data collection by then. What dawned on me is that experimental design is a statistical framework for conducting unbiased research. That concept permeates my course and this book and it is the one thing I most want my students to take away from suffering through me for a full semester. Heres how it hit me. I was working on my PhD in pharmacology within the medical school. But most of my classmates in Garys courses were from the other side of campus, working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. The problem my classmates shared, which was not a problem that really affected me, is having only a single growing or mating season by which to run a fully replicated experiment. Figure 1.1: One shot! They only had one shot. Which changes everything. Planning was a priority for them. They needed to map out their experimental design well in advance and with all the statistical thought on the front end. Once the experiment began, they didnt have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups. This planning was based upon statistical design principles, often in consultation with Gary. Defining variables and endpoints and comparisions and sample sizes and more. For them, statistics were mostly a priori planning. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the script they wrote up the previous winter. That is how unbiased research is conducted. In contrast, it is fair to say that a lot of biomedical scientists fail to incorporate statistical design into their plans at all, yet run statistical tests and inference as if they did. Failure to write and follow a script opens up a whole can of worms that, kindly, characterized as using statistics in ways our statistics were never meant to be run. The biomedical researcher who takes a more fly by the seat of their pants approach to running experiments and collecting data is too common. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation of what the data obviously show. Oops. Unfortunately, that approach is riddled with biases. A lot has been written in recent years about the replication and reproducibility crisis in biomedical research. When reproducibility is defined as achieving the same analytic result given identical raw data, then any failures are mostly a breadcrumb problem. Thats relatively easy to solve. We just need to change our data handling and analysis process and be more explicit about how we arrive at our numbers statistical solution. When replication is defined as an independent repeat of the same general result, then any failures are indicative of a bias. Bias is more complicated to fix then just a simple breadcrumb problem. Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made. Therefore, the focus of this course is in teaching statistics not as tests to run after the fact, but as an experimental design framework. The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance, to follow it (and to feel really, really guilty when they dont) and to appreciate why this is so important to reduce bias. That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs and data types. Most of which she will forget immediately. And since I emphasize pre-planning, this book is full of simulations. These mostly are simulations designed to determine a sample size necessary to run an experiment. However, they are based on a method in which the experimental groups are all pre-specified. This provides a way to map out and even visualize potential results in advance. Which is a great way to check assumptions. Im not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work under the hood. When I do, it is mostly for stuff I think should be helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just dont believe Im the person who should be teaching it. Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. Thats why operating within a statistical design framework is so important. Perhaps most importantly, for the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers the potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments that prolonged time to degrees. "],["history.html", "Chapter 2 A Brief History of Experimental Design", " Chapter 2 A Brief History of Experimental Design Researchers in the pre-statistics days lacked the statistical framework that todays researchers take for granted. Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they struggled with designing experiments, in summarizing the data, and in drawing unbiased inference from it. The statistical approach to experimental design we use today was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. Figure 2.1: RA Fisher in 1913, from the Adelaide Digital Archive At the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. He took that trajectory by accident. For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwins theory of evolution, which was the hot discovery of the day, of course. Fishers side hustle was to work on mathematical problems related to evolutionary genetics. Today, we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. Thats certainly where his career ambitions seem laid. He never lost an interest in evolution and would go on to become, unfortunately, a prominent eugenicist. The take-away from that, alone, is that statistics is not a fool-proof antibias framework. Still, one big contribution he made during this early stage was no small feat. He defined variance and its relationship to the mean of a population. He proposed that variance is useful as a descriptive statistic for the variability within a population. Further developed, it would soon become the foundation of the powerful multigroup experimental designs that are called ANOVA, the analysis of variance, which are widely used today in the biomedical sciences. In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar (WWI) agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of this data they already had on hand. Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher early in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the stations data, some of which represented serial agricultural experiments that had been running for decades. As he dug in Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of much of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds at all familiar then Ive achieved my objective for mentioning it. Theres nothing worse than spending months of painstaking effort collecting data that cant be analyzed. Heres when the paradigm shifted. Fisher began to think about the process by which experimental data should be collected. Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. As I mentioned above, his invention of the analysis of variance extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well. It was a truly remarkable period. In 1925 Fisher published a small book, Statistical Methods for Research Workers. In 1934 he published its extension, Design of Experiments. In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would soon cross oceansand then disciplines. It is telling that the first academic statistics department was at Iowa State University, in the heart of the corn belt, in the 1920s. George Snedecor started it. George Snedecor is also the author of the F-distribution and the F-test, which is used to make decisions on ANOVA experimental designs. By the 1950s statistical methods were spreading rapidly through the medical literature. Today, experiments that we would recognize as statistically rigorous are those in which Fishers early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. Theres much more to this history, including many additional players and plenty of controversy that remains unsettled to this day. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today. "],["software.html", "Chapter 3 Installing and Understanding the software 3.1 tl;dr 3.2 Spring 2021 term versions 3.3 After installation 3.4 What is R? 3.5 More on packages 3.6 How to install and use packages 3.7 Packages well need 3.8 Super immportant: Always avoid compiling 3.9 Summary", " Chapter 3 Installing and Understanding the software When buying commercial, GUI-based software, were mostly paying for a silky smooth install experience. Open source software is free. 3.1 tl;dr Download and install R binaries. Then download and install RStudio Desktop Free. In that order. If youre compiling, youre doing it wrong. 3.2 Spring 2021 term versions R: 4.0.3 Bunny-Wunnies Freak Out RStudio: 1.4.11103 3.2.1 What are these? R is the computational engine. RStudio is an integrated development environment (IDE) that makes using R easy. 3.2.2 Rule 1: Install R first, install RStudio second This will make it easier for RStudio to automagically find your R installation. 3.2.3 Rule 2: Dont pay! One sign you are doing it wrong is when you find yourself about to purchase something. Step away if thats the case. These are free. 3.2.4 Rule 3: Dont compile! Make sure you download and install the PRECOMPILED BINARY VERSIONS Unless you really really really know what you are doing, do not compile from source or source code. 3.2.5 Rule 4: Install the right R software for your machine R binaries for Windows R binaries for Mac/OsX R for Linux RStudio Desktop FREE 3.2.6 Doubts? Unsure? Like videos instead? Go to this link, find your machine system, and follow the directions therein. 3.3 After installation Launch only RStudio on your machine. RStudio should be able to find your R installation and will run that for you. You will see the R console as one of the panes in R studio. On the prompt (&gt;) line, type either 2+2, 2^2, 16/4 or 2*2. If you get 4 with any of these you are good to go. Problems? Just uninstall and try to reinstall again. 3.4 What is R? R is an object oriented programming language, useful for a wide assortment of data analysis projects. It is also open source and free. The R youve just installed is the core software along with some base packages. R packages are collections of R functions that accomplish specific tasks. People write and share their packages with all of us, for free. Thus, open source. For the most part, we will work with a fairly limited repertoire of R packages. We will use packages that allow us to read data files, to organize datasets so they can be analyzed, to plot the data, and to run statistical tests. 3.5 More on packages There are two types of packages, system or user. To see these, after installing R and then R Studio, on the top menu go to View &gt; Show Packages (or use Ctrl+7 on your keyboard while in RStudio). Or in one of your R Studio panes youll see a Packages thumbnail. The packages in the system library come along with your R installation. These are often referred to as the base packages. The user library are packages we those we install on our machine. The user library will grow over time. As we do more things with R, well find more uses for all sorts of additional packages. If you already had a prior version of R on your machine, updated versions of your favorite packages will need to be re-installed. For this reason, during the course I want everybody working with the same R version. Dont use a prior version. And dont upgrade to the next version if one comes out during the semester. 3.6 How to install and use packages This is really important. Whenever you need to install a package that you are missing, say, the foobar package, go to the console. Next to the &gt; prompt, type the command install.packages(\"foobar\") and then hit enter. R will search for the package on an internet repository, download it, and install it.1 You only need to install a given package once when working with a given R version. But installing a package doesnt make it available for use. Whenever we need to use functions within an installed user library package, you must first load it into your working environment during your current R session. We use the library() function for this. Or we can go to the Packages thumbnail and click on the box just left of the package name. For example, typing library(foobar) in the console will load the foobar package in your environment. The functions within foobar that we want to use will now be available. They will remain present in your environment until you clear the environment. For example, by shutting down and restarting R. Notice the installation command uses quotes \"\" whereas the load library command does not.2 The packages in the system library dont need to be called into the environment using the library() function. System packages work automagically. 3.7 Packages well need At the beginning of each chapter I list the packages that will be necessary to run the scripts within that chapter. In order for you to run my scripts, you will first need to install those packages on your machine and load their library when you need their functions in an R session. Note: When you are working in an RMarkdown document, you need to load those libraries in a code chunk within the RMarkdown document. And run it. Remember, you install a package only once. After that, the package and its function are stored on your machine. Whenever you need to use the package (or a function(s) within it) for the current session, you will need to load it into your environment using the library() command 3.8 Super immportant: Always avoid compiling When installing R or R packages you are sometimes presented with the option of compiling from source code. For example, after typing an install package command you might see a trigger message in the console like this: Do you want to install from sources the package which needs compilation (yes/no/cancel)? In almost ALL cases, the selection should be no. At which point R will download and install the binary version. Binary versions are pre-compiled. 3.8.1 Reasons not to compile No need to make things more complicated when you are starting out. Compiling lengthens the installation timeoften quite dramatically. Source code is often developmental, and may have bugs. Our machine may not be configured to compile properly (eg, our JRE version is out of date or not set up right.) Compiling might make it harder for you to perform the required tasks for this course. As your R experience and skills grow, down the road youll likely want to use packages you need to compile. Wait until then. 3.9 Summary R and RStudio are free. When we pay for commercial software we are mostly buying a smooth installation experience. Open source software installs are never completely idiot proof. For most people, the installations will go smoothly. If you experience any problems you can always uninstall, and then try to install again. Otherwise, contact me or the course TA for help. You only have to install packages onto your machine one time per R version. But you will have to run the package library command with each new session or when working inside an RMarkdown document. Never compile from source unless you know what you are doing. Alternately, on the RStudio top menu bar click on Tools &gt; Install Packages Seems weird but it makes a lot of sense. Before it got on our machine, the package had a name. Names are character strings. Strings get enquoted in R. Once the package is on our machine, it is an object. Objects are never enquoted in R. "],["getting-started.html", "Chapter 4 Start Learning R 4.1 Digging in with R/RStudio 4.2 The tidyverse package 4.3 Other resources", " Chapter 4 Start Learning R library(swirl) library(tidyverse) For this course I want you to read and understand everything in these chapters, including the R code. Especially the R code. When you see code leading to some output in this book, that means I think it is important for you to understand that code. I want you to read it and to understand it. Youll need it as a basis to write scripts to accomplish virtually every task assigned in this course. The other stuff Ive written in the chapters is about experimental design and statistics. Which are important, too. Writing R code is the method Ive chosen for you to learn this statistical material. My code demonstrations are approached with parsimony. For example, my plots are pretty minimal. There is a LOT more that could be done with them. I want you to focus on the very basics of throwing up data as a plot. Once you learn that, learning how to customize them more fully is super easy. 4.1 Digging in with R/RStudio After installation of R and RStudio, launching just RStudio on your machine should automagically start R. Explore the RStudio menu bar and the RStudio panes. Click on stuff. Change stuff. There is very little for you to break! Most of the menu bar functionality is overkill for a beginner. But a few things are immediately useful. Under Tools select Global Options and experiment with an font and color appearance configuration that pleases your eye. Set the Pane Layout to your liking. I like source on top left, console on top right. YMMV. Under Session you can select Restart R. Sometimes R needs a reboot. That will turn R off and on without having to close out RStudio and a document you might be working on within the document pane. 4.2 The tidyverse package The tidyverse is a collection of R packages designed for doing common tasks in data handling. Well use tidyverse commands a lot. The tidyverse is a more modern syntax and I think more intuitive. Also, well be doing data visualization with ggplot, which is part of the tidyverse. See the next section for installation of the tidyverse packages. 4.2.1 Do swirlstats Once you have R and RStudio up and running a homework assignment will be to work on the R Programming module in the interactive swirl package. Install and load the swirl library by typing the following into the R console within R studio, line-by-line. Hit enter after each line so the code will run. install.packages(&quot;tidyverse&quot;) install.packages(&quot;swirl&quot;) library(tidyverse) library(swirl) Swirl is basically a guided sandbox that will take you through an overview of the basics. There are a lot of basics, most of which youll forget! But thats OK. I want you to see the 30,000 ft view. As you go through swirl I suggest you go to the source pane to open up a fresh R script file or R markdown file to take notes (eg, copy/paste swirl code snippets) for a custom cheat sheet. Experiment with running those snippets from a source code window and saving the source code file. The first few weeks youll probably come pack to your cheat sheet, until things get less foggy. 4.3 Other resources Ive found these to be the most useful. R for Data Science is an outstanding resource for the scope of R in data analysis. Lots of code tips. The STHDA website has tips for coding most ggplots imaginable. You can scroll through, find the plot you want, and use their code to get started. The tidyverse website offers reproducible examples for using the functions of the core tidyverse packages. 4.3.1 Internet search Just Google it. I usually only click on Stack Overflow links that come up in Google searches because SO focuses on reproducible code and solutions. The strong moderation helps keep things clean and organized. Youll probably never have to ask a question on SO. Theres a good chance someone else asked it, and it has been answered already. 90% of the time the answers with the green checks will save you. Note. Were going to focus on using the tidyverse functions. So when you Google for help, be sure to include that your looking for tidyverse-based solutions. For example, how to add a data frame column in r tidy Adding tidy will show you solutions using tidyverse-based approaches. 4.3.2 Help Documents Another sources is Help within R. In the R console of RStudio you can type: ?ggplot which will launch a help document for the ggplot function in the Help viewer pane. It may take you some time to learn how to read Rs Help documentation. Which tend to be cryptic. Functions have arguments and the help documents mostly just tell you about the scope of these arguments. But the Help for all functions includes examples. I think the easiest and fastest way to learn how to use a function is by playing with the examples. Open a fresh R script document and paste example code into it. Run the example code line-by-line. Make some adjustments. See what happens. 4.3.3 Package Vignettes Many, but not all, packages have vignettes written by the package authors. These tend to offer deeper explanations than function Help documents. 4.3.4 People You cant learn without trying stuff. Dont be afraid to make errors. Lean on your classmates, lean on me, lean on the TAs. Engagement and interaction with a human is often the fastest way to get through a problem. 4.3.5 typos As you work with R youll make mistakes. Of these, &gt;90% will be typos. Misspellings, missing commas and unclosed parentheses or brackets will happen a lot. Figure 4.1: The weirder the R error messages the more probably I have a typo. The other 10% of mistakes come from assuming R can read your mind. Eventually you figure out R is very stupid (or obedient). That you need to tell R exactly what you want it to do, one instruction at a time. At that point, mistakes become lessfrustrating. A fast solution is often to copy and paste error messages into your browser search bar. Most of the errors you will make have happened before. Most of what you want to do someone has done before. But after a few searches if nothing is making sense, look for the typo. Code camps are nice, but as a PhD student they will probably bore you pretty quickly because they tend to work on problems that dont interest you. By far the best way to learn R is to just start using it to solve the problems you need to solve. Whether they are problems in this course or with your own data. Ill provide you the problems you need to solve in this course. They are mostly about learning statistics and reproducible data handling, and involve biomedical data sets. Youll end up learning how to use R as a side benefit. "],["philos.html", "Chapter 5 What is your philosophy? 5.1 Exploration and uncertainty", " Chapter 5 What is your philosophy? As a new PhD student you are in a significant transition state. Youve been a really good knowledge consumer. Otherwise you wouldnt have been accepted into a highly competitive Ph.D. program. Now things are different. An important condition of your Ph.D. will be to create an original contribution to knowledge. In other words, your graduate program training is mostly about the transition from a knowledge consumer into a knowledge producer. The two require different toolkits. For example, a knowledge consumer might use statistics as only rote processthat there are right ways and wrong ways to do things. They might say, Tell me the rules and procedures and Ill just follow them. Or they might ask, tell me about when one test works better than some other? A knowledge producer will want to go deeper to understand why things are done the way they are. To question their validity and wonder if they can be improved. They may even change the process, adapting rules and procedures that fit her philosophy better. Or they will crack open the computer to see how functions work and simulate data sets to see how tests perform under different assumptions. A knowledge producer makes adjustments and is able to defend their approach. Youll find this course is mostly about pointing out the various statistical decisions youll need to make as you go about creating new knowledge. In this course I am going to hammer home the point, over and over, that you have a responsibility to think critically about your statistical practice. Hopefully youll see that how we go about collecting and analyzing data is more important than the data itself. In a statistical design framework, our intentions and our data generation process matter. How we think about the analysis matters. We have a lot of judgments to make. To a large extent, the experimental design and analysis procedures we choose are driven by our philosophical approach to science. From time to time we need to lean on philosophy when called to defend our decisions (for example, when interpreting results, or in rebutting a reviewer, or when deciding to move forward or to stop some project). When I say philosophy Im thinking in terms of the really fundamental ideas. Do you think truth is absolute and attainable, or provisional? What is the nature of evidence? What are the merits of inductive and deductive reasoning and how much weight should you give to each in your problem solving process? How should you test hypotheses, through affirmation or falsification? Are you more Popperian (deduction, falsification) than Carnapian (induction, confirmation)? What are your biases? Do you and your workflow have integrity? Are you a perfectionist or a dontgiveafuctionist or somewhere in between? It is a big, diverse world. Reasonable people operate on either side of such philosophical and behavioral divides. These questions have a lot of gray areas. My sense is that most of us biomedical scientists are hybrids. We go about discovery through a mixture of inductive and deductive practices. The process of making observations then establishing models before seeking confirmation in related experiments is both inductive and empirical. Whereas experiments that test predictions based upon some model generate observations that require deductive reasoning. We often use deduction when formulating a logic for deciding to open up a new thread of research. 5.1 Exploration and uncertainty What everyone seems to agree upon is that creating knowledge occurs through exploration. And to explore means we have to deal with uncertainty. By what criteria will you ensure you are not making mistakes? By what criteria will you validate an observation? How will you ensure a hypothesis has been well-tested? The answers to this group of questions are found in inferential statistics, which in turn use probability. Probability is the mathematics of uncertainty. Probability provides a way to keep score when were working near the forefront of knowledge, confronted by improbable ideas and uncertain data. It turns out that there are a couple of statistical frameworks that people tend to apply to derive probability. One decision youll have to make is to choose which of these frameworks to apply for your work: Bayesian statistics or the Fisher/Neyman-Pearson based error statistics, aka frequentism? In the Bayesian framework, the rules of probability are used to represent the plausibility of a hypothesis under the observed data. In the Fisher/Neyman-Pearson framework, probability is used to assess the plausibility of the data under a null hypothesis. Equally reasonable people choose either framework. If you search for Bayesian v frequentism you will run smack dab into what are called the statistics wars. In one or two clicks you are sure to stumble into impassioned arguments on each side. knitr::include_graphics(&quot;images/statistics_wars.gif&quot;) Figure 5.1: When you wander into a chat between a frequentist and a Bayesian. Which is better? Im honestly agnostic. I learned, practice and teach error statistics, or something along the lines of the Fisher/Neyman-Pearson school. But I definitely appreciate the validity of Bayesian approaches in many circumstances. That sounds wishy-washy so here is a hill that I will stand on. It is less important that one framework might be better than the other. Whats more important is to operate within a framework, for which you fully understand its strengths and limitations. This course advocates adopting a framework that speaks to replicable experimental practices, to variables and models, to the implications of evidence, and also to a reproducible workflow. The inferential statistics you will learn are mostly frequentist. That choice is motivated for several reasons that I wont go into here, except for a nod towards a philosophical reason that makes a lot of sense to me, as spoken by Deborah Mayo: In the severe testing view, probability arises in scientific contexts to assess and control how capable methods are at uncovering and avoiding erroneous interpretations of data. That is what it means to view statistical inference as severe testing. A claim is severely tested to the extent it has been subjected to and passes a test that probably would have found flaws, were they present. The probability that a method commits an erroneous interpretation of data is an error probability. Statistical methods based on error probabilities I call error statistics. It is not probabilism or performance we seek to quantify, but how severely probed claims are. -Mayo "],["bigpic.html", "Chapter 6 Stats: The Big Picture 6.1 What are experimental statistics?", " Chapter 6 Stats: The Big Picture To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher library(tidyverse) library(Hmisc) Lets start by listing out some key characteristics that most biomedical experiments share in common. They tend to involve the use of relatively small sample sizes. are usually highly exploratory in nature, measuring new variables under new conditions. generate data that are either counted as discrete counts or measurements of continuous scalars. are structured as a small group of fairly common experimental designs. are usually interpreted in a binary way; as having worked, or not. test hypotheses (though too often these are unstated). aspire for rigor, replicability and reproducibility. aspire to be unbiased. The stakes of our work can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge, and opening the door to impact, particularly in the realm of human diseases and suffering. But no less motivating are the issues more related to the professional practice of science. These include ego, the completely natural impulse to seek out validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. The point is that the process of scientific discovery is driven both by ideals and by biases. This is nothing new. What may be new is the fact that you are not shielded from bias simply because you work in a university and your work is peer-reviewed. The one big concept that I hope you embrace is that the statistical design and analysis of experiments serves antibias framework. A biomedical researcher operates within that framework to conduct reasonably unbiased work. The statistical approaches covered in this course, it turns out, were invented long ago with this purpose in mind. 6.1 What are experimental statistics? Experimental statistics are used in experimental planning, to summarize data into simpler descriptive models, as procedures to make valid inferences, to serve as framework for conducting unbiased research. Chances are that you thought biostats was only applied math or that part of software used to produce summaries and p-values after a trove of data has been collected. It is much more than that. 6.1.1 Descriptive modeling Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics. Thus, we use parameters like the sample mean and standard deviation, or median and ranges or percentiles to describe what weve just measured. The alternative is unthinkable today (but common long ago), which is to discuss each replicate individually. Descriptive modeling is just summarizing what we found in a sample and extending that to the population that was just sampled. To emphasize that sample parameters differ from population parameters, the standard in statistical notation is to use roman characters to indicate samples and greek characters to indicate the population. For example, parameter sample population mean \\(\\bar y\\) \\(\\mu\\) standard deviation \\(s\\) \\(\\sigma\\) variance \\(s^2\\) \\(\\sigma^2\\) Thus, the sample mean, \\(\\bar y\\) is an estimate of the population mean, \\(\\mu\\). Statistical tests also have a descriptive element in that they convey information about the experimental design. If you say, Im working up a two-tailed paired t-test, say no more. From that alone I know something about your hypothesis, how your replicates are handled, the number of predictor groups, and the type of data youre measuring. Regression models also describe data. For example, here is the well-known Michaelis-Menten model that describes product formation as a function of substrate concentration. \\[[P]=\\frac{[S][Vmax]}{[S]+Km}\\] Thats a model we might fit to certain kinds of enzyme kinetic data. The model is used to estimate scientifically meaningful parameters, like \\(V_{max}\\) and \\(K_m\\), from experiments where we control \\([S]\\) while measuring \\([P]\\). In fact, mathematical statistics is actually just modeling. And modeling is the process of simplifying experimental data into something more coherent. Take a simple example of two groups shown here. Each group has been fit to a simple, very common model: sample mean and standard deviation. Clearly, that model fits the control group much better than it fits the treatment group. Figure 6.1: Is the mean for each these groups a good descriptive model? Why do I say that? The treatment group data are much more skewed. Most of the data values are greater than the mean of the group. Sure, a mean can be calculated for that group, but it serves as a fairly crappy summary. Perhaps some other model (or group of statistical parameters) would better convey how these data behave? This is to point out that learning statistics is about learning to make judgments about which models are best for describing a given data set. 6.1.2 Statistical inference There are two main types of inference researchers make. One type is to infer whether an experiment worked or notthe so-called significance test. This familiar process involves calculating a test statistic from the data (eg, t-test, F-tests, etc) and then applying a threshold rule to its value. If the test passes the rule, we conclude the experiment worked. I cover this type of inference in much more detail in the p-value chapter 12. Well talk about this one over and again throughout the course. A second type of inference is to use a sample parameters as estimates for values of the variable within the sampled population. For example, the average height students in an Emory graduate biostats course can be used as an estimate of all biostats students (or all Emory graduate students). Both descriptive and statistical inference are subject to error. By random chance/luck-of-the-draw our sample estimates can be off the mark. Usually, we dont how wrong we might be. A great example for this is seeing the various The real difficulty with inference is we can never know for certain whether we are right or wrong. They are called random variables for a reason. It pays to have a very healthy respect for the role played by random chance in determining the values of our parameter estimates. If we were to completely redo a fully replicated experiment once more, we would almost certainly arrive at different numbers. In a well behaved system, theyd likely be in the same ballpark as those of the first experiment. But they would still differ. To illustrate, copy and paste the code chunk below. It replicates a random triplicate sample six times, taking six means. Unlike in real life, the population parameters are known (because I coded them in): \\(\\mu=2\\) and \\(\\sigma=0.4\\). You can run that chunk tens of thousands of times and never get a sample with one mean that has a value of exactly 2, even though thats the true mean of the population that was sampled. x &lt;- replicate(6, rnorm(3, 2, 0.4)) apply(x, 2, mean) ## [1] 2.092121 2.025281 2.093270 1.612758 2.096955 2.276354 6.1.3 Experimental design Experimental planning that involves dealing with statistical issues is referred here as experimental design. This involves stating a testable statistical hypothesis and establishing a series of decision rules in advance of data collection. These rules range from subject selection and arrangement, predetermination of sample size using a priori power analysis, setting some data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, declaring a primary outcome, on up to what statistical analysis will be performed on the data. Experimental design is very common in prospective clinical research. Unfortunately, very few basic biomedical scientists practice anything remotely like this. Most biomedical researchers begin experiments with only vague ideas about the statistical analysis, which is usually settled on after the fact. Much of the published work today is therefore retrospective, rather than prospective. Yet, most researchers tend to use statistics that are largely intended for prospective designs. Thats a problem. 6.1.4 Statistics as an anti-bias framework If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and youre not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct. The main purpose of doing statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesnt provide us a magic woo-woo cloak to protect us from our biases. Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias. Here are some of the more important biases. 6.1.4.1 Cognitive biases From a statistical point of view biases can be classified into two major groupings. The first are Cognitive biases. These are how we think (or fail to think) about our experiments and our data. These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. Following a negative result, if you ever find yourself declaring, how could this not work!?! you are in the throes of a pretty deep cognitive bias. In bench research, cognitive biases can prevent us from building adequate controls into experiments or lead us to fail to think about alternative explanations for results, or prevent us from spotting confounding variables or recognizing telling glitches in the data as meaningful. Cognitive biases are the rose-tinted glasses worn by all scientists. 6.1.4.2 Systematic biases The second genaral bias are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed, and how data is handled or transformed. Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line. 6.1.4.3 Scientific misconduct More malicious biases exist, too. These include forbidden practices such as data fabrication and falsification. This is obviously a problem of integrity. Very few scientists working today are immune from the high stakes issues that pose threats to our sense of integrity. In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them. The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite livable. If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know youve given it your best shot. That is hard to conclude when the latter occurs. 6.1.4.4 Statistical bias In statistical jargon something that is said to be biased is inaccurate. Bias, as a statistical term, is a synonym for inaccurate. "],["sampling.html", "Chapter 7 Statistical Sampling 7.1 Experimental units 7.2 Independent Replicates 7.3 Random process 7.4 Statistically valid samples 7.5 Independence of replicates", " Chapter 7 Statistical Sampling An experiment is no more reliable than is its sample. -TJ Murphy A statistically valid sample is comprised of independent replicates of the experimental unit, which are generated using some random process. To unpack this lets think about each of the following terms: What are experimental units? What do we mean by independent replicates? What is a random process? When is statistical validity even important? 7.1 Experimental units The experimental unit is the source of the measurement. An experimental unit can generate one or many measurement values. I prefer the concept of an experimental unit to the concept of subject, though they often mean the same thing. Ive found the word subject carries more ambiguity, especially for people first learning sampling and sample size concepts. In some experimental designs (eg, unpaired or completely randomized) each experimental unit generates a single measurement value. Here there is a one-to-one correspondence exists between the number of experimental units and the number of measurement values within a data set. In other designs (eg, paired or matched or repeated/related measure), a single experimental unit can generate more than one measurement values for the same variable. Such data sets have more values than experimental units. Here are some guidelines for deciding what is the experimental unit in an experiment, with full recognition that sometimes there are gray areas. Ultimately the researcher has to use scientific judgment to recognize or define the experimental unit. 7.1.1 A simple test to define the experimental unit When defining an experimental unit I recommend using a simple test: Are these measurements intrinsically-linked? If two or more measurement values are intrinsically-linked then they would comprise paired or matched or related measures from a single experimental unit. So how could you judge whether two or more measurements are intrinsically-linked? For the most part, this happens when the source of those measurements doesnt differ. Here are a few examples: A before and after design. A mouse is scored on how well it performs a behavioral test at baseline, before a treatment. After that same mouse receives a treatment it is run through the behavioral test once more to get a second score. Those two scores are intrinsically-linked because they were taken from the same mouse. All that differs between the scores is the absence or presence of the treatment, the effect of which the researcher is trying to measure. We would also say those two scores are matched, paired or related/repeated measures. A single mouse from which two scores are derived is an independent replicate of the experimental unit. Twinning. Take for example a study involving human identical twins. In these studies identical twin pairs are modeled as a single experimental unit due to their high level of instrinsic relatedness. There are two human subjects but they are modeled statistically as a single experimental unit. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. One of the pair receives a control condition while the other receives a treatment condition. A measurement is taken from each person. There are two measurements in total, and two people, but only a single experimental unit. Given that the twins are so identical we could reasonably conclude these two measurements are intrinsically-linked. We can model the pair as one. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. Unpaired or completely randomized In contrast, imagine a study using the same control and treatment conditions using unrelated humans (or some other outbred animal species) as subjects. Each subject is assigned either a treatment or a control, and only a single measurement is taken from them. Since the subjects are each very different from each other, we could not conclude that measurements taken from them are intrinsically-linked. Each person stands alone as an experimental unit. The data would be analyzed using an unpaired, unmatched or completely randomized test. Intrinsically-linked measurements are very common in bench work. In fact they are too often overlooked for what they are and mistakenly analyzed as unmatched. Experiments involving batches of biological material, cultured cells and/or littermates of inbred animal strains routinely involve intrinsically-linked measurements. As a general rule, these should always be designed and analyzed using matched/paired/related measures procedures. Cell cultures Cell cultures are remarkably homogeneous. The typical continuous cell line is a monoculture passaged across many doubling generations. Imagine a test conducted on a 6 well multi-well cell culture plate. Each well receives a different level of some treatment condition, such as a dosing or time-course study. All of the wells were laid down at the same time from a common batch of cells. Each well is very highly related to all of the other wells. The intrinsic differences between wells would be relatively minor and mostly due to technical variation. Theres no real inherent biological variation from well-to-well other than that attributable to the level of treatment the well receives. As a result, all of the measurements taken from a plate of wells are intrinsically-linked to each other. The experimental unit is the plate. They should be designed and analyzed using matched/paired/related measure statistical procedures. Furthermore, any other plates laid down at the same time from the same source of cells are virtually identical clones of each other. If we were to expose the wells in all of those plates to various treatments followed by taking some measurement, then it is pretty easy to argue that all of those measurements taken on that passage of cells are intrinsically-linked. None of the wells are independent of any of the other wells, irrespective of the plate. Together, all of the plates represent a single experimental unit. Inbred mice In many regards, the high level of relatedness within inbred mouse strains doesnt differ from human identical twins, or from cultured cells, for that matter. A given strain of these animals are inbred to genetic homogeneity across several generations. For all intents and purposes all mice derived from a given strain are immortalized clones of each other. Two mice from the same litter are identical twins. Indeed, two mice from different litters from the same strain are identical twins. Due to their clonal identity all measurements taken from any of these highly related subjects are intrinsically-linked. Just as for cell culture, protocols must be contrived to break up the homogeneity. A common approach is to treat the litter as the experimental unit and take measures from littermates as intrinsically-linked. Split tissue Imagine two slices of an organ (or two drops of blood) taken from a single animal. Although the two slices (or drops of blood) are obviously different from each other, any measurements derived from each are intrinsically-linked. The experimental unit would be the animal from which that biological material is derived. Batches Finally, imagine a batch of a purified protein or other biochemical material. The batch was isolated from a single source and prepared through a single process. The material in the batch is highly homogeneous, irrespective of whether it is stored away in aliquots. Any measurement taken from that batch are highly related to any other measurement. They are intrinsically-linked. The batch would be the experimental unit. ###Blocking We have to contrive protocols to break up experimental units that have high inherent homogeneity. The statistical jargon used for this is blocking, such that blocks are essentially grouping factors that are not scientifically interesting. Going back to culture plates. Lets say we prepared three plates on Friday. An assay performed on one plate on Monday would represent one experimental unit of intrinsically-linked measures. An assay repeated on Tuesday on a second plate would represent a second experimental unit. Wednesdays assay on the third plate is also its own experimental unit. Here the blocking factor is the day of the week. Assuming we created fresh batches of reagents each day, there would be some day-to-day variation that wouldnt exist if we assayed all threee plates at once on a single day. But were not particularly interested in that daily variation, either. More conservatively, cell line passage number can be used as a blocking factor to delineate experimental units. Each passage number would represent an experimental unit and the overall replicated experiment would be said to be blocked on passage number. Defining the experimental unit and any blocking factors requires scientific judgement. That can be difficult to do when dealing with highly homogenous material. What should be avoided is creating a design that limits random chance too severely. To measure on Monday all three plates that were laid down on Friday will probably yield tighter results than if they were blocked over the course of the week. This has to be thought through carefully by the researcher in each and every case. Reasonable people can disagree what whether one approach is superior to some other. Therefore, what is important is to make defensible decisions. To do that, you need to think through this problem carefully. When in doubt, I suggest leaning towards giving random chance a fair shot at explaining the result youre observing. For example, you can make the case that measurements from two cell culture plates that were laid down on the same day but are collected on different days are not intrinsically-linked. Thats a harder case to make if they are collected on the same day. You will almost certainly have to make the case that measurements taken from two mice on different days or if they are from different litters are not intrinsically-linked. Before going there, we need to chat about what we mean by independent replication. 7.2 Independent Replicates That we should strive for biological observations that are repeatable seems self evident. An experiment is comprised of independent replicates of treatment conditions on experimental units. The total number of independent replicates comprises an experiments sample size. A primary goal in designing an experiment is to assess independent replicates that are not biased to the biological response of a more narrowly defined group of experimental units. A replicate is therefore independent when a repeat is on an experimental unit that differs materially from a previous experimental unit. A material difference could involve a true biological replicate. Measurements taken from two unrelated human subjects have a material difference. In bench biological work with fairly homogenous systems (eg, cell lines and inbred animals) a material difference will usually need to be some separation among replicates in time and space in applying the experimental treatments. 7.2.1 A simple test for independence How willing am I to certify this is a truly repeatable phenomenon when replicated in this way? A new scientific discovery would be some kind of repeatable phenomenon. 7.2.2 Some replication examples If we are performing an experiment using pairs of human twins, each pair that is studied stands as an independent replicate. Because the pair is the experimental unit, a study involving 5 pairs will have five, rather than ten, independent replicates. If we conduct an experiment using unrelated human volunteers, or someother out bred animals, each person or animal from whom a measurement is recorded is considered an independent replicate. Their biological uniqueness defines their independence. We wander into gray areas pretty quickly when thinking about the independence of experimental units in studies involving cultured cells, batches of biological material, and inbred mice. Working with these systems it is difficult to achieve the gold standard of true biological independence. The focus instead should be on repeatability.Working with new batches of reagents and different days do I get the same response? Imagine a 6 well plate of cultured cells. No well differs biologically from any other. If each well received a repeat of the same treatment at the same time we shouldnt consider any measurements from that plate independent from others. Otherwise, the sample would be biased to that plate of cells measured at that particular time with a given set of reagents under those particular conditions. It is too biased to that moment. What if we screwed up the reagents and dont know it? Rather than being independent, it is best to consider the 6 measurements drawn from the plate as technical replicates or pseudo replicates. The data from the 6 wells should be averaged or totaled somehow to improve the estimate of what happened on that plate that day. A better approach with cultured cells is to use passage numbers to delineate independence. Thus, a 6 well plates from any one passage are independent experimental units relative to all other passages. Obviously, given the homogeneity of cells in culture, its unlikely there is much biological variation even by these criteria. But to achieve true biological independence would require re-establishing the cell line each time an independent replicate was needed. Thats rarely feasible. Inbred mice pose much the same problem. Scientific judgment is needed to decide when 2 mice from the same strain are independent of each other. One mark of delineation is the litter. Each litter would be independent of other litters. Outcomes of two (or more) littermates could be considered matched or related-measures and thus one experimental unit. 7.3 Random process You can probably sense intuitively how randomization can guard against a number of biases, both systematic and cognitive. Systematic artifacts become randomly distributed amongst the sample replicates, whereas you are less tempted to treated a replicate as preferred if you dont know what is its treatment level. Mathematical statistics offers another important reason for randomization. In classical statistics the effect size of some treatment is assumed to be fixed. Our estimate of that real value is the problem. Thus, when we measure a value for some replicate, that value is comprised of a combination of these fixed effects and unexplained effects. The variation we observe in our outcome variables, the reason it is a random variable, arises from these unexplained effects. These can be particularly prominent in biological systems. Randomization procedures assures those random effects are truly random. Otherwise we might mistake them for the fixed effects that are of more interest us! This concept will be discussed more formally in the section on general linear models. Suffice to say for pragmatic purposes that random sampling is crucial for limiting intentional and unintentional researcher biases. Either the experimental units should be selected at random, or the experimental units should be assigned treatments at random, and/or the outcome data should be evaluated at random (eg, blind). Sometimes, doing a combination of these would be even better. Usually, the researcher supervises this randomization using some kind of random number generator. Rs sample() function gets that job done for most situations. Lets design an experiment that involves two treatments and a total of 12 independent experimental units. Thus, 6 experimental units will each receive either of the two treatments. Lets say that my experimental units each have an ID, in this case, a unique letter from the alphabet. Using sample(1:12) we randomly assign a numeric value to each ID. This numeric value will be the order by which the experimental unit, relative to the other experimental units, is subjected to the experimental treatment. IDs that are assigned even random numbers get one of the two treatments, and odd numbered IDs get the other treatment. What weve done here is randomize both the order of replication and the assignment of treatment. Thats a well-shuffled deck. You can see how this approach can be readily adapted to different numbers of treatment levels and sample sizes. set.seed(1234) ID &lt;- letters[1:12] order &lt;- sample(1:12, replace=F) plan &lt;- data.frame(ID, order) plan ## ID order ## 1 a 2 ## 2 b 7 ## 3 c 11 ## 4 d 6 ## 5 e 10 ## 6 f 5 ## 7 g 1 ## 8 h 12 ## 9 i 3 ## 10 j 8 ## 11 k 4 ## 12 l 9 7.4 Statistically valid samples For any statistical test to be valid, each replicate within a sample must satisfy the following two criteria: The replicate should be generated by some random process. The replicate must be independent of all other replicates. Why? Statistical tests are one of the last stages of a hypothesis testing process. All of these tests operate, formally, on the premise that at least these two conditions are true. When these conditions have not been met the researcher is collecting data without testing a hypothesis. To run a statsitical test is to pretend a hypothesis has been tested, when it has not. 7.4.1 Select random subjects Lets say we want to do an experiment on graduate students and need to generate a representative sample. There are 5 million people in the US who are in graduate school at an given time. Lets imagine they each have a unique ID number, ranging from 1 to 5,000,000. We can use Rs sample() function to randomly select three individuals with numbers corresponding to that range. Sampling with replacement involves throwing a selection back into a population, where it can potentially be selected again. In that way, the probability of any selection stays the same throughout the random sampling process. Here, the replace = FALSE argument is there to ensure I dont select the same individual twice. sample(x=1:5000000, size=3, replace = FALSE) ## [1] 1413668 4617167 1461579 All that needs to be done is to notify the three people corresponding to those IDs and schedule a convenient time for them to visit so we can do our experiment. You can imagine several variations to randomly select graduate students for measurements. You just need a way to find graduate students, then devise a way(s) to ensure the sampling is as representative as possible. Selecting subjects from a real population is pretty straight forward, a bit like picking 8 lotto balls from a spinning container. A lot of times in experimental work the number of subjects available to the researcher is fixed and smaller. The size of the population to be sampled can be much closer to the number of replicates needed for the experiment rather than a sample from a large pool. In these cases we have to come up with other ways to randomize. 7.4.2 Randomize to sequence For example, lets say we want to compare condition A to condition B. We have 6 subjects to work with, each of which will serve as an independent replicate. We want a balanced design so will have 3 replicates for each of the 2 conditions. Lets imagine we can only perform an experiment on one subject, one day at a time. In that case, it makes sense to randomize treatment to sequence. We can randomly generate a sequence of 6 even and odd numbers, and assign them to the daily sequence (MTWTFM) based on which random number is first on its list. We can make a rule that subjects assigned even numbers will receive condition A, whereas condition B is meted out to subjects associated with odd numbers. sample(x=11:16, size=6, replace = FALSE) ## [1] 16 12 15 11 13 14 7.4.3 Randomize to location Lets imagine 3 treatments (negative control, positive control, experimental), that we will code 1,1,2,2,3,3. These will be applied in duplicate to cells on 6-well cell culture plate. Well code the plate wells with letters, a, b, c, d, e, f from top left to bottom right (ie, a and b are wells in the top row). Now well generate a random sequence of those six letters. sample(letters[1:6], replace=F) ## [1] &quot;b&quot; &quot;a&quot; &quot;e&quot; &quot;d&quot; &quot;f&quot; &quot;c&quot; Next, well map the sequence 1,1,2,2,3,3 to those letters. Thus, negative control goes to the wells corresponding to the first two letters in that sequence, positive control to the 3rd and 4th letters, and so forth. 7.4.4 Randomize to block In statistical lingo, a block is a subgroup within a sample. A blocked subject shares some feature(s) in common with other members of its block compared to other subjects in the overall sample. But usually, were not interested in block as a variable, per se. Here are some common blocks at the bench are One purified enzyme preparation vs a second preparation of the same enzyme, nominally purified the same way. The two enzyme preps represent two different blocks. A bunch of cell culture dishes plated on Friday from passage number 15 vs ones plated on Tuesday from passage number 16. The two passages represent 2 different blocks. A litter of mouse pups born in January vs a litter born in February. The two different litters represent two different blocks. An experiment run with freshly prepared reagents on Monday vs one run on Tuesday, with a new set of freshly prepared reagents. Each experimental day represents a block. Frequently, each block is taken as an independent replicate. 7.5 Independence of replicates In biomedical research the standard is for biological independence; when we speak of biological replicates we mean that each independent replicate represents a distinct biological entities. That standard is difficult to meet when working with many common biological model systems, particularly cell lines and inbred animals. The definition of statistical independence is grounded in the mathematics of probability: Two events are statistically independent when they convey no information about the other, or \\[p(A \\cap B)=p(A)p(B)\\]. Here the mathematics is not particularly helpful. Imagine two test tubes on the bench, each receives an aliquot of biological material from a common prep (eg, a purified protein). One tube then receives treatment A and the other treatment B. As best we know, the two tubes arent capable of influencing each other. But we can reasonably assume their responses to the treatments will at least be correlated, given the common source of biological material. Should each tube be treated as if it were statistically independent? Replicate independence that meets statistical validity therefore has to take on a more pragmatic and nuanced definition. My preference is to define a replicate as the independent experimental unit receiving treatment. I like this because it allows for defining the experimental unit differently depending upon the experimental design. "],["data.html", "Chapter 8 Data Classification 8.1 Dependent and independent variables 8.2 Discrete or continuous variables 8.3 Working with data 8.4 Summary", " Chapter 8 Data Classification library(datapasta) library(tidyverse) library(viridis) The starting point in any statistical design is to understand the types of data that are involved. First ask whether the variables are discrete or continuous. Then ask if they measured, ordered or sorted? Then ask which variables are controlled by the researcher and which arise as responses? The answers will point in the proper analytical direction. I cannot emphasize enough the importance of understanding data classification for mastering a statistical framework. It is the foundation. Therefore, this material covers perhaps the most important learning objectives in this course: Given an experimental data set, be able to Describe the variables that are dependent or independent. Describe each variable as either continuous (measured) or discrete (ordered or sorted). A second focus of this chapter is on introducing some of the more common data handling procedures well do. The several scripts provide examples for how to import, inspect, subset, transform and visualize different types of data variables. 8.1 Dependent and independent variables The experimental researcher has two basic types of variables. An independent variable is the predictor or explanatory variable controlled by the researcher. Independent variables are chosen, not collected. Nevertheless, they have values just like the values of collected data. For example, In a blood glucose drug study, the independent variable Treatment would come in two levels, Placebo and Drug. In a study on how a gene influences behavior, the independent variable is Genotype, with three values: wild-type, heteroozygous and knockout In an enzyme activity assay, the independent variable is substrate, with a dozen or so values ranging over two orders of magnitude. Conventionally, the independent variable is plotted on the abscissa, or x-axis, of some graph. A dependent variable is the response or outcome variable collected in an experiment. The values that dependent variables can take on are determined by, or are dependent upon, the level of the independent variables. For example, The variable blood_glucose, collected from blood samples, has units of glucose concentration, with values over a large continuous range. In a Morris Water Maze test, memory is represented by a time variable, which is the time taken to find a remembered location, with unit values in seconds. Levels of product from an enzyme reaction are measured with an assay the emits fluorescence, in relative units. The dependent variable RFU/mg represents activity controlled by the amount of enzyme added and takes on many values over a continuous range. Most of the time the dependent variable is plotted on the ordinate, or y-axis, of some graph. 8.1.1 Statistical notation Variables are abstracted in statistical modeling. In the convention Ill use, the dependent variable is usually depicted by the uppercase symbol \\(Y\\), to represent a variable name. Sometimes experiments (eg, anything with an -omics) generate multiple dependent variables, or \\(Y_j\\). Here \\(j\\) represents the number of dependent variables and can have values from 1 to \\(k\\) different variables. The values one dependent variable can assume are symbolically represented as lowercase symbol \\(y_i\\), where \\(i\\) is the individual sample replicate, with values from 1 to \\(n\\) independent replicates. Univariate experiments will generate \\(y_i\\) measurements from \\(1 X n\\) replicates, whereas multivariate experiments will generate \\(y_{ij}\\) measurements from \\(k x n\\) replicates. Similarly, the independent variable is usually depicted by uppercase \\(X\\) (or some other letter) whereas the values are represented by lowercase \\(x_i\\). Im going to use that convention but with a twist. Independent variables denoted using \\(X\\) will represent continuous scaled variables, whereas independent variables denoted using \\(A\\) or \\(B\\), or \\(C\\), will represent discrete, factorial variables. Factorial variables will take on values denoted by lowercase, eg, \\(a_i\\), \\(b_i\\), \\(c_i\\)). Multivariable experiments have more than two independent variables. \\(X_j\\) represents the name of the \\(j^{th}\\) independent variable, where \\(j = 2\\ to\\ k\\). To illustrate dependent and independent variables think about a linear relationship between two continuous variables, \\(X\\) and \\(Y\\). This relationship can be expressed using the model \\(Y=\\beta_0 + \\beta_1 X\\). \\(X\\) is the variable the researcher manipulates, for example, time or the concentration of a substance. \\(Y\\) would be a variable that the researcher measures, such as absorption or binding or fluorescence. Each has multiple values. The parameters \\(\\beta_0\\) and \\(\\beta_1\\) are constants that modify the relationship between the two variables, which Im sure you recognize as representing the y-intercept and slope, respectively, of a straight regression line. Thus, \\(Y\\) takes on different values as the researcher manipulates the levels of \\(X\\). Which explains why \\(Y\\) depends on \\(X\\). For example, heres how the data for a protein standard curve experiment would be depicted. In the R script below the variable \\(X\\) represents known concentrations of an immunoglobulin protein standard in \\(\\mu g/ml\\). The researcher builds this dilution series from a stock of known concentration, thus it is the independent variable. The variable \\(Y\\) represents \\(A_{595}\\), light absorption in a spectrophotometer for each of the values of the standard protein. The \\(A_{595}\\) values depend upon the immunoglobulin concentration. Estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are derived from running a linear regression on the data with the lm(Y~X) script. #Protein assay data, X units ug/ml, Y units A595. X &lt;- c(0, 1.25, 2.5, 5, 10, 15, 20, 25) Y &lt;- c(0.000, 0.029, 0.060, 0.129, 0.250, 0.371, 0.491, 0.630) #derive the slope and intercept by linear regression lm(Y~X) ## ## Call: ## lm(formula = Y ~ X) ## ## Coefficients: ## (Intercept) X ## -0.0008033 0.0249705 The output indicates the regression line intercepts the y-axis at a value very close to zero, and that every one unit increment in the value of \\(X\\) predicts a 0.0249705 increment in the value of \\(Y\\). Fortunately, R functions rarely forces us to abstract by the mathematical statistics convention, X and Y. We can name variables in more comfortable terms. For example, #Protein assay data, standard units ug/ml, absorbtion units A595. standard &lt;- c(0, 1.25, 2.5, 5, 10, 15, 20, 25) A595 &lt;- c(0.000, 0.029, 0.060, 0.129, 0.250, 0.371, 0.491, 0.630) #derive the slope and intercept by linear regression lm(A595 ~ standard) ## ## Call: ## lm(formula = A595 ~ standard) ## ## Coefficients: ## (Intercept) standard ## -0.0008033 0.0249705 8.1.2 When there is no independent variable The problem of drawing causal inference from studies in which all of the variables are observed, as is common in public health and other social sciences, is beyond the scope of this course. Pearl offers an excellent primer on considerations that must be applied to extract causality from observational data here. 8.2 Discrete or continuous variables No matter if they are dependent or independent variables, all variables can be subclassified further into two categories. They are either discrete or continuous. Discrete variables can only take on discrete values, while continuous variables can take on values over a continuous range. This distinction is discussed further below. Variables can be subclassified further as either measured, ordered, or sorted. This subdivision fulfills a few purposes. First, its alliterative so hopefully easier to remember. It reminds me of Waffle House hash browns, which can be either scattered, smothered or covered. Different authors/software give these three types of variables different names, which creates some confusion. SPSS users must click buttons to classify data as either scalar, ordinal, or nominal, which correspond to measured, ordered and sorted. Another fairly common descriptive for the three types is interval, integer, and categorical. Conceptually, they mean the same things. Table 8.1: Common synonyms for types of variables measured ordered sorted scalar ordinal nominal interval integer categorical numeric factor/character/string/numeric factor/character/string continuous discrete discrete Even if we cant agree on names, everybody seems to agree that all variables can be reduced to three fundamental subtypes. Second, this taxonomy forms the basis for drawing a pragmatic statistical modeling heuristic to help you keep the relationship between variable type and experimental design (and analysis) straight. Figure 8.1: The type of data dictates how the experiment should be modeled and analyzed. Third, the measured, ordered, sorted scheme classifies variables on the basis of their information density, where measured &gt; ordered &gt; sorted. Youll see below how discrete variables lack the kind of information density of continuous variables. More pragmatically, measured, ordered and sorted variables behave differently because they represent different kinds of information. They tend to have very different probability distributions. As a result statisticians have devised statistical models that are more appropriate for one or the other. 8.2.1 Measured variables Measured variables are fairly easy to spot. Any derivative of one of the seven base SI units will be a measured variable. The gang of seven are the ampere, second, mole, kilogram, candela, Kelvin, and meter. When in doubt, a working hack is that these variables tend to need instruments to take measurements. So if youre using some instrument to measure something, and it is not counting events, it is a measured variable. Figure 8.2: The seven SI units Take mass as an example. The masses of physical objects can be measured on a continuous scale of sizes ranging from super-galaxian to subatomic. Variables that are in units of mass take on a smooth continuum of values over this entire range because mass scales are infinitesimally divisible. Heres a thought experiment for what that means. Take an object that weighs a kilogram, cut it in half and measure whats left. You have two objects that are each one half a kilogram. Now repeat that process again and again. After each split something with mass always remains. Even when you arrive at the point where only a single proton remains it can be smashed into even smaller pieces in a supercollider, yielding trails of subatomic particles.most of which have observable masses. But heres whats most different about continuous variables: That continuity between gradations means that continuous variables carry more information than other types of variables. On a scale of micrograms, objects weighing one kilogram would have one billion subdivisions. If you have an instrument that can weigh the mass of kilogram-sized objects at microgram precision, you would say that one kilogram is comprised of a billion bits of information. Thats more information than sorting an object by naming it heavy or light. 8.2.1.1 Visualizing measured variables In the precourse survey biostats students self-reported their height, in cm, which is a continuous scale. Height is a measured variable. Viewed in scatter plots, it is easy to see how the height variable takes on many different values on the y-scale. (The points have been colored by sex, mostly to illustrate how cool it is to segment one variable (height) using another (sex)). pcd &lt;- read_csv(&quot;datasets/precourse.csv&quot;) # omit heights &lt;125 cm and &gt;250 cm as unreliable entries # nobody that short or tall has ever taken the course! ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250)) + geom_jitter(aes(x=factor(0), y = height, color = sex), height = 0, width = 0.4, size = 4, alpha = 0.8) + scale_color_viridis_d(begin= 0.1, end = 0.6)+ scale_y_continuous(breaks = seq(140, 200, 10)) + xlab(&quot;biostats students&quot;) + ylab(&quot;height(cm)&quot;) + theme(axis.ticks.x = element_blank(), axis.text.x = element_blank() ) Figure 8.3: Scatterplot of biostat student heights segmented by sex Its always important to do a quick histogram of measured data. Histograms categorize height values into bins of a given number or width. The geom_histogram then counts the number of values in each bin. Histograms therefore show the frequency distribution of a variable. ggplot(data=pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250)) + geom_histogram(aes(x=height), binwidth = 2.5, color = &quot;#f2a900&quot;, fill = &quot;#012169&quot;) Figure 8.4: Histogram of biostat student heights, the most frequent height value is 170 +/- 1.25 cm. 8.2.2 Discrete categorical and ordinal variables Discrete variables are discontinuous. The units of discrete variables are indivisible. Unlike continuous variables, discrete variables have no information between their unit values. There are two types of discrete variables, either ordered or sorted. 8.2.2.1 Sorted data These variables represent objects somehow scored as belonging to one nominal category of attributes or some other. In other words, the objects are sorted into categories. You can also think of this as being sorted into buckets or bins based upon some common feature(s). The scoring process can be based upon either objective or subjective criteria. Statisticians generally frown at researchers who sort replicates on the basis of perfectly good measured data. For example, sorting people into low, mid and high blood pressure on the basis of a cuff blood pressure measurement. These instruments generate continuous variables on inherently more informative scales. Statisticians hold that sorting after measurement discards valuable information. And cut offs can be arbitrary. In a survey biostats students identify their sex as either male or female. The name of the variable is sex. The values that sexcan have are either male or female. The variable has only two nominal levels (or two categories): male and female. Perhaps most important, sorted variables have no intrinsic numeric value. Male is neither higher nor lesser than female; the values of the sex variable are just different. Wanting more of one or the other shouldnt be conflated with a variables inherent properties. Sorted variables are also called factors. Thats the dominant jargon in R. Well find ourselves converting variables to factors at times. In stats jargon, sorted, nominal, categorical, and factor are words used to represent the same kind of variable. Sorry Try not to let the jargon confuse you. Whats most important to understand is that sorted variables lack numeric values. So we count how many are sorted into one category or the other. That changes everything about the statistical methods we apply to experiments involving sorted variables. A count is an integer value. The count cannot take on any values other than integer values. For example, we dont have cases where there is a partial student of one sex or the other. These counts exist as discrete, discontinuous values. Of course, the categorization of sex is sometimes ambiguous. When important to accommodate more than two categories, we would add additional categories to account for all possible outcomes. For example, the sex variable could be set to take on values of male, female, and other. 8.2.2.2 Visualizing sorted data Lets look at the precourse survey data frame we imported above. Each row represents the data for one biostats student. Notice how the values for the sex variable are characters. There are no numbers to plot!! I could assign them numbers if I wished, for example \\(Male = 0\\) and \\(Female = 1\\), but that is just arbitrary numeric coding, using numbers rather than characters. The numeric values are meaningless. I could just as easily code one \\(1.2345\\) and the other \\(0.009876\\) and accomplish the same thing. The point being the values of sorted variables are arbitrary. head(pcd) ## # A tibble: 6 x 6 ## term enthusiasm height friends sex fav_song ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2014 6 170 685 Female NA ## 2 2014 7 158. 154 Female NA ## 3 2014 7 163. 1333 Female NA ## 4 2014 10 168 0 Female NA ## 5 2014 7 168. 250 Female NA ## 6 2014 5 157 476 Female NA Well plot the sex variable data using a function, geom_bar. This geom not only makes bars, it also sums up the number of Male and Female instances in the variable sex. Note there are no error bars. Counts are just a sum, there is no variation. In this course, well avoid bar plots for everything other than visualizing variables that summarize as counts. We could have also used pie charts or stacked bars, but those are harder to read. The chosen method is a better way to illustrate the magnitude of the counts and the difference between the two levels within the sex variable. Note how this bar plot is, effectively, a histogram of the sex variable. Here the bins are defined by the two values of the variable. ggplot(pcd)+ geom_bar(aes(x=sex), width = 0.5) Figure 8.5: Use bar plots to display the relative counts of sorted variable values. Many biomedical experiments generate discrete sorted data, too. Imagine the following: Neurons are poked with an electrode. Counts are recorded of the number of times they depolarize over a certain time period. Counts are compared between various treatments, such activating an upstream neuron at different levels. Cells are stained for expression of a marker protein. The number of cells in which the protein is detected are counted. Counts are compared between various treatments, such as a knockdown and its control. By some criteria, cells are judged to be either alive or dead and counted as such. The number of alive cells are counted after manipulating expression of a tumor suppressor gene, and compared to a control. By some criteria, mice are judged to either show a disease phenotype or not, and counted as such. Disease incidence is counted in response to levels of a therapeutic agent, or a background genotype, or in response to some stressor. There are an infinite number of examples for experiments that can be performed in which a dependent variable is sorted. For each replicate the researcher decides what nominal category it should be sorted into. Sometimes sorting occurs after some fairly sophisticated biological manipulation, instrumentation or biochemical analysis, where in the end all the researcher has are a count of the number of times one thing happened or some other. 8.2.2.3 Ordered data Ordered data is, in one sense, a hybrid cross of sorted and measured data. Ordinal is another common jargon term used to describe such variables. If youve ever taken a poll in which youve been asked to evaluate something on a scale ranging from something akin to below low to super very highthen youve experienced ordinal scaling (such scales are called Likert scales). The ordered variables values are, in one sense, categorical: low, med, and high differ qualitatively. Each lacks a precise quantitative value. In that sense, they are like sorted variables. However, there is a quantitative, if inexact, relationship between the values. High implies more than med which implies more than low. An underlying gradation exists in the variables scale. This is not just the absence or presence of an attribute. Rather, the scale allows for some amount of the attribute relative to other possible amounts of the attribute. Ordinal variables are discrete, because no values exist between the categories on the scale. The precourse survey is chock full of questions that generate data on an ordered scale. For example, one asks students about their enthusiasm for taking the course. They can select answers ranging from 1 (Truly none) to 10 (Absolutely giddy with excitement). Even though numbers are used as categories, their values are arbitrary, not quantitative, except to imply a 2 means more enthusiasm than a 1, and so on. The variable values could just as easily be: Truly none, Very little, Slightly more than very little,  and so on. In a survey, the participant selects the level for the variable. When we do an experiment, replicates are evaluated and then categorized to one of the values of the ordinal scale. Disability status scales are classic ordinal scales. These are used to assess neurological abnormalities, for example, those associated with experimental multiple sclerosis. Each replicate in a study is evaluated by trained researchers and assigned the most appropriate value given its condition: 0 for no disease, 1 for limp tail, 2 for mild paraparesis, 3 for moderate paraparesis, 4 for complete hind limb paralysis, and 5 for moribund. Obviously, in this ordinal scale, as the numeric value increases so to does the severity of the subjects condition. Heres what a very small set of ordinal data might look like. Imagine doing a small pilot study to determine whether ND4 gene deletion causes neurological impairment. Heres how the data from a lab notebook would be transcribed into an R script to make a summary table: genotype &lt;- c(rep(&quot;wt&quot;, 3), rep(&quot;ND4&quot;, 3)) DSS_score &lt;- as.integer(c(0,1,1,5,3,5)) results &lt;- tibble(genotype, DSS_score); results ## # A tibble: 6 x 2 ## genotype DSS_score ## &lt;chr&gt; &lt;int&gt; ## 1 wt 0 ## 2 wt 1 ## 3 wt 1 ## 4 ND4 5 ## 5 ND4 3 ## 6 ND4 5 Lets make sure we understand the variables. The independent variable is genotype, a factorial variable that comes in two nominal levels, wt or ND4. DSS_score is a dependent variable that comes in 6 levels (integer values ranging from 0 to 5). Im forcing R to read DSS_score for what it is, an integer rather than as a numeric. However, the values 0 to 5 are nominally arbitrary. This only differs from a sorted varialble because a value of 1 means more than zero, 2 means more than 1, and so on. 8.2.2.4 Visualizing ordered variables Lets visualize the enthusiasm biostats students have for taking a course. Heres a bar plot. Here again, geom_bar not only draws bars, it also counts the total number of biostat students who voted for each of the 10 levels. A bar plot of counted data is just a poor mans histogram. ggplot(pcd) + geom_bar(aes(x=enthusiasm))+ scale_x_continuous(breaks=seq(1, 10, 1)) Viewed using the histogram function, we see the same thing, if we dial the binwidth to the unit of the ordinal scale or the number of bins to the length of the ordinal scale. It is easy to see the data are skewed a bit to the right of the scale center. I mention this because ordered data are frequently skewed. Variables of counts should not be expected to obey a symmetrical normal distribution. Although it is tempting to compute descriptives like means and standard deviations and even run t-tests and ANOVA on such data, slow down and have a thought about that. The values are arbitrary. If instead of 1, 2, 3, the variable values were Truly none, Very little, Slightly more than very little, , how exactly would you compute a mean and standard deviation? You could not. A cognitive bias is equating numeric discrete yet arbitrary scale with a continuous, equal interval scale. ggplot(pcd) + geom_histogram(aes(x=enthusiasm), binwidth = 1, color = &quot;#f2a900&quot;, fill = &quot;#012169&quot;) + scale_x_continuous(breaks=seq(1, 10, 1)) Figure 8.6: An ordered variable plotted as histogram. Viewing scatter plots of discrete data really brings forth their discretiness. They show how there is obviously no information between levels of the ordered scale used in this survey instrument. # the height argument in geom_jitter is crucial to seeing this pattern # adjust the height value to see what happens ggplot(pcd) + geom_jitter(aes(x=factor(1), y=enthusiasm), height=0, width = 0.4, size = 4, alpha = 0.6) + scale_y_discrete(limits = c(1:10)) + xlab(&quot;biostats students&quot;) + theme(axis.ticks.x = element_blank(), axis.text.x = element_blank() ) ## Warning: Continuous limits supplied to discrete scale. ## Did you mean `limits = factor(...)` or `scale_*_continuous()`? Although you can calculate an mean for a variable such as this, whether you should report it to one significant digit or beyond is fraught with portent. 8.2.3 Rescaling variables Treating an ordered scale as if it were continuous is called rescaling. Commonly researchers transform discrete variables onto a continous scale, and vice versa. Rescaling requires some judgment. The most frequent transformation is of raw count data to percents. This usually has descriptive value but can introduce problems when used inferentially. For example, the two percent values for the sex variable below are useless for asking whether there is a statistical difference from, say, 50%. The transformation converted 386 bits of information into 2. By keeping it in counts we can test the null hypothesis that the proportion of 144/242 does not differ from 50/50, by performing a proportion test. pcd %&gt;% count(sex) %&gt;% mutate(total=sum(n), percent=100*n/total) ## # A tibble: 2 x 4 ## sex n total percent ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Female 242 386 62.7 ## 2 Male 144 386 37.3 Another common transformation is treating ordinal independent variables as if they are, in fact, continuous. In addition to the fact that these scales are arbitrary, there are a couple of other problems that should be thought through before transforming. First, an ordinal variable like enthusiasm is bounded. Values below 1 and above 10 are not possible. Continuous sampling distributions, such as the t-distribution, are premised upon variables that have no bound. Another major question that arises with ordinal scales is whether it is safe to assume the intervals between the units are equal. Is the difference between a 1 and a 2 score on the enthusiasm variable the same as the difference between a 9 and a 10 score? Descriptive parameters of continuous scales, such as mean and standard deviation, can apply when these interval magnitudes equal. And tests based upon the t sampling distribution operate on an assumption of equal interval scaling. When all of these issues cannot be assumed, the nonparametric tests provide perfectly good inferential tools for ordinal data. 8.3 Working with data Some of the code chunks above show scripts for manipulating and viewing data. The section below is a more formal but quick primer on R data entry, inspection, summation and visualization with ggplot. The focus here is conceptual with simple quick examples to help you get started. 8.3.1 Entering data There are three main ways: Type from your lab notes by keyboard into an R source. Copy to clipboard from some file, paste using the datapasta package. Read from a file. 8.3.1.1 Typing in variables and making a data frame Lets say we have some data in a lab notebook. These are results from a small pilot experiment comparing the effect on neurological function of knockout of the ND4 gene to wild-type. Lab meeting is in 10 minutes. The knockout and wild-type are two values of the variable genotype, which is sorted. Neurological function is assessed using the 6-value disability status scale, which is an ordered variable. Both variables are discrete. Saving the Rscript (or Rmarkdown) file stores the data. The function tibble() creates a tidyverse data frame. # understand the logic of creating vector objects for the # independent and the dependent variables # note how a tibble-type data frame is created # vector objects represent the variables genotype &lt;- c(rep(&quot;wt&quot;, 3), rep(&quot;ND4&quot;, 3)) DSS_score &lt;- as.integer(c(0,1,1,5,3,5)) # put the variables in a data frame object results &lt;- tibble(genotype, DSS_score); results ## # A tibble: 6 x 2 ## genotype DSS_score ## &lt;chr&gt; &lt;int&gt; ## 1 wt 0 ## 2 wt 1 ## 3 wt 1 ## 4 ND4 5 ## 5 ND4 3 ## 6 ND4 5 ggplot(results, aes(x=genotype, y=DSS_score)) + geom_jitter(height=0, width=0.3, size=4) 8.3.1.2 Using datapasta This is a good way to import large chunks of data from other formats. View a data file on your machine, such as an excel sheet. Copy the data you want to your machines clipboard. In an Rscript or chunk, name an empty object (eg, song &lt;-c()). Place the cursor inside the parentheses. Click on the Addins icon below the RStudio menu bar and select an option. Here I copied the fav_song column (including header) from the precourse.csv file. Probably due to that header, datapasta coerced the values as characters. Some munging will be necessary to clean things and turn everything into numeric values. But at least its in R now. Woot. song &lt;-c(&quot;fav_song&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;570&quot;, &quot;576&quot;, &quot;232&quot;, &quot;204&quot;, &quot;524&quot;, &quot;252&quot;, &quot;588&quot;, &quot;201&quot;, &quot;212&quot;, &quot;221&quot;, NA, &quot;265&quot;, &quot;294&quot;, &quot;217&quot;, &quot;246&quot;, &quot;481&quot;, &quot;480&quot;, &quot;200&quot;, &quot;180&quot;, &quot;210&quot;, &quot;203&quot;, &quot;210&quot;, &quot;172&quot;, &quot;255&quot;, &quot;375&quot;, &quot;698&quot;, &quot;234&quot;, &quot;242&quot;, &quot;256&quot;, &quot;288&quot;, &quot;272&quot;, &quot;273&quot;, &quot;601&quot;, &quot;293&quot;, &quot;260&quot;, &quot;120&quot;, &quot;300&quot;, &quot;147&quot;, &quot;272&quot;, &quot;202.2&quot;, &quot;274&quot;, &quot;293&quot;, &quot;262&quot;, &quot;555&quot;, &quot;362&quot;, &quot;260&quot;, &quot;180&quot;, &quot;237&quot;, &quot;300&quot;, &quot;373&quot;, &quot;121&quot;, &quot;361&quot;, &quot;179&quot;, &quot;169&quot;, &quot;201&quot;, &quot;194&quot;, &quot;246&quot;, &quot;139&quot;, &quot;210&quot;, &quot;234&quot;, &quot;292&quot;, &quot;217&quot;, &quot;412&quot;, &quot;172&quot;, &quot;289&quot;, &quot;214&quot;, &quot;180&quot;, &quot;448&quot;, &quot;180&quot;, &quot;207&quot;, &quot;74&quot;, &quot;253&quot;, &quot;256&quot;, &quot;243&quot;, &quot;187&quot;, &quot;420&quot;, &quot;200&quot;, &quot;235&quot;, &quot;185&quot;, &quot;246&quot;, &quot;172&quot;, &quot;258&quot;, &quot;213&quot;, &quot;188&quot;, &quot;258&quot;, &quot;201&quot;, &quot;214&quot;, &quot;210&quot;, &quot;230&quot;, &quot;5421&quot;, &quot;259&quot;, &quot;222&quot;, &quot;171&quot;, &quot;184&quot;, &quot;220&quot;, &quot;225&quot;, &quot;295&quot;, &quot;240&quot;, &quot;250&quot;, &quot;204&quot;, &quot;208&quot;, &quot;254&quot;, &quot;341&quot;, &quot;233&quot;, &quot;232&quot;, &quot;455&quot;, &quot;423&quot;, &quot;246&quot;, &quot;570&quot;, &quot;319&quot;, &quot;0&quot;, &quot;205&quot;, &quot;240&quot;, &quot;260&quot;, &quot;126&quot;, &quot;480&quot;, &quot;210&quot;, &quot;226.93&quot;, &quot;0&quot;, &quot;300&quot;, &quot;180&quot;, &quot;298&quot;, &quot;290&quot;, &quot;202&quot;, &quot;194&quot;, &quot;246&quot;, &quot;232&quot;, &quot;210&quot;, &quot;150&quot;, &quot;236&quot;, &quot;174&quot;, &quot;260&quot;, &quot;300&quot;, &quot;565&quot;, &quot;273&quot;, &quot;245&quot;, &quot;360&quot;, &quot;174&quot;, &quot;276&quot;, &quot;293&quot;, &quot;460&quot;, &quot;240&quot;, &quot;583&quot;, &quot;90&quot;, &quot;180&quot;, &quot;240&quot;, &quot;244&quot;, &quot;127&quot;, &quot;210&quot;, &quot;211&quot;, &quot;240&quot;, &quot;220&quot;, &quot;241&quot;, &quot;180&quot;, &quot;230&quot;, &quot;180&quot;, &quot;198&quot;, &quot;133&quot;, &quot;272&quot;, &quot;293&quot;, &quot;296&quot;, &quot;230&quot;, &quot;212&quot;, &quot;195.6&quot;, &quot;187&quot;, &quot;200&quot;, &quot;234&quot;, &quot;200&quot;, &quot;240&quot;, &quot;295&quot;, &quot;183&quot;, &quot;216&quot;, &quot;286&quot;, &quot;192&quot;, &quot;270&quot;, &quot;360&quot;, &quot;262&quot;, &quot;268&quot;, &quot;234&quot;, &quot;1560&quot;, &quot;197&quot;, &quot;261&quot;, &quot;279&quot;, &quot;195&quot;, &quot;251&quot;, &quot;295&quot;, &quot;461&quot;, &quot;360&quot;, &quot;218&quot;, &quot;185&quot;, &quot;250&quot;, &quot;257&quot;, &quot;210&quot;, &quot;199&quot;, &quot;210&quot;, &quot;210&quot;, &quot;200&quot;, &quot;261&quot;, &quot;237&quot;, &quot;226&quot;, &quot;331&quot;, &quot;261&quot;, &quot;190&quot;, &quot;222&quot;, &quot;180&quot;, &quot;284&quot;, &quot;120&quot;, &quot;255&quot;, &quot;222&quot;, &quot;3&quot;, &quot;200&quot;, &quot;235&quot;, &quot;277&quot;, &quot;314&quot;, &quot;249&quot;, &quot;75&quot;, &quot;300&quot;, &quot;277&quot;, &quot;250&quot;, &quot;241&quot;, &quot;209&quot;, &quot;250&quot;, &quot;200&quot;, &quot;210&quot;, &quot;402&quot;, &quot;360&quot;, &quot;205&quot;, &quot;345&quot;, &quot;252&quot;, &quot;305&quot;, &quot;201&quot;, &quot;170&quot;, &quot;213&quot;, &quot;656&quot;, &quot;240&quot;, &quot;339&quot;, &quot;172&quot;, &quot;263&quot;, &quot;239&quot;, &quot;135&quot;, &quot;210&quot;, &quot;230&quot;, &quot;185&quot;, &quot;292&quot;, &quot;210&quot;, &quot;432&quot;, &quot;200&quot;, &quot;192&quot;, &quot;21&quot;, &quot;215&quot;, &quot;240&quot;, &quot;240&quot;, &quot;200&quot;, &quot;480&quot;, &quot;247&quot;, &quot;200&quot;, &quot;253&quot;, &quot;230&quot;, &quot;355&quot;, &quot;214&quot;, &quot;332&quot;, &quot;2450&quot;, &quot;300&quot;, &quot;180&quot;, &quot;237&quot;, &quot;379&quot;, &quot;516&quot;, &quot;180&quot;, &quot;184&quot;, &quot;218&quot;, &quot;222&quot;, &quot;194&quot;, &quot;160&quot;, &quot;234&quot;, &quot;238&quot;, &quot;208&quot;, &quot;250&quot;, &quot;123&quot;, &quot;240&quot;, &quot;226&quot;, &quot;237&quot;, &quot;176&quot;, &quot;200&quot;, &quot;192&quot;, &quot;347&quot;, &quot;255&quot;, &quot;254&quot;, &quot;255&quot;, &quot;240&quot;, &quot;480&quot;, &quot;240&quot;, &quot;456&quot;, &quot;223&quot;, &quot;236&quot;, &quot;201&quot;, &quot;190&quot;, &quot;226&quot;, &quot;300&quot;, &quot;288&quot;, &quot;681&quot;, &quot;180&quot;, &quot;352&quot;, &quot;270&quot;, &quot;245&quot;, &quot;180&quot;, &quot;229&quot;, &quot;180&quot;, &quot;221&quot;, &quot;320&quot;, &quot;217&quot;, &quot;236&quot;, &quot;352&quot;, &quot;233&quot;, &quot;214.8&quot;, &quot;262&quot;, &quot;179&quot;, &quot;221&quot;, &quot;227&quot;, &quot;241&quot;, &quot;180&quot;, &quot;2640&quot;, &quot;364&quot;, &quot;240&quot;, &quot;382&quot;, &quot;243&quot;, &quot;204&quot;, &quot;185&quot;, &quot;240&quot;, &quot;298&quot;, &quot;155&quot;, &quot;216&quot;, &quot;24840&quot;, &quot;180&quot;, &quot;202&quot;) 8.3.1.3 Reading a file Of the three data entry methods, reading a file is the most reproducible. Raw data are read from files into R objects using a script. The data object represents all the values from the file and is now in the environment. The object can be worked on to fix, segment, summarize, and visualize the data. After the initial read step, the raw data file remains untouched. Which is very good. As a general rule, just save the script and never write over raw data because doing so changes the raw data. When you need to work on the data later in a new R session, just re-run the saved script, including the read step. This is a very different way of working with data compared to using GUI-based software. We dont create and save all sorts of new sheets of edited or interpreted data. Or overwrite the original data sheet. We just write a script, and change it until we get exactly what we want. The script provides the reproducible record of how the data was manipulated. Well (mostly) use .csv files as a data source in this course. The same basic reading approach used for that is applicable to all kinds of other data sources. Many different R functions exist to read from many different types of data sources. Heres how to import the precourse survey data using the read_csv function from the tidyverse readr package. # my working directory has a subdirectory named datasets # the precourse.csv file is in the datasets folder # If the csv file is in your working directory, just do: pcd &lt;- read_csv(&quot;precourse.csv) pcd &lt;- read_csv(&quot;datasets/precourse.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## term = col_double(), ## enthusiasm = col_double(), ## height = col_double(), ## friends = col_number(), ## sex = col_character(), ## fav_song = col_double() ## ) Inspect the data to make sure it is what you think it is. # get in the havit of inspecting data, several ways str(pcd) ## tibble [386 x 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ term : num [1:386] 2014 2014 2014 2014 2014 ... ## $ enthusiasm: num [1:386] 6 7 7 10 7 5 5 10 7 10 ... ## $ height : num [1:386] 170 158 163 168 168 ... ## $ friends : num [1:386] 685 154 1333 0 250 ... ## $ sex : chr [1:386] &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ fav_song : num [1:386] NA NA NA NA NA NA NA NA NA NA ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. term = col_double(), ## .. enthusiasm = col_double(), ## .. height = col_double(), ## .. friends = col_number(), ## .. sex = col_character(), ## .. fav_song = col_double() ## .. ) head(pcd) ## # A tibble: 6 x 6 ## term enthusiasm height friends sex fav_song ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2014 6 170 685 Female NA ## 2 2014 7 158. 154 Female NA ## 3 2014 7 163. 1333 Female NA ## 4 2014 10 168 0 Female NA ## 5 2014 7 168. 250 Female NA ## 6 2014 5 157 476 Female NA view(pcd) There are a handful of functions in R land for reading .csv data. They dont all work the same, 8.3.1.4 Subsetting, transforming and summarizing data Before diving into data munging learn some more about R syntax/logic: Functions have one or more arguments, placed inside (). Commas always separate arguments, and one argument is usually some sort of data object or a function that has a data argument. # prototype function foo(arg1, arg2, arg3, ...) # for example: x &lt;- c(1, 2, 3, 4 ,5, NA) # x is an argument # na.rm = T is also an argument # two arguments needed because x has NA, and mean won&#39;t ignore NA unless told mean(x, na.rm = T) Functions can work sequentially inside other functions. The pipe function, %&gt;%, from the dplyr package will be your best friend in the course if you allow it. In English, %&gt;% means then or next. For example: # old fashioned R syntax, hard to read and write, NTTAWWT: work(commute(shower(breakfast(walk_dog(bathroom(wake_up(me))))))) # piping is more natural workday &lt;- me %&gt;% wake_up() %&gt;% bathroom() %&gt;% walk_dog() %&gt;% breakfast() %&gt;% shower() %&gt;% commute() %&gt;% work() # I saw this analogy on twitter and wish I&#39;d invented it Your process should be making sure each step works before going to the next. And if youre not getting what you want, maybe your steps are not in the right order? Five MUST KNOW functions (there will be a few more later, including ggplot and pivot_longer, nee gather) Here is a goal: Generate some descriptive statistics, for only valid heights of male and female biostats students, in units of inches rather than centimeters. Here is a solution. pcd %&gt;% select(height, sex) %&gt;% filter(height &gt; 125 &amp; height &lt; 250) %&gt;% mutate(height = height/2.54) %&gt;% group_by(sex) %&gt;% summarise(mean = mean(height), sd = sd(height), n = length(height)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 4 ## sex mean sd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Female 64.7 2.93 237 ## 2 Male 70.3 3.02 142 Think about these functions: select chooses variables (columns) from a data frame filter chooses rows mutate is how you transform variables, clever, huh? group_by segmentation, based upon values of the argued variable summarise calculates any summary statistic you can imagine There are many ways to do the same thing, or pretty much the same thing # need a data frame object of random normal values? a &lt;- rnorm(5) b &lt;- rnorm(5) oneway &lt;- data.frame(a, b) another &lt;- data.frame(a = rnorm(5), b = rnorm(5)) prettyMuchSame &lt;- tibble(a, b) 8.3.1.5 Evolution of a visualization We use ggplot in this course. The ability to make plots is essential in statistical design and analysis of experiments. The 10 step procedure below is mostly conceptual, but has code you can learn a lot from right out of the gate. Also, although this is about building a ggplot, conceptionally, the same step-by-step process is used to do any other coding task. Lets begin. Creating a presentation quality ggplot is no different than a Bob Ross oil painting. It is a process. We begin with an idea of what we want to create. Then we make a blank canvas, to which stuff is added, one step at a time. Improve it through iteration, tweaking, slowly but surely, argument by argument, line by line. There you go. Anybody can ggplot. Lets make a cool image of biostat student heights. The default ggplot creates the blank canvas ggplot() Figure 8.7: Not much, but it is a start. The data must be in a data frame. Its there now, but still a blank canvas. ggplot(data = pcd) Figure 8.8: The nothingness belies real progress Assign variables to the canvas coordinates using the aesthetics function. No longer blank. ggplot(data = pcd, aes(x=sex, y=height)) Figure 8.9: Woot! Something happened. Geoms bring the data into view. Aesthetics can also be argued in geom functions. ggplot(data = pcd) + geom_point(aes(x=sex, y=height)) Figure 8.10: Woot! Data! Ugh, outliers, no grad student is ever that tall or short. Munge the data inside the ggplot function. On the fly! ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250), ) + geom_point(aes(x=sex, y=height)) Figure 8.11: Fix the problem right in the data frame. Not a good use case for geom_point. ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250), aes(x=sex, y=height)) + geom_jitter() Figure 8.12: Try a different geom. Customize the geom. ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250), aes(x=sex, y=height)) + geom_jitter(shape = 22, height = 0, width = 0.2, color = &quot;blue&quot;, size = 4, alpha = 0.6) Figure 8.13: Lets call this Bob-Rossing the plot. Customize the axis ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250), aes(x=sex, y=height)) + geom_jitter(shape = 22, height = 0, width = 0.2, color = &quot;blue&quot;, size = 4, alpha = 0.6)+ labs(title= &quot;Precourse Survey&quot;, subtitle= &quot;2014-2020&quot;, caption=&quot;IBS538 Spring 2020&quot;, tag = &quot;Biostats&quot;, x=&quot;sex chromosome&quot;, y=&quot;verticality, cm&quot;)+ scale_x_discrete(labels= c(&quot;XX&quot;, &quot;XY&quot;)) Figure 8.14: More Bob-Rossing. Customize the frame &amp; theme ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250), aes(x=sex, y=height)) + geom_jitter(shape = 22, height = 0, width = 0.2, color = &quot;blue&quot;, size = 4, alpha = 0.6)+ labs(title= &quot;Precourse Survey&quot;, subtitle= &quot;2014-2020&quot;, caption=&quot;IBS538 Spring 2020&quot;, tag = &quot;Biostats&quot;, x=&quot;sex chromosome&quot;, y=&quot;verticality, cm&quot;)+ scale_x_discrete(labels= c(&quot;XX&quot;, &quot;XY&quot;))+ theme_classic() Figure 8.15: And more Bob-Rossing. Last minute changes. The audience wont get cm. Transform variable to inches. Oh, and segment using a third variable and applying scaling colors. ggplot(data = pcd %&gt;% filter(height &gt; 125 &amp; height &lt; 250) %&gt;% mutate(height = height/2.54), aes(x=sex, y=height, color = term)) + geom_jitter(shape = 18, height = 0, width = 0.2, size = 3, alpha = 1)+ labs(title= &quot;Precourse Survey&quot;, subtitle= &quot;2014-2020&quot;, caption=&quot;IBS538 Spring 2020&quot;, tag = &quot;Biostats&quot;, x=&quot;sex chromosome&quot;, y=&quot;verticality, INCHES&quot;)+ scale_x_discrete(labels= c(&quot;XX&quot;, &quot;XY&quot;))+ theme_classic()+ scale_color_viridis(begin = 0.1, end =0.9) Figure 8.16: Finish with a dramatic combination data munge, Bob-Rossing flourish. Much more could still be done, but I promised myself to stop at 10 steps. Start small, grow it. Play with it until you like it. If that seems like a lot to code just for one figure, note that it is reproducible and modular. These latter features can be exploited with only slightly higher level R skills to repeatedly reuse the same custom format on many different data sets and variables. 8.4 Summary Were going to talk about data a lot more the rest of the semester. So far weve covered these fundamentals. An experimental researchers collects measures for dependent variables after imposing independent variables on a system. Both dependent and independent variables can be either discrete or continuous. Discrete variables can be either ordered or sorted. Continuous variables are measured. Statistical treatment depends upon the type of variables involved in the experiment. Use R to inspect, modify and visualize variables. All R coding is like building a ggplot, through an iterative, step-by-step process. It pays to understand the starting material you have (date) to get the end product you want (analysis). "],["dispersion.html", "Chapter 9 Variability, Accuracy and Precision 9.1 Illustration of a model with error 9.2 Variance: Quantifying variation by least squares 9.3 Standard deviation 9.4 Precision and Accuracy 9.5 Standard error 9.6 Confidence intervals 9.7 Key take aways", " Chapter 9 Variability, Accuracy and Precision Your estimate can be precisely inaccurate. library(tidyverse) library(ggformula) One of my favorite sayings is models are perfect, data are not, because its a simple way to say that statistical models will always predict perfect data.3 Experimentalists draw random samples from a population we are interested in studying. In statistics, the population parameters that interest us are held to have true, fixed values. Such parameters are important because they have biological meaning: a parameter can mean the differences between two treatment levels, affinities between proteins, maximum responses, enzyme activity rates, depolarization frequencies,. Every experiment is about a population being modeled. The models we conjure up to estimate these parameters are also perfect. When you simulate data using models (without an error term) you generate data that fit the model perfectly. I imagine a population with a mean of 5 and a standard deviation of 0. Heres data I could expect to see from such a perfect model: rnorm(n=3, mean=10, sd=0) ## [1] 10 10 10 Whats imperfect, unfortunately, are the real data that we generate through sampling. Stood up against a perfect model, they usually come off looking pretty crappy. The models arent always good fits for the data, and very often it is ambiguous whether one model fits best, or some other should be applied. Therefore, a major focus of statistics is to evaluate how well our perfect models are fit by these messy data. Quantifying the uncertainty of that fit is one basis for asserting how reliable are our parameter estimates. Which is very important because in turn that addresses how realiable is our understanding of the system that we are researching. All of this points to the fact that it is not possible to understand statistical analysis without wrapping our heads around the concepts related to data dispersion: variability, accuracy and precision. Together, these concepts comprise ways of thinking about and describing the uncertainty of data. 9.0.1 Data are messy The reason data are messy is that sample data possess inherent variability. This means there is always some error between the true values of the population parameters and their estimated values within the sample. In running experiments on biological systems we think about two main contributors to this error. The first is biological variation between independent replicates. There are confounding factors inherent within the sampled specimens that are driving the observed variation. We dont understand them enough to know how to control for these confounders. Oddly enough, in statistical jargon the unexplained biological variation between replicates is often called random error. This is meant to imply the random biological differences between replicates are due to things we dont understand, not random chance. The second source is so-called systematic error, having to do with our processes, the techniques and equipment, the way we measure the sample. These ways of thinking about different sources for variation will hopefully become more clear when we get to ANOVA and to regression. 9.1 Illustration of a model with error As you recall from middle school, a straight line drawn between two variables is described by the linear model \\(Y=\\beta_0+\\beta_1 X\\). This model has two parameters, \\(\\beta_0\\) represents the y-intercept while \\(\\beta_1\\) represents the slope of the line. The predictor variable is \\(X\\). Thus, passing values of \\(X\\) into this model generates values for the response variable, \\(Y\\). I mention this so I can illustrate error, perfect models, and all of that. Heres a simulation of response data from two linear models, \\(Y1\\) and \\(Y2\\). Reading the code, you notice that one of the models, \\(Y1\\) has no error term, so it is perfect, which is to say the values of \\(Y1\\) are perfectly predicted by the models intercept and slope parameters. A linear regression goes through every data point and the output of that regression yields the exact same parameter values that were input (a bit of a circular proof, if you will). The model perfectly fits its data. The other model (\\(Y2\\)) has an error term tacked onto it. As a result it yields imperfect values of \\(Y2\\) even when given the exact same model parameters as for \\(Y1\\). Applying a linear regression to this data yields an imperfect fitthe line doesnt go through all of the data points. Error is associated with the estimate (the gray shade), and the intercept and slope parameters estimated through linear regression differ somewhat from the parameter input values. #intital model parameter values b0 &lt;- 0 b1 &lt;- 5 X &lt;- seq(1,10,1) error &lt;- rnorm(length(X), mean=0, sd=10) #models Y1 &lt;- b0+b1*X Y2 &lt;- b0+b1*X + error #put the simulations into a data frame so it can be plotted df &lt;- data.frame(X, Y1, Y2) %&gt;% gather(model, response, -X) #plotting function ggplot(df, aes(X, response))+ geom_point()+ geom_smooth(aes(X, response), method=lm)+ facet_grid(cols=vars(model)) Figure 9.1: Models are perfect, data are not. You can appreciate how the second model, with the random error term, is more real life. Whats even more interesting is that every time you run the second model youll generate a completely different result. Copy and paste that code chunk into R and see for yourself. That error term is just a simulation of a random process. And thats just how random processes behave! (Play with that code if you dont understand how it works. Run it on your machine. Change the initializers) In real life, unlike in this simulation, we wouldnt know the true values of the parameters (as we know them in this simulation). In real life we sample and test to see how well imperfect data fit a perfect model. At this point in the course its not important to interpret what all of this error analysis means. The point Im making right now, and that hopefully youll grow to appreciate, is that to a large extent, statistics is mostly about residual error analysismeasuring and accounting for the differences between perfect models and imperfect data. 9.2 Variance: Quantifying variation by least squares The two primary methods to quantify variation are termed ordinary least squares and maximum likelihood estimation. Mathematically they are distinct, and that difference is beyond the scope of this moment. What is important is that, in most cases, either method will arrive at roughly the same solution. Ordinary least squares is the method used for quantifying variation for most of the statistics discussed in this course. The only exception is when we get to generalized linear models. So well omit consideration of maximum likelihood estimation until then. Ordinary least squares arises from the theoretical basis of variance, and is a deceptively simple concept. It can be shown (by mathematicians) that the variance of a random variable, \\(Y\\), with an expected value equivalent to its mean, \\(E(Y) = \\mu\\), is the difference between its squared expected value and its expected value squared: \\[Var(Y) = E(Y^2)-E(Y)^2 \\] In practice, imagine a sample of size \\(n\\) random independent replicates drawn from a population of the continuous random variable, \\(Y\\). The replicate sample values for \\(Y\\) are, \\(y_i\\), and thus \\(y_1, y_2, ...y_n\\) is a vector of values of the variable \\(Y\\). The mean of these replicate values provides an estimate for the mean of the sampled population, \\(\\mu\\), and is \\[\\bar y = \\frac{{\\sum_{i=1}^n}y_i}{n}\\] The value by which each replicate within the sample varies from the mean is \\(y_i-\\bar y\\). In jargon these are described alternately as a variate, or a deviate or as a residual. Each word represents the same thing. A vector of sample replicates will have values that are smaller than and larger than the mean. Thus, some variate values are positive, while others are negative. In a random sample of a normally distributed variable we would expect that the values for variates are roughly equally dispersed above and below a mean. Thus, if we were to sum up the deviates (or variates or residuals) wed expect that that sums value to approach zero. \\[E(\\sum_{i=1}^n(yi-\\bar y))=0\\] By squaring the deviates, negative values of the deviates are removed, providing a parameter that paves the way to more capably describe the variation within the sample than can the sum of the deviates. This parameter is called the sum of squares: \\[SS=\\sum_{i=1}^n(yi-\\bar y)^2\\] 9.2.1 Definition of variance A samples variance is the sum of the squared deviates divided by the sample degrees of freedom4, \\(df=n-1\\). \\[s^2=\\frac{\\sum_{i=1}^n(yi-\\bar y)^2}{n-1}\\] A samples variance, \\(s^2\\) is used as an estimate of the variance, \\(\\sigma^2\\), of the population that was sampled, in the same way \\(y_bar\\) estimates the population mean \\(\\mu\\). Since it is derived through division using a number that approximates sample size you can think of variance as an approximate average of \\(SS\\). Therefore, a synonym for variance is mean square, a jargon that is used in ANOVA. The reason \\(s^2\\) is arrived at through dividing by degrees of freedom, \\(n-1\\), rather than by \\(n\\) is because doing so produces a better estimate of the population variance, \\(\\sigma^2\\). Mathematical proofs of that assertion can be found all over the internet. Variance is a hard parameter to wrap the brain around. It describes the variability within a dataset, but geometrically: The units of \\(s^2\\) are squared. For example, if your response variable is measured by the \\(gram\\) mass of objects, then the variance units are in \\(grams^2\\). Thats weird. Later in the course, well discuss statistical testing using analysis of variance (ANOVA) procedures. The fundamental idea of ANOVA is to test for group effects by partitioning the error. Thats done with \\(SS\\). When a factor causes some effect, the \\(SS\\) associated with that factor get largerby the square of the variation. Statistical testing is then done on the variance in groups, which in ANOVA jargon is called the mean square, or \\(MS\\). \\(MS\\) is just another way for saying \\(s^2\\). 9.3 Standard deviation The sample standard deviation solves the problem of working with an unintuitive squared parameter. The standard deviation is a more pragmatic descriptive statistic than is variance. The standard deviation is the square root of the sample variance: \\[sd=\\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{n-1}}\\] Poof! The weird squared value for the varible is gone. No more need to think about blood glucose in squared units of mg/dl. 9.3.1 What does the standard deviation tell us The sample standard deviation is two things at once. It is * a statistical parameter that conveys the variability within the sample. * a point estimate of the variability within the population that was sampled. There arent many factoids in statistics worth committing to memory, but the one that follows is: A bit over two thirds of the values for a normally-distributed variable will lie between one standard deviation below and above the mean of that variable. To get statistics it will be very important to get an intuitive feel for the standard deviation. I think the best way to understand it is to run R scripts that explore its properties. Heres one way to calculate the two-thirds rule using Rs pnorm function, the cumulative normal distribution function: pnorm(-1, lower.tail = T) #Calculates the AUC below a zscore of -1. ## [1] 0.1586553 pnorm(1, lower.tail = F) #Calculates the AUC above a zscore of 1 ## [1] 0.1586553 1-pnorm(-1)-pnorm(1, lower.tail = F) #The middle range ## [1] 0.6826895 Explore: Use pnorm to calculate the AUC between z scores of -2 and 2, visualizing the outcome. Figure 9.2: About 86% of the values for a normally distributed variable are within +/- one standard deviation from the mean. No matter the scale for the variable, the relative proportion of values within 1 standard deviation for normally distributed variables will always behave this way. Heres the distribution of serum glucose concentration values, where the average is 100 mg/dl and the standard deviation is 10 mg/dl: Figure 9.3: Modeling the distribution of a blood glucose variable. 9.3.2 Ways of describing sample variability Just show all of the data as scatter plots! Dont hide the variability in bar plots with error bars. When using bar plots with error bars, always show SD. Violin plots are pretty ways to illustrate the spread and density of variation graphically. The coefficient of variation, \\(cv=\\frac{sd}{mean}\\), is a dimensionless index that describes the noise-to-signal ratio. Percentiles and ranges. In particular, the interquartile range. This is usually reserved for non-normal data, particularly discrete data. The SEM does not describe sample variability. 9.4 Precision and Accuracy Even with well-behaved subjects, state-of-the-art equipment, and the best of technique and intentions, samples can yield wildly inaccurate estimates, even while measuring something precisely. My favorite illustration of this is how estimates for the masses of subatomic particles have evolved over time. We can assume that the real masses of these particles have remained constant. Yet, note all of the quantum jumps, pun intended, in their estimated values. What seems very accurate today could prove to be wildly inaccurate tomorrow. In particular, just because something is measured precisely doesnt mean it is accurate. If there is one clear take away from this its that all such statistical estimates are provisional. 9.5 Standard error This will sound counter-intuitive, but we can actually know how precise our estimate of some parameter is without knowing the true value. Thats because precision is the repeatability of a measurement, and it is possible to repeat something very, very reliably but inaccurately. The standard error is the statistical parameter used to express sample precision. Standard error is calculated from standard deviation and the sample size \\[precision:\\ SE = \\frac{sd}{\\sqrt n}\\] Thus, the SE is not something that estimates a fixed parameter of the sampled population. Since it reduces as sample size increases, it provides information only about the sample. 9.5.1 What exactly does the standard error represent? The central limit theorem predicts a few important things: A distribution of many sample means will be normally distributed, even if a non-normal distribution is sampled. A distribution of sample means will have less dispersion with larger sample sizes. If a sample population has a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means of sample size \\(n\\) drawn from that population will have a mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt n}\\). Therefore, the standard error for a mean is the standard deviation of a theoretical population of sample means. These features are illustrated below, as a prelude to further defining standard error. The first graph showing a big boring black box. It simply plots out a uniform distribution of the random variable \\(Y\\) ranging in values from 0 to 1, using the duniffunction. It just means that any value, \\(y\\), plucked from that distribution is as equally likely as all other values. Well sample repeatedly from this distribution and show that the means of those samples are normally distributed. ggplot( data.frame( x=seq(0, 1, 0.01)), aes(x))+ stat_function( fun=dunif, args=list(min=0, max=1), xlim=c(0, 1), geom=&quot;area&quot;, fill=&quot;black&quot;)+ labs(y=&quot;p(y)&quot;, x=&quot;Y&quot;) Figure 9.4: A variable having a uniform distribution The probability of sampling any single \\(y_i\\) value is equivalent to that for all other \\(y_i\\) values. Uniform distributions arise from time-to-time. For example, important actually, the distribution of p-values derived from true null hypothesis tests is uniform. As you might imagine, the mean of this particular uniform distribution is 0.5 (because it is halfway between the value limits 0 and 1). Probably less obvious is the standard deviation for a uniform distribution with limits \\(a\\) and \\(b\\) is \\(\\frac{b-a}{\\sqrt{12}}\\). So the standard deviation for this particular uniform distribution \\(\\sigma\\) = 0.2887. Just so you trust me, these two assertions pass the following simulation test: mean(runif(n=100000, min=0, max=1)) ## [1] 0.5003241 sd(runif(n=100000, min=0, max=1)) ## [1] 0.2887711 The next graph illustrates the behavior of the central limit theorem. In the following a little custom function, called meanMaker, random samples are taken from this uniform distribution many times. The script generates either 1000 means of very small random samples (n=3) or 1000 means from somewhat larger random samples (n=30). Those two sets of means are then plotted. Notice how the distributions of sample means are normally distributed, even though they comes from a uniform distribution! That validates the first prediction of the CLT, made above. Note also that the distribution of means corresponding to the larger sample sizes has much less dispersion than that for the smaller sample sizes. That validates the second CLT prediction made above. meanMaker &lt;- function(n){ mean(runif(n)) } small.sample &lt;- replicate(1000, meanMaker(3)) large.sample &lt;- replicate(1000, meanMaker(30)) ggplot(data.frame(small.sample, large.sample))+ geom_histogram(aes(small.sample), fill=&quot;red&quot;)+ geom_histogram(aes(large.sample), fill=&quot;blue&quot;, alpha=0.5 )+ xlab(&quot;n=3 (red) or n=30 (blue&quot;) Figure 9.5: The central limit theorem in action, distributions of sample means of small (red) and large (blue) sample sizes. As for the third point, the code below calculates a mean of all these means, and the SD of all these means, for each of the groups. mean(small.sample) ## [1] 0.505869 sd(small.sample) ## [1] 0.1609439 mean(large.sample) ## [1] 0.4998256 sd(large.sample) ## [1] 0.05257406 Irrespective of sample size, the mean of the means is a great estimator of the mean of the uniform distribution. But passing the sample means into the sd function shows that neither provides a good estimate of the \\(\\sigma\\) for the population sampled, which we know for a fact has a value of 0.2887. Obviously, the standard deviation of a group of means is not an estimator of the population standard deviation. So what is it? The standard deviation of the distribution of sample means is what is known as the standard error of the mean. This goes a long way to illustrate what the standard error of the mean of a sample, SEM, actually represents. It is an estimator of the theoretical standard deviation of the distribution of sample means,\\(\\sigma_{\\bar y}\\). Now what are the implications of THAT? There are a few. First, SEM is not a measure of dispersion in a sample. It is a measure that describes the precision by which a mean has been estimated. The SEM for the large sample size group above is much lower than that for the small sample size group. Meaning, \\(\\mu\\) is estimated more precisely by using large sample sizes. That should make intuitive sense. Second, statistically naive researchers use SEM to illustrate dispersion in their data (eg, using SEM as error bars). Ugh. They do this because, invariably, the SEM will be lower than the SD. And that looks cleaner. But they should not do this, because SEM is not an estimator of \\(\\sigma\\). Rather, SEM estimates \\(\\sigma_{\\bar y}\\). Those are two very different things. When we do experiments we sample only once (with \\(n\\) independent replicates) and draw inferences about the population under study on that bases of that one sample. We dont have the luxury or resources to re-sample again and again, as we can in simulations. However, these simulations illustrate that, due to the central limit theorem, a long run of sampling is predictably well-behaved. This predictability is actually the foundation of statistical sampling methodology. The SEM estimates a theoretical parameter (\\(\\sigma_{\\bar y}\\) that we would rarely, if ever, attempt to validate. Yet, on the basis of one sample it serves a purpose by providing an estimator of precision. In a sense, the SEM provides an estimate for how well something is estimated. 9.5.2 Should I use SD or SEM? The SEM does not illustrate variation. I suggest you use SEM if it is important to illustrate the precision by which something is measured. Thats usually only important for parameters that are physical constants. And the exact n should be coupled to every reported SEM. More often, your reader is better served by showing true dispersion. Use SD to summarize the variability in your data using SD. 9.6 Confidence intervals Confidence intervals are among the most useful yet misunderstood statistics. They are calculated from a sample. They are a range of values for the variable parameter being estimated. They can be calculated for virtually any variable parameters. There are many ways to calculate CIs They can be set at any confidence level the researcher prefers, 95%, 90%, 99%, 87.26% For example, assume the variable is blood glucose, in units of mg/dl, and the parameter we are interested in estimating is the mean blood glucose level in a population. The experiment involves taking a random sample of that population through \\(n\\) independent replicates, measuring the blood glucose level in each. The sample mean is calculated, along with a 95% confidence interval for that mean. 9.6.1 CI Definition Lets assume our experiment involves random sampling of a population to estimate its mean, and we are calculating the 95% confidence interval from that sample. What does that 95% CI tell us? Definition: If we repeated the same experiment many, many times, where each experiment is a fresh random sample of \\(n\\) independent replicates, we would expect the true value of the population mean to fall within 95% of those confidence intervals. In other words, for any given experiment there is a 1 in 20 chance the true value of the population mean is not within the 95% CI calculated from the sample. Or, we are 95% confident our interval includes the value of the true population mean. Let me add that this is the beating heart of the frequentism in frequentist statistics. The CI is a statistic that is calculated from the sample of one experimentand yet its interpretation is assumes an infinite number of repeats of that experiment that we would never do. This drives people who practice Bayesian statistics apoplectic. What does the 95% CI not mean? The 95% CI is not a probability. We should neither say, the probability of the point estimate is 95% nor the probability the true value lies within this range is 95%. The CI merely asserts our level of confidence in a range of values that might include the true parameter value. To have more or to have less confidence means that we would need to calculate some other interval, such as a 99% or 90% interval, respectively. When that is done for the same sample date, the range of the interval changes. Because our confidence in the sample changes! 9.6.2 Evaluating confidence intervals The CI has duality as both and inferential and descriptive statistic. Standard errors and confidence intervals and test statistics and p-values are calculated from the same sample data. So there is a bit of a circular argument at play here. The confidence interval serves as a statistical parameter used to express sample accuracy. We use the variability within the sample to generate two related parameters, standard error and confidence intervals. If the former tells us about the precision of the point estimate (eg, the sample mean), let the latter speak to accuracy (eg, a range of sample means consistent with the sample that we can be confident of). To be sure, this is conditioned on the experiments ability to detect a true point estimate. There are a lot of ways for the sample to be untrue. If the measurement machines and techniques are uncalibrated, or the sampled population is the wrong one, or there are uncontrolled confounder variables, or the experiment was underpowered, or the researcher picked and chose replicates to keep, the sample may be wildly inaccurate. The CI also reports the accuracy of point estimates that are wrong. Wrong point estimates are something that can only be addressed scientifically, not statistically. The confidence interval can be used inferentially, to make decisions on hypothesis tests Assume three experiments, each by one of three different groups, are conducted to test whether a drug lowers blood glucose in diabetics. They all use a paired, before/after paradigm. From each random independent replicate blood glucose is measured just before (B) and after (A) giving the drug. The estimated sample parameter is the mean difference between the before and after blood glucose values (a difference equal to \\(B-A\\) is computed within each replicate, the experiment tests whether that difference is reliable). The null hypothesis is the mean difference is zero, or: \\[H_0:\\beta - \\alpha = 0\\] CIs for the mean difference by the groups are listed: Group1: 95% CI: 57 to 25 mg/dl Group2: 95% CI: 34 to 2 mg/dl Group3: 95% CI: 40 to -5 mg/dl Each group is 95% confident the true mean difference in blood glucose lies within their own calculated range. The range of values in Group1 does not include the null value, zero. We can reject the null at 95% confidence. Further, given our scientific experitise in blood glucose, we also assert every value in that range is scientifically meaningful. The range of values in Group2 does not include the null value, zero. We can reject the null at 95% confidence. However, given our scientific expertise in blood glucose, we note the range includes low values that represent scientifically inconsequential effects. The range of values in Group3 does include the value of zero, which represents no mean difference. We are not prepared to reject the null at 95% confidence. The result is inconclusive. 9.6.3 CI Simulation tools Ive found simulation is by far the best way to gain an intuitive feel for how confidence intervals operate. 9.6.3.1 Evaluating one sample CI The first random sampler is a script that generates a single random sample from a known distribution before calculating several sample parameters, including the confidence interval, which it plots as a pretty CI line over the distribution curve for the population that was sampled. Its particularly useful to illustrate the relationships between confidence intervals, sample sizes, standard deviations, confidence levels, and the sampled population. When the confidence interval includees the mean of the population sampled, the confidence interval is accurate, and a blue-colored bar appears. When the confidence interval is not accurate, you get a red bar. The confidence interval for a sample mean is calculated as follows: \\[CI=\\bar x \\pm t_{df(n-1)}\\cdot\\frac{sd}{\\sqrt{n}}\\] Pla #set.seed(1234) # change these initializers n &lt;- 3 pop.mean &lt;- 100 pop.sd &lt;- 25 sig.dig &lt;- 2 conf.level &lt;- 0.95 x &lt;- c(seq(1, 200, 0.1)) # this draws a sample and calculate its descriptive stats mysample &lt;- rnorm(n, pop.mean, pop.sd) mean &lt;- round(mean(mysample), sig.dig) sd &lt;- round(sd(mysample), sig.dig) sem &lt;- round(sd/sqrt(n), sig.dig) ll &lt;- round(mean-qt((1+conf.level)/2, n-1)*sem, sig.dig) ul &lt;- round(mean+qt((1+conf.level)/2, n-1)*sem, sig.dig) #print to console print(c(round(mysample, sig.dig), paste( &quot;mean=&quot;, mean, &quot;sd=&quot;, sd, &quot;sem=&quot;, sem, &quot;CIll=&quot;, ll, &quot;CIul=&quot;, ul))) ## [1] &quot;80.02&quot; ## [2] &quot;98.04&quot; ## [3] &quot;86.56&quot; ## [4] &quot;mean= 88.21 sd= 9.12 sem= 5.27 CIll= 65.54 CIul= 110.88&quot; # graph the sample confidence interval on the population pretty &lt;- ifelse(ll &gt; pop.mean | ul &lt; pop.mean, &quot;red&quot;, &quot;blue&quot;) #graph (note: using ggformula package) gf_line(dnorm(x, pop.mean, pop.sd)~x)%&gt;% gf_segment(0 + 0 ~ ll + ul, size = 2, color = pretty)%&gt;% gf_labs(subtitle = paste(100*conf.level, &quot;% CI =&quot;,ll, &quot;to&quot;,ul)) Figure 9.6: Confidence interval illustrator 9.6.3.2 Comparing two samples with CI Heres a second script that compares the confidence intervals of two means, from random samples of known distributions. When two CIs dont overlap, they remain colored blue and green, and the two means they correspond to are unequal. Its no different inferentially than test of a statistical difference for comparing two means and using p-values. If the two CIs overlap, one of them turns red. Its a sign the two samples might not differ, since the 95% CI of one sample includes values in the 95% CI of the other. Its also the same as a test of significance, for a null outcome. Use this simulator to gain an intuitive understanding about how confidence intervals operate. Practice changing the sample size (n), the population means and standard deviations, and even the confidence level. Under what conditions are overlapping intervals diminished? What factors influence narrower intervals? n &lt;- 5 m &lt;- 100 conf.level=0.95 t &lt;- qt((1+ conf.level)/2, n-1) pop.mean.A &lt;- 125 pop.mean.B &lt;- 200 pop.sd.A &lt;- 25 pop.sd.B &lt;- 25 x &lt;- c(seq(1, 300, 0.1)) y &lt;- seq(0.0005, m*0.0005, 0.0005) #simulate mydat.A &lt;- replicate( m, rnorm( n, pop.mean.A, pop.sd.A ) ) ldat.A &lt;- apply( mydat.A, 2, function(x) mean(x)-t*sd(x)/sqrt(n) ) udat.A &lt;- apply( mydat.A, 2, function(x) mean(x)+t*sd(x)/sqrt(n) ) mydat.B &lt;- replicate( m, rnorm( n, pop.mean.B, pop.sd.B ) ) ldat.B &lt;- apply( mydat.B, 2, function(x) mean(x)-t*sd(x)/sqrt(n) ) udat.B &lt;- apply( mydat.B, 2, function(x) mean(x)+t*sd(x)/sqrt(n) ) ci &lt;- data.frame( y, ldat.A, udat.A, ldat.B, udat.B ) alt &lt;- ifelse( udat.A &gt;= ldat.B, &quot;red&quot;, &quot;blue&quot; ) #plots made with ggformula package gf_line(dnorm( x, pop.mean.A, pop.sd.A)~x, color = &quot;dark green&quot; )%&gt;% gf_line(dnorm( x, pop.mean.B, pop.sd.B)~x, color = &quot;blue&quot; )%&gt;% gf_segment( y+y ~ ldat.A + udat.A, data = ci, color = &quot;dark green&quot; )%&gt;% gf_segment( y+y ~ ldat.B + udat.B, data = ci, color = alt )%&gt;% gf_labs( y = &quot;dnorm(x, pop.mean, sd.mean&quot; ) Figure 9.7: Comparing two samples using confidence intervals. Red-colored indicates the two CI overlap, meaning the two groups for that sample test out as no different. 9.7 Key take aways Focus less on point estimates (eg, the mean of samples) and much more on the statistical parameters that give insight into variation (eg, SD, CIs, range, and even SE) Variability is inherent in biological data. The two main sources are intrinsic biological variationwhich has so many causes, and variability associated with our measurements and procedures. Statistics operates on the presumption that the values of parameters in the populations we sample are fixed. Residual error is unexplained deviation from those fixed values. About two-thirds of the values of a normally distributed variable will lay, symmetrically, within one standard deviation on either side of the variables mean. Variance expresses the variability within a sample, but geometrically, in squared units. The standard deviation, the square root of variance, estimates the variability of a sampled population. The standard error of the mean estimates the precision by which a sample mean estimates a population mean. The standard error of the mean grows smaller as sample size gets larger, by the square root of n. The standard error of the mean is the standard deviation of a theoretical distribution of sample means. A confidence interval estimates the accuracy by which a parameter, such as a mean, has been estimated. If the confidence intervals of two samples do not overlap, the sampled distributions likely differ. For a 95% CI for a mean, there is a 95% confidence the interval includes the true mean. A 99% CI will be wider than a 95% CI, given the same data. A 90% CI will be narrower. The best way to understand these various statistics of dispersion is to simulate and view the outcomes. This may sound like a contrarian view, but is not. Epidemiologists like to say that all models are wrong, but they imply the problem of choosing the wrong model for a dataset. Even a wrong model is perfect. The sample mean is used to calculate SS; deriving that mean cost 1 degree of freedom. Given that mean, the values for all but one replicate are free to vary for the SS to be true.  "],["hypotheses.html", "Chapter 10 Framing statistical hypotheses 10.1 The decision process 10.2 Popper and falsification 10.3 Statistical hypothesis rubric", " Chapter 10 Framing statistical hypotheses There is no more to science than its method, and there is no more to its method than Popper has said.-Hermann Bondi Hypothesis-driven research tests predictions about the nature of the world. Testing hypotheses statistically provides a pragmatic framework for making decisions about the validity of those predictions. When planning an experiment the primary goal should be to bring hyper-focused clarity to the hypothesis. This is the time to distill your thinking down to the exact question you want answered. What are you studying? What is not known? What is your prediction? How will you measure it? What are your variables? Are they discrete or continuous? How will you test it? How will you decide whether what you predicted happened or not? Will this actually answer the question youre asking? The statistics taught in this course are for assessing the validity of experimental outcomes in a somewhat odd way: Formally, we test the null hypothesis. The expectation is to generate observations of such extreme magnitude that we can reject the null, the hypothesis that nothing happened. At first blush that might come off as absurd. Like a Seinfeld episode, where nothing is what is most important. Hopefully this wont seem so odd after I describe what this accomplishes and explain why it is done this way. 10.1 The decision process Everybody knows something about the p-value. When its low enough, the experiment worked. Before diving into the nitty gritty of p-values, lets jump into a wider angle format to flesh out how they are used. The framework can be broken down into 5 key steps: We begin with a null hypothesisyes, the boring one about nothing. Experiments generate data. The data are transformed into test statistics. P-values are calculated from the experiments test statistic value. Based upon a priori thresholds, a decision is made to reject a null hypothesis, or not, depending upon the extremeness of the result. Figure 10.1: Statistical hypotheses test the null in a multistep process Low p-values are associated with extreme values of a test statistic. Extreme values of test statistics happen when the effect sizes of the results are high. Rejecting a null on the basis of a p-value means our test statistic value is too extreme to belong in the distribution of null test statistic values. Thus, a low p-value means the effect size is improbably high if the treatment is, in fact, not truly effective. If you learn nothing more in this course, learn that the statistics discussed here are tests of the null hypothesis. Learn that every p-value you see in R output is coupled to a test statistic value. These p-values represent a cumulative probability of a null test statistic distribution. 10.2 Popper and falsification Using data to falsify an hypothesis, even if that hypothesis is the null, is a decision framework that plays well with philosopher Karl Poppers assertion that scientific theories are probative and that unscientific theories are not. To Popper, the grandest scientific theories are those that can be falsified. In Popperian logic, the truth of nature is unknowable and unproveable.even though it is testable. Thus, the scientific method advocated by Popper doesnt allow for proving an hypothesis, but at the same time it doesnt forbid us from rejecting hypotheses that are inconsistent with observations. Thus enters the null hypothesis, which predicts, of course, that nothing happens. The null is an incredibly handy device because if we make observations that are extremely inconsistent with the null, meaning we have observed that something happens, we are obligated to reject the null. Thus, the null is falsifiable when we have positive results! Imagine an experiment to test whether a drug lowers blood glucose in people who have diabetes. When the glucose-lowering effect size for the drug in the sample is large enough, we can reject the hypothesis that the drug didnt have any effect. In other words, we will accept an observation as evidence for a positive result by formally concluding that same evidence is inconsistent with a negative result. Some argue that this logic forces the researcher to test the wrong hypothesis and to also accept an alternate hypothesis that itself may not be true. For example, although blood glucose may be lower in the drug treatment arm of the sample, that may have occured by random chance. Or an unknown confounder variable that wasnt controlled for could be responsible for the observation that drug-treatment is associated with lower blood glucose. In that case we would make an error by rejecting the null when it is actually true. Of course, rejecting the null is provisional. All gained knowledge is provisional. Drawing a conclusion from one experiment doesnt preclude testing the experiment some other way. If the problem is important enough (and real), it will be tested from multiple angles. It will need to survive the preponderance of the evidence. Im personally convinced the alternative approach, which is to seek out evidence that affirms an hypothesis, is not better. This lies at the heart of what Popper stood against. There is an inherent confirmation bias in seeking out affirmation of ideas. In the proper light, any evidence can be made to look attractive. Furthermore, what if, in seeking affirmation, nothing happens? Negative results are very difficult to interpret because the absence of evidence cannot be interpreted as the evidence of absence. So Id hope the researcher who gives this some thought will find null falsification at least pragmatic, if not ingenious. It allows us to move forward on the basis of positive evidence (granted, which may be wrong and we dont know it), while at the same time practicing a more sound, more unbiased scientific methodology (hypothesis falsification rather than affirmation). Meanwhile, this statistical framework does allow for the possibility of designing experimental conditions to minimize false positive (type1) and false negative (type2) errors. We can operationalize our tolerance for those kinds of mistakes in meaningful ways such that we are less likely to become victims of bad luck. Finally, the decision to reject a null hypothesis can stand alone. It need not be the same as a decision to accept the alternate. Rejecting the null only asserts that the experimental evidence is inconsistent with the null. In no way does that prove\" the alternative hypothesis. For most, some of these concerns should become even less of a problem when the null and alternate hypotheses are explicitly framed in terms of population parameters and their mutually exclusive and collectively exhaustive outcomes. This approach doesnt leave much room for ambiguity about what is being declared at the decision step. For example, the null hypothesis for the diabetes case is very explicit: \\(null, H_0: \\mu_{placebo} = \\mu_{drug}\\), Here \\(\\mu\\), since it is greek notation, represents the mean blood glucose in concentration units in the population from which the sample was drawn. Now that we have a bona fide null hypothesis, we can state the alternate hypothesis as everything the null cant be: \\(alternate, H_1: \\mu_{placebo}\\ \\ne \\mu_{drug}\\) In other words, the inference operates on the basis of straightforward mathematical principles. Two parameters that are compared either meet our prescribed expectations, or they do not. In this case, if we reject the hypothesis that the means of the two groups are equal, then they can only be not equal. Are they truly not equal? We can never know for sure, but we are operating within a framework of known error tolerances. 10.3 Statistical hypothesis rubric Researchers have to grapple with two types of hypotheses. One type is the grand, paradigm-driving assertion of some key insight, which is designed to express the big picture in forward thinking terms. It is also designed to wow study sections and seminar audiences. The other type is the null hypothesis, which is designed to be tested statistically. The null predicts nothing will happen. The null is as boring as it gets. Youd never propose the null in a specific aims page, but you should get in the habit of thinking in terms of testing the null with your statistics. Only the null hypothesis has any statistical utility, whereas the grand hypothesis has no statistical utility. This is a conceptual hurdle that most students struggle with. The grand hypothesis is for marketing, the null hypothesis is for mattering. For that reason Ive created a rubric for forming a statistically testable hypothesis. The rubric begins with a conceptual overview of a problem, and it ends with how the results will be interpreted. At some point during the semester youll have a major assignment that asks you to go through this rubric for a problem of your own choosing. That assignment is a major test for whether you get statistical design of experiments. Step 1: Lay out the big picture of the problem in a way that leads to a What is not yet known assertion. Type 2 diabetes is associated with high blood glucose levels and obesity, which each have long term effects associated with high morbidity. Exenatide is GLP-1 receptor agonist that can control blood glucose levels. When delivered as an osmotic minipump exenatide lowers blood glucose. A standard of care for type2 diabetics is to put them on a weight loss program while giving them drugs that manage blood glucose. It is not known if continuous administration via osmotic minipump can lead to greater weight loss while on this standard of care. Step 2: Transform the What is not known statement into a bold and simple scientific prediction, as if what is not known were answered: Long-term administration of exenatide via osmotic minipump to type-2 diabetics will cause weight loss. Step 3: Now frame the experimental plan in terms of the independent and dependent variables, written as an if/then statement. In narrative format, if you manipulate what predictor variables, then what outcome do you expect to observe? If an exenatide osmotic minipump is implanted into type-2 diabetics, then their weight loss will differ compared to placebo. Step 4: Define the dependent and the independent variables of the experiment. What type of variables are these? What are the experimental units? Are the measurements intrinsically-linked, or not? The dependent variable will be weight loss, calculated as the weight difference between pre-study to post-study for each human subject. Each subject is the experimental unit. The independent variable is treatment. Treatment is a discrete, factoral variable that will be at two levels, placebo and exenatide. Although pre- and post-study weights will be measured for each subject and are themselves intrinsically-linked, they are used to derive the dependent variable (weight loss), which are not instrinsically-linked. Step 5: Write the null and alternate hypothesis on the basis of the statistical parameters to be tested. Note here that greek notation is used to symbolize that the hypothesis is about the sampled population parameters, rather than the sample. Where \\(\\mu\\) represents the mean weight loss of the populations corresponding to the sampled groups, the null and alternate hypotheses are \\[H_0:\\mu_{exenatide}=\\mu_{placebo}\\] and \\[H_1: \\mu_{exenatide}\\ne\\mu_{placebo}\\] Step 6: What statistical test will be used to test the null hypothesis? What are the decision rules? A two-sided, unpaired t-test for comparing group means. The sample size will be based upon a power of 90%, which means that the tolerance level for type2 error will be 10%. The decision threshold for type1 error will be 5%. Thus, the null hypothesis will be rejected at a p-value of less than 0.05. 10.3.0.1 Two-sided vs one-sided hypothesis The above is an example for a two-sided hypothesis. In a two-sided hypothesis \\(\\ne\\) is mutually exclusive and collectively exhaustive of \\(=\\). By rejecting the null that two things are equal, we implicitly (and provisionally) accept the alternative hypothesis that they are not equal. Notice how this hypothesis doesnt predict the direction of an effect. It only predicts there will be a difference between the two groups. If youre willing to predict the direction of an effect, you would choose to make a one-sided hypothesis. One-sided hypotheses can happen in either of two ways. In one case we can predict one mean will be greater (\\(&gt;\\))than another mean. In the other case, we can predict one mean will be less than (\\(&lt;\\)) another mean. The mutually exclusive and collectively exhaustive alternatives to these one sided hypotheses are therefore \\(\\ge\\) and \\(\\le\\), respectively. In other words, if one mean is not greater than another mean, then the only alternative possibilities are that it is less than or equal to it. The decision to test a one- or two-sided hypothesis should be based upon scientific reasoning. In the example above, Im unwilling to test a one-sided hypothesis that exenatide will cause a greater weight loss than placebo, even though that is the expectation (and hope!). Were I willing to test the direction of the effect, and predict that the mean weight would be lower with exentatide, the one-sided hypothesis test would be written like this: \\[H_0:\\mu_{exenatide} \\ge \\mu_{placebo}\\] and \\[H_1: \\mu_{exenatide}&lt;\\mu_{placebo}\\] If the data show that mean weight loss is greater in the exenatide group, as expected, that null hypothesis can be rejected. But what if, unexpectedly, weight loss is greater in the placebo group? It would generate a high p-value. According to the pre-planned hypothesis, the null could not be rejected. Worse, given they are already enrolled in a standard of care weight loss program, to know the drug actually impairs weight loss would be an important finding. But in choosing the incorrect one-sided hypothesis, there is nothing to do with the result. It is a negative result. I cant flip the tail to the other direction to get a significant result that I wasnt planning upon. That would be extremely biased! In practice, some researchers caught in this conundrum create a whole new can of worms by simply changing the pre-planned hypothesis after the fact. Its done flippantly but is actually a fairly serious violation of scientific integrity. Changing the hypothesis so that it is consistent with the results is not what anybody would consider sound scientific method. 10.3.0.2 Stick to two-sided hypotheses Unlike the case above, when being wrong about the direction of an effect is not a big deal, then one-sided tests are not a bad option. The example above serves to illustrate how a two-sided hypothesis would have been a better choice than a one-sided hypothesis. There are a few other reasons why it is probably better to get in the habit of always testing two-sided nulls: the two-sided test is more conservative because the p-value threshold is a bit lower. Furthermore, multiple tests and confidence intervals easier perform and to interpret, respectively. "],["error.html", "Chapter 11 Error 11.1 Setting type 1 and type 2 error thresholds 11.2 Striking the right balance 11.3 False discovery rate", " Chapter 11 Error library(tidyverse) library(treemapify) library(pwr) In a jury trial under the American system of justice the defendant stands accused of a crime by a prosecutor. Both sides present evidence before a jury. The jurys duty is to weigh the evidence then vote in favor of or against a conviction. The jury doesnt know the truth. A jury is at risk of making two types of mistakes: * An innocent person might be convicted, or * a guilty person might be acquitted. They can also make two correct calls: * Convict a guilty person or * acquit someone who is innocent. Without ever knowing for sure what is actually true, they are instructed by the judge to record their decision on the basis of a threshold rule. In a trial the rule is vote to convict only when you believe it is beyond a reasonable doubt the accused is guilty. In science the researcher is like a jury. The experiment is like a trial. At the end, the researcher has the same problem that jurors face. There is a need to conclude whether the experiment worked or not. And theres no way to know with absolute certainty. Mistaken judgments are possible. Whereas the jury works within the beyond a reasonable doubt framework, researchers operate within a framework that establishes tolerance limits for error. Every hypothesis tested risks two types of error. A type 1 error is committed when the researcher rejects the null when in fact there is no effect. This is also known as a false positive. A type 2 error is not rejecting the null when it should be rejected, which is known as a false negative. Or the researcher might not make an error at all. The sensitivity of an experiment is conclude correctly there is no effect, and power (also known as specificity^{Sensitivity and specificity as statistical jargon predominates in the clinical trial and epidemiology worlds. Experimentalists speak of power more than they do of specificity. And were too crestfallen to speak of sensitivity.}) is concluding correctly there is an effect. Sensitivity and power are the complements of type 1 and type 2 error, respectively 11.1 Setting type 1 and type 2 error thresholds In the planning stages of an experiment the researcher establishes tolerance for these errors. It is a juggling act of multiple competing interests. A balance has to be struck between aversion for each error type, having conditions that favor the ability to make the right call, and how much it costs to be either wrong or right. The latter might seem weird, how can being right have any cost? Well, it may cost a very large number of replicates and time and resources to be right, more than were willing to pay for. 11.1.1 Setting alpha-the type 1 error In the biological sciences the standard for type 1 error is 5%, meaning in any given experiment (no matter the number of comparisons to be made), wed need to see a signal to noise ratio in the top 5th percentile of signal to noise ratios to be willing to accept it might be a mistake. The acceptable type 1 error limit is labeled alpha, or \\(\\alpha\\). In several R statistical functions, it is controlled by adjusting its complement, the confidence level. An \\(\\alpha\\) of 0.05 corresponds to a 0.95 confidence level. Why is \\(\\alpha\\) 5% and not some other value? Credit for that is owed largely to R.A. Fisher who offered that a 1 in 20 chance of making such a mistake seemed reasonable. That number seems to have stuck, at least in the biological sciences. The researcher is always free to establish, and defend, some other level of \\(\\alpha\\). In the field of psychology, for example, \\(\\alpha\\) is historically 10%. Or perhaps you have an exploratory gene screen study involving a large number of comparisons. In that case, youre willing to accept a few more errors over the risk classifying too many negatives. Raise your \\(\\alpha\\)! There is nothing to stop a researcher from selecting a threshold below or above 5%. She just needs to be prepared to defend the choice. 11.1.1.1 The decision rule The \\(\\alpha\\) is stated before an experiment begins, but operationalized during the final statistical analysis on the basis of p-values generated from statistical tests. The null hypothesis is rejected when a p-value is less than this preset \\(\\alpha\\). Thats it. A relatively straight forward threshold decision. Not unlike the decision to obey the expiration date on a food item. 11.1.1.2 Experimentwise error An experiment that just makes one comparison between two groups (eg, placebo vs drug) generates only one hypothesis. An experiment comparing \\(k\\) groups (eg, placebo vs drug1, vs drug2drugk-1) generates \\(m=\\frac{k(k-1)}{2}\\) hypotheses. When an experiment tests multiple hypotheses it is important to maintain the overall \\(\\alpha\\) for the experiment at 5% (or whatever level is chosen). If not checked, the overall exposure to experiment-wise error would inflate with each hypothesis tested. Several methods have been devised to maintain experiment-wise \\(\\alpha\\) for multiple comparisons. The most conservative of these is the Bonferroni correction \\(\\alpha_m=\\frac{\\alpha}{m}\\). Thus, if \\(m = 10\\) hypotheses are tested, the adjusted threshold for each, \\(\\alpha_m\\), is 0.5%, or a p-value of 0.005. If 1000 hypotheses are tested, such as in a mini-gene screen, the p-value threshold for each would be 0.00005. 11.1.2 Power: Setting beta-the type 2 error In the biological sciences the tolerance for type 2 error, otherwise symbolized as \\(\\beta\\), is generally in the neighborhood of 20%. Its a bit easier to discuss \\(\\beta\\) through its complement, \\(1-\\beta\\) or power. Thus, experiments run at 80% power are generally regarded as well-designed. These run at 20% risk of type 2 error. Recall, the type 2 error is the mistake of not rejecting the null, when it is actually false. Operationally, an experiment is designed to hit a specific level of power via planning of the sample size. Power calculations are mostly designed to return sample size by integrating intended power, \\(\\alpha\\), and an estimate for a scientifically meaningful effect size. Students tend to fret over effect size estimates. They are nothing more than a best guess of what to expect. A crude estimate. In one approach, the researcher should use values representing a minimum for a scientific meaningful effect size. The effect size is estimated on the basis of scientific judgment and preliminary data or published information. If the anticipated effect size is much larger than what is considered a scientifically meaningful effect size, then go with that. If the effect size estimate turns out to be accurate, an experiment run at that sample size should be close to the intended power. In a perfect world of unlimited resources, we might consider powering up every experiment to 99%, dramatically minimizing the risk of \\(\\beta\\). As youll see in the simulation below, the incremental gain in power beyond ~80% diminishes with larger sample size. In other words, perfect power and very low \\(\\beta\\) comes at a high cost. The choice of what power to run an experiment should strike the right balance between the risk of missing out on a real effect against the cost burden of additional resources and time. Fortunately, these scenarios can all be simulated in advance. What if games can be played on the computer at very low cost. Rs pwr package has a handful of functions to run power calculations for given statistical tests. These, unfortunately, do not cover all of the statistical tests, particularly for the most common experimental designs (eg, ANOVA). In this course, we will emphasize performing power calculations using custom Monte Carlo functions, which can be custom adapted for any type of experiment involving a statistical test. Heres a custom Monte Carlo-based power function for a t-test. To illustrate the diminishing returns argument, the function calculates power comparing samples drawn from \\(N(0,1)\\) to samples drawn from \\(N(1,1)\\). The graph is generated by passing a range of sample sizes into the function. Note how the gain in power plateaus as sample size increases. # If you feed t.pwr a sample size, it will calculate power t.pwr &lt;- function(n){ # Intitializers. Means and SD&#39;s of populations compared. # Change these values to the units of what you expect. m1=1; sd1=1; m2= 0; sd2=1 ssims=1000 p.values &lt;- c() i &lt;- 1 # this next step is THE monte carlo, it resamples based # upon the initializer settings repeat{ x=rnorm(n, m1, sd1); y=rnorm(n, m2, sd2); p &lt;- t.test(x, y, paired=F, alternative=&quot;two.sided&quot;, var.equal=F, conf.level=0.95)$p.value p.values[i] &lt;- p if (i==ssims) break i = i+1 pwr &lt;- length(which(p.values&lt;0.05))/ssims } return(pwr) } # Run t.pwr over a range of sample sizes and plot results frame &lt;- data.frame(n=2:50) data &lt;- bind_cols(frame, power=apply(frame, 1, t.pwr)) #plot ggplot(data, aes(n, power))+ geom_point() + scale_y_continuous(breaks=c(seq(0, 1, 0.1)))+ scale_x_continuous(breaks=c(seq(0,50,2)))+ labs(x=&quot;n per group&quot;) ## Validation by comparison to pwr package results pwr.t.test(d=1, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 16.71472 ## d = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 11.2 Striking the right balance The script below provides a way to visualize how the relationship between correct (green) and incorrect (red) decisions varies with error thresholds. The idea is to run experiments under conditions by which green is the dominant color. Unfortunately, most published biomedical research appears to be severely underpowered findings. alpha &lt;- 0.05 beta &lt;- 0.20 panel &lt;- data.frame(alpha, sensitivity=1-alpha, power=1-beta, beta) panel &lt;- gather(panel, key=&quot;threshold&quot;, value=&quot;percent&quot;) panel &lt;- bind_cols(panel, truth=c(&quot;no effect&quot;, &quot;no effect&quot;, &quot;effective&quot;, &quot;effective&quot;), decision=c(&quot;effective&quot;, &quot;no effect&quot;, &quot;effective&quot;, &quot;no effect&quot;), choice=c(&quot;error&quot;, &quot;correct&quot;, &quot;correct&quot;, &quot;error&quot;)) panel ## threshold percent truth decision choice ## 1 alpha 0.05 no effect effective error ## 2 sensitivity 0.95 no effect no effect correct ## 3 power 0.80 effective effective correct ## 4 beta 0.20 effective no effect error ggplot(panel, aes(area=percent, fill=choice, label=threshold))+ geom_treemap(color=&quot;white&quot;)+ geom_treemap_text( fontface = &quot;italic&quot;, colour = &quot;white&quot;, place = &quot;centre&quot;, grow = F )+ scale_fill_manual(values = alpha(c(&quot;green3&quot;, &quot;red&quot;), .3)) 11.3 False discovery rate The false discover rate, or FDR is another way to estimate experimental error. \\[FDR=\\frac{false\\ positives}{false\\ positives + false\\ negatives}\\] FDR varies given \\(\\alpha\\), \\(\\beta\\) and the probability of the effect. The probability of the effect bears some comment. Think of it as a prior probability that an effect being studied is real. It takes some scientific judgment to estimate these probability values. And the values can be vague. The graph below illustrates how FDR inflates, particularly when running experiments for low probability effects when tested at low power, even at a standard \\(\\alpha\\). These relationships clearly show that the lower the probability of some effect that you would like to test in an experiment, the higher the stringency by which it should be tested. px &lt;- seq(0.1, 1.0, 0.1) #a range of prior probabilities tests &lt;- 10000 fdr_gen &lt;- function(beta, alpha){ real_effect &lt;- px*tests true_pos &lt;- real_effect*(1-beta) false_neg &lt;- real_effect*beta no_effect &lt;- tests*(1-px) true_neg &lt;- tests*(1-alpha) false_pos &lt;- no_effect*alpha FDR &lt;- false_pos/(true_pos + false_pos) return(FDR) } upss &lt;- fdr_gen(0.6, 0.05)#under-powered, standard specificity wpss &lt;- fdr_gen(0.2, 0.05)#well-powered, standard specificity uphs &lt;- fdr_gen(0.6, 0.01)#under-powered, high specificity wphs &lt;- fdr_gen(0.2, 0.01)#well-powered, high specificity fdrates &lt;- data.frame(px,upss, wpss, uphs, wphs) colnames(fdrates) &lt;- c(&quot;Probability&quot;, &quot;5% alpha, 60% beta&quot;, &quot;5% alpha, 20% beta&quot;, &quot;1% alpha, 60% beta&quot;, &quot;1% alpha, 20% beta&quot;) #convert to long format fdrates &lt;- gather(fdrates, tests, FDR, -Probability) ggplot(fdrates, aes(Probability,FDR, group=tests))+ geom_point(aes(color=factor(tests)))+ geom_line(aes(color=factor(tests))) "],["pvalues.html", "Chapter 12 P Values 12.1 Definition 12.2 Null test statistic distributions 12.3 Computing p-values 12.4 P-values from null tests are uniform 12.5 How P-values become error probabilities 12.6 P-values as dichotomous switches 12.7 P-values in true effects 12.8 Criticisms of p-values 12.9 Summary: How to use p-values", " Chapter 12 P Values library(tidyverse) The p-value represents the probability that your dataset is too small. -Karendeep Singh on twitter, (kdpsinghlab?) The p-value is an instrument by which a decision will be made. As such, it is worth understanding how that instrument works. Within the frequentist inferential framework we use for this course, hypothesis-driven experiments are designed to test the null hypothesis. That null will be falsified when the effect size we measure in our data is large enough to generate what we classify as an extreme value for a given test statistic. The p-value is the basis of that classification. Strictly, the p-value is the area under the curve drawn by a distribution function for a given test statistic. The p-value is just an integral for the frequency of a test statistic over a specific range of values. In R, as for most statistical software, a cumulative distribution function of a test statistic is used to calculate p-values. Thats all just math and computation. How that area under the curve is re-interpreted as an an error probability comes from inferential theory. 12.1 Definition This is probably the most widely accepted definition: A p-value is the probability that a test statistic value could be as large as it is, or even more extreme, under the null hypothesis. This definition includes some inferential slight of hand. The test statistic probability distributions are also called sampling distributions, because they represent all of the test statistic values that could be produced from an infinite number of experimental samples. And the probability it represents represents what is also called an error probability because every time we do an experiment we assume we are sampling from a null distribution. The p-value is therefore taken as the probability we would mistakenly declare there to be evidence against the null in our data.5 Read further below to understand that one. Mathematically, p-values are just simple frequency probabilities. Areas under the curve of a function. They convey what proportion of the total area under a curve is represented by a range of test-statistic values. They take on additional inferential meaning only when the intention is to test a hypothesis. 12.2 Null test statistic distributions The first thing to know about all test statistics is they all represent a conversion of experimental data into something resembling a signal-to-noise value. Second, there are many test statistics. Each has a corresponding sampling distribution function in R: t (dt), F (df), z (dnorm), \\(\\chi^2\\) (dchisq), sign rank (dsignrank), and more. Third, R also has cumulative probability functions for each test statistic (eg, for the t-statistic, this function is pt). The cumulative probability functions are useful to calculate the area under the distribution curve to the right (lower.tail = F) or to the left (lower.tail =T) of a given test statistic value. The output of these functions is therefore the proportion of the total area under the curve to the right or left of a given test statistic value. Fourth, when our intentions move to using p-values to draw inference on experimental data, all of these sampling distributions are meant to represent the distribution of test statistic values under the null hypothesis. And the p-values take on added meaning as error probabilities, rather than proportions of area. The script below simulates the behavior of null test statistics. It repeatedly compares the means of random samples drawn from two identical populations. What follows is based upon sampling from two populations that we know are identical (because they are coded as such), unlike in real life. Using the rnorm function, the code repeatedly conducts a random sampling to compare two groups, A and B, at 5 replicates per group per sample. These two groups are coded to have the same populaton parameter values, \\(\\mu=100\\) and \\(\\sigma=10\\). A test statistic value (and also a p-value) is collected each time a random sample is generated. Because we know the two groups are identical, we have no inference to make. No matter how extreme, every test between any samples generated from them will be null. The question is, what is the distribution of test statistic values under this condition? The gray histogram in the figure below is distribution of 10,000 null test statistic values. Think of this as the insanity of performing 10,000 individual experiments comparing the means of two identical groups, such as placebo v a drug that doesnt work (like hydroxychloroquine for Covid). You can see the test statistic frequency distribution fits favorably to the perfect curve drawn from the theoretical t-statistic distribution function (dt(df=8)). The average value of t is about zero (no signal to noise), which we would expect if comparing two identical populations. But there is a lot of dispersion around that mid point. About 2/3rds of the t-statistic values are within +/- a standard deviation. About 95% of these values are within two standard deviations, and so forth. Thats not surprising for a random process. To be sure, of the 10,000 comparisons between these two identical groups, at relatively low frequencies, there appear some extreme test statistic values. Some are very extreme. And they are extreme on both sides of the center line. But we know even these extreme values are null, because the code is set up to compare two identical populations. We can conclude those extreme test statistic values simply arise by random chance. A real life experiment comparing two populations is very different. The most important difference is we dont know for sure whether the two populations differ. The othr important difference is we only get to draw one sample, not 10,000, and we have to conclude from that one sample whether the two populations differ. We could make an error doing that. set.seed(1234) #this is for reproduciblity ssims=10000 #number of repeat cycles to run t &lt;- c() #empty vector, to be filled in the repeat p &lt;- c() #ditto i &lt;- 1 #an index value, for counting repeat{ groupA &lt;- rnorm(5, mean=100, sd=10); #generates 5 independent random replicates from N(100,10) groupB &lt;- rnorm(5, mean=100, sd=10); #ditto result &lt;- t.test(groupA, groupB, paired=F, var.equal=F, alternative = &quot;greater&quot;, conf.level=0.95) #&#39;result&#39; is a list of t-test output t[i] &lt;- result$statistic[[1]] #grabs t-test statistic value p[i] &lt;- result$p.value #ditto but for p-value if (i==ssims) break #logic for ending repeat function i = i+1 } output &lt;- tibble(t, p) #need this to ggplot #draws canvas, adds histogram, adds blue line, then customizes x scale ggplot (output, aes(x=t))+ geom_histogram(aes(y=..density..), binwidth=0.05)+ stat_function(fun=dt, args=list(df=8), color=&quot;blue&quot;, size=1)+ scale_x_continuous(&quot;t Statistic&quot;, breaks=-8:8) Figure 12.1: The distribution of null t-tests includes extreme values of the test statistic. 12.3 Computing p-values P-values are derived from test statistic values, according to the cumulative probability distribution function for that statistic. The easiest way to understand what the output means is by playing with Rs cumulative probability distribution functions. The pt function is that for the t-statistic. pt produces a p-value when fed a t-statistic value and the degrees of freedom for a test. See Chapter 24 for more on the R functions for the t- distribution. To see how this works, lets randomly select a t-statistic value from our function above, and then calculate a p-value for it. set.seed(8675309) paste(&quot;here&#39;s a random t-statistic:&quot;, sample(output$t, 1)) ## [1] &quot;here&#39;s a random t-statistic: 1.5364988920531&quot; Now well use pt to calculate the p-value. paste(&quot;here is its p-value: &quot;, pt(q=1.536499, df = 8, lower.tail =F)) ## [1] &quot;here is its p-value: 0.0814845608085302&quot; Viewed graphically, the p-value for t = 1.536499 is the gold shaded area under the blue curve. Strictly, the gold area represents 8.1484% of the total area under the blue curve. So when stripped of all inferential meaning a p-value is just a fraction of the area under a test statistic distribution curve. ggplot (output, aes(x=t))+ geom_histogram(aes(y=..density..), binwidth=0.05)+ stat_function(fun=dt, args=list(df=8), color=&quot;blue&quot;, size=1)+ stat_function(fun=dt, args=list(df=8), geom=&quot;area&quot;, xlim=c(1.536499, 8), fill=&quot;gold&quot;)+ scale_x_continuous(&quot;t Statistic&quot;, breaks=-8:8) Figure 12.2: The p-value for t = 1.536499 is the gold shaded area under the curve. 12.4 P-values from null tests are uniform Now one last really important point about p-values from null tests. Lets make a histogram of the distribution of p-values from those 10,000 null tests above. Clearly, no p-value occurs more frequently than another. They have a classic uniform distribution.6 ggplot(output, aes(x=p))+ geom_histogram(color=&quot;black&quot;, fill=&quot;blue&quot;, binwidth=0.05, na.rm=T)+ scale_x_continuous(&quot;p-value&quot;, limits=c(0,1)) Figure 12.3: The distribution of p-values from tests between identical populations is uniform. When testing between two identical populations a low p-value (&lt;0.05) is just as likely as a high p-value (&gt;0.95) or as an intermediate p-value (0.50).7 Importantly, note how there are even very low p-values. By random chance, even though the two populations are identical, it is possible to draw a sample whose test generates a p-value below, hmmmsearching for some meaningful threshold, I dunno, say, 0.05. 12.5 How P-values become error probabilities When the intention of the researcher is to test a hypothesis, our interpretation of the p-value morphs from a proportion of area under the curve to a probability. Mathematically, it is derived from the same function and is still a proportion. It just takes on additional meaning. Now lets pretend, in the simulation above, that we dont know that the two populations are actually the same. Lets also imagine that we want to test the hypothesis that the mean of one of the simulated populations is greater than the mean of the other. Lets also imagine we only draw a single random sample, just like in a real life experiment. Lets also imagine that we make a rule that we will declare one of the population means will be statistically greater if the p-value associated with our single test is less than 0.05. Imagine by random chance, we just happen to draw a sample that causes a very extreme test statistic value. The p-value is very low. output %&gt;% filter(t==max(t)) ## # A tibble: 1 x 2 ## t p ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.67 0.0000313 The test statistic value for this draw is extremely different from zero and the p-value is below our 0.05 threshold. On the basis of this outcome we would infer that the mean of one of the populations is greater than the other. Of course, in fact, the two populations are identical. An error is clearly made to conclude that one mean is statistically greater than the other. We would go on to say that the probability of erroneously concluding that one group is larger, when in fact they are identical, is 3.131..e-05. In other words, the error probability is 3.131..e-05. 12.6 P-values as dichotomous switches We also shouldnt frame p-values as signs of strength of evidence (that p-values of null tests are uniformally distributed should scare us away from there). In the same regard lets not use p-values as measures of credibility (eg, the p-value is very significant)p-values arent Bayesian probabilities. The uniform distribution of nul p-values and the fact that low p-values can come from null tests should scare us away from such practices. Instead, p-values are instruments used in a dichotomous decision making process. When the p-value of a test falls below a preset threshold for type1 error, alpha, then we reject the notion that the sample belongs in the distribution of null tests. In other words, our decision is based upon the assumption that our single test is a random draw from an infinite number of tests we might have otherwise conducted. When the result is extreme, it is less likely our sample belongs in the null distribution. The p-value gives us a sense of how likely of a mistake it is to reject the null hypothesis. P-values allow for this decision making process to be standardized across a variety of experimental designs and test statistics. Thus, learningn how to use them for one type of test puts us in good shape for others. 12.7 P-values in true effects To emphasize the latter point, lets no look at how p-values behave when there truly is a true effect? Take the simulation script from above and change only one element. Now the mean of Group A is higher than the other, by 25 units. The first thing to note is the distribution of test-statistic values is right-shifted, dramatically, away from the null distribution for the test statistic. Higher test statistic values are more likely when group differences are true. But note how they are not all extreme. With random sampling it is possible to generate test statistic values consistent with the null distribution, even when differences are true. Those are type2 errors, of course. set.seed(1234) ssims=10000 t &lt;- c() p &lt;- c() i &lt;- 1 repeat{ groupA &lt;- rnorm(5, mean=125, sd=10); #the mean value is the only difference groupB &lt;- rnorm(5, mean=100, sd=10); result &lt;- t.test(groupA, groupB, paired=F, alternative=&quot;greater&quot;, var.equal=F, conf.level=0.95) t[i] &lt;- result$statistic[[1]] p[i] &lt;- result$p.value if (i==ssims) break i = i+1 } output2 &lt;- tibble(t, p) #I&#39;ll draw our new distribution next to the theoretical null ggplot (output2)+ geom_histogram(aes(x=t, y=..density..), binwidth=0.05, na.rm=T)+ stat_function(fun=dt, args=list(df=8), color=&quot;blue&quot;, size=1)+ scale_x_continuous(&quot;t-statistic value&quot;, limits = c(-8,30), breaks=-8:30) Figure 12.4: A true difference between groups generates many more extreme values of the test statistic than in tests of null groups. The second thing to note is that the distribution of p-values in tests of non-null groups is not uniform, as was the case under the null distribution. They are much, much more skewed to lower values. When there is a true effect were much more likely to generate low than high values. ggplot(output2, aes(x=p))+ geom_histogram(color=&quot;blue&quot;, binwidth=0.005, na.rm=T)+ scale_x_continuous(&quot;p-value&quot;, limits=c(0, 1))+ scale_y_continuous(limits = c(0, 3000)) Figure 12.5: Tests of non-null groups yields skewed p-value distributions The main take away is that when there is no difference between tested populations, were equally likely to generate high and low p-values. When there are true differences, we can expect to see low p-values at much higher frequencies. 12.8 Criticisms of p-values There are several common criticisms of p-values, many of which are legitimate. Ill address a few key ones here. They are too confusing, nobody understands them. I confess that p-values are a struggle to teach in a way thats simple and memorable. Especially for researchers who only consider statistics with any intensity episodically, perhaps a few times a year. But like any tool they use in the lab, it is incumbent upon the researcher to learn how it works. A good way to get a better intuitive understanding for p-values is to play around with the various test statistic probability and quantile distributions in R (pnorm, qnorm, pt, qt, pf, pf, pchisq, qchisq, psignrank, qsignrank etc). Use them to run various scenarios, plot them outget a sense for how the tools work by using them. p-Values poorly protect from false discovery This is undoubtedly true. Since David Colquhoun goes over this in blistering detail I wont repeat his thorough analysis here. The researcher MUST operate with skepticism about p-values. Since Colquhouns argument is largely based on simulation of typical underpowered experiments. For well-powered experiments, p-values are far less unreliable. Using Monte Carlo simulation a priori, a researcher can design and run experiments in silico that strikes the right balance between the threshold levels she can control (eg, \\(\\alpha\\) and \\(\\beta\\)) and feasibility in a way that best minimizes the risk of false discovery. All that before ever lifting a finger in the lab. p-Values arent the probability Im interested in Researchers who raise this criticism generally are interested in something the p-value was never designed to deliver: the probability that their experiment worked, or the credibility for their observed results, or even the probability of a false discovery. A p-value doesnt provide any of that information because it is an error probability. It is meant to give the researcher a sense of the risk of making a type 1 error by rejecting the null hypothesis. The p-value is just a threshold device. For these researchers, embracing Bayesian statistics is probably a better option. People use p-values as evidence for the magnitude of an effect. It is common for people to conflate statistical significance with scientific significance. This criticism is really about mistaking a statistically difference p-value and as a statistically significant scientific finding. A low p-value doesnt provide evidence that the treatment effect is scientifically meaningful. If small, scientifically insignificant effect size is measured with high enough precision, that can come with a very low p-value. Some low p-values are uninterpretable. A simple example of this comes from 2 way ANOVA F test analysis. When the test suggests a positive result for an interaction effect, low p-values for the factors individually are uninterpretable because they are confounded by the interaction effect. Researchers should therefore always analyze p-values in conjunction with other parameters, such as effect and sample sizes and the confidence intervals, and always with scientific judgment. 12.9 Summary: How to use p-values Before starting an experiment, we write in our notebook that we can tolerate some level of type1 error. Usually we choose 5% because everybody seems to choose that one, but we are free to use any value that were comfortable with. Thats our judgment to make and defend. We may be at a time or place in our career where we are willing to accept more or less error. As aloof, unbiased, highly skeptical scientists, we go about running the experiment under the null or to test the null. This means that we operate, coolly and calmly, under the assumption that nothing will happen. Better to plan not to be disappointed. No big deal. Whatever. We make a decision rule. Our rule is that if p-value calculated from our test statistic falls below our threshold for type1 error (say, p &lt; 0.05 or whatever is specified), we will reject the null hypothesis. In effect, by rejecting the null, we declare that our random sample is too extreme to belong within a null distribution of test statistic values. We now fully understand that the test statistic from null distributions can have extreme values all by chance alone. Such values, however, occur at acceptably low frequencies. We accept we might be in error in concluding a low p-values is associated with a true effect. And thats why the p-value is the probability we are making this error. Over the long run of many experiments we could anticipate making that error 5% of the time. The next figure illustrates all of the samples that we would have reject as null, in error. ggplot (output, aes(x=t))+ geom_histogram(binwidth=0.05)+ scale_x_continuous(&quot;t Statistic&quot;, breaks=-8:8)+ geom_histogram(data=output %&gt;% filter(t&gt;0 &amp; p&lt;0.05), aes(x=t), binwidth=0.05, fill =&quot;gold&quot;) Figure 12.6: When the two populations are in fact identical, the samples we would erroneously declare as statistically different are gold colored. In a one-sided test Cox, D. R. &amp; Hinkley, D. V. (1974). Theoretical Statistics, London, Chapman &amp; Hall., p66 To understand how a uniform distribution works, play with Rs uniform distribution functions: dunif, punif, qunif and runif Think about this the next time you hear someone declare, disappointing result, but at least the p-value was trending toward significance. It can be said equally that the p-values was trending toward non-significance. "],["jaxwest7.html", "Chapter 13 Reproducible Data Munging in R 13.1 Jaxwest7 data 13.2 Munge the glucose data into R 13.3 Munge the bodyweight data 13.4 Merge and explore the glucose and bw data 13.5 Summary", " Chapter 13 Reproducible Data Munging in R library(datapasta) library(tidyverse) library(cowplot) library(viridis) Data munging is the process of taking data from one source(s) and working it into a condition where it can be analyzed. Every data set will differ and so every data munge will be custom. Data munging is a bit like organic chemistry. We know what we want to create. We know the starting materials that we have on hand. We work work with reactions and intermediates necessary to produce that final product. Here is one example of the munging process. The exercise converts an excel file data source into a format that can be used in R. The data source is formatted in such a way that reading the file is buggy, and not pragmatic. We will use the import function (datapasta) and a handful of functions in the tidyverse package to get it into R and into a long table tidy format. Note especially how every transaction with the data is recorded. Anyone who has this source data file can start with the same excel file and get an identical outcome. Thats reproducibility. 13.1 Jaxwest7 data The Jaxwest7 data set is a Jackson Labs experiment conducted using an obese mouse model of diabetes type 2. The experiment tests whether a drug used to treat type2 diabetes, rosiglitazone, is effective in the model. From the protocol, half the subjects receive the antidiabetic drug, the other receive vehicle as placebo. The syndrome is assessed by measuring two response variables: body weight and blood glucose concentrations. There are two explanatory variables: day of study and drug treatment. The experimental design is therefore multivariate (weight, blood glucose) two-factor (drug treatment, day) ANOVA with repeated measures on the day variable. Our ultimate goal is to create a dataset for MANOVA analysis. The purpose of this chapter, for now, is to illustrate how to retrieve and process data to prepare it for analysis. We will use datapasta to copy the data from the spreadsheet into code, which illustrates the Addin feature of RStudio. 13.1.1 Inspect the source data Download the Jaxwest7.xls file from the mouse phenome database to your machine and open it with spreadsheet software such as Excel. First go to the BloodGlucoseGroups tab. This is readable file, but complex. In fact, this sheet illustrates what unstructured data looks like. Almost every column has cells containing multiple types of values. The first 8 rows have various descriptor text, including a logo. Rows 9-14 have some other definitions. Scroll way over to the right and some graphs pop up. The data we are interested in are in rows 15 to 42, and in columns F to S. Each of those columns has two column names, a date and a day. A variable column should have only one name. Additionally Columns T and U have several missing values, because those animals were used for autopsy. Were going to have to ignore their response values. Cell F21 is a character value indicative of an out-of-range test result. Columns 43 to 146 are missing entirely. Below the array are some summary statistics, each of which is a different parameter. Finally, cage and mouse IDs are not recorded. Now go to the BodyWeightsGroups tab. The structural issues are about the same as for the BloodGlucoseGroups sheet. Here there are cage IDs but no mouse IDs. Most notably, there are only 7 rows of body weights in the rosiglitazone treatment group, whereas there are 8 rows in the corresponding glucose group. These are not a spreadsheets that can be imported whole scale directly into R with ease. Instead, we need to grab only the data we need. Then well use R to structure it for analysis. 13.2 Munge the glucose data into R Lets start with the glucose data R. The goal is to create a dataframe object with the following variables: animal id, day, treatment, and glucose value. Glucose concentrations were measured twice per day on odd-numbered days plus day 12. Each column represents a blood draw session. This was done on each of 16 animals. Half were in a placebo group, half were in a drug group. Well omit day 15 due to the NA values (those mice were harvested for autopsy, and so day 15 breaks the time series). Well also omit the last row in the rosiglitazone group because it doesnt have a corresponding match in the body weight data. We assume each row represents a unique mouse and that the rows in the glucose and body weight data correspond. This is tenuous and not ideal. 13.2.0.1 Step 1 Deal with cell F21. Its value in the excel spreadsheet is Hi, a character value rather than a numeric. We have two options: Assign it an NA value, or impute. Since this is a related-measures time series with multiple other glucose measurements for that specific replicate, well impute by using the average of all these other measurements. Calculate the value that will be imputed: #Use datapasta to paste in vector values. Calculate their mean. Then impute value for cell F21 in original data set by exchanging the value &quot;Hi&quot; with the mean produced here. F21 &lt;- mean(c(449L, 525L, 419L, 437L, 476L, 525L, 499L, 516L, 485L, 472L, 535L, 500L, 497L) ); F21 ## [1] 487.3077 Ideally, youd import the data with the Hi value and fix it in R, to have a contiguous reproducible record for the imputation. In this case, Im getting some anomalies using read_excel from readr, which seem related to the spreadsheet formatting. Leaving Hi in F21 causes some another issue with the datapasta importer that require additional not-fun munging. So it will be fixed in situ. 13.2.0.2 Step 2 Fix the F21 cell in the spreadsheet file by imputing the value from above. 13.2.0.3 Step 3 Copy the spreadsheet array F15:S41 to the clipboard. This is 14 columns and 15 rows of glucose data. All values are numeric and represent the same variable: blood glucose concentration. This omits the last replicate from the rosiglitazone glucose group. This is due to the fact that the body weight data only are for four rosiglitazone animals. Use the datapasta package Addin for this procedure. Create an object name, put the cursor next to it, and select Paste as Tribble from the Addins drop down menu. Youll find the Addins drop down just below the RStudio main menu. jw7gluc &lt;- tibble::tribble( ~V1, ~V2, ~V3, ~V4, ~V5, ~V6, ~V7, ~V8, ~V9, ~V10, ~V11, ~V12, ~V13, ~V14, 136, 270, 162, 165, 192, 397, 172, 148, 291, 239, 192, 172, 235, 153, 345, 518, 429, 413, 456, 487, 468, 419, 507, 559, 420, 415, 511, 464, 190, 301, 311, 361, 398, 465, 388, 392, 453, 421, 355, 381, 394, 444, 434, 504, 453, 392, 350, 400, 458, 387, 342, 368, 355, 429, 373, 501, 424, 486, 447, 417, 496, 484, 468, 423, 472, 507, 458, 456, 519, 570, 170, 208, 134, 129, 147, 141, 241, 128, 162, 163, 222, 438, 307, 252, 487, 449, 525, 419, 437, 476, 525, 499, 516, 485, 472, 535, 500, 497, 218, 273, 254, 265, 338, 386, 287, 236, 347, 235, 432, 450, 509, 326, 179, 184, 124, 107, 108, 149, 142, 143, 112, 233, 113, 137, 106, 150, 260, 381, 174, 140, 132, 138, 164, 137, 122, 140, 102, 174, 120, 135, 115, 191, 132, 132, 169, 158, 129, 120, 122, 157, 94, 141, 120, 166, 526, 517, 465, 394, 310, 269, 213, 185, 145, 201, 131, 258, 114, 160, 325, 252, 203, 158, 135, 162, 164, 181, 150, 177, 162, 192, 170, 162, 329, 296, 212, 159, 156, 200, 139, 143, 164, 150, 119, 193, 148, 188, 230, 414, 408, 179, 432, 288, 163, 240, 185, 208, 138, 208, 153, 140 ) Notice how R coerces unique variable names for each column. Thats fine, but well need to fix them. 13.2.0.4 Step 4 Convert from a wide to a long format, and check. gluc &lt;- jw7gluc %&gt;% pivot_longer(cols=V1:V14, names_to = &quot;V&quot;, values_to=&quot;glucose&quot;) gluc ## # A tibble: 210 x 2 ## V glucose ## &lt;chr&gt; &lt;dbl&gt; ## 1 V1 136 ## 2 V2 270 ## 3 V3 162 ## 4 V4 165 ## 5 V5 192 ## 6 V6 397 ## 7 V7 172 ## 8 V8 148 ## 9 V9 291 ## 10 V10 239 ## # ... with 200 more rows 13.2.0.5 Step 5 Create variables for ID, day, blood draw and treatment. id &lt;- rep(LETTERS[1:15], each=14) day &lt;- rep(rep(c(1, 3, 5, 7, 9, 11, 12), each=2), 15) draw &lt;- rep(rep(c(&quot;early&quot;, &quot;late&quot;), 7),15) treat &lt;- c(rep(&quot;placebo&quot;, 8*14 ), rep(&quot;rosiglitazone&quot;, 7*14)) 13.2.0.6 Step 6 Add the variables to the long data frame while removing the irrelevant V variable. gluc &lt;- add_column(gluc, id, day, draw, treat, .before=T) %&gt;% select(-one_of(&quot;V&quot;)) gluc ## # A tibble: 210 x 5 ## id day draw treat glucose ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A 1 early placebo 136 ## 2 A 1 late placebo 270 ## 3 A 3 early placebo 162 ## 4 A 3 late placebo 165 ## 5 A 5 early placebo 192 ## 6 A 5 late placebo 397 ## 7 A 7 early placebo 172 ## 8 A 7 late placebo 148 ## 9 A 9 early placebo 291 ## 10 A 9 late placebo 239 ## # ... with 200 more rows 13.2.0.7 Step 7 Convert every variable except for glucose to a factor. cols &lt;- c(&quot;id&quot;, &quot;day&quot;, &quot;draw&quot;, &quot;treat&quot;) gluc[cols] &lt;- lapply(gluc[cols], factor) gluc ## # A tibble: 210 x 5 ## id day draw treat glucose ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 1 early placebo 136 ## 2 A 1 late placebo 270 ## 3 A 3 early placebo 162 ## 4 A 3 late placebo 165 ## 5 A 5 early placebo 192 ## 6 A 5 late placebo 397 ## 7 A 7 early placebo 172 ## 8 A 7 late placebo 148 ## 9 A 9 early placebo 291 ## 10 A 9 late placebo 239 ## # ... with 200 more rows 13.2.0.8 Step 8 Average the early and late blood draws, to get one glucose value per day. gluc &lt;- gluc %&gt;% group_by(id, day, treat) %&gt;% summarise(glucose=mean(glucose)) ## `summarise()` regrouping output by &#39;id&#39;, &#39;day&#39; (override with `.groups` argument) gluc ## # A tibble: 105 x 4 ## # Groups: id, day [105] ## id day treat glucose ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 1 placebo 203 ## 2 A 3 placebo 164. ## 3 A 5 placebo 294. ## 4 A 7 placebo 160 ## 5 A 9 placebo 265 ## 6 A 11 placebo 182 ## 7 A 12 placebo 194 ## 8 B 1 placebo 432. ## 9 B 3 placebo 421 ## 10 B 5 placebo 472. ## # ... with 95 more rows 13.2.0.9 Step 9 Now heres a summary of all the replicate values in tabular form. gluc %&gt;% group_by(day, treat) %&gt;% summarise( n=length(glucose), mean=round(mean(glucose)), sd=round(sd(glucose)), sem=round(sd/sqrt(n)), min=round(min(glucose)), max=round(max(glucose)) ) ## `summarise()` regrouping output by &#39;day&#39; (override with `.groups` argument) ## # A tibble: 14 x 8 ## # Groups: day [7] ## day treat n mean sd sem min max ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 placebo 8 338 128 45 189 469 ## 2 1 rosiglitazone 7 300 120 45 153 522 ## 3 3 placebo 8 330 131 46 132 472 ## 4 3 rosiglitazone 7 213 111 42 116 430 ## 5 5 placebo 8 378 115 41 144 490 ## 6 5 rosiglitazone 7 200 89 34 128 360 ## 7 7 placebo 8 352 132 47 160 512 ## 8 7 rosiglitazone 7 162 30 11 124 202 ## 9 9 placebo 8 379 132 47 162 533 ## 10 9 rosiglitazone 7 162 22 8 131 196 ## 11 11 placebo 8 386 99 35 182 504 ## 12 11 rosiglitazone 7 154 29 11 118 194 ## 13 12 placebo 8 410 117 41 194 544 ## 14 12 rosiglitazone 7 145 17 6 128 168 Woot! Well visualize at the end. 13.3 Munge the bodyweight data The goal here is to create a data frame of the body weight data that is symmetric to the glucose dataframe created above. Thats because the ultimate goal is to join the two together into a single dataframe. The major difference between the two sheets in the excel file is the body weights are measured daily rather than every other day as for glucose. Well therefore toss out some data. Sad. 13.3.0.1 Step 1 From the BodyWeightsGroups sheet we copy cells F15:T45 to the clipboard. Name an object in the code junk. Then on the Addins drop down menu select Paste as tribble jw7bw &lt;- tibble::tribble( ~V1, ~V2, ~V3, ~V4, ~V5, ~V6, ~V7, ~V8, ~V9, ~V10, ~V11, ~V12, ~V13, ~V14, ~V15, 31.9, 32.1, 32.8, 33, 33.3, 33.2, 33.2, 32.8, 33.5, 34, 34.2, 34.9, 35.2, 35.8, 36.1, 38.1, 38.4, 38.7, 38.5, 38.8, 38.8, 38.9, 38.6, 39.4, 39.4, 39, 39.4, 39.7, 39.3, 39.1, 31, 31.7, 31.9, 32, 32.9, 33.1, 33.5, 33.6, 34.3, 34.4, 34.7, 35.5, 35.4, 35.5, 35.4, 36.4, 35.9, 36.8, 36.9, 37.3, 37.1, 37.4, 37.4, 36.8, 37.4, 37.1, 38, 37.4, 38, 37.9, 38.9, 38.5, 39.2, 39.1, 39.8, 39.3, 39.3, 39.5, 39.7, 40.1, 40.3, 41, 40.9, 41.4, 41.4, 33.8, 34.2, 34.2, 33.9, 34.7, 35, 35.3, 35.3, 35.8, 36.1, 36.7, 37.3, 37.5, 38.4, 38.7, 34.6, 35, 35, 35.1, 35.6, 35.5, 35.7, 36.1, 35.9, 35.9, 35.5, 35.3, 35.2, 35, 34.6, 33.4, 33.6, 34.2, 33.8, 34.3, 34.6, 34.7, 34.9, 34.9, 35.4, 35.6, 35.8, 36.1, 35.9, 35.8, 31.9, 31.9, 32.6, 33.2, 34.3, 34.7, 35.3, 35.1, 35.4, 35.6, 36, 36.9, 37, 38.7, 38.8, 33.4, 34.1, 35.2, 35.8, 36.7, 37.4, 38.2, 38.7, 39.5, 40.1, 40.2, 40.6, 41, 41.7, 42, 32.8, 33.7, 34.5, 34.9, 35.5, 36, 36, 36.2, 36, 36.9, 36.9, 37.6, 38.1, 39.2, 39.5, 37.9, 38.8, 39.9, 40.4, 41.5, 42.5, 43.3, 43.8, 44.5, 44.7, 45.1, 45.7, 46.3, 47.1, 47.1, 35.9, 37.2, 38, 38.6, 39.4, 40, 40.5, 41, 41.2, 42.3, 43.2, 43.9, 44.2, 44.3, 44.9, 35.2, 36.5, 37.4, 38, 39, 39.5, 40.4, 40.7, 40.9, 41.9, 42.5, 43.5, 44, 44.1, 44.7, 35.8, 36.6, 37.3, 37.7, 38.4, 38.4, 39.3, 40.1, 40.9, 41.4, 41.7, 42.3, 42.2, 42.9, 43.3 ) 13.3.0.2 Step 2 Select only the days that correspond to the glucose measurement days. bw &lt;- jw7bw %&gt;% select(c(V1, V3, V5, V7, V9, V11, V12)) bw ## # A tibble: 15 x 7 ## V1 V3 V5 V7 V9 V11 V12 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31.9 32.8 33.3 33.2 33.5 34.2 34.9 ## 2 38.1 38.7 38.8 38.9 39.4 39 39.4 ## 3 31 31.9 32.9 33.5 34.3 34.7 35.5 ## 4 36.4 36.8 37.3 37.4 36.8 37.1 38 ## 5 38.9 39.2 39.8 39.3 39.7 40.3 41 ## 6 33.8 34.2 34.7 35.3 35.8 36.7 37.3 ## 7 34.6 35 35.6 35.7 35.9 35.5 35.3 ## 8 33.4 34.2 34.3 34.7 34.9 35.6 35.8 ## 9 31.9 32.6 34.3 35.3 35.4 36 36.9 ## 10 33.4 35.2 36.7 38.2 39.5 40.2 40.6 ## 11 32.8 34.5 35.5 36 36 36.9 37.6 ## 12 37.9 39.9 41.5 43.3 44.5 45.1 45.7 ## 13 35.9 38 39.4 40.5 41.2 43.2 43.9 ## 14 35.2 37.4 39 40.4 40.9 42.5 43.5 ## 15 35.8 37.3 38.4 39.3 40.9 41.7 42.3 13.3.0.3 Step 3 Change from wide to long format and check. bw &lt;- bw %&gt;% pivot_longer(cols=V1:V12, names_to=&quot;V&quot;, values_to=&quot;weight&quot;) bw ## # A tibble: 105 x 2 ## V weight ## &lt;chr&gt; &lt;dbl&gt; ## 1 V1 31.9 ## 2 V3 32.8 ## 3 V5 33.3 ## 4 V7 33.2 ## 5 V9 33.5 ## 6 V11 34.2 ## 7 V12 34.9 ## 8 V1 38.1 ## 9 V3 38.7 ## 10 V5 38.8 ## # ... with 95 more rows 13.3.0.4 Step 4 Add variables for id, day and treatment. id &lt;- rep(LETTERS[1:15], each=7) day &lt;- rep(c(1, 3, 5, 7, 9, 11, 12), 15) treat &lt;- c(rep(&quot;placebo&quot;, 8*7), rep(&quot;rosiglitazone&quot;, 7*7)) bw &lt;- add_column(bw, id, day, treat, .before=T) %&gt;% select(-one_of(&quot;V&quot;)) bw ## # A tibble: 105 x 4 ## id day treat weight ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A 1 placebo 31.9 ## 2 A 3 placebo 32.8 ## 3 A 5 placebo 33.3 ## 4 A 7 placebo 33.2 ## 5 A 9 placebo 33.5 ## 6 A 11 placebo 34.2 ## 7 A 12 placebo 34.9 ## 8 B 1 placebo 38.1 ## 9 B 3 placebo 38.7 ## 10 B 5 placebo 38.8 ## # ... with 95 more rows 13.3.0.5 Step 5 Factorize each of the variables except for weight. cols &lt;- c(&quot;id&quot;, &quot;day&quot;, &quot;treat&quot;) bw[cols] &lt;- lapply(bw[cols], factor) bw ## # A tibble: 105 x 4 ## id day treat weight ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 1 placebo 31.9 ## 2 A 3 placebo 32.8 ## 3 A 5 placebo 33.3 ## 4 A 7 placebo 33.2 ## 5 A 9 placebo 33.5 ## 6 A 11 placebo 34.2 ## 7 A 12 placebo 34.9 ## 8 B 1 placebo 38.1 ## 9 B 3 placebo 38.7 ## 10 B 5 placebo 38.8 ## # ... with 95 more rows 13.3.0.6 Step 6 Calculate some descriptive statistics. bw %&gt;% group_by(day, treat) %&gt;% summarise( n=length(weight), mean=mean(weight), sd=sd(weight), sem=sd/sqrt(n), min=min(weight), max=max(weight) ) ## `summarise()` regrouping output by &#39;day&#39; (override with `.groups` argument) ## # A tibble: 14 x 8 ## # Groups: day [7] ## day treat n mean sd sem min max ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 placebo 8 34.8 2.83 1.00 31 38.9 ## 2 1 rosiglitazone 7 34.7 2.09 0.791 31.9 37.9 ## 3 3 placebo 8 35.4 2.65 0.938 31.9 39.2 ## 4 3 rosiglitazone 7 36.4 2.45 0.927 32.6 39.9 ## 5 5 placebo 8 35.8 2.55 0.900 32.9 39.8 ## 6 5 rosiglitazone 7 37.8 2.48 0.936 34.3 41.5 ## 7 7 placebo 8 36 2.32 0.820 33.2 39.3 ## 8 7 rosiglitazone 7 39 2.77 1.05 35.3 43.3 ## 9 9 placebo 8 36.3 2.26 0.798 33.5 39.7 ## 10 9 rosiglitazone 7 39.8 3.17 1.20 35.4 44.5 ## 11 11 placebo 8 36.6 2.11 0.747 34.2 40.3 ## 12 11 rosiglitazone 7 40.8 3.33 1.26 36 45.1 ## 13 12 placebo 8 37.2 2.19 0.775 34.9 41 ## 14 12 rosiglitazone 7 41.5 3.30 1.25 36.9 45.7 13.4 Merge and explore the glucose and bw data Joining the two data frames is so simple it is almost stupid easy. jw7 &lt;- left_join(bw, gluc) ## Joining, by = c(&quot;id&quot;, &quot;day&quot;, &quot;treat&quot;) jw7 ## # A tibble: 105 x 5 ## id day treat weight glucose ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 placebo 31.9 203 ## 2 A 3 placebo 32.8 164. ## 3 A 5 placebo 33.3 294. ## 4 A 7 placebo 33.2 160 ## 5 A 9 placebo 33.5 265 ## 6 A 11 placebo 34.2 182 ## 7 A 12 placebo 34.9 194 ## 8 B 1 placebo 38.1 432. ## 9 B 3 placebo 38.7 421 ## 10 B 5 placebo 38.8 472. ## # ... with 95 more rows And now for the visualizations. p1 &lt;- ggplot(jw7, aes(day, glucose, color=treat, group=id))+ geom_point()+geom_line()+ theme_half_open(12) + theme(plot.margin = margin(6, 0, 6, 0), legend.position=&quot;top&quot;)+ scale_color_viridis_d(begin=0, end=0.8) p2 &lt;- ggplot(jw7, aes(day, weight, color=treat, group=id))+ geom_point()+geom_line()+ theme_half_open(12) + theme(plot.margin = margin(6, 0, 6, 0), legend.position=&quot;top&quot;)+ scale_color_viridis_d(begin=0, end=0.8) plot_grid(p1,p2) Figure 13.1: Spaghetti plots of jaxwest7 data. p3 &lt;- ggplot(jw7)+ geom_histogram(aes(glucose, fill=treat))+ theme_half_open(12) + theme(plot.margin = margin(6, 0, 6, 0), legend.position=&quot;top&quot;)+ scale_fill_viridis_d(begin=0, end=0.8) p4 &lt;- ggplot(jw7)+ geom_histogram(aes(weight, fill=treat))+ theme_half_open(12) + theme(plot.margin = margin(6, 0, 6, 0), legend.position=&quot;top&quot;)+ scale_fill_viridis_d(begin=0, end=0.8) plot_grid(p3,p4) Figure 13.2: Histograms of the jaxwest7 data. p5 &lt;- ggplot(jw7, aes(day, glucose,color=treat)) + stat_summary(fun.data = &quot;mean_sdl&quot;, fun.args = list(mult = 1), geom =&quot;pointrange&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group=treat) ) + theme_half_open(12) + theme(plot.margin = margin(6, 0, 6, 0), legend.position=&quot;top&quot;)+ scale_color_viridis_d(begin=0, end=0.8) ## Warning: `fun.y` is deprecated. Use `fun` instead. p6 &lt;- ggplot(jw7, aes(day, weight, color=treat)) + stat_summary(fun.data = &quot;mean_sdl&quot;, fun.args = list(mult = 1), geom =&quot;pointrange&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group=treat) ) + theme_half_open(12) + theme(plot.margin = margin(6, 0, 6, 0), legend.position=&quot;top&quot;)+ scale_color_viridis_d(begin=0, end=0.8) ## Warning: `fun.y` is deprecated. Use `fun` instead. plot_grid(p5,p6) ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function Figure 13.3: Group means and std devs of the jaxwest7 glucose and body weight data. 13.5 Summary It is important to have a final product in mind before starting a munge. In this case, the goal was to create a dataframe for multivariate statistical analysis. These analyses need for each dependent variable to share a common set of independent variables. That product is the jw7 object above. * Many data sets are like the Jaxwest7, with plenty of good information but unstructured. * datapasta is your friend. * Every munge is a custom munge. "],["binomial.html", "Chapter 14 The Binomial Distribution 14.1 dbinom 14.2 pbinom 14.3 qbinom 14.4 rbinom", " Chapter 14 The Binomial Distribution library(ggplot2) The binomial probability distribution models the discrete outcomes of dichotomous processes. In other words, events that can be categorized as either a successes or as a failure. 14.1 dbinom The binomial probability mass function in R is dbinom. Rs dbinom function returns p(x), a probability value produced by the binomial probability mass function: \\[p(x)={n\\choose x}(p)^x(1-p)^{(n-x)}\\] Where \\(n\\) is the number of trials, \\(x\\) is the value of the number of successes, \\(p\\) is the probability of a single success, and therefore \\(1-p\\) is the probability of a single failure. The coin toss serves as the classic explainer for binomial events. A fair coin can land as either heads, or tails with equal probabilities. Unless youre Patriots quarterback Tom Brady, for whom it always lands as heads. If Matt Ryan tossed a coin 10 times, whats the probability of him getting EXACTLY 7 heads? dbinom(x=7, size=10, prob=0.5) ## [1] 0.1171875 Thats almost a 12% chance! Eight heads would be even more unlikely. And so on. Female mice enter estrus one out of five days. This implies that if a female mouse is mated on any random day, the probability of any single mating resulting in pregnancy is 0.2. On a given day, if you set up 12 female mize for mating, whats the probability that exactly half of them would become pregnant? dbinom(x=6, size=12, prob=0.2) ## [1] 0.01550215 Theres only a 1.55% chance of getting EXACTLY 6 dams out of that mating set up. The script below illustrates the probabilities over a full range of possible pregnancy outcomes, for a trial of size 12 (ie, 12 matings set up) x &lt;- 1:10 size &lt;- 12 prob &lt;- 0.2 df &lt;- data.frame(x, px=dbinom(x, size, prob)) ggplot(df, aes(x, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) Its evident that binomial distributions where the probabilities of successes and failures are uneven are skewed. The only way to make these appear more normally distributed is to have equal probabilities for successes and failures. 14.2 pbinom Rs pbinom is the cumulative probability distribution function for the binomial. \\[p(x)={\\sum_{i=0}^{x}}{n\\choose i}(p)^i(1-p)^{(n-i)}\\] This function returns the cumulative probability value for a number of successes in n trials. This can be a very useful value to model. For example, if you set up 12 matings of mice, where each had a 0.2 probability of pregnancy, what is the probability that you would have up to 6 pregnant dams? pbinom(6, 12, 0.2, lower.tail=T) ## [1] 0.9960969 Theres is a very high probability of getting UP TO 6 pregnancies from 12 matings! If we turn the lower.tail argument from TRUE to FALSE the pbinom returns a p-value like probability. Whats the probability of getting 6 or more pregnancies from 12 matings where the probability of a single pregnancy is 0.2? pbinom(6, 12, 0.2, lower.tail=F) ## [1] 0.003903132 Thats about 0.39%! Which would be a very rare outcome from the mating trial, indeed! Maybe even scientifically significant were it to occur. Perhaps its useful to visualize both the upper and lower tails of this cumulative function: q &lt;- 1:10 size &lt;- 12 prob &lt;- 0.2 df &lt;- data.frame(q, px=pbinom(q, size, prob)) ggplot(df, aes(q, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;pbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;lower.tail=TRUE&quot;,&quot;)&quot;)) df &lt;- data.frame(q, px=pbinom(q, size, prob, lower.tail=F)) ggplot(df, aes(q, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;pbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;lower.tail=FALSE&quot;,&quot;)&quot;)) 14.3 qbinom The quantile binomial distribution function in R is qbinom. qbinom is the inverse of the pbinom function. This predicts the number of successes that might occur given a percentile of the distribution. Assuming 12 matings are set up, where the probability of any one pregnancy success is 0.2, what number of pregnancies would be expected if the group performed at the 90th percentile? qbinom(p=0.90, size=12, prob=0.2, lower.tail=T) ## [1] 4 Thats only 4 litters. That should make sense, since only 1 in 5 would be pregnant on average. To out perform this expectation at the 90th percentile is still not a very large numer The graph below illustrates this. Notice the step-wise distribution, which is diagnostic of discrete functions. #define variables p &lt;- seq(0, .99, 0.03) #cumulative probability quantiles size &lt;- 12 #number of trials prob &lt;- 0.2 #probability of success of one trial df &lt;- data.frame(p, q=qbinom(p, size, prob)) ggplot(df, aes(p, q)) + geom_col(fill=&quot;blue&quot;) + xlab(&quot;p(q)&quot;) + ylab(&quot;q&quot;) + labs(title = paste(&quot;qbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) 14.4 rbinom The rbinom function is for random simulation of n binomial trials of a given size and event probability. The output is the number of successful events per trial. Lets simulate 12 matings 12 times, as if we do one a mating involving 12 females, once per month. How many successes will we see per month? The output below represents the number of litters we would produce on each of those months. The point is, we dont get the average every month. Some times its more successes, others its fewer. Models are perfect, data are not. rbinom(n=12, size=12, prob=0.2) ## [1] 1 4 3 2 3 3 0 5 1 1 3 4 Heres a histogram from a very large number of simulations of the same scenario. You can clearly see the binomial distribution for this trial size and probability of success is skewed. You can also clearly see the average of the distributionwhich is somewhere between 2 and 3. n &lt;- 10000 #number of simulations size &lt;- 12 #number of trials prob &lt;- 0.2 #probability of success of one trial df &lt;- data.frame(x=rbinom(n, size, prob)) #x=number of successful trials ggplot(data=df, aes(df$x)) + stat_count(fill=&quot;blue&quot;) + xlab(&quot;x&quot;)+ ylab(&quot;count&quot;)+ labs(title = paste(&quot;rbinom&quot;,&quot;(&quot;,&quot;number=&quot;,n,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) ## Warning: Use of `df$x` is discouraged. Use `x` instead. "],["poisson.html", "Chapter 15 The Poisson Distribution 15.1 Poisson Events 15.2 dpois 15.3 ppois 15.4 rpois 15.5 Overdispersion", " Chapter 15 The Poisson Distribution library(tidyverse) 15.1 Poisson Events Counts of random, discrete events that occur in blocks of time or space are said to have the property of frequency. They can be modeled by the Poisson distribution. The values of these events are always integers. For example, the number of times a neuron depolarizes over a fixed period of time would be frequency. The number of cells on which an antigen can be detected that are in a fixed volume of fluid would also be a frequency. The number of facebook friends a random biostats student has would be a frequency. 15.2 dpois dpois is the Poisson probability mass function in R: \\(p(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) dpois takes as arguments i) the scalar \\(x\\), and ii) lambda, an average or expectation of the distribution, and returns the value of the probability, or otherwise known as the probability mass, for that scalar. \\(x\\) can be either a single value, or a vector comprised of many values. We use the latter, conveniently, to produce nice graphs. For example, assume a neuron, on average, can be expected to depolarize spontaneously 8 times per second. What is the probability it would only depolarize half that number of times? Use dpois to calculate the probability that a random number of events would occur in a time or space, given some expected average frequency. dpois(x=4, lambda=8) ## [1] 0.05725229 Therefore, the probability that a randomly selected neuron would depolarize exactly 4 times per second is 5.72%. The probability of some frequency we might expect to see is sometimes useful to calculate. Whats most notable about the the dpois is how it loses symmetry and becomes more skewed as its average (lambda) gets lower. x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=dpois(x, lambda)) ggplot(df, aes(x=x,y=px)) + geom_col(fill = &quot;red&quot;) + xlab(&quot;x&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dpois&quot;,&quot;(&quot;,&quot;lambda=&quot;, lambda,&quot;)&quot;)) 15.3 ppois Rs ppois function is the Poisson cumulative mass function \\[p(x)=\\sum_{i=0}^{x} \\frac{e^{-\\lambda}\\lambda^i}{i!}\\] This calculates a cumulative probability value for a certain frequency, given the average frequency of the distribution. Lets say, for example, that a neuron depolarizes on average 8 times per second. If you took a random measure of depolarization activity, what is the probability that youd observe a frequency as high as 4 depolarizations per second? ppois(4, 8, lower.tail=T) ## [1] 0.0996324 The value of almost 10% is higher than what we determined using the dpois because the ppois is a cumulative function! What is the probability that youd observe a frequency of 16 or more depolarizations per second? To answer this question we have to reverse the functions default lower.tail argument. As you might suspect, when set with the lower.tail=FALSE argument, as below, the ppois function returns a p-value. ppois(16, 8, lower.tail=F) ## [1] 0.003718021 Thus, the probability of observing a frequency twice as high or higher than the average for this distribution is quite low, at about 0.37%! Heres the cumulative Poisson distribution for a phenomenon that has an average of 8, over a range of frequency values: x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=ppois(x, lambda, lower.tail=T)) ggplot(df, aes(x, px)) + geom_col(fill=&quot;red&quot;) Its probably worth reversing the lower.tail argument to visualize the distribution of p-values for a Poisson x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=ppois(x, lambda, lower.tail=F)) ggplot(df, aes(x, px)) + geom_col(fill=&quot;red&quot;) ## qpois The qpoisfunction is the inverse of the cumulative Poisson mass function. It takes a probability as an argument and returns a frequency value. What depolarization frequency can we expect as the 90th percentile for a neuron that has an average frequency of 8 depolarizations per second? qpois(0.9, 8) ## [1] 12 Visualized, notice the stair-step pattern, which is diagnostic of discrete probability distributions p &lt;- seq(0, 1, 0.05) lambda &lt;- 8 df &lt;- data.frame(p, frequency=qpois(p, lambda, lower.tail=T)) ggplot(df, aes(p, frequency)) + geom_col(fill=&quot;red&quot;) 15.4 rpois The rpois function in R is used to generate random Poisson data. It takes arguments of lambda, the average frequency of a population, and the number of random counts to generate. To simulate a sample of 10 measurements of a neuron that on average depolarize 8 times per sec: rpois(10, 8) ## [1] 8 14 7 7 5 6 6 3 9 4 Heres a histogram of a larger sample. Notice how it isnt as perfect as the Poisson distribution would suggest. Thats because models are perfect, but samples are not. Even computer-generated samples! Also notice the low but detectable frequency of extreme values..frequencies higher than 20, by random chance. df &lt;- data.frame(s=rpois(10000, 8)) ggplot(df, aes(s)) + geom_histogram(binwidth=1, color=&quot;red&quot;) 15.5 Overdispersion Its very common to observe discrete variables that disobey the Poisson. In other words, these are systems that are discrete counts in time or space, and thus whose very nature is frequency. But yet they are poorly fit by the Poisson distribution model. For example, the two graphs below. The first is a distribution of the counts of facebook friends of biostats students. Their average number of friends is 610. The second is based upon a Poisson distribution whose lambda is 610. The dpois model obviously is a poor fit for the data. Models are perfect, data are not. The friends distribution is said to be over-dispersed. The reason for this overdispersion is likely that it is a more complex system than what a Poisson would assume. A key assumption for a Poisson model is that the counts occur randomly. It seems quite likely that biostats students probably dont choose friends randomly. survey &lt;- read_csv( &quot;datasets/precourse.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## term = col_double(), ## enthusiasm = col_double(), ## height = col_double(), ## friends = col_number(), ## sex = col_character(), ## fav_song = col_double() ## ) ggplot(survey, aes(x=friends)) + geom_histogram(binwidth=100) ## Warning: Removed 1 rows containing non-finite values (stat_bin). x &lt;- c(0:5000) lambda &lt;- 500 df &lt;- data.frame(x, px=dpois(x, lambda)) ggplot(df, aes(x=x,y=px)) + geom_col(fill = &quot;red&quot;) + xlab(&quot;x&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dpois&quot;,&quot;(&quot;,&quot;lambda=&quot;, lambda,&quot;)&quot;)) "],["normal.html", "Chapter 16 The Normal Distribution 16.1 dnorm 16.2 pnorm 16.3 qnorm 16.4 rnorm", " Chapter 16 The Normal Distribution 16.0.1 The Standard Normal If the variable \\(X\\) represents a population of normally distributed values with a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the variable \\(Z\\) \\[Z=\\frac{X-\\mu}{\\sigma}\\] has a standard normal distribution with a mean of 0 and a standard deviation of 1. Converting experimental sample data to standard normal z-scores is very common. Assume we have a sample of 210 peoples heights, whose sample mean = 169.9 and sd = 10.5. The tallest height in the sample is 205. The z-score of the tallest individual is derived using the sample parameters and is (205-169.9)/10.5 = 3.34. The z-score could be interpreted this way: the tallest individual is said to be 3.34 standard deviations taller than the mean height of the students in the sample. 16.1 dnorm dnorm is the normal probability density function in R: \\(p(x)=\\frac{e^\\frac{-(x-\\mu)^2}{2\\sigma^2}}{\\sigma\\sqrt{2\\pi}}\\) dnorm takes i) the scalar \\(x\\), ii) a mean and iii) sd as arguments and returns the value of the probability, or otherwise known as the probability density, for that scalar. \\(x\\) can be either a single value, or a vector comprised of many values. For example, assume the average height of an adult in the US is 168.8 cm with a standard deviation of 7.1 cm. Use dnorm to calculate the probability that a randomly selected US adult will be 176 cm tall: dnorm(x=176, mean=168.8, sd=7.1 ) ## [1] 0.03360041 Therefore, the probability that a randomly selected US adult would be 176 cm tall is 3.36%. The probable height of one specfic individual is usually not very useful or interesting to calculate. Whats a bit more useful is to see how the function operates over a range of values. For example, we can model the distribution of adult heights using dnorm. We might be interested in the fraction of adults whose heights are 176cm and taller compared to the rest of the population. To illustrate the answer with a plot, we pass a range of x values range = c(130, 210) into the dnorm function, rather than a single value. dnorm calculates probabilities over that full range. We can make a plot to visualize the answer, shading anybody as tall or taller than 176 in blue: range &lt;- c(130,210) pop &lt;- list(mean=168.8, sd=7.1) auc &lt;- c(176,210) ggplot(data.frame(x=range), aes(x)) + stat_function(fun=dnorm, args = pop, color = &quot;red&quot;) + stat_function(fun=dnorm, args = pop, xlim = auc, geom=&quot;area&quot;, fill=&quot;blue&quot;) + xlab(&quot;heights, cm&quot;) + ylab(&quot;p(heights)&quot;) Later on, well use other functions to quantify the blue shaded area. Heres plot of the standard normal distribution over a range of z-values that are 4 SDs below and above the mean: z &lt;- seq(-4,4,0.1) mean &lt;- 0 sd &lt;- 1 df &lt;- data.frame(z, pz=dnorm(z, mean, sd)) ggplot(df, aes(x=z,y=pz)) + geom_line(color = &quot;red&quot;) + xlab(&quot;z&quot;) + ylab(&quot;p(z)&quot;) + labs(title = paste(&quot;dnorm&quot;,&quot;(&quot;,&quot;mean=&quot;, mean,&quot;,&quot;,&quot;sd=&quot;,sd,&quot;)&quot;)) 16.2 pnorm When a height value is given, calculating the probabilities for heights up to or greater than than that limit can be of considerable interest. pnorm is the R function for thatgive it the value of a normally-distributed variable, such as height, and it returns a cumulative probability for the distribution on either side of that value. Thus, pnorm is called the normal cumulative distribution function in R: \\[p(x)=\\int^x_{-\\inf}\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi}}\\] By default pnorm will return the cumulative value of the normal pdf up to the value of the input scalar. In otherwords, given the value of a variable, pnorm returns the probability of that value or less. However, this can be reversed by changing to the lower.tail=FALSE argument. In this case, pnorm returns something akin to a p-value. Given some value of a variable, pnorm returns the probability of that value or greater. The first line in the script below calculates the probability that a US adult will be less than or equal to 175.9 tall, which in this instance turns out to be 1 sd taller than the mean height. It returns a value of about 84%. The second line calculates the probability that a US adult will be greater than or equal to 175.9 cm tall. It returns a value of about 16%. pnorm(175.9, 168.8, 7.1, lower.tail = T) ## [1] 0.8413447 pnorm(175.9, 168.8, 7.1, lower.tail = F) ## [1] 0.1586553 Subtracting the latter from the form illustrates that about 68% of US adults will be within 1 standard deviation of the average US adult height: pnorm(175.9, 168.8, 7.1, lower.tail = T) - pnorm(175.9, 168.8, 7.1, lower.tail = F) ## [1] 0.6826895 About 95% of the values for any continuous, normally-distributed variable will be between 2 standard deviations of the mean: pnorm(q=2, mean=0, sd=1, lower.tail=T) ## [1] 0.9772499 pnorm(2, 0, 1, lower.tail=F) ## [1] 0.02275013 pnorm(2) - pnorm(2, lower.tail=F) ## [1] 0.9544997 (The script above illustrates a few of the different shorthands that can be taken working with Rs probability functions). Calculating p-values\" using pnorm Lets go back to human heights. Whats the probability of an US adult being as tall or taller than 205 cm? Notice the lower.tail=F argument: pnorm(205, 168.8, 7.1, lower.tail=F) ## [1] 1.71095e-07 Thats a very low probability value, because the height is so extreme. Calculating percentiles using pnorm What is the height percentile of a 205 cm tall US adult? pnorm(205, 168.8, 7.1)*100 ## [1] 99.99998 16.3 qnorm qnorm is the inverse of the cumulative distribution function of a continuous normally-distributed variable. By default, qnorm takes a cumulative probability value (eg, a percentile) as an argument (along with the mean and sd of the variable) and returns a limit value for that continuous random variable. Heres what the qnorm distribution looks like: p &lt;- seq(0.0, 1.0, 0.01) mean &lt;- 0 sd &lt;- 1 df &lt;- data.frame(p, z=qnorm(p, mean, sd)) ggplot(df, aes(p, z)) + geom_line(color = &quot;red&quot;) + xlab(&quot;p(z)&quot;) + ylab(&quot;z&quot;) + labs(title = paste(&quot;qnorm&quot;,&quot;(&quot;,&quot;mean=&quot;, mean,&quot;,&quot;,&quot;sd=&quot;,sd,&quot;)&quot;)) Youre probably very familiar with quantiles since percentiles are a class of quantiles. For example, if your standardized exam score is in the 90th percentile, then you did as well or better than 90% of test takers. If you forgot your score, but remember your percentile, you could use qnorm to return your specific test scoreso long as you also know the mean and sd values of the test scores. Back to heights. Whats the height in cm of a US adult who is at the 99th percentile? What about the 5th percentile? qnorm(0.99, 168.8, 7.1) ## [1] 185.3171 qnorm(0.05, 168.8, 7.1) ## [1] 157.1215 The first script below returns the height of the 84th% quantile of US adults. 84% are about 175.9 cm tall or less. The 84th is the upper 1 SD quantile. The second script returns the height of the complement of the 84th quantile of US adults, by switching the default lower.tail argument. This complement is the lower 1 SD quantile. qnorm(0.84, 168.8, 7.1, lower.tail = T) ## [1] 175.8607 qnorm(0.84, 168.8, 7.1, lower.tail = F) ## [1] 161.7393 About two thirds of US adults are between 161.7 and 175.9 cm tall. 16.3.0.0.1 Confidence interval limits and qnorm The qnorm distributon has pragmatic utility for finding the limits for confidence intervals when using the normal distribution as a model for the data. In a standard normal distribution, the limits for the lower and upper 2.5% of the distribution are about \\(\\pm\\) 1.96 standard deviation units. Thus, the 95% confidence interval for standard normal z-values is -1.96 to 1.96. qnorm(0.025) ## [1] -1.959964 qnorm(0.025, lower.tail=F) ## [1] 1.959964 Based upon the average height and sd of US adults, the 95% confidence interval for US adult height is: paste(round(qnorm(0.025, 168.8, 7.1),1), &quot;to&quot;, round(qnorm(0.025, 168.8, 7.1, lower.tail=F),1)) ## [1] &quot;154.9 to 182.7&quot; The confidence interval means there is a 95% chance the true average US adult is within that range. Rember that height sample from near the very start of this document? It had 210 replicates, a mean = 169.9, sd = 10.5. What is the 95% confidence interval of US adult heights, based upon that sample? paste(round(qnorm(0.025, 169.9, 10.5),1), &quot;to&quot;, round(qnorm(0.025, 169.9, 10.5, lower.tail=F),1)) ## [1] &quot;149.3 to 190.5&quot; Drawing inference to the whole population on the basis of this sample, and assuming a normal model, there is a 95% chance the true US adult height is within this slightly wider range. The truth is, a better model to derive confidence intervals for normally-distributed samples is the t-distribution. 16.4 rnorm Rs rnorm is the random number generator function for the normal distribution. This has a lot of utlity in synthesizing data sets, for example, when running simulations. It returns random normally distributed values given size, mean and standard deviation arguments. For example, here is a randomly generated sample of 10 US adult heights, rounded to 1 significant digit. Well use this function a LOT in this course to generate normally-distributed data for various purposes. Heres 10 random, simulated heights of US adults, rounded to the first digit to make it cleaner looking: round(rnorm(10, 168.8, 7.1), 1) ## [1] 158.3 171.9 181.6 184.0 168.1 161.9 165.3 165.0 176.8 174.1 16.4.1 Plotting histograms of some rnorm samples Histograms are a powerful way to explore the underlying structure in a dataset. They are a descriptive tool. Adjusting the bin parameters provides a way to evaluate the data. A histogram takes one variable, plotting its values on the x-axis while displaying the counts or the density of those values on the y-axis. The distribution of the data in a histogram is controlled by adjusting the number of bins, or by adjusting the binwidth. 16.4.2 Bins and Binwidth Think of bins as compartments. If using, for example 100 bins as a plotting argument, the range of the sample values are split, from the shortest to the tallest heights, into 100 evenly spacedcompartments. For normally distributed variables, the central bins are more dense or have more counts then those on the tails of the histogram, because values near the average are more common. A binwidth argument can be used instead of bin. Setting the binwidth to 10 on the graph below would look the same as setting the number of bins to 6. Adjusting bins or binwidths has the effect of rescaling the y-axis. 16.4.2.1 Histograms of counts vs density Because histograms are so commonggplot2 has a geom_histogram function to simplify creating the plots. The default argument creates a histogram of values of a random variable of interest. To create a histogram of densities, use a density aes argument in the function, instead as youll see below. Counts are the number of cases within a given bin. Density is the fraction of all counts within a given bin. Here are 1000 random, simulated heights, plotted as a histogram of counts. set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=168.8, sd=7.1)) ggplot(df, aes(s)) + geom_histogram(bins=100) Same thing, plotted as a probability density: set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=168.8, sd=7.1)) ggplot(df, aes(s)) + geom_histogram(aes(y = ..density..), bins=100) 16.4.2.2 Histogram with Distribution For an example of plotting a model along with the raw data, heres a density histogram of the rnorm sample, plotted along with a dnorm distribution model, given the population mean and sd. By eyeball, the model seems to be a decent fit for the sample. But it is not a perfect fit. Its not the models fault. Models are perfect, data are not. How does the sample size affect how well the model fits the data? set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=0, sd=1)) ggplot(df, aes(s)) + geom_histogram(aes(y = ..density..), bins=100) + stat_function(fun = dnorm, args=list(mean=0, sd=1), color = &quot;red&quot;, size = 2) "],["categorical.html", "Chapter 17 Statistics for Categorical Data 17.1 Types of categorical data 17.2 Exact v Asymptotic Calculations of p-values 17.3 Overview of the types of hypothesis testing 17.4 Comparing proportions 17.5 Exact tests for two proportions 17.6 Goodness of fit Tests 17.7 Contingency Testing 17.8 Power analysis for proportion tests 17.9 Power analysis functions for proportion tests 17.10 Graphing Proportions 17.11 Key Takeaways", " Chapter 17 Statistics for Categorical Data library(PropCIs) library(tidyverse) library(knitr) library(kableExtra) library(binom) library(pwr) library(statmod) library(EMT) Biomedical research is full of studies that count discrete events and sort them into nominal categories. A common mistake made by many researchers is to use statistics designed for measured (continuous) variables on discrete count variables. For example, they transform count data into scalar measures (eg, percents, folds etc) and then apply statistics designed for continuous variables to events that are fundamentally discrete by nature. The problem with that is there are inherent differences in the behaviors of continuous and discrete variables. Recognize that there are statistical tests that can be used to analyze discrete data directly, without resorting to these transformations. 17.1 Types of categorical data What proportion of cells express a specific antigen and does an experimental treatment cause that proportion to change? What proportion of rats treated with an anxiolytic drug choose one chamber over others in a maze test? How many people who express a certain biomarker go on to have cancer? In these three scenarios the primary data are counts. All of the study results have integer values. The counts are categorized with variable attributes, thus they are called categorical data. In fact, the three scenarios above are very different experimental designs. The first represent experiments that compare simple proportions. The second compare frequencies, and the third is an association study. The analysis of these require using a common suite of statistical tools in slightly different ways. Broadly, all of these tools boil down to dealing with proportions. A few types of proportions (eg, odds ratio, relative risk) that can be calculated from these datasets are sometimes used as effect size parameters. Other times wed use a confidence interval as a way of conveying an effect size. Statistical tests are then used to evaluate whether these effect sizes, or frequencies, or simple proportions, are extreme enough relative to null counterparts so that we can conclude the experimental variable had some sort of effect. 17.1.1 Proportions We might Inactivate a gene hypothesized to function in the reproductive pathway. To test it, mating triangles would be set up to count the number of female mice that become pregnant or not. Implant a tumor in mice, before counting the number of survivors and non-survivors at a given point later. Mutate a protein that we hypothesize moves in and out of an intracellular compartment, before staining cells to count the number of cells where it is and is not located in a compartment of interest. Each of the examples above have binomial outcomespregnant vs not, dead vs alive, or inside vs outside. In each case above, both the successful and the failed events are counted in the experiment. A proportion is a simple ratio of counts of success to counts of failures. 17.1.2 Frequencies Other kinds of counted data occur randomly in space and time. The examples below illustrate this. Note how only the number of events are recorded, rather than categorizing them as successes or failures. These counts therefore have the statistical property of frequency, which are counts of events in space or time. We can Expose a neuron to an excitatory neurotransmitter, then count the number of times it depolarizes. In a model of asthma, count the number of immune cells that are washed out in a pulmonary lavage protocol after treatment with an immunosuppressive agent. The key difference for these compared to binomial events is that their non-event counterparts are meaningless. For example, it is not possible to measure the number of depolarizations that dont occur, or know the number of immune cells that dont wash out in the lavage. Importantly, the dependent variable is simple discrete counts. If we count 555 events in a 5 minute window, the recorded value and units is 555 counts, not 111 counts per minute. 17.1.3 Associations Lastly, the examples below illustrate the design of association studies, which are based upon, according to the null hypothesis, independent predictors and outcomes. Here are some examples of association study designs: You might wish to identify causal alleles associated with a specific disease phenotype by counting the number of people with and without the disease, who have or dont have a particular allelic variant. determine if a history of exposure to certain carcinogens is associated with a higher risk of cancer by counting people with cancer who have been exposed. know if a drug treatment causes a higher than expected frequency of a side-effect by counting the people on the drug with the side-effect. In the simplest (and most general) case, association studies are 2X2 in design: A predictor is either present or absent as the row factor, and an outcome was either a success for a failure as the column factor. Subjects are categorized into groups on the basis of where they fall in the 4 possible combinations that these 2X2s allow for. It should be noted that analysis of higher order association studies is also possible, which can be either symmetric (eg, 3x3) or non-symmetric (eg,) 9X2, 2x3, and so on. 17.1.4 Statistics Covered Here Confidence intervals of proportions One-sample proportions test Two-sample proportions test Goodness of fit tests Tests of associations (aka contingency testing) Power analysis of proportions (including Monte Carlo simulation) Plotting proportions with ggplot2 17.2 Exact v Asymptotic Calculations of p-values The statistical tests for hypotheses on categorical data fall into two broad categories: exact tests (binom.test, fisher.test, multinomial.test) and asymptotic tests (prop.test, chisq.test). Exact tests calculate exact p-values. Thats made possible using factorial math. The prop.test and chisq.test generate asymptotic (aka, approximate) p-values. They calculate a \\(\\chi^2\\) test statistic from the data before mapping it to a \\(\\chi^2\\) probability density function. Because that function is continuous, the p-values it generates are asymptotically-estimated, rather than exactly calculated. That should strike you as odd that a continuous sampling distribution is used for discrete data. The reasons are historical. In the early days, it was harder to calculate exact p-values than \\(\\chi^2\\) values. Tables existed (in text books) with critical values of the \\(\\chi^2\\) and associated p-values. 17.2.1 Choosing exact or asymptotic As a general rule, given the same datasets and arguments, exact and approximate hypothesis test functions will almost always give you p-values that differ, but only slightly. Thats usually not a problem unless youre near a threshold value. Typically, an integrity crisis is evoked when that happens. For example, you dont want to put yourself in a position to run the same data through two tests that generate p-values on either side of a threshold. To ask Which test isright???\" is not the right question. Instead, if you find yourself in such a position ask yourself whether p-hacking is right (it is not). You should use the test you said youd use when you first designed the experiment. And if you didnt pre-planor at least have some idea about where you are goingrecognize that the exact tests are more accurate, even when they give you the answer you dont want. Another issue that arises is how well the tests perform with low count numbers. For example, as a rule of thumb, were advised to avoid using the chisq.test when the data have counts less than 5 in more than 20% of the cells because the accuracy of the chisq.test is less at these low cell counts. Use an exact test instead. 17.3 Overview of the types of hypothesis testing Well go through each below in more detail, emphasizing practical experimental design and interpretation principles. 17.3.1 Proportion analysis You can learn a lot about experimental statistics by thinking about proportions. Proportions are derived from events that can be classified as either successes or failures. Sometimes we want to compare simple proportions to decide if they are the same or not. 17.3.2 Goodness of fit testing We do this when we want to compare the frequency distribution we observe in an experiment to the null expectation for that frequency distribution. 17.3.3 Contingency Analysis Contingency analysis, otherwise known as tests of independence, are very different from goodness-of-fit test and simple proportion tests, in design and in purpose. They allow us to ask if two (or more) variables are associated with each other. Unlike a lot of the statistics well deal with, there is a hint of a predictive element associated with these types of studies because the effect sizes we use to explain their results are related to odds and risk and likelihood. Which is not to say that we couldnt use the same predictive concepts in proportions and goodness of fit testing. Contingency tests are very common in epidemiology and in clinical science. You recognize them by their names as cohort studies, case control studies, and so forth. 17.4 Comparing proportions In their simplest use, the tests here can be used to compare one proportion to another. Is the proportion of successes to failures that results from a treatment different from the proportion that results from control? Well dive into this further below. 17.4.1 A Mouse T Cell Pilot Experiment: The Cytokine-inducible antigen gradstudin Lets imagine a small pilot experiment to see how a cytokine affects T cells. This is a very crude thought experiment designed mostly to illustrate some principles. A cytokine is injected into a single mouse. There is no control injection, just one mouse/one cytokine injection. A time later, blood is harvested from the mouse to measure an antigen on T cells. Lets call the antigen gradstudin. Assume a method exists to detect T cells that express gradstudin and that dont express gradstudin. That method implies some nominal criteria are established to categorize T cells as either expressing gradstudin or not. FACS machines are very useful for this. The machine typically produces continuous fluorescent data, where intensity is proportional to gradstudin levels. But we dont care about the magnitude of the expression level, we just care whether it is there or not. Based upon our scientific expertise, we establish cutoff gating criteria. A cell shows a level of fluorescence above which it is sorted as gradstudin positive. The machine therefore returns simple counts of both gradstudin-positive and gradstudin-negative cells. 17.4.2 Calculating Proportions Heres the data, counts of cells expressing and not expressing gradstudin. It is a very simple dataset: pos &lt;- 5042 neg &lt;- 18492 A proportion is the count of a particular outcome relative to the total number of events. Its customary to use the number of successes as the numerator. Whereas its customary to refer to the total number of events, n, as the trial number rather than as sample size, but they mean the same thing. n &lt;- pos+neg #trial size prop &lt;- pos/n prop ## [1] 0.2142432 17.4.3 What A Proportion Estimates This sample proportion is a descriptive statistic. It serves as a point estimate of the true proportion of the population we sampled. The only way to know the true proportion would be to count every T cell in every drop of the subjects blood, which is not going to happen. This point estimate is statistically valid if our sample meets two conditions. First, that this is a random sample of the T cells in the subjects blood. Second, if we consider every T cell in the sample as statistically independent of every other T cell. We can safely assume those conditions are met. Strictly, as an estimate this proportion only infers the population of blood borne T cells IN THAT ONE subject. We really cant generalize much further than that. Surely T cells are sequestered in other compartments (thymus, nodes, etc) that a blood draw cannot estimate. Which is fine for our purposes now because were trying to keep this simple. 17.4.4 Confidence Intervals of Proportions Confidence intervals (CI) have features of both descriptive and inferential statistics. 17.4.4.1 Definition of a 95% CI The range of proportions within which 95% of the time we would expect the true population proportion. Theres a lot going on there. The value of the proportion we measured in the sample is a mathematical fact that is not in dispute. It is what it is. The question is, what does it represent? Although there might be some error associated with measuring it, our single sample offers no real information about what that error might be. What is unknown is the true proportion of gradstudin+ T cells in the population we sampled. CIs are designed to give us some insights into that unknown. 95% CIs are a range estimate of what that true population proportion might be. CIs are calculated in part upon the quality of the point estimate. In the case of proportions, the quality of the point estimate is driven by the size of the sample, the number of counts that are involved in calculating the proportion. As you might imagine intuitively, the more counts we have in the sample, the more confidence we should have that our proportion provides a good estimate of the populations proportion. 17.4.4.2 Calculating CI with R Their are several ways to calculate the CI for a proportion. Because of that, when publishing CIs of proportions it is important is to state which CI method is used. Is one better than the other? Sometimes, yes. For now, lets not worry about that. Wilsons score interval with continuity correction] is suggested as the most accurate for proportions. Other methods are more commonly used than Wilsons because they gained traction as being easier to compute by hand, and old habits die slowly. Taking the data on cytokine induced gradstudin+ T cells, the chunk below illustrates how to use PropCIs to derive a Wilson score interval-based 95% CI: scoreci(pos, n, conf.level=0.95) ## ## ## ## data: ## ## 95 percent confidence interval: ## 0.2090 0.2195 17.4.4.3 Interpretation of a CI The value of our sample proportion, 0.214, falls within this 95% CI. Thats not a big surprise, given the 95% CI was calculated from our proportion! On the basis of the sample proportion, we can conclude from this CI that there is a 95% confidence the true proportion of gradstudin positive T cells falls within this very narrow range of 0.02090 to 0.2195. 17.4.4.4 Using the CI as a quick test of statistical significance. Lets say, for example, that we have tremendous experience and great scientific reason to expect that only 15% of T cells would be gradstudin-positive. Does our sample proportion differ from that expectation? Since a proportion of 0.15 is not within the 95% CI calculated above, we can conclude that the cytokine-induced sample proportion differs from this expectation at the 5% level of statistical significance. We just did a statistical test, without running any software (sorta) or generating any p-values!! And it is perfectly valid inference. 17.4.5 A One-Sample Proportion Test Now well use prop.test to run a test that generates a p-value to decide if the sample proportion we have above differs from 0.15. If you know something about a one sample t-test, then this should sound familiar to you. 17.4.5.1 Hypothesis Tested in a One-Sample Proportion Test In this test the sample antigen-positive proportion is compared to a theoretical proportion. If the typical proportion of antigen-positive T cells within a blood sample is 15%, is the result after cytokine treatment different from this proportion? Lets say that our scientific hypothesis is that the cytokine induces the antigen on T cells. Since we are predicting an increase, for logical coherence we should establish a one-sided alternative (thus using greater as an argument in prop.test below) as our statistical hypothesis. Our statistical hypothesis is the null. Well decide whether or not to reject the null on the basis of the test results. Philosophically, were using a falsification method. The statistical alternate hypothesis: \\(\\pi&gt;15\\%\\) The statistical null hypothesis: \\(\\pi\\le15\\%\\) Note how logically the two hypotheses are mutually exclusive and comprehensive. We use Greek notation to represent the true population proportion. This reminds us that a statistical hypothesis is an inferential test about the population proportion. Again, there is no question that the sample proportion differs from a proportion of 15%. 21% does not equal 15%. Thats a simple numerical fact. Statistical tests are not necessary to make that assertion. We use a test to draw inference on the composition of all of the T cells in the blood of the subject given this one sample. Thus, the sample p is only a point estimate of a true \\(\\pi\\) (which we notate using Greek letters). The chunk below lays out these arguments using Rs prop.test function: #pos and n in the test arguments are objects that were defined above! prop.test( pos, n, p=0.15, alternative = &quot;greater&quot;, conf.level = 0.95, correct = TRUE ) ## ## 1-sample proportions test with continuity correction ## ## data: pos out of n, null probability 0.15 ## X-squared = 761.29, df = 1, p-value &lt; 2.2e-16 ## alternative hypothesis: true p is greater than 0.15 ## 95 percent confidence interval: ## 0.2098559 1.0000000 ## sample estimates: ## p ## 0.2142432 17.4.5.2 Interpreting one-sample prop.test output Like all statistical tests, this one is evaluated under the assumption that the null hypothesis is true. We use the test outcome to decide whether the null hypothesis should be rejected. The prop.test conducts a chi-square analysis. The value of \\(\\chi^2\\) for this sample is very large, extreme. Read more about the chi-square sampling distribution in Chapter 18 to get a feel for what these values mean. The p-value represents the probability of obtaining a \\(\\chi^2\\) value as larger or larger from this test, if the null is true. If the null hypothesis is true in this case, the probability of a \\(\\chi^2\\) value as large or larger than we obtained is 2.2e-16, which is very, very low. The 95% CI is 0.2098 to 1.0. There is a 95% chance the population proportion is greater than 0.2098. The reason it differs from the Wilsons CI calculated above is that we used greateras a one-sided hypothesis argument in the prop.test. These CIs from one-sided hypothesis tests are not particularly useful when attempting to use the CI as an index of accuracy. But perhaps this oddity helps you understand how CIs are tied to the hypothesis and thus inference. 17.4.5.3 How to write this up The proportion of gradstudin positive T cells after cytokine treatment in the subject differs from an expected value of 0.15 (one-sided one-sample proportions test, p-value=2.2e-16, 95% CI = 0.209 to 1.0) 17.4.6 An exact test for one proportion Rs binom.test function is an exact test for whether a proportion differs from a theoretical expectation. It compares proportions using an entirely different procedure. As a one-proportion test the binom.test gives an exact p-value derived from the binomial distribution, whereas the prop.test gives approximate p-values because it uses the chi-square distribution. That distinction is hard to see with our examples here, but the differences will become more noticeable when analyzing samples with far fewer events. Heres the binomial test run on the sample we ran above in prop.test. binom.test(pos, n, p=0.15, alternative = &quot;greater&quot;) ## ## Exact binomial test ## ## data: pos and n ## number of successes = 5042, number of trials = 23534, p-value &lt; 2.2e-16 ## alternative hypothesis: true probability of success is greater than 0.15 ## 95 percent confidence interval: ## 0.209849 1.000000 ## sample estimates: ## probability of success ## 0.2142432 The results are about the same and the write up would be the same. 17.4.7 Comparing Two Proportions We can stick with the T cell-cytokine-gradstudin scenario, but lets change up the experiment a tad. Lets imagine weve withdrawn a sample of blood from a subject and enriched for T cells. Half of the sample is exposed in vitro to a cytokine for a few hours. The other half is exposed to vehicle as a control. We count the gradstudin-positive and gradstudin-negative T cells in both groups. We now have a predictor group at two levels (treatment = vehicle or cytokine) and an outcome group at two levels (antigen = positive or negative) 17.4.7.1 Hypothesis Tested Lets test the hypothesis that the proportion of positive T cells in the two samples differ. The choice is not to test whether one proportion is greater than the other. We just want to know if they differ. The statistical hypotheses here differs from the one sample hypotheses in two ways. First, notice how were comparing the population proportion of cytokine- to that for vehicle-treatment. Second, were making this a two-tailed (two.sided) test instead of one-tailed (greater). The statistical alternate hypothesis: \\(\\pi_c\\ne\\pi_v\\) The statistical null hypothesis: \\(\\pi_c=\\pi_v\\) A second way of writing these hypotheses to say the same thing: Alternate: \\(\\pi_c-\\pi_v\\ne0\\) Null: \\(\\pi_c-\\pi_v=0\\) 17.4.7.2 Running the test Lets say that here are the outcome data: The data can be passed directly into prop.test. # note x = successes, n = successes + failures prop.test( x=c(567, 412), n=c(1778, 1897), conf.level=0.99 ) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: c(567, 412) out of c(1778, 1897) ## X-squared = 48.066, df = 1, p-value = 4.121e-12 ## alternative hypothesis: two.sided ## 99 percent confidence interval: ## 0.06368227 0.13974295 ## sample estimates: ## prop 1 prop 2 ## 0.3188976 0.2171850 17.4.7.3 Interpretation of Two-Sample proportions test output This test is evaluated under the assumption that the null hypothesis is true. The test results helps us decide whether the null hypothesis should be rejected. Well do that if the p-value is less than our type1 error threshold. The prop.test conducts a chi-square analysis using the Yates continuity correction. This \\(\\chi^2\\) value is very large; extremely large. You can learn more about the chisquare test statistic in Chapter @(chisquare). The p-value represents the probability of obtaining a \\(\\chi^2\\) value as larger or larger than that in the null distribution. If the null hypothesis is true in this case, the probability of that sized \\(\\chi^2\\) value is 4.12e-12, which is very low, and still much lower than our type1 error threshold. We can reject the null. If we subtract prop2 from prop1 we get a value of about 0.1017, as a point estimate for the difference between the proportion of gradstudin=positive cells in the vehicle and cytokine treated samples. The 95% CI represents a range of values that might include the difference between the two proportions. We are 95% confident that the true difference between these proportions is from 0.0638 to 01397. The 95% CI for the difference between two proportions does not include the value of 0. Since the 95% CI does not overlap with 0, we can conclude from it alone that there is a statistical difference between the two proportions. We have to use scientific judgment to assess whether the lower and upper ranges of the CI represent values that are scientifically meaningful or relevant. It is possible for a CI to include values that are scientifically unremarkable, even if the value of zero is not included. 17.4.7.4 Write Up The proportion of gradstudin positive T cells after cytokine differs from that in vehicle treated cells (two-sided two-sample proportions test, p-value=4.12e-12, 95% CI for proportion difference = 0.0638 to 01397) Note how we dont say statistically significantly different. 17.5 Exact tests for two proportions An alternative to prop.test to compare two proportions is the fisher.test, which like the binom.test calculates exact p-values. The fisher.test requires that data be input as a matrix or table of the successes and failures, so that involves a bit more munging. In R a matrix differs from a dataframe in many ways. The cells within the matrix must all be of the same data type. In this case, numerical values. The matrix can be labeled with dimension names. Finally, it helps to understand that the matrix world operates in a row by column way. Thus, the vector of 4 values below fills the rows before filling the columns. We need to make a matrix of the data first, then perform the fisher.test on that matrix: Notice below how to enter the successes and the failures in the matrix. The matrix is also labeled with some dimension (row and column) names so it doesnt get confusing. The fisher test is run by passing the matrix into the fisher.test function: M &lt;- matrix( c(567, 412, 1211, 1485), nrow=2, dimnames = list( Treat=c(&quot;Cytokine&quot;, &quot;Vehicle&quot;), Antigen=c(&quot;Positive&quot;, &quot;Negative&quot;) ) ) M ## Antigen ## Treat Positive Negative ## Cytokine 567 1211 ## Vehicle 412 1485 fisher.test(M) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: M ## p-value = 3.433e-12 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.451760 1.962188 ## sample estimates: ## odds ratio ## 1.687312 Notice the output differs from the prop.test. In addition to a p-value, the Fisher test produces an odds ratio and its confidence interval. The p-value leads to the same result and write-up as for the two proportion test. As for the odds ratio Recall that the odds of an event that has a probability \\(p\\) are \\[odds = \\frac{p}{1-p}\\] Thus, the odds of gradstudin-positive cells are \\[odds = \\frac{positive}{negative}\\] An odds ratio is the ratio of two odds. Here, it is the ratio of the odds of gradstudin-positive cells in response to cytokine to the odds of gradstudin-positive cells after vehicle.\\[OR = \\frac{odds_{cyto}}{odds_{veh}}\\] It is probably best to leave the the OR as is. Report the ratio per se as the effect size as in the write up below. However, one way to think of the OR is as a percent by multiplying the OR by 100. Thus, the odds of gradstudin positive cells in the cytokine group are 168% of their odds in the vehicle group. 17.5.0.1 Write up The cytokine increases the odds of gradstudin-positive T cells by 1.687 compared to vehicle treatment (Fishers Exact Test for Count Data, p = 3.433e-12, OR 95% CI = 1.45 to 1.96). 17.6 Goodness of fit Tests Goodness-of-fit tests are useful for testing hypotheses about patterns of counts in time or space. The tests questions whether the distribution of their observed frequencies differs from expectations of a null case. In other words, do the count patterns occur in a non-random pattern? The shape of these datasets in their simplest form is either as 1 row or 1 column, where every data cell represents a time or space or nominal feature and the cell value is the the number of counts that occurred in that time or space or have that nominal feature. Either the multinomial.test (for exact p-values) or the chisq.test (for approximate p-values) can be used for Goodness-of-fit testing. The latter is most commonly used, but may be less accurate when the number of counts in cells is low. These designs compare the distribution of events to a hypothetical (or expected) model null distribution of those events. These expected counts are entered in the test script as a prop or p argument. This can be confusing. Its important to recognize you should enter a vector of null probabilities in p or prop! Dont enter the counts you hope to see if the test were positive!!! You always test the null. Say we had a spatial memory test in which 28 independent subjects (rats) are placed into a maze for testing (one at a time) and we count which of 4 chambers they enter first. The results indicate they do so at the following frequencies: A=14, B=3, C=7, D=4. Does this frequency distribution differ from the null expectation, A=7, B=7, C=7, D=7, where no chamber is more likely to be entered than another? Failure is not an option in this design! Only successes are counted. Given enough time, a subject will always choose one of the 4 chambers. If a subject fails the task and never enters a chamber, it should be censured. If failure to enter a chamber is a fairly common outcome, then perhaps it should be the fifth nominal option. Lets test this at the 5% type1 error threshold: 17.6.1 An Exact Goodness of Fit test NB:The multinomial.test function form the EMT package requires you to assert the null frequency distribution explicitly and as fractions whose sum is 1. This is unlike using the chisq.test, which coerces expected values for you unless you argue otherwise. Also note that in this example the probabilities are uniform. That need not be the case. The only requirement is that the sum of the probabilities equals one. The multinomial test shines brightest when you expect varied probs. x &lt;- c(A=14, B=3, C=7, D=4) prob &lt;- c(A=.25, B=.25, C=.25, D=.25) multinomial.test( x, prob=prob ) ## ## Exact Multinomial Test, distance measure: p ## ## Events pObs p.value ## 4495 1e-04 0.0235 Note on the multinomial.testoutput: Please see this site for further reading for further information on what is represented by events. In this case the value for Events is choose(28+3-1,4-1). pObs is a (rounded) multinomial probability for observing that exact permutation (dmultinom(x=c(14,3,7,4), prob = c(0.25,0.25,0.25,0.25))), which serves as a critical value. The p.value is the sum of all pObs for other permutations that are lower (more extreme) than this pObs. Were only interested in the p-value, since were using this function as an exact goodness-of-fit hypothesis test for the null hypothesis. Why use an exact test rather than a chisq.test? Because we have two cells in the dataset with counts &lt; 5. An exact p-value test will be more accurate than chisq.test. The test compares our sample frequency distribution to that in the null model. We actually wrote the latter explicitly in the function argument: null is a uniform distributionthe subjects are equally likely to enter each chamber. H0: The probability of choice is equal for each chamber. \\(\\pi_A=\\pi_B=\\pi_C=\\pi_D\\) H1: The probability of choice is not equal for each chamber. \\(\\pi_A\\ne\\pi_B\\ne\\pi_C\\ne\\pi_D\\) Note: this is an omnibus test. It doesnt explicitly tell us which chambers are preferred by the subjects. Because the observed p-value is less than a pre-chosen threshold of 0.05, we reject the null hypothesis and conclude that the chamber choice is not equitable across the four options. 17.6.1.1 Write up In a 4 chamber maze test, the subjects displayed a non-uniform chamber preference (Exact Multinomial Test, n=28, p=0.0235) Note how this phrasing implies the null hypothesis. 17.6.2 An Approximate Goodness of Fit test The \\(\\chi^2\\) test of the same data is really simple to execute. We pass into the function the same vector of observed counts, x, that we configured above. It offers the same conclusion, but note how the p-value is very different. Note also that we didnt enter the null frequency argument. The chisq.testfunction will coerce the null distribution if it is not entered as an argument explicitly, as you can see from the output for the second line. If for some reason to test against a non-uniform null distribution, youll need to write that in your argument explicitly (eg, p = c(A=0.5, B=0.25, C &amp; D = 0.125). chisq.test(x) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 10.571, df = 3, p-value = 0.01428 17.6.2.1 Write up The interpretation is no different than for the exact test. The write up is: In a 4 chamber maze test, the subjects displayed a nonuniform chamber preference (Chi square test for uniform probabilities, \\(\\chi^2\\)=10.571, df=3, p=0.01428) 17.6.2.2 Post hoc: which chamber is the preferred? The result of the multinomial test can be unsatisfying in the sense that it only indicates the observed frequency pattern differs from that expected by chance. It doesnt say which chamber preference explains the overall outcome. This can be answered using a series of binomial tests. These would address whether a given choice differs from all the others. The key here is that multiple comparisons will be made. Therefore an adjustment must be made to the type1 error threshold. Using a Bonferroni correction, if the experiment-wise type1 threshold is 5%, and 4 comparisons are to be made, then the threshold for any one comparison would be 5%/5, or 1.25%. In later chapters (on ANOVA) we will talk about this adjustment in more detail. Of the following four tests, you see that only the frequency of the A chamber differs from the null expectation. The test p-value of 0.004099 is lower than that for the reset type1 threshold of 0.0125. # A chamber binom.test(x=14, n=28, p=0.25) ## ## Exact binomial test ## ## data: 14 and 28 ## number of successes = 14, number of trials = 28, p-value = 0.004099 ## alternative hypothesis: true probability of success is not equal to 0.25 ## 95 percent confidence interval: ## 0.306471 0.693529 ## sample estimates: ## probability of success ## 0.5 # B chamber binom.test(3, 28, p=0.25) ## ## Exact binomial test ## ## data: 3 and 28 ## number of successes = 3, number of trials = 28, p-value = 0.123 ## alternative hypothesis: true probability of success is not equal to 0.25 ## 95 percent confidence interval: ## 0.02266509 0.28226440 ## sample estimates: ## probability of success ## 0.1071429 # C chamber binom.test(7, 28, p=0.25) ## ## Exact binomial test ## ## data: 7 and 28 ## number of successes = 7, number of trials = 28, p-value = 1 ## alternative hypothesis: true probability of success is not equal to 0.25 ## 95 percent confidence interval: ## 0.1069080 0.4487155 ## sample estimates: ## probability of success ## 0.25 # D chamber binom.test(4, 28, p=0.25) ## ## Exact binomial test ## ## data: 4 and 28 ## number of successes = 4, number of trials = 28, p-value = 0.2738 ## alternative hypothesis: true probability of success is not equal to 0.25 ## 95 percent confidence interval: ## 0.04033563 0.32665267 ## sample estimates: ## probability of success ## 0.1428571 Another way to report p-values from multiple comparisons is to state them as adjusted. p.adjust(p=0.004099, method=&quot;bonferroni&quot;, n=4) ## [1] 0.016396 Thus, a p-value of 0.004099 when adjusted for 4 multiple comparisons is equivalent to a p-value of 0.016396. This value yet remains below the pre-chosen threshold of 0.05. 17.6.2.3 Write-up including post hoc test In a 4 chamber maze test, the subjects display a clear chamber preference (Chi square test for uniform probabilities, \\(\\chi^2\\)=10.571, df=3, p=0.01428). Multiple post hoc tests indicate only the chamber A frequencies differ from the null (Exact binomial test, adjusted p-value= 0.016396). 17.7 Contingency Testing Contingency testing is for deciding whether two or more variables are associated or not. These either explicitly (ie, when using fisher.text) or implicitly (ie, when using chisq.test) use ratios of proportionsthe odds ratio, or relative risk, or the likelihood ratio, or sometimes other proportionsas parameters that express the magnitude of these associations. In other words, the hypothesis test asks whether these ratios of proportions are more extreme than the null (which would be 1). Lets take the cancer marker data from the contingency analysis lecture. As you recall, a biomarker has been discovered that is hoped to be strongly associated with cancer. 100 people were tested for whether or not they have the marker, and whether or not they also go on to have cancer. Well create a simple matrix then pass it through the fisher.test function to illustrate the procedure and interpretation. x &lt;- matrix( c(14, 16, 6, 64), ncol=2, dimnames = list( Marker=c(&quot;Present&quot;, &quot;Absent&quot;), Cancer = c(&quot;Present&quot;, &quot;Absent&quot;) ) ) x ## Cancer ## Marker Present Absent ## Present 14 6 ## Absent 16 64 fisher.test(x) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x ## p-value = 3.934e-05 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.762514 33.678765 ## sample estimates: ## odds ratio ## 9.061278 #more argument customization than this is possible 17.7.1 Interpretation of Contingency Results The odds of a person with the marker having cancer are 9.06 times greater than the cancer odds for those who dont have the marker. There is 95% confidence the true odds ratio in the population is between 2.76 and 33.68. This is consistent with an association between the presence of this marker and the probability that cancer occurs. 17.7.2 Write Up The large OR indicates the presence of this marker is strongly associated with cancer (n=100, OR = 9.06, 95% CI = 2.76 to 33.68, Fishers Exact Test for Count Data, p = 3.934e-05). The word strongly is used to emphasize the effect size, which is OR, not the smallness of the p-value. Here are the other tests you might use to conduct for a contingency analysis, to illustrate how they differ. First, a proportion test and a sort of proof that the prop test is just a chisquare test: x &lt;- matrix( c(14, 16, 6, 64), ncol=2, dimnames = list( Marker=c(&quot;Present&quot;, &quot;Absent&quot;), Cancer = c(&quot;Present&quot;, &quot;Absent&quot;) ) ) x ## Cancer ## Marker Present Absent ## Present 14 6 ## Absent 16 64 prop.test(x) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: x ## X-squared = 16.741, df = 1, p-value = 4.284e-05 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.2496194 0.7503806 ## sample estimates: ## prop 1 prop 2 ## 0.7 0.2 chisq.test(x) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x ## X-squared = 16.741, df = 1, p-value = 4.284e-05 chisq.test(x, correct=F) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 19.048, df = 1, p-value = 1.275e-05 First, note that the prop.test is just the chisq.test. You get the same \\(\\chi^2\\) value and p-value from both. They just differ in parameter output and input options. If youd like a confidence interval use prop.test. Second, note how turning off the Yates continuity correction changes the \\(\\chi^2\\) value and the p-value. Thats to be expected, it changes the calculation! Both the prop.test and chisq.test use Yates by default. The best way to think about Yates is that it acts as a smoothing function to take off some of the jigger in the calculation of the \\(\\chi^2\\) value. 17.7.3 Interpretation of chi-square output There is an association between the presence of this marker and the probability that cancer occurs. We could take the prop tests calculation of the proportions and their difference, along with the 95% CI of their difference and make some hay out of that (the probability of getting cancer with the marker is 70%, and without the marker is 20%). But its more customary to use the odds ratio or relative risk rather than differences between probabilities to make effect size assertions in association studies. 17.7.4 Write Up You would want to derive the odds ratio and its 95% CI, even though the \\(\\chi^2\\) test doesnt produce it for you. The easiest way to do that is with fisher.test. Having that: The large OR indicates the presence of this marker is strongly associated with cancer (n=100, OR = 9.06, 95% CI = 2.76 to 33.68, Pearsons Chi-square test with Yates continuity correction, p = 4.284e-05). As before, the word strongly is used to emphasize the effect size, which is the OR, rather than the extremeness of the p-value. 17.7.5 Which contingency test is best? With so many options, the question Im sure you are asking is which is best to use for contingency analysis? The answer is, * be sure to make this decision in advance and use what you said you would use before you started the experiment. * when it comes to p-values, are you an exactophile or an exactophobe? Use exact tests if the former. * for datasets with low cell numbers (eg, counts less than 5 in a cell), exact tests tend to provide more accurate p-values. * the fact that fisher.test generates the OR and its CI is very, very convenient. I prefer the fisher.exact test. However, in R youll need to understand how to configure its arguments to get it to work on higher dimension contingency tables (eg, 2x3, 3x2, 3x3, etc). 17.7.6 Higher dimension contingency analysis Not infrequently we have studies with more than one level of a predictor variable and with two or more categorical outcomes (eg, 7x2, 2x3, 5x4, etc). In these instances we are interested in comparing a few to several combinations of outcomes. A step-wise approach for these more complex analyses is to first run an omnibus chisq.test or fisher.test on the fully intact dimension. If the test is positive (the p-value is lower than a pre-chosen threshold), analyze 2x2 segments of the grid post hoc. That is accomplished by using one of the exact or the chisq.test on more discrete groupings. These tests can explain which of the proportions or associations explain the overall skew detected by the initial omnibus test. These post hoc analyses must include correction for multiple comparisons (eg, the Bonferroni correction) when drawing inference. To do this, collect a vector of p-values, on from each post hoc test, and pass it into Rs p.adjust function. Heres an example of a higher dimension array: Gammaherpesviruses, such as EBV and KSHV, establish lifelong infection in B cells. At the peak of infection the cells have either germinal center B cell or plasma cell phenotypes. Very little is known about the immunoglobulins expressed in gammaherpesvirus infected cells. In a study using a mouse model of viral exposure to YFP-tagged gammherpesvirus, immunoglobulin isotypes (IG) were determined in randomly collected single cell clones that include both germinal center B cells and in plasma cells that were either infected or not (YFP+ or YFP-). As a first step, the data are constructed as a matrix. counts &lt;- matrix(c(3,0,0,0,10,9,15,29,17,11,40,50,3,2,2,13,25,59,61,43), nrow=4, ncol=5, dimnames=list(Cell=c(&quot;YFP- GC B&quot;, &quot;YFP+ GC B&quot;, &quot;YFP- plasma&quot;, &quot;YFP+ plasma&quot;), IG =c(&quot;G1&quot;, &quot;G2b&quot;, &quot;G2c&quot;, &quot;G3&quot;, &quot;M&quot;))) counts ## IG ## Cell G1 G2b G2c G3 M ## YFP- GC B 3 10 17 3 25 ## YFP+ GC B 0 9 11 2 59 ## YFP- plasma 0 15 40 2 61 ## YFP+ plasma 0 29 50 13 43 paste(&quot;Total number of unique cells isotyped =&quot;, sum(counts)) ## [1] &quot;Total number of unique cells isotyped = 392&quot; Next, an omnibus Fisher test run at a type1 threshold of 5%. The combination calculations necessary to solve for an exact p-value in these higher order matrices wont always converge to a solution. The simulate.p.value argument is often necessary in these cases. The p-value is not exact. fisher.test(counts, simulate.p.value = T) ## ## Fisher&#39;s Exact Test for Count Data with simulated p-value (based on ## 2000 replicates) ## ## data: counts ## p-value = 0.0004998 ## alternative hypothesis: two.sided 17.7.6.1 Post hoc testing of higher dimensions A 4x5 matrix contains 20 groups, \\(k\\). The total possible comparisons is \\(c=\\frac{k(k-1)}{2}\\), or 190 in this case. Many of these would be uninteresting. Let scientific reasoning guide the posthoc testing. Thinking about the experiment two sets of comparisons seem good. The first set ask whether two rows differ in isotype frequencies. A second, and larger, set asks whether two rows differ in frequency for a specific isotype. In one sense, the first of these sets can serve as a sub-omnibus to guide the comparisons made in the second set. Here are two examples for what the first set of comparisons might look like. Does viral infection affect immunoglobin isotype frequencies in germinal center cells? fisher.test(counts[1:2,1:5]) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: counts[1:2, 1:5] ## p-value = 0.002753 ## alternative hypothesis: two.sided Do isotype frequencies differ between virally-infected germinal center and plasma cells? fisher.test(counts[c(2, 4), 1:5]) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: counts[c(2, 4), 1:5] ## p-value = 9.09e-08 ## alternative hypothesis: two.sided A few other questions could be asked. Heres an example of what the second set of questions would look like which focus on the frequencies of specific isotypes. Although the p-value is unadjusted, the exact test result to the second question above suggests isotype frequencies do differ between virally-infected germinal center and plasma cells. To focus on the isotype(s) that might explain this difference we would compare a series of two proportions, These would compare the counts for a given isotype to the sum of the counts for all other isotypes. Although the matrix-making code below looks a bit chunky, it clearly illustrates what Other counts represent: Does viral infection cause a difference in M isotype expression between germinal center and plasma cells? m15 &lt;- matrix( c(counts[2,5], counts[4,5], sum(counts[2,1:5])-counts[2,5], sum(counts[4,1:5])-counts[4,5]), nrow=2,ncol=2, dimnames=list(YFp=c(&quot;+ GC B&quot;, &quot;+ plasma&quot;), serotype=c(&quot;M&quot;, &quot;Other&quot;) )) ftm15 &lt;- fisher.test(m15) ftm15 ## ## Fisher&#39;s Exact Test for Count Data ## ## data: m15 ## p-value = 5.056e-09 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.998242 11.094080 ## sample estimates: ## odds ratio ## 5.686205 Although this p-value is also unadjusted, it certainly appears that gammaherpesvirus infection causes a difference in M isotype expression between germinal center and plasma cells. 17.7.7 Other experimental designs involving categorical data Imagine an experiment to compare two or more conditions (eg, placebo v drug, wildtype vs mutant) and the outcome variable is discrete (eg, frequency counts or success and failure counts). The experiment involves several independent replications. For example, cell depolarizations are counted both in the absence or presence of a condition on several different occasions. Table 17.1: Hypothetical replicates comparing depolarizations in mutant and wildtype cells should be analyzed using Poisson regression. replicate predictor counts one wildtype 110 two wildtype 99 three wildtype 100 one mutant 165 two mutant 138 three mutant 139 Alternately, the fraction of cells in a culture dish that have died in the absence or presence of a condition is repeated a handful of times. Table 17.2: Hypothetical replicates comparing in mutant and wildtype cells should be analyzed using logistic regression. replicate predictor alive dead one wildtype 32 71 two wildtype 30 73 three wildtype 31 69 one mutant 64 40 two mutant 68 38 three mutant 65 39 The key distinction here, compared to whats been discussed in this chapter up to this point, is that within each replicate, all of the events are intrinsically-linked. Through replication were establishing whether the frequencies of these counts are repeatable. Logistic (for dichotomous data) or Poisson (for frequency data) regression are the appropriate analytical tools for these designs. These involve using the generalized linear model, conducted with the function glm or the function glmer (for so-called mixed models). These are discussed later in the logistic regression chapter. 17.8 Power analysis for proportion tests Power analysis should be done before starting an experiment. The purpose of a conducting power calculations a priori is to determine the number of trials, or subjects or sample size, to use for the study. This is a two step process. Step 1: What effect size will you test? Using scientific judgement, decide what is the value of a null proportion and an alternate that you think would be a scientifically meaningful proportion to observe. You need to have some insight into the system youre studying to make these decisions. Often that comes from pilot studies or from previously published (or unpublished) work. Whats important is to establish an expectation of what a minimally scientifically significant outcome would look like. Step 2: Calculate the number of subjects (or trials) youll need to study, given these proportions (and also given some type1 and type2 error tolerances). There are several options in R for the second step. In the examples below, were declaring a 5% difference between the null (0.15) and alternate (0.20) proportions would be a scientifically meaningful. Were also using 5% for type1 error and 20% for type2 error (80% power) as error tolerance thresholds. 17.9 Power analysis functions for proportion tests The function pwr.p.test is for one-sample proportion tests. The calculations below return a sample size n that should be used in the study, given a null and an alternate proportions, in addition to error rates. NB: Since pwr.p.test takes a Cohens effect size as an argument, you 1st must calculate a Cohen effect size, h, given the alternate and null proportions you expect. The function Es.h() in the pwr package is useful for this. Then plug that effect size, h, into the proportion test power calculator. h &lt;- ES.h(p1=0.2, p2=0.15) h ## [1] 0.1318964 pwr.p.test( h, sig.level=0.05, power=0.8, alternative=&quot;two.sided&quot; ) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.1318964 ## n = 451.1706 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided The output indicates that a total of 450 random, independent events will be necessary for 80% power to detect a difference between a sample proportion of 0.2 to a hypothetical proportion of 0.15. binom.power is a function from the binom package. Instead of returning sample size, this function returns power, given sample size. As an a priori test, one iterates through (by hand) entering sample sizes until it returns an acceptable power. Then run the experiment at that sample size. Note that it doesnt give exactly the same result as pwr.p.test. The calculation differs, but the result is close. binom.power( 0.2, n=451, p=0.15, alpha=0.05, alternative = &quot;two.sided&quot;, method=&quot;exact&quot; ) ## [1] 0.7908632 To estimate sample size needed for a two-sample proportion test design, for example, to compare a negative control to a treatment, use the power.prop.test function. power.prop.test( p1=0.15, p2=0.2, sig.level = 0.05, power=0.8 ) ## ## Two-sample comparison of proportions power calculation ## ## n = 905.3658 ## p1 = 0.15 ## p2 = 0.2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Finally, the statmod package has the power.fisher.test, which returns the power for a Fishers exact test, given arguments of proportion, trial size and type1 error. Note how it is in close but not exact agreement with power.prop.test. power.fisher.test( 0.15, 0.2, 905, 905, 0.05, alternative = &quot;two.sided&quot; ) ## [1] 0.79 17.9.0.1 Monte Carlo power simulations The Monte Carlo is a general technique in statistical computing to randomly repeat some calculation or process. Monte Carlo is the basis of all sorts of things, like permutation analyses, machine learning, fitting Bayesian models to datasets and more. Monte Carlos are very simple. Monte Carlo power simulations is the insane thing you would never do in real life, which is to repeat the same full experiment replete with random, independent replicates thousands of times. Just to get a long run average of how well it works. The basic gist is to simulate and test a very large number of experiments. Each of these experiments is comprised of a random sample of some size, corresponding to your minimal effect size you define as scientifically meritorious. Sample responses are randomly generated. These are run through the test of significance, to calculate a p-value. The fraction of simulations that are hitsfor example, that have p-values &lt; 0.05, is the power of the experiment! Simulations are re-run by adjusting the sample size until a desired power is achieved. Thats the sample size youd take for use in a real experiment! The question the script addresses is this: What is the power of an experiment, given this trial size n, the null and alternate proportions evaluated, and the type1 error threshold? If n is too low, the test will return a power below the threshold you set, meaning it is not adequately powered to test the difference between the null and alternate proportions. Iterate through some a few sample sizes (n) until you arrive at an acceptable value for power. 17.9.0.1.1 Monte Carlo simulation for prop.test power #these first few objects are the initializers #number of experiments to simulate, each of trial size n sims &lt;- 1000 #expected null proportion null &lt;- 0.15 #expected minimal effect proportion alternate &lt;- 0.20 #binomial trial size, just a guess, ha! n &lt;- 450 #type 1 error threshold alpha &lt;- 0.05 #s1 is a random sample vector #each value is the number of #successes observed in a trial #of size n, given the alternate proportion. #it simulates the outcome of one experiment &quot;sims&quot; times s1 &lt;- rbinom(sims, n, alternate) #t1 is a vector of p-values, #derived from a one sample proportion test #on each of the values in s1. #read from inside the function to see the logic t1 &lt;- unlist( lapply( s1, function(s1){ prop.test( s1, n, null, alternative=&quot;two.sided&quot;, conf.level=1-alpha, correct=T)$p.value } ) ) power &lt;- length(which(t1 &lt; alpha))/sims power ## [1] 0.806 17.10 Graphing Proportions Heres a few ggplot2-based ways of visualizing proportion data. First thing is to create a dataframe of the proportion data since data fed into ggplot2 must be in dataframe format. prop.df &lt;- tibble( group=c(&quot;positive&quot;, &quot;negative&quot;), value=c(pos, neg) ) prop.df ## # A tibble: 2 x 2 ## group value ## &lt;chr&gt; &lt;dbl&gt; ## 1 positive 5042 ## 2 negative 18492 17.10.0.0.1 Simple stacked bar chart These certainly emphasize proportion differences well, but make it difficult to visually estimate the differences between the two groups. ggplot( prop.df, (aes(x=&quot;&quot;, y=value, fill=group) ) ) + geom_bar(stat=&quot;identity&quot;, fill=c(&quot;red&quot;, &quot;blue&quot;)) Figure 17.1: Stacked bar plots of raw counts can be difficult to interpret 17.10.0.0.2 Side-by-side bar chart The gold standard for clear presentation of total counts and differences in counts between groups. Note: When comparing independent counts there is no error to report. Theres no variation. Cells were classified as either having or not having the antigen. Every count is independent of all other counts. ggplot( prop.df, (aes( x=group, y=value, fill=group) ) ) + geom_bar(stat=&quot;identity&quot;, fill = c(&quot;red&quot;, &quot;blue&quot;)) Figure 17.2: Side-by-side bar charts make it easier to see difference between raw counts. 17.10.0.0.3 Plot proportions with confidence intervals However, adding confidence intervals to bar plots of proportions is a good way to visualize the uncertainty of counted data and comparing to other proportions. Table 17.3: Hypothetical counts of gradstudin-stained T cells, myCounts group pos neg Control 37 193 Cytokine1 44 186 Cytokine2 63 167 Munge the data directly within ggplot function to calculate values for the proportion and its upper and lower 95% confidence intervals. Proportions with overlapping CIs do not differ from each other. Note how the CIs are adjusted for the 3 possible comparisons using the Bonferroni correction (adjusted 95% confidence level = 1-0.05/3). Note that the rowwise function is necessary for certain legacy functions like scoreci. ggplot(myCounts %&gt;% rowwise() %&gt;% mutate( prop=pos/(pos+neg), lower=scoreci(pos, pos+neg, 0.98333)$conf.int[1], upper=scoreci(pos, pos+neg, 0.98333)$conf.int[2]), aes(x=group))+ geom_bar(aes(y = prop), stat =&quot;identity&quot;, fill=&quot;blue&quot;)+ geom_errorbar(aes(ymin=lower, ymax=upper), width=0.2, size=2)+ ylab(&quot;Proportion of gradstudin positive cells&quot;) Figure 17.3: You can infer two proportions differ from plots when their confidence intervals do not overlap. 17.10.0.0.4 Pie charts and mosaic plots I suggest you explore these options on Google. They can be visually appealing for showing relative differences in some cases, but they make it more difficult to easily see absolute counts and differentials. 17.11 Key Takeaways Sorted variables have nominal values that are classified into categories and that are quantified as discrete integer counts. Each count must be an independent replicate. Sorted events can be modeled using rbinom or rmultinom. The latter is used when the event has more than two discrete outcomes. The three basic types of experimental designs are questions about proportions, or about frequencies, or about associations. The test functions mostly differ in whether they generate approximate or exact p-values. To avoid p-hacking, choose the test in advance. When designing an experiment perform a power analysis to determine the number of independent events that will be necessary. When testing for small differences or effect sizes, these numbers needed for a stringent test can be surprisingly high. "],["chisquare.html", "Chapter 18 The Chi-square Distribution 18.1 Background 18.2 dchisq 18.3 pchisq 18.4 qchisq 18.5 rchisq", " Chapter 18 The Chi-square Distribution library(tidyverse) 18.1 Background The chi-square permeates biostatistics in several important ways. For example, the \\(\\chi^2\\) distribution is used to generate p-values for Pearsons chi-square test statistic, \\[\\chi^2=\\sum_{i=1}^{n}\\frac{(O_i-E_i)^2}{E_i}\\] which is used in goodness-of-fit and independence tests. The \\(\\chi^2\\) distribution is used to generate p-values for tests of homogeneity and also to calculate the confidence intervals of standard deviations. A good way to think of the chi-square distribution more generally is as a probability model for the sums of squared variables. As such, the \\(\\chi^2\\) test statistic only takes on positive values. If \\[X_1,..,X_k\\] is a set of standard normal variables, then the sum of their squared values is a positive, random variable \\(Q\\), \\[Q=\\sum_{i=1}^{k}{X^2_i}\\] that will take on a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom. The mean of any \\(\\chi^2\\) distribution is \\(k\\) while its variance is \\(2k\\). The distributions are skewed (mode = \\(k-2\\)) except that those having very large degrees of freedom, which are approximately normal. 18.2 dchisq dchisq is the \\(\\chi^2\\) probability density function in R. The simplest way to think of dchisq is as the function that gives you the probability distribution of the \\(\\chi^2\\) test statistic. The \\(\\chi^2\\) probability density function is: \\[p(x)=\\frac{e^\\frac{-x}{2}x^{\\frac{k}{2}-1}}{2^\\frac{k}{2}\\Gamma(\\frac{k}{2})}\\] Given a chi-square value and the degrees of freedom of the dataset as input, dchisq returns the probability for a given chi-square value. For example, if a 2X2 test of independence (df=1) returns a \\(\\chi^2\\) value of 4, the probability of obtaining that value (its density) is equal to 0.02699: dchisq(x=4, df=1) ## [1] 0.02699548 For continuous distributions like the \\(\\chi^2\\) such point density values are usually not particularly useful. In contrast, inspecting how the function behaves over a range of \\(\\chi^2\\) values and at different dfs is informative. Thats plotted below: df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- dchisq(x, i) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;p(chi-square)&quot;) 18.3 pchisq pchisq is the \\(\\chi^2\\) cumulative distribution function in R. The simplest way to think about the pchisq function is as the probabilities under the \\(\\chi^2\\) curve. The \\(\\chi^2\\) cumulative distribution function is: \\[p(x)=\\frac{\\gamma(\\frac{k}{2}\\times\\frac{x}{2})}{\\Gamma(\\frac{k}{2})}\\] It returns the cumulative probability for an area under the curve up to a given \\(\\chi^2\\) value. For example, the probability that a \\(\\chi^2\\) value with 1 degree of freedom is less than 4 is 0.9544997: pchisq(q=4, df=1, lower.tail = T) ## [1] 0.9544997 df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- pchisq(x, i, lower.tail = T) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;cumulative p(chi-square)&quot;) 18.3.1 Calculating p-values from pchisq The pchisq function is used to calculate a p-value. A p-value is a probability that a \\(\\chi^2\\) value, x, is as large or larger. \\[p-value = P(\\chi^2 \\ge x)\\] This can be calculated using the pchisq function simply by changing lower.tail argument in the function tolower.tail = F pchisq(q=4, df=1, lower.tail = F) ## [1] 0.04550026 Here p-values for the \\(\\chi^2\\) at various dfs: df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- pchisq(x, i, lower.tail = F) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;p-value&quot;) 18.4 qchisq qchisq is the inverse of the \\(\\chi^2\\) cumulative distribution function. This function takes a probability value as an argument, along with degrees of feedom, and returns a \\(\\chi^2\\) value corresponding to that probability. Heres the \\(\\chi^2\\) value corresponding to the 95th percentile of a distribution with 3 degrees of freedom: qchisq(0.95, 3) ## [1] 7.814728 Here is what the quantile \\(\\chi^2\\) distribution looks like for several different dfs: df = 9 p = seq(0.01, .99, 0.01) xpd &lt;- matrix(ncol=df, nrow=length(p)) for(i in 1:df){ xpd[,i] &lt;- qchisq(p, i) } xpd &lt;- data.frame(xpd) colnames(xpd) &lt;- c(1:df) xpd &lt;- cbind(p, xpd) xpd &lt;- gather(xpd, df, qchisq, -p) ggplot(xpd, aes(p, qchisq, color=df)) + geom_line() + xlab(&quot;probability&quot;)+ylab(&quot;chi-square&quot;) 18.5 rchisq Rs rchisq function is used to generate random values corresponding to \\(\\chi^2\\)-distributed data. This has utility in simulating data sets with skewed values, for example, to mimic overdispersed Poisson distributions. Here are 10 random values from a \\(\\chi^2\\) distribution with 3 degrees of freedom: rchisq(10, 3) ## [1] 2.4287709 1.1650965 3.1655686 2.6182321 1.1958868 1.1320062 0.7003490 ## [8] 0.3521667 3.5975258 7.6505483 "],["nonparametrics.html", "Chapter 19 Nonparametric Statistical Tests 19.1 Nonparametric sampling distributions 19.2 Experiments involving discrete data 19.3 Experiments involving deviant Data 19.4 Sign Test 19.5 Wilcoxon Sign Rank Test for One Group 19.6 Wilcoxon Mann Whitney Rank Sum Test for 2 independent groups 19.7 Wilcoxon Sign Rank Test for paired groups 19.8 Kruskal-Wallis 19.9 Friedman test 19.10 Nonparametric power calculations 19.11 Summary", " Chapter 19 Nonparametric Statistical Tests library(tidyverse) library(PMCMR) library(PMCMRplus) library(PropCIs) library(knitr) Non-parametric statistical tests are quite versatile with respect to the dependent variables they tolerate and the experimental designs. Nonparametric tests allow for statistical analysis of discrete ordinal data, such as likert survey data or other assessment instruments comprised of discrete scoring levels. Nonparametric tests can also be used for data on measured, equal interval scales, particularly when the normality and equal variance assumptions of parametric statistical testing are not satisfied or cannot be assumed. Nonparametric statistics are parameter-less. They dont compare means, or even medians. Though people frequently treat nonparametrics as tests of medians, that is not strictly true. They dont involve least squares calculations, or standard deviations, or variance, or SEs. They do compare distributions of data. This happens by transforming the data into a standardized measure of rankseither into signs, or sign ranks or rank sums. The experimental design dictates which of these measures of ranks is used for testing. The tests, essentially, evaluate whether the distribution of ranks in an experimental outcome differs from a null distribution of ranks, given a sample size. That can seem pretty abstract. But its actually a simple and elegant way to think about these tests. With the exception of the Sign Test, which has a probability as an effect size, strictly speaking there really isnt an effect size that describes non-parametric outcomes other than the value of the test statistic. However, it is possible to use confidence interval arguments in Rs tests to coerce them into providing effect size output as estimates of medians. This can be sometimes useful in write ups of the results. Non-parametric analogs exist for each of the major parametric statistical tests (t-tests and one-way completely randomized anova and one-way repeated/related measures anova). Which nonparametric analog to use for a given data set analysis depends entirely upon the experimental design. Sign Test -&gt; analog to the binomial Test -&gt; when events are categorized as either successes or failures. Wilcoxon Sign Rank Test for one group -&gt; analog to the one sample t-test -&gt; compare the values of a one group data set to a standard value. Mann Whitney Rank Sum Test for 2 independent groups -&gt; analog to the unpaired t test -&gt; for comparing two groups in a data set. Wilcoxon Sign Rank Test for paired groups -&gt; analog to the paired t-test -&gt; comparing a group of paired outcomes in a data set to no effect null. Kruskal-Wallis Test -&gt; analog to one way completely randomized ANOVA -&gt; comparing 3 or more groups Friedman Test -&gt; analog to one way repeated/related measures ANOVA -&gt; comparing 3 or more differences. In R, the wilcox.testfunction is a work horse for non-parametric analysis. By simply changing the functions arguments it can do either a WSRT, or MW, or a WSRT for paired groups analysis. For the Sign Test, well just use the binom.test function in R which was discussed previously in chapter @(categorical). 19.1 Nonparametric sampling distributions As for other statistical tests, when conducting nonparametric analysis the experimental data are transformed into a test statistic that represents the signal-to-noise in the results. The question, as always, is whether this signal-to-noise is too extreme for a null effect. The key nonparametric test statistics are the Wilcoxon Signed Rank (see chapter @(signrank)) and the Wilcoxon Rank Sum (see chapter @(ranksum)). For each, there is a unique distribution for every sample size and the p-values are themselves discrete. 19.2 Experiments involving discrete data Discrete data can be either sorted or ordered. Discrete data arises from counting objects or events, which is sorted data. They also occur when measurements taken from the experimental units are assigned discrete values, which is ordinal data. Counted objects are easy to spotthey are indivisible. They belong in one bucket or some other bucket(s). Dependent variables that have discrete values are also easy to spot. On scatter plots they exist as discrete rows. There is no continuum of values between the rows. 19.2.1 Ordered data When planning an experiment ask whether the data will be sorted into categories on the basis of nominal characteristics (eg, dead vs alive, in vs out). Or will the data be categorized on some ordered basis. For example, a score of 1 = the attribute, a score of 2 = more of the attribute, a score of 3= even more of the attribute, and so on. The sum of the discrete counts within one category of an ordered scale mean that they have more or less of some feature than do the counts in another category in the ordered group. Thus, compared to nominal sorted data, ordered data have more information. Whereas nominal events are just sorted into one bucket or another, ordered events are inherently categorized by rank. Ordered data are common in survey instruments. These are likert scales. For example, you might take a survey that asks you to rate your enthusiasm for taking a biostats course. Your choice options are discrete values, on a scale of 1 to 10, where 1= undetectable enthusiasm and 10 = giddy with excitement. Obviously, a selection of 2 implies more enthusiasm than 1, and so on up the scale. The values are ordered. Certain experimental designs generate inherently ordered data as well. For example, imagine a test that scores dermal inflammatory responses. Given a subject, * Score 1 if we dont see any signs of inflammation. * Score 2 if there was a faint red spot. * Score 3 for a raised pustule. * Score 4 for a large swollen area that feels hot to the touch. * Score 5 for anything worse than that, if it is possible! Using that ordered scale system, wed run experiments, for example, to compare a steroid treatment that might reduce inflammation compared to a vehicle control. Or wed look at a gene knockout, or CRISP-R fix, or whatever, and score an outcome response compared to a control group. After an evaluation by a trained observer, each experimental unit receives a score from the scale. In quantifying effect sizes for such studies, a mistake you often see is parametric analysis. The researcher uses parameters such as means, and standard deviations, performs t-tests, and so forth on the scored rank values. This isnt always bad, but it assumes a couple of things. First, that the distribution of the residuals for the data is approximately normal, as is the population that was sampled. Second, the scoring scale is equal interval. That is to say, the difference between inflammation scores of 1 and 2 is the same as the difference between scores 2 and 3, and so on. Suffice to say that researchers should validate whether these assumptions are true before resorting to parametric tests. If the assumptions cannot be validated nonparametric tests are perfectly suitable for such data. It happens the other way, too. Sometimes we take measurements of some variable on a perfectly good measurement scale, one that satisfies these assumptions, but then break the data out to some ordered scale. Take blood pressure, for example, which is a continuous variable, usually in standardized units of mm-Hg. We might measure its value for each subject, but on the basis of that measurement sort the subjects into ordered categories of low, medium and high pressure. Our scientific expertise drives what blood pressure values are used to define the margins of those categories. And we should have good reasons to resort to a categorization because doing so tends to throw away perfect good scalar information. Which is not a good idea, generally. It is on this ordered scale, of discrete events, rather than the original measurements on a continuous scale, that we might then run statistical tests. My point for this latter example is, of course, that not all ordered scales are based upon subjective assessments. 19.3 Experiments involving deviant Data Any scale, whether discrete or continuous, can yield deviant data. What I mean by deviant data is non-normal, skewed, has unequal variances among groups, has outliers, and is just plain ugly. When data are deviant there are two options: Use reciprocal or log transform functions to transform the data distribution into something more normal-like. Run the statistical tests intended for normal data on the transformed values. Run non-parametric statistical tests on the raw, untransformed data. These tests transform the data into a rank-based distribution. These (the sign rank and the rank sum distributions), are discrete normal-like, and are used to cough up p-values. Tossing outliers is almost always a bad and unnecessary option. Outlier tossing introduces bias when the only reason to toss it is because it is an outlier. Because they are based on ranks, the nonparametric tests condense outliers back with the rest of the variables, providing a very slick way to deal with deviant data. 19.4 Sign Test The Sign Test is a non-parametric way of saying a binomial test. An experiment is conducted on a group of subjects, who are graded in some way for either passing (+) or failing (- ) some test. Did a cell depolarize, or not? Is a stain in the cell nucleus, or not? Did the animal move fast enough, or not? Did the subject meet some other threshold youve established as a success, or not? Simply count the number that passed. Given them a + sign. The number that failed receive a - sign. Using scientific judgement, assume a probability for the frequency of successes under the null hypothesis. For example, the null might be to expect 50% successes. If after analyzing the data the proportion of successes differs from this null proportion, you may have a winner! Heres an analysis of a behavioral test, the latency to exit a dark chamber into a brief field, as an index of anxiety. Lets say that exiting a chamber in less than 60 seconds is a threshold for what wed consider non-anxious behavior. Scientific judgement sets that threshold value. Fifteen subjects are given an anti-anxiety drug. The null probability of exiting the chamber is 0.5. Which is to say there is a 50/50 chance a mouse will, at random, exit the chamber at any given time before or after 60 sec. Or put another way, under the null, neither exiting nor remaining in the chamber by 60 seconds is favored. Lets imagine we have an alarm set to go off 60 seconds after placing the subject in the chamber. When the alarm sounds, we score the subject as either (+) or (-). If the subject is out of the chamber, a success is (+). If still in the chamber after 60 s it is a failure (-). This experiment tests the null hypothesis that the probability of successes are less than or equal to 50%. Well write in our notebook this hypothesis. We are testing the null. The alternate is mutually exclusive and exhaustive of the null, so it is that the probability of successes are greater than 50%. Well also jot down our error tolerance. Our tolerance for a type1 error is 5%, or 0.05. Thus our threshold rule is that we will reject the null if the p-value of the statistical test is below this tolerance level. Finally, well write down that once the data are collected, well run the binomial test to analyze the results. The results are that twelve exited the chamber in less than 60 seconds, and 5 did not. We have not recorded times because that isnt part of the protocol. If something is not less than or equal to another, it can only be greater. Thus, we choose the greater for the alternative hypothesis argument in the binomial test function. We think on an anti-anxiety drug the probability is greater that the subjects will successfully exit the chamber. binom.test(x=12, n=15, p=0.5, alternative =&quot;greater&quot;, conf.level=0.95 ) ## ## Exact binomial test ## ## data: 12 and 15 ## number of successes = 12, number of trials = 15, p-value = 0.01758 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.5602156 1.0000000 ## sample estimates: ## probability of success ## 0.8 scoreci(x=12, n=15, conf.level = 0.95) ## ## ## ## data: ## ## 95 percent confidence interval: ## 0.5481 0.9295 19.4.1 Interpretation The effect size is 0.8, which represents the fraction of subjects that left the chamber prior to the 60 second threshold we set. The p-value is the probability of observing an effect size this large or larger, if the null hypothesis is actually true. The interpretation of the confidence interval is as follows. On the basis of this sample, there is 95% confidence the true value for the fraction of subjects that would leave the chamber prior to 60 seconds is within the range of 0.56 to 1. Note that this 95% CI range includes values, eg, 0.56, that are pretty close to the null expectation. The fact that p-value beat the threshold should be tempered by the fact that we are also 95% confident that the proportion of successes includes values that are pretty darn close to what we predicted is the null. To get a clearer sense of whats going on, here is the distribution of the binomial function for the null hypothesis. You can see that it is symmetrical. If the drug were ineffective, wed expect half to exit prior to 60 s, and the other half after 60 s. # I&#39;ll use the rbinom function to simulate data &lt;- tibble(x = c(0:15),y=dbinom(c(0:15), 15, prob=0.5)) ggplot(data, aes(x, y))+ geom_col(fill=&quot;blue&quot;) + xlab(&quot;exits before 60s&quot;) + ylab(&quot;prob of that many exits&quot;) + geom_text(aes(x=1, y=.20, label=&quot;H0 distribution&quot;)) Figure 19.1: Distribution of chamber exits in a trial sized 15 for the null for a 50% chance of chamber exits priort to 60 seconds, on a binomial model. And here is the distribution for the alternate hypothesis, given the effect size: If the drug were effective at reducing anxiety, wed expect 80% would exit the chamber prior to 60 second. data &lt;- tibble(x = c(0:15),y=dbinom(c(0:15), 15, prob=0.8)) ggplot(data, aes(x, y))+ geom_col(fill=&quot;green&quot;) + xlab(&quot;exits before 60s&quot;) + ylab(&quot;prob of that many exits&quot;) + geom_text(aes(x=1, y=.20, label=&quot;H1 distribution&quot;) ) Figure 19.2: Distribution of chamber exits in a trial sized 15 for the alternate expectation that 80% would exit prior to 60 seconds, based upon the binomial. This is to emphasize that the binomial distribution is used here as a model of the experimental data. Thus, we might also conclude that our data is consistent with a binomial distribution of 15 trials wherein the probability of event success is 80%. 19.4.2 Write Up Heres a way to write up the result. Drug treatment increases fearlessness (one-sided binomial test, p = 0.01759). The fraction exiting the chamber (0.8) is greater than expected for the null of 0.5 (95% CI = 0.55 to 1.0, Wilsons CI) 19.5 Wilcoxon Sign Rank Test for One Group The test statistic for the Wilcoxon Sign Rank is determined as follows. Assert a theoretical threshold value of the dependent variable, such as a median. Calculate the difference between the theoretical threshold value and the values recorded for each independent replicate. Rank those differences from lowest (rank = 1) to highest (rank = n). Assign a negative value to the replicate values that are less than the median. The test statistic V is the sum of the positive values. (software other than wilcox.test in R may calculate W, the sum of the positive and negative values). The test statistic V has a symmetrical discrete distribution. The cumulative function of V is psignrank and is used to compute p-values. 19.5.1 Wilcoxon Sign Rank Experimental Designs This experimental design is similar to the Sign Rank test above except in one important detail: We actually measure the time it takes for the subjects to exit the chamber. No alarm sounds to end the game at 60 sec. If subjects dawdle about and take longer than 60 sec to exit, we wait and record that time! Thus, because the data set is comprised of the actual values for the latency variable, rather than counts of a simple (+) or (-) score, the Wilcoxon Sign Rank design collects more information than does the Sign Rank Test. Lets say we have a chamber test on 7 subjects whove all been given an anti-anxiety drug. After placement in the chamber, their exit times (in seconds) are 3, 5, 8, 15, 19, 21 and 108. Based upon our scientific expertise, we assume exiting sooner than 60 s would represent fearlessness (less anxiety). This test ranks each subjects performance relative to that reference time and then signs it as negative or positive based on whether its original value was below or above the 60 second threshold. In our data, only one subject exceeded that value108 sec. Our prediction is that less anxious subjects should exit the comfort of the dark chamber sooner than would be expected. The null hypothesis is that the location\" of the null distribution is greater than or equal to 60 seconds. The alternate is the location of the distribution is less than 60 seconds, since less is everything that greater than or equal to cannot be. We run the Wilcoxon Sign Rank test to test this hypothesis using the arguments below. wilcox.test(x=c(3,5,8,15,19,21,108), mu=60, alternative = &quot;less&quot;, conf.level = 0.95, conf.int = 0.95 ) ## ## Wilcoxon signed rank exact test ## ## data: c(3, 5, 8, 15, 19, 21, 108) ## V = 4, p-value = 0.05469 ## alternative hypothesis: true location is less than 60 ## 95 percent confidence interval: ## -Inf 61.5 ## sample estimates: ## (pseudo)median ## 14 19.5.2 Interpretation The value of the test statistic, V is four. How extreme is that? It is pretty far to the left on the test statistic distribution (see below) for this particular sample size. However, the p-value is above the threshold of 5%. The evidence is not enough to reject the null hypothesis. Otherwise, the probability of making an error doing so would be 0.05469. What does V = 4 mean? It is the value corresponding to the sum of the positively signed ranks in the sample. The pseudo-median of the latency time is 14 seconds. The one-sided 95% confidence ranges from -infinity to 61.5. Heres a null signrank distribution for a sample size of 7. The values of the x scale are V, the test statistic. These are all the possible values that V can take on, given the sample size. For example, if all the signed ranks were positiveif every subject took longer than 60 sec to exit)then V would equal 28. If all subjects exited before 60 sec, then V would equal zero. Which is to say the location of this distribution is, by coincidence, also centered on 14. The value of 4 is less than this location, but not extremely-enough lower to be considered as belonging to some other distribution with a different location! The 95% confidence interval of the location on the V test statistic ranges from -infinity to 62.5. psignrank(q=4, n=7) ## [1] 0.0546875 psignrank(q=1, n=7) ## [1] 0.015625 # sign rank distribution for sample size of 7 upper &lt;- 28 n &lt;- 7 df &lt;- tibble(x=0:upper, y=dsignrank(0:upper, n)) ggplot(df, (aes(x,y)))+ geom_col() + xlab(&quot;V&quot;) + scale_x_continuous(breaks=(seq(0,upper,1))) Figure 19.3: Null sign rank test statistic distribution for sample size of 7 19.5.3 Write Up Analysis of the chamber test results indicates the anti-anxiety drug has no effect (Wilcoxon Signed Rank test, V = 4, n = 7, p= 0.0547) 19.6 Wilcoxon Mann Whitney Rank Sum Test for 2 independent groups This nonparametric test, often referred to simply as the Mann-Whitney test, is analogous to the parametric unpaired t-test. It is for comparing two groups that receive either of 2 levels of a predictor variable. For example, in an experiment where one group of m independent replicates is exposed to some control or null condition, while a second group with n independent replicates is exposed to some treatment. More generally, the two groups represent two levels of a predictor variable given to m+nindependent replicates. The rank sum is calculated as follows: The data are collected from any scale, combined into a single list, whose values are ranked from lowest (rank 1) to highest (rank m+n), irrespective of the level of the predictor variable. Let \\(R_1\\) represent the sum of the ranks for the one level of the predictor variable (eg, group2). Let \\(U_1\\) represent the number of times a data value from group2 is less than a data point from group1. \\(U_1=m*n+\\frac{m(m+1)}{2}-R_1\\) And \\(U_2=m*n-U_1\\) The rank sum test computes two test statistics, \\(U_1\\) and \\(U_2\\) that are complementary to each other. Heres another in the line of the mighty mouse experiments. 55 independent subjects were split into two groups. One group received an anti-anxiety drug and the second a vehicle as control. The subjects were run through the dark chamber test. The scientific prediction is the drug will reduce anxiety levels and so the drug treated mice will exit the chamber more quickly compared to the control mice. Since this is a parameter-less test, the null hypothesis is that location of the distribution of the drug-treated population is greater than or equal to the location of the vehicle distribution. The alternative hypothesis is that the location of the distribution of the drug-treated population is less than that of the vehicle distribution. The alternative is consistent with our scientific prediction and represents an outcome that is exclusive and comprehensive of the null! We choose the less option for the alternative argument in the test. mightymouse &lt;- read_csv(&quot;datasets/mightymouse.csv&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## Group = col_character(), ## Time = col_double(), ## Rank = col_double() ## ) # note use of naming variables with the formula syntax wilcox.test(Time ~ Group, data = mightymouse, alternative =&quot;less&quot;, conf.level=0.95, conf.int=T) ## ## Wilcoxon rank sum exact test ## ## data: Time by Group ## W = 55, p-value = 0.1804 ## alternative hypothesis: true location shift is less than 0 ## 95 percent confidence interval: ## -Inf 6 ## sample estimates: ## difference in location ## -9.8 19.6.1 Interpretation The test statistic you see in the output, \\(W\\), warrants some discussion. \\(W\\) is equal to \\(U_2\\) as defined above. By default, R produces \\(U_2\\) (labeled W!) as the test statistic. Most other software packages use \\(U_1\\), which in this case would be 88 (easy to compute in the console given \\(U_2\\)). Think of \\(W\\) as a value on the x axis of a rank sum distribution for a sample size of m+n. The rank sum distribution has a function in R called dwilcox. Here it is (note the large value this distribution can take on is m*n and the smallest is zero): df &lt;- tibble(x=0:143, y=dwilcox(0:143, 11, 13)) ggplot(df, (aes(x,y)))+ geom_col()+ xlab(&quot;W&quot;)+ scale_x_continuous(breaks=c(55, 88)) Figure 19.4: The null rank sum test statistic distribution for a sample size of m=11 and n=13 All this can seem confusing, but it is very elegant. First, the rank sums of samples, like the rank signs of samples, take on symmetrical, normal-like distributions. The greater the sample sizes, the more normal-like they become. Second, the bottom line is the same as for all other statistical tests: test statistic values at either extreme of these null distributions are associated with large effect sizes. The non-extreme-ness of the test statistic value for our sample is illustrated in that plot. Clearly, W=55, it is well within the null distribution. I calculated its symmetrical counterpart, \\(U_1\\) = 88, from the relationship above. As you can see, the value of the test statistic and 88 frame the central location of this null ranksum distribution null quite nicely: The p-value for W=55 indicates that the probability of creating a false positive by rejecting the null is 18.04%, well above the 5% type1 error threshold. So we should not reject the null given wed have a 1 in 5 chance of being wrong if we did! The effect size is in the output is the magnitude of the difference between the location parameters (pseudo-medians) of the two groups, on the scale of the original data. The 95% confidence interval indicates there is a 95% chance the difference in locations is between negative infinity and 6. Since the 95% confidence interval includes zero, the possibility exists that there is zero difference between the two locations. That provides additional statistical reasoning not to reject the null. 19.6.2 Write Up There is no difference in performance using the closed chamber test between subjects randomized to anti-anxiety drug (n=11) or to vehicle (n=13) (Mann-Whitney test, W = 55, p = 0.1804). 19.6.3 Plot This is a two group experiment. Every data point represents an independent replicate. The aes function defines the plot axis. The x-axis is the factor Group and it has two levels, Drugs and Vehicle. The y-axis is Time in seconds. It is customary to illustrate summary statistics of nonparametric data with box plots. By default these illustrate the median (mid horizontal bar), interquartile ranges (outer horizontal bars) and full ranges (verticle bars). The small dot is a value the geom_boxplot function recognizes as an outlier. It can be surpressed with an argument. It is becoming customary to show all data points. Thus, overlaying the points on a box plot shows all the data and its summary. The visualization illustrates quite well the deviant data. Nonparametric analysis handles this well. This isnt suitable for a t-test. ggplot(mightymouse, aes(x=Group, y=Time))+ geom_boxplot(width=0.3)+ geom_jitter(height=0, width = 0.1, size = 4, alpha=0.4) Figure 19.5: A minimal configuration for plotting a 2 group nonparametric experimental design. 19.7 Wilcoxon Sign Rank Test for paired groups The classic paired experimental design happens when two measurements are taken from a single independent subject. For example, we take a mouse, give it a sham treatment, and measure its latency in the chamber test. Later on we take the same mouse, give it an anti-anxiety drug treatment, and then measure its latency once again. This kind of design can control for confounding factors, like inter-subject variability. But it can also introduce other confounds. For example, what if the mouse remembers that there is no real risk of leaving the dark chamber? Pairing can happen in many other ways. A classic pairing paradigm is the use of identical twins. Individuals of inbred mouse strains are all immortal clones. Two litter mates would be identical twins and would also be, essentially, clones of their parents and their brothers and sisters from prior litters! Two dishes of cultured cells, passed together and now side-by-side on a bench are intrinsically-linked. All of these can be treated, statistically, as pairs. In this example, we take a pair of mice from each of 6 independent litters produced by mating two heterozygotes of a nogo receptor knockout. One of the pair is nogo(-/-). The other is nogo(+/+). We think the nogo receptor causes the animals to be fearful, and predict animals in which the receptor is knocked out will be more fearless. The independent experimental unit in this design is a pair. We have six pairs, Therefore, the sample size is 6 (even though 12 animals will be used!) Well measure latency in the dark chamber test. Our random variable will be the difference in latency time between the knockout and the wild type, for each pair. Heres the data, latency times are in sec units. Each animal generates two measurements and has an id value: # enter the data by hand, no need to use csv files mmko &lt;- tibble( id=c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;), knockout=c(19, 24, 4, 22, 15, 18), wildtype=c(99, 81, 70, 62, 120, 55) ) #create a long data fram to do formula arguments in wilcox test mmkotidy &lt;- pivot_longer(mmko, - id, names_to = &quot;genotype&quot;, values_to = &quot;latency&quot; ) Scientifically, we predict there will be a difference in latency times within the pairs. Specifically, the knockout will have lower times than their paired wild-type. The null hypothesis is that the difference within pairs will be greater than or equal to zero. The alternative hypothesis is the difference will be less than zero. wilcox.test(latency ~ genotype, data=mmkotidy, paired=T, conf.level=0.95, conf.int=T, alternative=&quot;less&quot; ) ## ## Wilcoxon signed rank exact test ## ## data: latency by genotype ## V = 0, p-value = 0.01563 ## alternative hypothesis: true location shift is less than 0 ## 95 percent confidence interval: ## -Inf -40 ## sample estimates: ## (pseudo)median ## -61.5 19.7.1 Interpretation Note that this is not a rank sum test as for the Mann-Whitney, but a signed rank test. So we have seen the V test statistic before. Its value of 0 is as extreme as can be had on the null distribution, as is evident in the distribution below! That happened because in each of the six pairs, the knockout had a lower latency time than its paired wildtype. All of the signed ranks were negative! In terms of position differences, it is as strong of an effect size as possible. upper &lt;- 21 n &lt;- 6 df &lt;- tibble(x=0:upper, y=dsignrank(0:upper, n) ) ggplot(df, (aes(x,y)))+ geom_col() + xlab(&quot;V&quot;) + scale_x_continuous(breaks=(seq(0,upper,1))) Figure 19.6: Null sign rank test statistic distribution when n= 6 pairs. The p-value is exactand it can never be lower, given this sample size. We can reject the null since it is below our 5% threshold and it says the probably that we are accepting a type1 error is 1.563%. The pseudo-median is in units of latency time. It represents the median for the differences in latency within the pairs. In other words, there are six values of differences, one difference value for each pair. -61.5 is the median of those 6 differences. There is a 95% chance the true median of the differences lies between negative infinity and -40. Note that the 95% CI does not include the value of zero. 19.7.2 Write up Dark chamber test latency differs markedly within pairs of knockout and wildtype subjects (Wilcoxon Signed Rank Test for pairs, n=6, V = 0, p=0.01563) 19.7.3 Plot Since this is a paired experimental design we create a dot-line-dot plot that illustrates this point. A very common mistake is for people who have a paired experiment to plot means of the two groups, or two bars, without connecting the paired data. The statistical test, in fact, is not performed on the latency values for the two groups.Group means or medians are irrelevant. Instead, it is performed on the differences within the pairs. In other words, the test is run, essentially, on sign ranked values related to the slopes of the red lines, not on the values of the dots. The plot should always reflect the experimental design. Here is a minimal concept for this. The group aesthetic is the key and definitely a trick worth remembering. If you want to really bob ross this, add color = id to the aesthetics. ggplot(mmkotidy, aes(genotype, latency, group=id))+ geom_line(color=&quot;red&quot;)+ geom_point(size=4) Figure 19.7: Paired experimental designs should always be plotted dot-line-dot. 19.8 Kruskal-Wallis The kruskal.test is a non-parametric method for comparing 3 or more treatment groups. It serves as an omnibus test for the null hypothesis that each of the treatment groups belong to the same population. If the null is rejected, post hoc comparison tests are then used to determine which groups differ from each other. A post hoc test for this purpose in base R is pairwise.wilcox.test. The PMCMRplus package has others. Documentation within the PMCMRpackage vignette provides excellent background and instructions for these tests. The Kruskal-Wallis test statistic is computed as follows. Values of the outcome variables across the groups are first converted into ranks, from high to low. Tied values are rank-averaged. The test can be corrected for large numbers of tied values. The Kruskal-Wallis rank sum test statistic is H: \\[H=\\frac{12}{n(n+1)}\\sum_{i=1}^k\\frac{R_{i}^2}{n_i}-3(n+1)\\] \\(n\\) is the total sample size, \\(k\\) is the number of treatment groups, \\(n_i\\) is the sample size in the \\(ith\\) group and \\(R_i^2\\) is the squared rank sum of the \\(ith\\) group. Under the null, \\(\\bar{R_i} = (n+1)/2\\). Because it is, effectively, the sum of a squared value, the H statistic is approximated using the \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom to produce p-values. 19.8.1 The experimental design Lets analyze the InsectSprays data set, which comes with the PMCMRplus package. This is a multifactorial experiment in which insects were counted in agricultural field plots that had been sprayed with 1 of 6 different insecticides. Each row in the data set represents an independent field plot. Do the insecticides differ? ggplot(InsectSprays, aes(spray, count))+ geom_violin(fill=&quot;blue&quot;, color=&quot;red&quot;, size = 1.5) Figure 19.8: Violin plot for counts of insects in crop fields somewhere in Kansas following treatment with different insecticides. The violin plots (modern day versions of box plots) illustrate how the groups have unequal variance. Such data are appropriate for non-parametric analysis. #insectsprays &lt;- read.csv(&quot;insectsprays.csv&quot;) kruskal.test(count ~ spray, data=InsectSprays ) ## ## Kruskal-Wallis rank sum test ## ## data: count by spray ## Kruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10 kwAllPairsConoverTest(count ~ spray, data=InsectSprays, p.adjust =&quot;bonf&quot; ) ## Warning in kwAllPairsConoverTest.default(c(10, 7, 20, 14, 14, 12, 10, 23, : Ties ## are present. Quantiles were corrected for ties. ## ## Pairwise comparisons using Conover&#39;s all-pairs test ## data: count by spray ## A B C D E ## B 1.000 - - - - ## C 5.6e-13 4.5e-14 - - - ## D 4.7e-07 3.6e-08 0.021 - - ## E 1.1e-09 8.5e-11 1.000 1.000 - ## F 1.000 1.000 2.1e-14 1.7e-08 3.9e-11 ## ## P value adjustment method: bonferroni We first do a Kruskal-Wallis rank sum omnibus test to test the null hypothesis that the locations of the insect distributions for several insecticide groups are the same. The null is rejected given the large \\(\\chi^2\\) test statistic, which has a p-value well below the threshold of 5% type1 error. Thats followed by a pairwise post hoc test with a p-value adjustment. The number of pairwise tests for 6 groups is choose(6, 2)= 15. Each pairwise test is a single hypothesis test associated with 5% type1 error risk. If we dont make a correction for doing the multiple comparisons, the family-wise type1 error we allow ourselves would inflate to \\(15 x 5% = 75%\\)! The Bonferroni adjustment is simple to understand. It multiplies every unadjusted p-value by 15, the number of comparisons made. Thus, each of the p-values in the grid is 15X larger than had the adjustment not been made. Every p-value less than 0.05 in the grid is therefore cause for rejecting the null hypothesis those two compared groups. The highest among these is the comparisons between sprays C and D, which has a p-value of 0.03977. 19.8.2 Write Up A non-parametric omnibus test establishes that the locations of the insecticide effects of the six sprays differ (Kruskal-Wallis, \\(\\Chi^2\\) = 54.69, df=5, p=1.511e-10). Posthoc pairwise multiple comparisons by the Mann-Whitney test (Bonferroni adjusted p-values) indicate the following sprays differ from each other: A v(0.00058), D(0.00117), E(0.0051), and so on 19.9 Friedman test The Friedman test is used for nonparametric analysis of three or more levels of an independent variable for experimental designs that involve repeated/related measures. These are otherwise known as block designs. The Friedman test is a nonparametric analog of the one-way repeated/related measures ANOVA. The test statistic is calculated in a multistep process from matrix-like data \\({x_{i,j}}\\). Each row, \\(n_i\\), is an independent block within which repeated/related measures are collected. Each column, \\(k_j\\) represents one level of the independent variable. The friedman.test function works as follows: The first step involves ranking response values in ascending order within each row, transforming into a matrix of ranks, \\(r_{i,j}\\). The mean ranks are derived for each column \\(\\bar r_j=\\frac{1}{n}\\sum_{i=1}^n(r_{i,j})\\). The Friedman test statistic is then \\[Q=\\frac{12n}{k(k+1)}\\sum_{j=1}^k(\\bar r_j-\\frac{k+1}{2})^2\\] Note how this is in the form of a sum of squared values. The distribution of \\(Q\\) is therefore approximately \\(\\chi^2\\). The function output is very simple: a p-value derived from the \\(\\chi^2\\) distribution. 19.9.1 Blocked experimental design Protein kinase A activity (PKA) was measured in extracts of cultured VSMC after exposure to either of four possible treatments. There are a few critical questions this experiment hopes to answer. Does isoproterenol stimulate activity? Does the presence of a GFP_PKI inhibit stimulated activity? Is that inhibitory effect absent in a negative control for GFP_PKI? Each week serves as a block, which is an independent replicate wherein all recorded measurements are more related to each other than they are to measurements in other weeks. The VSMC used each week have a common source, are treated in parallel, and are subject to vagaries peculiar to that week for the multi-step protocol. The results are entered in the code below, creating a wide format data frame: pki &lt;- tibble( replicate=c(&#39;week1&#39;, &#39;week2&#39;, &#39;week3&#39;, &#39;week4&#39;, &#39;week5&#39;), basal=c(20.0, 5.0, 2.1, 9.0, 23.7), isoproterenol=c(38.3, 9.1, 3.6, 15.5, 38.9), iso_GFP_PKI=c(27.5, 6.7, 2.7, 12.1,28.0 ), iso_GFP=c(36.1, 8.5, 4.2, 16.0,39.9 ) ) Here is a plot of the VSMC data. Note how it was converted to long format within the ggplot function: ggplot(pki %&gt;% pivot_longer(-replicate, names_to=&quot;stimulus&quot;, values_to =&quot;pka&quot;), aes(stimulus, pka, group=replicate, color=replicate) )+ geom_point(size=4)+ geom_line()+ labs(y=&quot;PKA activity, pmole/mg/min&quot;) Figure 19.9: PKA activity in VSMC, note related measures within each replicate. (#tab:Table of PKA activity data)Replicate measurements of PKA enzyme activity (pmole/mg/min) in cultured rat vascular smooth muscle cells (VSMC). replicate basal isoproterenol iso_GFP_PKI iso_GFP week1 20.0 38.3 27.5 36.1 week2 5.0 9.1 6.7 8.5 week3 2.1 3.6 2.7 4.2 week4 9.0 15.5 12.1 16.0 week5 23.7 38.9 28.0 39.9 As seen in the graph and the table, for a given predictor variable there is as much as a 10-fold difference in activity from week to week. But within each row are consistent fold-differences between treatments. The inconsistent levels of raw enzymatic activity from week-to-week make this suitable for nonparametric testing, whereas the weekly block design calls for related measures analysis. Thus the Friedman test. The Friedman test can answer this question: Do any of these treatments differ? The test does not provide information on which groups differ, serving instead as an omnibus. The test output is simply a p-value, derived from the so-called Friedman chisquare. There are a couple of options for performing the test. The first involves converting the data to a matrix, omitting the values of the replicate variable, and passing that matrix of responses into the friendman.test function. The final row names decoration is not essential for friedman.test, but does help the posthoc tests play nicer. pkiM &lt;- pki %&gt;% select(basal:iso_GFP) %&gt;% as.matrix() rownames(pkiM) &lt;- 1:5 friedman.test(pkiM) ## ## Friedman rank sum test ## ## data: pkiM ## Friedman chi-squared = 13.56, df = 3, p-value = 0.00357 The second approach is the so-called formula method, which well use a lot when we get to regression. First, convert the data to a long format data frame. pkiL &lt;- pki %&gt;% pivot_longer(-replicate, names_to = &quot;stimulus&quot;, values_to=&quot;pka&quot;) Now pass that format into friedman.test. In English, this formula reads, run the Friedman test on pka activity by stimulus levels, with related measures by replicate. friedman.test(pka~stimulus|replicate, data=pkiL) ## ## Friedman rank sum test ## ## data: pka and stimulus and replicate ## Friedman chi-squared = 13.56, df = 3, p-value = 0.00357 Because this p-value is below a 5% type1 error threshold, this result indicates that levels of the stimulus variable cause differences in PKA activities. However, posthoc comparison tests are necessary to find out which differences between stimulus levels are reliable. frdAllPairsConoverTest(pkiM, p.adjust=&quot;bonf&quot;) ## ## Pairwise comparisons using Conover&#39;s all-pairs test for a two-way balanced complete block design ## data: y ## basal isoproterenol iso_GFP_PKI ## isoproterenol 0.087 - - ## iso_GFP_PKI 1.000 0.733 - ## iso_GFP 0.056 1.000 0.489 ## ## P value adjustment method: bonferroni Youll find a handful of pairwise posthoc comparison functions like the two used in this chapter in the PMCMRplus package. They are based upon different formulas and perform differently. It is not possible to conclude that one is better than another, except those that allow for a p-value adjustment should be used. In this particular case, we have an ambiguous and underpowered data set. If you play with this data on your own machine, youll find there is ample latitude for prosperous p-hacking in the post hoc testing. For example, if the p-values are unadjusted for multiple comparisons, all of the post hoc functions yield statistical differences. But unadjusted comparisons are not very stringent. Alternately, certain post hoc tests with other p-value adjustment options yield statistical differences. The question is not which one is better? The approach to take is to run realistic simulations in advance of collecting data, learn about the options by playing with familiar variables, and choose accordingly. Write down your choices for how you you intend to post hoc once the new data is collected, and stick with that plan. 19.9.2 Interpretation Because the p-value of the Friedman test statistic is below the 5% threshold, we can reject the null that there are no differences in PKA activity caused by the four stimulus conditions. However, the post hoc multiple comparison testing, with appropriate p-value adjustment, is inconclusive. The bloody obvious evidence (as seen by inspecting the table and figure) that the GFP-PKI construct inhibits PKA is unsupported by the statistical analysis. The experiment is underpowered. There are too few independent replicates. Although there is a proverbial trending towards significance8 for two of the comparisons, neither of these speak to the most important outcome, which is to observe that GFP-PKI inhibits PKA activity. It turns out transfection inefficiency explained this failure. The data reflect that the inhibitor was working, just not in enough cells. This was overcome by a significant protocol adjustment so that more cells expressed the inhibitor.9 19.9.3 Write up Although a positive nonparametric Friedman test indicates there are differences in PKA activity among levels of the stimuli (Friedman chi-squared = 13.56, df = 3, p-value = 0.00357), no statistical differences are observed in pairwise post hoc comparisons, likely because the effects are small and the experiment is underpowered (Conover test with Holm p-value adjustment, experimentwise type 1 error threshold of 5%). 19.10 Nonparametric power calculations The function below, nonpara.pwr is configured to simulate the power of nonparametric one- and two-group comparisons using the Wilcoxon test function of R. The intended use of this function is to establish a priori the sample size needed to yield a level of power that you deem acceptable. The function simulates a long run sample results at a given sample size, running a statistical test on each. Power is simply the fraction of these tests that have p-values below a chosen threshold. This is a Monte Carlo method. The function strictly takes the argument of a sample size value and returns an experimental power that sample size generates. For example, after loading the function in your environment, typing nonparam.pwr(n=5) in the console will return the power for the test you configured. That configuration customization of the functions initialization. Do this by entering estimates for parameters of the population you expect to sample. All you need for that is a scientific hunch about the values for the outcome variable you expect your experiment will produce. 19.10.1 How it works Monte Carlo is general statistical jargon for repeated simulation. This is a technique widely used in many applications across statistics and data science. In this course, Monte Carlo power calculations means that we are mostly running repeated simulations to determine a sample size for a planned experiment. 19.10.1.1 tl;dr Simulate a random sample. Run a statistical test on the sample. Collect the p-value from that test. Repeat steps 1 through 3 ~1000 times or more. Count the p-values that are below a type1 threshold, such as 0.05. 19.10.2 Simulating response data Nonparametric tests are useful for a diverse array of data types. So it is useful to know there are many ways to mimic this diversity. Whats most important is mimicking the effect sizes (eg, differences) and variation as realistically as possible. Using the random number generator functions of the various probability distributions in R is suitable for many cases. Bear in mind, the base distributions we cover in this course are just tip of the iceberg.10 For other problems, random sampling functions like sample can be configured to do the trick. For example, use rnorm for measured variables for which you believe you can predict means and standard deviations. Make sure your sd estimate is realistic. Most people underestimate sd. # simulate a sample size of 5 for a normally-distributed #variable with mean of 100 units and standard deviation of 25 units control.1 &lt;- rnorm(n=5, mean=100, sd=25) treatment.1 &lt;- rnorm(n=5, mean=150, sd=25) sim.set1 &lt;- tibble(control.1, treatment.1) sim.set1 ## # A tibble: 5 x 2 ## control.1 treatment.1 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 127. 158. ## 2 167. 138. ## 3 126. 187. ## 4 86.6 125. ## 5 53.9 131. Use rpois to simulate variables that represent discrete events, such as frequencies. How many depolarizations do you expect at baseline? How many more (or less) do you think your treatment will cause? #simulate 5 events, where each occurs at an average frequency of 7. control.2 &lt;- rpois(n=5, lambda=7) treatment.2 &lt;- rpois(n=5, lambda=10) sim.set2 &lt;- tibble(control.2, treatment.2); sim.set2 ## # A tibble: 5 x 2 ## control.2 treatment.2 ## &lt;int&gt; &lt;int&gt; ## 1 4 7 ## 2 6 7 ## 3 9 5 ## 4 8 14 ## 5 6 8 Use rlnorm to simulate variables that have log normal distributions, which just happens a lot in biological systems. It takes some playing to get used to simulating with rlnorm but the results can look scary realistic compared to rnorm. # simulate 5 events, from a log normal distribution control.3 &lt;- rlnorm(n=5, meanlog=0, sdlog=1) treatment.3 &lt;- rlnorm(n=5, meanlog=2, sdlog=1) sim.set3 &lt;- tibble(control.3, treatment.3); sim.set3 ## # A tibble: 5 x 2 ## control.3 treatment.3 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.78 2.68 ## 2 0.794 4.58 ## 3 1.91 7.81 ## 4 0.858 30.8 ## 5 0.427 6.65 Its even possible to simulate ordinal data, such as the outcome of likert tests. The sample() function can be configured for this purpose. Note how a probability vector argument is used to cast expected distributions of the values #simulate 6 replicates scored on a 5-unit ordinal scale, #where 1 is the lowest level of an observed outcome and 5 is the highest. control.4 &lt;- sample(x=c(1:5), size=6, replace=T, prob=c(.70, .25, .05, 0, 0)) treatment.4 &lt;- sample(x=c(1:5), size=6, replace=T, prob=c(0, 0, 0.15, 0.70, 0.15)) results &lt;- tibble(control.4, treatment.4); results ## # A tibble: 6 x 2 ## control.4 treatment.4 ## &lt;int&gt; &lt;int&gt; ## 1 2 4 ## 2 1 4 ## 3 1 5 ## 4 1 4 ## 5 1 4 ## 6 2 3 If a more sophisticated simulation of ordinal data is needed, youll find it is doable but suprisingly not trivial. Please recognize that simulating data for these functions mostly depends upon your scientific understanding of the variables. You have to make scientific judgments about the values your variable will take on under control and treatment conditions. To get started statistically, go back to the basics. Is the variable discrete or continuous. Is it measured or ordered or sorted? What means and standard deviations, or frequencies, or rank sizes should you estimate? Those are all scientific judgments. Either select the values you predict will occur, OR enter values that you believe would be a minimal, scientifically meaningful effect size. 19.10.3 An example Lets say we study mouse models of diabetes. In non-diabetic control subjects, mean plasma glucose concentration is typically ~100 mg/dL and the variability tends to be quite low (15 mg/dL). Most everybody in the field agrees that average blood glucose concentration of 300 mg/dL represents successful induction of diabetes in these models. However, experience shows these higher levels of glucose are associated with considerably greater variability (SD=150 mg/dL) than under normal states. Large differences in variability between two groups are called heteroscedaticity. The presence of heteroscedaticity can preclude the use of parametric statistical tests since it raises type1 error risk. Thus, nonparametric testing would be used when expecting such outcomes. Lets say you want to test a new idea for diabetes induction. What sample size would be necessary, assuming an nonparametric testing and an unpaired design, to reliably detect a difference between these two groups at 80% power or better? The function below calculates the power of experiments that might be expected to generate such results, at given sample sizes. 19.10.4 nonpara.pwr Running this first code chunk reads the custom function into the environment. Theres no output, yet. Youll see some code later that uses this function. Read it line-by-line to see if you can follow the logic. Or read the comments which explain it. nonpara.pwr &lt;- function(n){ #Intitializers. Place expected values for the means and standard deviations of two groups to be compared here. These values will be called by the function a few lines below m1= 100 #mean of group 1 sd1= 10 #standard deviation of group 1 m2= 300 #mean of group 2 sd2= 150 #standard deviation of group 2 # the monte carlo just repeats the random sampling i times. It runs a t-test on each sample, i, grabs the p-value and places it in a growing vector, p.values[i] ssims=1000 #The function below produced p-values. This empty vector has to exist before the first p-value is generated. It will be filled successfully with p-values as the repeat component below...repeats. p.values &lt;- c() #This repeat function is a function inside a function. It repeats the simulation ssims times, collecting a p-value each time, and depositing that new p-value in our growing vector. Importantly, change the arguments for the statitical test to suit your experimental design. Want to simulate a &quot;greater&quot; hypothesis? Change it here. What about a paired model? Change it here, not after you run an experiment. i &lt;- 1 repeat{ x=rnorm(n, m1, sd1); # random data generator motifs y=rnorm(n, m2, sd2); p &lt;- wilcox.test(x, y, paired=F, alternative=&quot;two.sided&quot;, var.equal=F, conf.level=0.95)$p.value p.values[i] &lt;- p if (i==ssims) break i = i+1 #This calculates the power from the values in the p.value vector. pwr &lt;- length(which(p.values&lt;0.05))/ssims } return(pwr) } To get some output, pass a sample size of n= 5 per group into the function. # test it! set.seed(1234) nonpara.pwr(5) ## [1] 0.626 At this seed, the power is 62.6% for a sample size of 5 per group. YMMV if on a Mac. A slightly higher power would be desirable. Change the value of n in the test it code chunk to see whats necessary to get 80% power. Or, more easily, just run nonpara.pwr over a range of sample sizes, plotting a power vs sample size curve. There are a couple of keys here. First, make a simple dataframe with only one variable, a bunch of n values. #frame is a data frame with only one variable, sample size frame &lt;- tibble(n=2:50) Second, use Rs apply function to, row by row, calculate power for each value of n. This will take some time to run. Hopefully just a few seconds if you have a decent machine. #data is a data frame with two variables, sample size and a power value for each data &lt;- bind_cols(frame, power=apply(frame, 1, nonpara.pwr)) Now plot it out! #plot ggplot(data, aes(n, power))+ geom_point() + scale_y_continuous(breaks=c(seq(0, 1, 0.1)))+ scale_x_continuous(breaks=c(seq(0,50,2))) Figure 19.10: A power curve for simulated Wilcox testing for an antidiabetic drug effect. This result shows that a sample size of 7, 8, or 9 per group would give ~85% power. Thus, a sample size of 7 per group would be the minimal size necessary to test the two-sided null that there is no difference between the groups, at a 95% confidence level. Notice how the power is discrete for the early runs? Thats a byproduct of the discrete nature of nonparametric sampling distributions. Think about the general way this Monte Carlo technique worked: simulate, test, repeat that many times, count up the p-values. It shouldnt be much of a leap for you to realize this approach can be run for any experimental design. If you know what experiment you want to run, you can simulate how it will work thousands of times in just a few seconds. Minutes at the worst. 19.11 Summary If youre used to comparing means of groups, nonparametrics can be somewhat disorienting. There are no parameters to compare! And the concept of location shifts or vague differences seems rather abstract. The tests transform the values of experimental outcome variables into either sign rank or into rank sum units. That abstraction can be disorientating, too. But it is important to recognize that sign ranks and rank sum distributions are approximately normal. Therefore, perhaps its best to think of nonparametrics as a way to transform non-normal data into more normal data, from which predictable p-values can be derived. The nonparametrics are powerful statistical tests that should be used more widely than they are. Power analysis is really important when designing experiments. It allows you to determine a proper sample size to give your idea the severe test it deserves. The process can also help you evaluate the feasibility and even cost of an experiment. Are there enough mice in the world to test your idea? Will you still be young after doing your laborious protocol on as many replicates as the power analysis suggests is necessary? Embrace the Monte Carlo technique if you agree it is better to try to work out these questions over the course of a few hours coding in R, rather than jumping into a rabbit hole and hoping for an exit. http://www.academiaobscura.com/still-not-significant/ http://molpharm.aspetjournals.org/content/54/3/514 For a comprehensive list of distribution functions in R see https://cran.r-project.org/web/views/Distributions.html "],["signrank.html", "Chapter 20 Signed Rank Distribution 20.1 Transformation of data into sign ranks 20.2 Rs Four Sign Rank Distribution Functions", " Chapter 20 Signed Rank Distribution The signed rank per se represents a type of data transformation. Experimental data are first transformed into signed ranks (see below). When signed ranks for a group of data are summed that sum serves as a test statistic. The sign rank test has a symmetrical normal-like distribution, which is discrete rather than continuous. In R, when using the wilcox.test to evaluate data from one sample or for paired sample nonparametric designs, the sums of only the postive signed ranks are calculated as the test statistic \\(V\\). The distribution of \\(V\\) is the focus of this document. There is a unique sign rank sum distribution for sample size \\(n\\). These represent the distributions that would be expected under the null hypothesis. In practice, values of \\(V\\) that are on the extreme ends of these distributions can have p-values that are so low that we would reject the null \\(V\\) distribution as a model for our dataset and accept an alternative. The use of \\(V\\) as a test statistic and its null distributions can be a bit confusing if youve worked with other software to do nonparametric hypothesis testing. For example, Prism reports \\(W\\) as a test statistic for sign rank-based nonparametric experimental designs. On that platform, \\(W\\) is the sum of all signed ranks, not just the positives. The center location of \\(W\\) as a sign rank test statistic is at or near zero on the \\(W\\) distributions, whereas zero is the lowest possible value for \\(V\\). Although calculated slightly differently, \\(W\\) and \\(V\\) are equivalent ways to express the same relationship between the values within the original dataset. 20.1 Transformation of data into sign ranks 20.1.1 For a one group sample A one group sample represents a single vector of data values. Let a sample of size \\(n\\) for a variable \\(X\\) take on the values \\(x_1, x_2, ... x_n\\). The location of this vector will be compared to a location value \\(x = \\mu\\), such that \\(z_i=x_i-\\mu\\) for \\(i=1\\ to\\ n\\). To transform the sample to a vector of signed ranks, first rank \\(|z_i|\\) from lowest to highest. \\(R_i\\) is the rank value for each value of \\(|z_i|\\). \\(\\psi = 0\\) if \\(z_i &lt; \\mu\\), or \\(\\psi= 1\\) if \\(z_i &gt; \\mu\\). The signed ranks of the original vector values are therefore \\(R_1\\psi, R_2\\psi,...R_n\\psi\\). 20.1.2 For a paired sample The differences between paired groups in an experimental sample also represent a single vector of data values, which explains why a sign rank-based test are used, rather than a rank sum test. A sample of \\(n\\) pairs of the variables \\(X\\) and \\(Y\\) has the values \\((x_1,y_1); (x_2,y_2); ... (x_n,y_n)\\). The difference between the values of \\(X\\) and \\(Y\\) will be compared to zero. For \\(i=1\\ to\\ n\\), the difference is \\(z_i=x_i-y_i\\). The sign rank transformation of \\(z_i\\) for each pair is performed as above. 20.1.3 The sign rank test statistic in R The sign rank test statistic for both the one- and two-sample cases is \\(V = \\sum_i^{n}R_i\\psi_{&gt;0}\\), which has a symmetric distribution ranging from a minimum of \\(V = 0\\) to a maximum of \\(V = \\frac{n(n+1)}{2}\\). This test statistic is produced by the wilcox.test for one-sample and paired-sample expriments. Again, \\(V\\) is the sum of the positive signed-ranks only. Other softer produces a test statistic \\(W\\), which is the sum of all the signed ranks 20.1.3.1 More about the test statistic The expectation of \\(V\\) (ie, its median) when the null hypothesis is true is \\(E_0(V)=\\frac{n(n+1)}{4}\\) and its variance is \\(var_0(V)=\\frac{n(n+1)(2n+1)}{24}\\) The standardized version of \\(V\\) is \\(V^*=\\frac{V-E_0(V)}{var_0(V)}\\). As \\(n\\) gets large, \\(V^*\\) asymptotically approaches a normal distribution \\(N(0,1)\\). Zero and tied values of \\(|Z_i|\\) occur in datasets for both the one group and paired group transformations of raw data. When values of \\(|Z|=0\\) occur, they are discarded in the calculation of \\(V\\) and the \\(n\\) is readjusted. The integer value of ranks for tied \\(|Z&#39;s|\\) are averaged. Although these ties dont change \\(E_0(V)\\), they do reduce the variance of \\(V\\). \\(var_0(V)=\\frac{n(n+1)(2n+1)-\\frac{1}{2}\\sum_{j=1}^{g}t_j(t_j-1)(t_j+1)}{24}\\) 20.2 Rs Four Sign Rank Distribution Functions 20.2.1 dsignrank The function dsinerank returns a probability value given two arguments: \\(x\\) is an integer value, meant to represent \\(V\\), which is the value of the sign rank test statistic; and \\(n\\) is the sample size of either a one-sample or a paired experiment. The probabilities for individual values of \\(V\\) are sometimes useful to calculate. For example, the probability of obtaining \\(V=3\\) for an \\(n\\)=10 experiment is: dsignrank(3, 10) ## [1] 0.001953125 More ofen it is useful to visualize the null distribution of the sign rank test statistic over a range of values. Heres a distribution of \\(V\\) for an experiment of sample size 10. n &lt;- 10 #number of independent replicates in experiment max &lt;- n*(n+1)/2 df &lt;- data.frame(pv=dsignrank(c(0:max), n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p(V)&quot;) 20.2.2 psignrank This is the cumulative distribution function for the sign rank test statistic. \\(q\\) is an integer to represent the expected value of \\(V\\), and \\(n\\) is the sample size. Given these arguments, psignrank will return a p-value for a given test statistic value. psignrank returns the sum of the probabilities of \\(V\\) over a range, and is reversible using the lower.tail argument. Here we use psignrank to generate the p-value when \\(V\\)=3 and \\(n\\)=1. Then we show the sum of the dsignrank output for \\(V\\)=0:3 is equal to psignrank. Finally, the symmetry of the distribution is illustrated: psignrank(3, 10, lower.tail=T) ## [1] 0.004882812 sum(dsignrank(c(0:3), 10)) == psignrank(3, 10, lower.tail=T) ## [1] TRUE psignrank(51, 10, lower.tail=F) ## [1] 0.004882812 psignrank(51, 10, lower.tail=F)==psignrank(3, 10, lower.tail=T) ## [1] TRUE The distributions of psignrankfor its lower and upper tails: n &lt;- 10 max &lt;- n*(n+1)/2 df &lt;- data.frame(pv=psignrank(c(0:max), n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p-value for V&quot;) df &lt;- data.frame(pv=psignrank(c(0:max), n, lower.tail=F)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p-value for V&quot;) 20.2.3 qsignrank This is the quantile signrank function in R. If given a quantile value (eg, 2.5%) and sample size \\(n\\), qsignrank will return the value of the test statistic V. For example, here is how to find the critical limits for a two-sided hypothesis test for an experiment with 10 independent replicates, when the type1 error of 5% is evenly distributed to both sides: qsignrank(0.025, 10, lower.tail=T) ## [1] 9 qsignrank(0.025, 10, lower.tail=F) ## [1] 46 Interpretation of critical limits output: When the null two-sided hypothesis is true, \\(9\\le V\\le46\\). In other words, the null hypothesis would not be rejected if an experiment generated a \\(V\\) between 9 and 46. Alternative interpretation: \\(p&lt;0.05\\) when \\(9&gt;V&gt;46\\). In other words, a two sided null hypothesis would be rejected if it generated a \\(V\\) below 9 or greater than 46. And here are the critical limits for one-sided hypothesis tests for an experiment with 10 independent replicates, when the type1 error is 5%. Notice how the critical limits differ between one-sided and two-sided tests: qsignrank(0.05, 10, lower.tail=T) ## [1] 11 qsignrank(0.05, 10, lower.tail=F) ## [1] 44 Standard interpretion: For one-sided type1 error of 5%, When the null hypothesis is true, \\(V\\ge11\\), or \\(V\\le44\\). Alternative interpretation: \\(p &lt; 0.05\\) when \\(V&lt; 11\\) or \\(V&gt;44\\) Here is a graphical representation of the qsignrank function. Note the stairstep pattern, characteristic of discrete distributions: n &lt;- 10 x &lt;- seq(0,1,0.01) df &lt;- data.frame(v=qsignrank(x, n, lower.tail=T)) ggplot(df, aes(x=x, v)) + geom_point() + xlab(&quot;p-value&quot;) + ylab(&quot;V&quot;) 20.2.4 rsignrank This function is used to simulate random values of the test statistic for null distributions for \\(V\\). Heres a group of 5 random values from a distribution for a sample size of 10. The output values represent the sum of the positive sign ranks, aka the \\(V\\) test statistic values that would be generated from doing 10 different random experiments of this sample sizeif the null hypothesis were true: rsignrank(5, 10) ## [1] 35 30 35 43 24 Lets simulate 1,000,000 such experiments. And then lets count the number of extreme values of \\(V\\) that would randomly occur in that number of experiments. We know from the qsignrank function that 11 is the critical value for a one-sided test at \\(\\alpha=0.05\\) for a sample size of 10. Thus,our significance test for each of the 1,000,000 random experiments is to ask whether \\(V\\) is below the critical value of 11 for that sample size. Given weve set a 5% false positive rate, wed expect to see around 50000 false positives (5% of 1,000,000): sim &lt;- rsignrank(1000000, 10) test &lt;- sim&lt;11 length(which(test==TRUE)) ## [1] 42334 If you ran that chunk repeatedly youd come up with something around 4.2% each time. Why is it not exactly 5%? What do you think that means? Are there experimental sample-sizes where the type1 error would be even further or closer to 5%? "],["ranksum.html", "Chapter 21 Rank Sum Distribution 21.1 Rs Four Sign Rank Distribution Functions", " Chapter 21 Rank Sum Distribution The Wilcoxon rank sum per se represents a type of data transformation. The transformation is then used to calculate the nonparametric test statistic \\(W\\) for the Wilcoxon Test for independent two group samples, which is equivalent to the Mann-Whitney test. The distribution of \\(W\\) is discrete but normal-like. Given two groups for comparison, such as a control population, \\(X\\) vs a treatment population, \\(Y\\). Under the null hypothesis, the distributions of the values of \\(X\\) and \\(Y\\) are equal. Any effect due to treatment can be defined as \\(\\Delta = E(Y)-E(X)\\), where \\(E(Y)\\) and \\(E(X)\\) are the averages for the treatment and control effects, respectively. Under the null hypothesis, \\(\\Delta = 0\\). 21.0.1 Transformation of data into rank summs \\(X\\) has a sample size \\(m\\) and \\(Y\\) has a sample size \\(n\\) and the total sample size is \\(N=m+n\\). Combine all values of \\(X\\) and \\(Y\\) and rank them from smallest to largest. If \\(S_1\\) is the rank of \\(y_1,..,S_n\\) then W is the sum of the ranks assigned to the \\(Y\\) values: \\(W=\\sum_{j=1}^nS_j\\) 21.0.2 The sign rank test statistic in R When the null is true, the average of W is \\(E_0(W)=\\frac{n(m+n+1)}{2}\\) and the variance is \\(var_0(W)=\\frac{mn(m+n+1)}{12}\\). \\(W\\) can take on the values from zero to \\(mn\\). 21.1 Rs Four Sign Rank Distribution Functions 21.1.1 dwilcox Given a value of \\(W\\) and group sample sizes \\(m\\) and \\(n\\), dwilcox will return the value of the probability for that \\(W\\). For example, here is the probability of getting a \\(W\\) of exactly 10 with group sizes of 5 and 6: dwilcox(10, 5, 6) ## [1] 0.04978355 That probability is NOT a p-value. The p-value would be the sum of the probabilities returned from the dwilcox function for the range of \\(W\\) from 0 to 10. Given the range of possible values for \\(W\\) its distribution can be plotted for any combination of group sizes. We can see that a value of 10 for \\(W\\) is left-shifted, but not too extreme. m &lt;- 5 #number of independent of group 1 n &lt;- 6 #number of independent replicates of group 2 max &lt;- m*n df &lt;- data.frame(pv=dwilcox(c(0:max), m, n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p(W)&quot;) 21.1.2 pwilcox The pwilcox function returns a p-value when given \\(W\\) along with the sample sizes \\(m\\) and \\(n\\) corresponding to the two groups. Thus, the probability of obtaining a \\(W\\) value of 10 or less with group sample sizes of 5 and 6 is: pwilcox(10, 5, 6, lower.tail=T) ## [1] 0.2142857 The probability of obtaining a \\(W\\) value of 10 or more with group sample sizes of 5 and 6 is: pwilcox(10, 5, 6, lower.tail=F) ## [1] 0.7857143 The relationship of pwilcox to dwilcox is as depicted here there is a slight inequality between the two functions somewhere beyond 10 significant digits, thus the rounding: round(sum(dwilcox(c(0:10), 5, 6)), 10) == round(pwilcox(10, 5, 6), 10) ## [1] TRUE Here are the lower and upper tailed cumulative distributions of the Wilcoxon distribution: m &lt;- 5 n &lt;- 6 max &lt;- m*n df &lt;- data.frame(pv=pwilcox(c(0:max), m, n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p-value for W&quot;) df &lt;- data.frame(pv=pwilcox(c(0:max), m, n, lower.tail=F)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p-value for W&quot;) 21.1.3 qwilcox This is the quantile signrank function in R. If given a quantile value (eg, 2.5%) and sample sizes \\(m\\) and \\(n\\), qwilcox will return the value of the corresponding test statistic \\(W\\). For example, this can be used to find the critical limits for a two-sided hypothesis test for an experiment with 10 independent replicates, when the type1 error of 5% is evenly distributed to both sides: qwilcox(0.025, 5, 6, lower.tail=T) ## [1] 4 qwilcox(0.025, 5, 6, lower.tail=F) ## [1] 26 Interpretation of critical limits output: When the null two-sided hypothesis is true, the values of \\(W\\) are \\(4\\le V\\le26\\). In other words, the null hypothesis would not be rejected if an experiment generated a \\(W\\) between 4 and 26. Alternative interpretation: \\(p&lt;0.05\\) when \\(4&gt;V&gt;26\\). In other words, a two sided null hypothesis would be rejected if it generated a \\(W\\) below 4 or greater than 26. And here are the critical limits for one-sided hypothesis tests for an experiment with 10 independent replicates, when the type1 error is 5%. Notice how the critical limits differ between one-sided and two-sided tests: qwilcox(0.05, 5, 6, lower.tail =T) ## [1] 6 qwilcox(0.05, 5, 6, lower.tail = F) ## [1] 24 Thus, having obtained a \\(W\\) of less than 6 or 24 and greater we would reject the null hypothesis. Do notice the distribution for these two sample sizes lacks perfect symmetry: pwilcox(6, 5, 6) ## [1] 0.06277056 pwilcox(24, 5, 6, lower.tail=F) ## [1] 0.04112554 The quantile distribution of \\(W\\) is depicted below: m &lt;- 5 n &lt;- 6 x &lt;- seq(0,1,0.01) df &lt;- data.frame(w=qwilcox(x, m, n, lower.tail=T)) ggplot(df, aes(x=x, w)) + geom_point() + xlab(&quot;p-value&quot;) + ylab(&quot;W&quot;) 21.1.4 rwilcox We would use rwilcox to generated random values of \\(W\\) from null distributions. Here are 7 random values of \\(W\\) for experiments involving sample sizes of \\(m=5\\) and \\(n=6\\) rwilcox(7, 5, 6) ## [1] 17 19 10 22 6 15 17 Lets simulate 1,000,000 such experiments. And then lets count the number of extreme values of \\(W\\) that would randomly occur in that number of experiments. We know from the qwilcox function that 24 is the critical value for a one-sided test at \\(\\alpha=0.05\\). Thus, our significance test for each of the 1,000,000 random experiments is to ask whether \\(W\\) is equal to or greater than 24. Recall, the cumulative distribution function indicates the p-value for a one-sided test of a \\(W=24\\) is: pwilcox(24, 5, 6, lower.tail=F) ## [1] 0.04112554 Given weve set a 5% false positive rate, wed expect to see around 50000 false positives (5% of 1,000,000) by simulation of \\(W=24\\) under that scenario: sim &lt;- rwilcox(1000000, 5, 6) test &lt;- sim&gt;=24 length(which(test==TRUE)) ## [1] 62199 In this instance, we see a much higher number of false positive 6.2% than the expected number of 5%! Why? "],["ttests.html", "Chapter 22 The t-tests 22.1 Assumptions for t-tests 22.2 The t Statistic 22.3 One sample t tests 22.4 Unpaired t tests 22.5 Paired t tests 22.6 t Test Hypotheses 22.7 One-sided or two-sided?? 22.8 Confidence Intervals 22.9 Reliability of small samples 22.10 Running the analysis in R 22.11 Plotting t Tests 22.12 t Test Power 22.13 Summary", " Chapter 22 The t-tests Easily, in the biomedical science world t-tests are the most used and abused of all of statistical tests. The t-tests are for hypotheses comparing responses of either one group to a nominal standard or two groups to each other, or paired measures within replicates. Use the t-tests when the dependent variables are continuous measured data and when two or fewer levels of discrete (factorial) independent variable are tested. See Chapter 8 to understand what is meant by continuous measured data. A common misuse of t-tests are for experiments where the dependent variables are discrete sorted or ordered values. The menagerie of ANOVA posthoc tests go by various names, but most are fundamentally close cousins of t-tests. 22.1 Assumptions for t-tests The following assumptions should be checked to use t-tests properly. 22.1.1 Strong assumptions, t-tests are invalid if these not met: The replicate data arise from a randomized process. Each sample replicate is independent of every other sample replicate. A randomization process in the experimental protocol ensures that the distribution of residual error across the groups is truly random. Dont confuse a measurement with a replicate. Paired designs have two measurements from one replicate. These two measurements are not independent. See Chapter 7. 22.1.2 Weak assumptions: t-tests are less robust when these are not met: The residual error of the measurements for each group is approximately normally distributed. The variances of the compared groups are equivalent. 22.1.3 On random residual error We have arrived at the simplest application of the linear model. Wherein one level \\(x_1\\) or two levels \\(x_1,x_2\\) of the independent variable \\(X\\) predicts the expected response of the dependent variable \\(Y\\) as \\[\\begin{equation} E(Y) = \\beta_0 +\\beta_1X+\\epsilon \\tag{22.1} \\end{equation}\\] The focus of t-testing is to estimate for the value of \\(\\beta_1\\) to ask whether it differs statistically from zero. When \\(\\beta_1=0\\), a line drawn between the pair of points \\(x_1,y_1\\) and \\(x_2, y_2\\) has a slope of zero. This indicates \\(X\\) has no effect on \\(Y\\) and thus \\(E(Y)\\) equals the value of the intercept, \\(Y = \\beta_0\\). Think of \\(\\beta_0\\) as the background or basal or control level the dependent variable. \\(\\beta_1\\) is the magnitude of the change in the dependent variable caused by one unit of \\(X\\). We assume that both population parameters \\(\\beta_0\\) and \\(\\beta_1\\) have fixed valuesbut we dont know for sure what those values are (thats why they are in Greek letters). Our job as researchers is to use sampling-based experiments to estimate these fixed values. Lets say I come up with the following estimates for the betas. Then I want to run an experiment to determine values of \\(Y\\) in response to two different values of \\(X\\). I just run it as an R code model. beta0 &lt;- 1 beta1 &lt;- 2 X &lt;- c(1 , 2) Y &lt;- beta0 +beta1*X Y ## [1] 3 5 Thats a perfect model and it generates perfect data. Every time I run the chunk I get exactly the same result. Perfectly. There is no variation. Of course, data are never that perfect in real life. Real life data has noise. In statistical jargon noise is called residual error. Residual error is, simply, an accounting trick. It is a way to account for all the sources of variation in our data that our perfect model parameters fail to explain. The source of this unexplained variation could be biological, or it could be measurement error, or it could be both. Who knows? If we knew what caused it we would explain it by putting it in our model as another variable, like \\(Z\\), and another parmeter, like \\(\\beta_2\\). But we cant explain it other than to blame it on random. So we account for it in the linear model using the epsilon \\(\\epsilon\\) term. Gauss discovered that a property of continuous data is that residual error, \\(\\epsilon\\), tends to be normally distributed. The R code model is updated below to account for residual error. Notice how were adding a term to the model. This term is the random normal \\(N(\\mu=0,\\sigma=1)\\) and it represents the residual error term, \\(\\epsilon\\), from equation(22.1). If we run that model, you get a different result every timedue to the random normal term. Our only explanation for the variation is that it comes from a random normal process. This is why incorporate a protocol in our experimental design to allocate treatments using some random process! # everytime you run this chunk you&#39;ll get a different outcome Y &lt;- beta0 +beta1*X + rnorm(length(X)) Y ## [1] 2.882296 4.819684 22.1.4 On the normality assumption The normality assumption for t-testing (and other parametric statistical testing) is really about the normality of the residual error in the sample. All parametric models have a random normal term \\(\\epsilon\\) to account for that which we cannot yet explain with other model parameters. \\(\\epsilon\\) modifies the influence of the beta coefficients on the values of \\(Y\\) generated by the model. \\(\\epsilon\\) accounts for the unexplained variation in the data. 22.1.4.1 Usually it is hard to verify normality Pragmatically, assessing whether the distribution of \\(\\epsilon\\) is normal from small sample sizes is difficult. Which makes it hard to validate the weaker two assumptions above. As a researcher, particularly a small sample researcher, there are two ways to approach these weaker assumptions. The first is to just assume normality and not worry about it. This tends to be a safe bet when working with measured variables, particularly when were confident that we are working within their linear range. Further, at the core of the t-test and other parametric statistics are frequentist assumptions based upon the distributions of means. The distributions of means ARE normally distributed via the central limit theorem. The second approach is to run normality tests and then act on those test results. That action is either a data transformation or a data exclusion decision. Normality tests such as the Shapiro-Wilk (shapiro.test), tests the null hypothesis that the values of a random variable are normally-distributed. Levenes test car::leveneTest is useful to test the homogeneity of variance assumption. 22.1.4.2 On dealing with outliers If the nulls of the Shapiro or Levene tests are rejected, something must be done. The data might be normalized through log or reciprocal transformation. Then run the t-tests on these transformed values. If the measurement data include values of zero, something must be done that in the transformation protocol. Or the test result triggers an outlier removal protocol. Alternately, the decision tree calls for reverting to nonparametric analysis with the Wilcoxon tests instead of t-tests. See Chapter @(nonparametrics) for those. Im not a fan of these normality tests. The smaller the sample size, the more suspicious I am of their utility or that I have enough information to heed the weaker assumptions. Thats mostly why I dont teach them in the course. The other big reason is that, in practice, these decision trees tend to be made up as you go rather than pre-planned protocol. Thats hacking. If you actually wrote these into the statistical design in advance, it would read pretty hackish, too. Something like this: then I will do normality tests, and if normalization doesnt fix the problem, Ill start tossing data What most are really asking from a normality test is for permission to p-hack, perhaps hoping to rescue statistical significance from an otherwise moribund result. knitr::include_graphics(&quot;images/phacking.jpg&quot;) Figure 22.1: This is p-hacking. Uncritical allegiance to t-testings weaker assumptions is frequently a gateway to an integrity crisis. 22.1.4.3 On nonparametrics The performance of t-tests compared to their nonparametric counterparts are virtually identical when the normality and homoscedasticity assumptions are met (see the right panel in the next graph). The t-tests are generally held to be robust (aka, perform well) to violations of the weaker assumptions. But YMMV. Heres an illustration for when t-tests are not robust. The panel on the left shows a scenario where the data is lognormal and the nonparametric test has far greater power than the t-test to detect true differences. Figure 22.2: Comparison of Wilcoxon test (w) power to t-tests when sampling from skewed (lognormal) and normal distributions. Whats right for you? It depends on your experimental design and the data you intend to collect (ie, how much it diverges from normality), and what you believe to be a scientifically significant result. Take some time to learn the Monte Carlo simulation method. If you expect deviant data, model the deviant data as best you can in these simulations. Compare different ways for a statistical analysis of the simulated outcome data. Do this in advance. And then go run the experiment. 22.2 The t Statistic The \\(t\\) statistic is a ratio signifying the signal to noise ratio in a sample. See Chapter 24 for the distribution functions that help understand more about its properties. The numerator of \\(t\\) is a measure for the magnitude of the average effect, whereas the denominator represents the precision by which those mean differences are determined. I show below 4 different ways to calculate \\(t\\), which correspond to three very different experimental designs (the unpaired design has two ways to calculate \\(t\\)). The decision to use one or the other t-tests is based upon how the experiment is conducted. If we understand our experimental design, well know which t-test is applicable. The three t-tests answer fundamentally different questions: One sample tests inspect the difference between a sample mean and a hypothetical mean. Unpaired tests inspect the difference between the means of two groups. The paired t test inspects the mean of the differences between paired observations. The most common mistake made by statistically naive researchers is running unpaired t-tests on experiments that have a paired design. 22.3 One sample t tests This is for analyzing experiments comparing the mean response \\(\\bar y\\) evoked by a single level of a predictor variable to a hypothetical population mean of that response, \\(\\mu\\). The group is comprised of \\(n\\) independent replicates, one measurement per replicate. The mean of the replicate measurements is \\(\\bar y\\). The one sample t test has \\(n-1\\) degrees of freedom. \\[\\begin{equation} t=\\frac{\\bar y-\\mu}{\\frac{sd}{\\sqrt n}} \\tag{22.2} \\end{equation}\\] 22.3.1 Example Everybody accepts that the mean normal blood glucose level in mice is 100 mg/dl. In an experiment blood glucose is measured in a random sample of 12 mice in which the insulin receptor gene has been disrupted. A one-sample t-test compares the mean of these measured glucose values to the nominal value, \\(\\mu\\)=100 mg/dl. The experiment has 12 independent replicates and the t-test has 11 degrees of freedom. 22.4 Unpaired t tests This test is used for experiments in which the replicate measurements are all independent from each other. There are two ways to calculate \\(t\\) for these designs. 22.4.1 Students t-test The unpaired Students t-test compares the difference between the mean responses for each of two levels, A and B, of an independent variable. The design is comprised of a total of \\(n_A + n_B\\) observations. A degree of freedom is lost for each of the two calculated group means, thus the t-statistic has \\(n_A + n_B-2\\) degrees of freedom. \\(s_p\\) is the pooled standard deviation of the sample. \\(s^2_p\\) is the pooled variance of the sample. \\[\\begin{equation} t=\\frac{\\bar y_A-\\bar y_B}{\\sqrt{\\frac{s^2_p}{n_A}+\\frac{s^2_p}{n_B}}} \\tag{22.3} \\end{equation}\\] The pooled variance is calculated using the sum of the squared deviates \\(SS_A\\ and\\ SS_B\\) from each group as follows: \\[\\begin{equation} s^2_p=\\frac{SS_A+SS_B}{df_A+df_B} \\tag{22.4} \\end{equation}\\] where \\(df_A=n_A-1\\) and \\(df_B=n_B-1\\). The denominator of the \\(t\\) ratio is the standard error of the test. The standard error represents the tests precision but note how it represents the standard error for two means. Inspection of the pooled variance equation and how it factors into the calculation of the test statistic should give you some indication for how unequal sample sizes or unequal variance between groups can be problematic. When unbalanced the calculation can be biased to one of the groups. 22.4.2 Welchs t-test Welchs test differs from Students t-test and so will generate different output. First, Welch does not use a pooled variance term. \\[\\begin{equation} t=\\frac{\\bar y_A-\\bar y_B}{\\sqrt{\\frac{s^2_A}{n_A}+\\frac{s^2_B}{n_B}}} \\tag{22.5} \\end{equation}\\] Second, the degrees of freedom \\(\\nu\\) for the null \\(t\\) distribution of the test is calculated differently: \\[\\begin{equation} \\nu=\\frac{(\\frac{s^2_A}{n_A}+\\frac{s^2_B}{n_B})^2}{\\frac{s^4_A}{n^2_A\\nu_A}+\\frac{s^4_B}{n^2_b\\nu_B}} \\tag{22.6} \\end{equation}\\] 22.4.2.1 Which is better? In Rs t.test function the default option var.equal=F calls for Welchs test. When set to TRUE, it runs the unpaired Students t-test. Under some conditions, the two tests can yield quite different values for \\(t\\) and thus \\(p-values\\). The Welch t-test is held to perform better under a variety of sampling conditions, but such generalities are not as useful as you might think. YMMV. When conducting Monte Carlo analysis during the experimental design phase, particularly when planning unequal sample sizes or for unequal variances, it is useful to compare how Welch and Students perform against each other. Waiting until after the data have arrived to see which performs better is p-hacking. Figure it out in advance. 22.4.3 Example Irrespective of choosing to analyze the data using Welchs or Students t-test, the experimental design is the same. A mouse model of diabetes has been established. It is used to test whether novel blood glucose lowering drugs work. A sample of 28 mice are randomly assigned to treatment with either a placebo \\(n\\)=14 or with a drug\\(n\\)=14. Blood glucose measurements are taken from all mice. Every measurement is independent of all other measurements. There are two means: \\(\\bar y_{placebo}\\) and \\(\\bar y_{drug}\\) . The numerator of the t-statistic is the difference between these two group means. The denominator will be either Students or Welchs way of calculating the standard error \\(se\\). The degrees of freedom will differ also, depending upon whether Students or Welchs test is used. 22.5 Paired t tests This test is used when measurements are intrinsically-linked. Each of \\(n\\) replicates is exposed to both levels, A and B, of a predictor variable. There are \\(n\\) pairs of measurements in the design. The mean difference \\(\\bar d\\) of the paired responses is compared to the value of zero. \\(sd_d\\) is the standard deviation of the \\(n\\) differences. The experiment has \\(n-1\\) degrees of freedom, because only the mean of the differences is taken. \\[\\begin{equation} t=\\frac{\\bar d}{\\frac{sd_d}{\\sqrt n}} \\tag{22.7} \\end{equation}\\] 22.5.1 Example A random sample of 14 diabetic mice is selected from a colony. In a classic before and after design, for each mouse a blood sample is drawn after administration of a placebo. Blood glucose is measured in the sample. Each mouse is next administered a drug. Later, a second blood sample is drawn, in which glucose is measured again. There are 28 measurements. The difference in blood glucose between the two measurements is calculated within each mouse. There are 14 differences. The mean of those differences, \\(\\bar y_{diff}\\) is the numerator of the t statistic. The denominator is the standard error of the mean of the differences, \\(semd\\). The total sample size \\(n\\)=14 and the test has 13 degrees of freedom. Each pair of measurements stands as one independent replicate. 22.6 t Test Hypotheses The choice of t-test hypotheses depends upon the experimental design and the scientific question at hand. In R, the hypothesis is set via options in the alternative argument of the t.test function. Since t-tests are parametric, hypotheses are stated on the basis of the statistical parameters of the sampled population. In this case, the means of the samples are meant to infer the sampled population, so we revert to Greek notation. To put this another way, that the sample shows a difference is a mathematical fact. We dont need a statistical test to tell us that. The test helps us to infer from the sample of differences whether the populations that were sampled differ. One-sided hypotheses predict the direction of an effect. For example, the response to treatment will be greater than placebo. Or, the response to treatment will be less than placebo. Two-sided hypothesis do not predict the direction of an effect: The response to treatment will differ from control, either higher or lower. Therefore, we use a one-sided hypothesis if we think our treatment will go in a specific direction. We choose a two-sided test when were not willing to bet on an effects direction. This matters because at the 95% confidence level the threshold value of the t statistic will be lower for a one-sided test ( eg,qt(0.05, 2) than for a two-sided test (qt(0.05/2, 2)) given the same data. Put another way, the significance threshold will always be a higher bar to cross for a two-sided hypothesis. For a one-sided test, all of the 5% cumulative probability is on one side of the distribution. For a two-sided test, that 5% is evenly split to both sides. Therefore, two-sided tests are slightly more stringent. 22.7 One-sided or two-sided?? There is a school of thought that all testing should be two-sided. One reason for this is the enhanced stringency. Another is that confidence intervals of the estimate are more readily interpretable because they have both upper and lower bounds. Youll soon see (or noticed when we were in the nonparametrics) that one-sided tests have only one bound, the other side being infinite. Those are harder to interpret. A third reason is that when testing one-sided and the effect goes extreme but in the direction opposite of predicted, the null cannot be rejected formally. No inference could be made, formally. On the other hand, one-sided hypotheses feel very normal. We usually have good scientific rationale for expecting a treatment to be larger (or smaller) than a control. Why shouldnt we test that statistically? To keep the stringency of the two sides, we can run the one-sided t-test at conf.level=0.975. Problem solved. To have a 95% CI with upper and lower bounds we can always calculate and report one: has mean glucose of 100 (two-sided 95% CI 90 to 100, one-sided t-test, p = 0.024) We use the p-value for inference and the CI to report on the accuracy of the point estimate. Thats a bit clunky, but problem solved. I dont have an easy solution for the third point. If we start an experiment testing greater but the data show a statistical difference that is less, then we should admit we are wrong. Sorry. To do otherwise is to invoke the HARK bias, hypothesizing after receiving knowledge. 22.7.1 One sample hypotheses Two sided: Use when, going into an experiment, you are not sure which direction the predictor variable will change the outcome variable relative to the population mean. \\(H_0: \\bar y = \\mu\\) \\(H_1: \\bar y \\ne \\mu\\) One sided: Use when, going into an experiment, you are confident the predictor variable will cause the outcome response to be higher than the population mean. \\(H_0: \\bar y \\le \\mu\\) \\(H_1: \\bar y &gt; \\mu\\) Or you are confident the predictor variable will cause the outcome response to be lower than the population mean. \\(H_0: \\bar y \\ge \\mu\\) \\(H_1: \\bar y&lt; \\mu\\) 22.7.2 Unpaired hypotheses Two sided: Use when, going into an experiment, you are not sure whether level A or B of the predictor will cause a higher outcome response. \\(H_0: \\bar y_A = \\bar x_B\\) \\(H_1: \\bar y_A \\ne \\bar x_B\\) One sided: Use when, going into an experiment, you are confident level A of the predictor variable will cause the outcome response to be higher than that for level B. \\(H_0: \\bar y_A \\le \\bar x_B\\) \\(H_1: \\bar y_A &gt; \\bar x_B\\) Or when you are confident the level A of the predictor variable will cause the outcome response to be lower than that for level B. \\(H_0: \\bar y_A \\ge \\bar x_B\\) \\(H_1: \\bar y_A &lt; \\bar x_B\\) 22.7.3 Paired hypotheses Two sided: Use when, going into an experiment, you are not sure whether the mean difference between levels of the predictor variable will be less than or greater than zero. \\(H_0: \\bar y_{diff} = 0\\) \\(H_1: \\bar y_{diff} \\ne 0\\) One sided: Use when, going into an experiment, you are confident the mean difference between levels of the predictor will be greater than zero. \\(H_0: \\bar y_{diff} \\le 0\\) \\(H_1: \\bar y_{diff} &gt; 0\\) Or when, going into an experiment, you are confident the mean difference between levels of the predictor will be less than zero. \\(H_0: \\bar y_{diff} \\ge 0\\) \\(H_1: \\bar y_{diff} &lt; 0\\) 22.8 Confidence Intervals A confidence interval has duality. It is an inferential statistic that also allows for providing some insights as to the accuracy of a sample. A two-sided 95% CI can be calculated as follows: \\[\\begin{equation} \\bar y \\pm qt(0.975, df)*\\frac{sd}{\\sqrt n} \\tag{22.8} \\end{equation}\\] For example, given a sample size of n=3, the mean is 100. and the standard deviation is 25: upper.limit &lt;- 100+qt(0.975, 2)*(25/sqrt(3)) lower.limit &lt;- 100-qt(0.975, 2)*(25/sqrt(3)) paste(&quot;upper limit=&quot;, upper.limit) ## [1] &quot;upper limit= 162.103442793758&quot; paste(&quot;lower limit=&quot;,lower.limit) ## [1] &quot;lower limit= 37.8965572062417&quot; Thus, the point estimate for the mean is 100 and, on the basis of this single sample, we can assert 95% confidence the true population mean lies between 37.89 and 162.10. A slightly more accurate way to think about this is in frequentist terms. If we sampled 100 times, where each sample has n independent replicates, 95% of the resulting confidence intervals would include our point estimate of 100, but their point estimates could range from 37.89 to 162.10. The CI is useful inferentially. For the hypothesis rejection decision, the CI provides the same information as the p-value. For example, imagine the sample mean from above (100, 95% CI 37.89 to 162.10) comes from an experiment to test the hypothesis that the mean differs from zero (\\(H_0:\\mu=0\\)). Since the CI does not include 0, we can reject the null. We would also find a p-value less than 0.05 would be associated with this outcome. In contrast, if we have a mean of 69 and a 95% CI of -12.1 to 150 that includes the value of zero. In that case, even though our point estimate is 69, we have 95% confidence a value of zero for our point estimate is possible. The null would not be rejected. And you would find a p-value &gt; 0.05 associated with this outcome The CI should also be interpreted scientifically. The CI range may include values that are scientifically unimpressive. For example, the point estimate for the population mean from the sample is 100. There is 95% confidence the other values in the interval may also explain our variable, too. Perhaps we dont think much, scientifically, of values that are 40 or lower? Yet, since they are in the interval they raise concerns about the scientific quality of the sample, including the point estimate. I like to think that the standard error of the mean, \\(SEM\\), or \\(\\frac{sd}{\\sqrt n}\\), provides information about the precision of the mean estimate. As the sample size increases \\(SEM\\) lowers. As we sample more, our estimate of the mean gets more precise. I like to think about the 95% confidence interval as providing information about the accuracy of the mean; the range of values for what the true mean might be based upon this one sample. The CI tells us not to fall in love with the point estimatethe CI is a range of values that make just as much sense. Of course, when sampling from unknown hypothetical populations its never possible to know if something is accurate with perfect certainty. However, it is useful from time to time assert how confident we are in an estimate in terms of the values it might otherwise taken on. Therefore, be less concerned about whether a CI is wide or narrow. That is not the right question to ask. Be more concerned about whether the CI includes values that dont impress scientifically. That thinking provides a check against blind obedience to a p-value. 22.9 Reliability of small samples The t-tests are based upon the sample parameters \\(\\bar y, n, sd, where\\ sem=\\frac{sd}{\\sqrt n}\\). Furthermore, it is very common to base relatively small experiments on the t-test experimental design. At this stage we know enough to make worthwile pointing out the relationship between sample size and the properties of these parameters. The figure below illustrates the outcome of repeated random sampling from the known population \\(N(\\mu=100, \\sigma=25)\\). The coefficient of variation for this simulation is \\(cv=\\frac{\\sigma}{\\mu}=0.25\\), which my sense says is pretty typical for bio-variable with a well behaved signal to noise ratio. But YMMV. At first, this script samples at size \\(n=2\\), repeating the sampling 50 times. For each of these 50 samples it calculates the sample \\(\\bar y\\), \\(sd\\), and \\(SEM\\). Then it moves up to a sample size of \\(n=3\\), collecting these parameter values from each sample. And so on and so on up to a sample size of \\(n=100\\) Figure 22.3: Simulation of the relationships between the sample parameters used in t-tests. These can represent either group parameters or the parameters for the difference between groups. This result illustrates two big things. First, small samples are crappy. Since we coded the values we know the mean and standard deviation are 100 and 25, respectively. Yet, at low sample sizes, \\(n\\), a random sample can often be pretty far off. And these vary quite a lot, too. In real life, wed never know whether our small sample is yielding a crappy estimate for a population parameter! The researcher is not at fault for this. This is the luck of the random draw. This happens even with exquisite technique and perfectly calibrated pipettes and other instrumentation, and slavish attention to detail. Biological systems have variance. Variance can cause our sample estimates to be far off from the central tendancy. When exactly do small samples get less crappy? In some respects that is in the eye of the beholder. To my eye, for this case \\(n &gt;30\\) seems less crappy than under 30. Importantly, the coefficient of variation will have a big role in this. Small samples of variables with low \\(cv\\) will be less crappy, relatively speaking. And the larger the effect size the greater latitude we have in tolerating crappy small samples, irrespective of their \\(cv\\). This is where power calculations come in very handy. They are designed to tell us how low our sample size can be before we become victims of the craptastic. The second thing big thing this illustrator reveals is that \\(sd\\) and \\(sem\\) are not telling us the same thing. The sample \\(sd\\) measures dispersion in the sample, but that is also an estimate of \\(\\sigma\\), the dispersion in the population. This is in the same way that the sample mean \\(\\bar y\\) estimates the population mean \\(\\mu\\). As sample size increases the \\(sem\\) gets lower and lower. That is due to math because \\(sem=\\frac{sd}{\\sqrt n}\\). The \\(sem\\) in fact is the standard deviation of a theoretical population of sample means. This concept is derived from the central limit theorem11 As a result, we infer from the \\(sem\\) of a single sample how precisely that sample estimates the population mean. As you can see from this illustrator, the precision by which a mean is estimated improves as the sample size used to estimate that mean increases. Whereas \\(sd\\) oscillates with each sample around a fixed value, like the mean, reflective of the dispersion of values in the population, \\(sem\\) gets better and better (when one assumes low is better). 22.9.1 Reporting SD or SEM? When the precision of a measurement is important, show the \\(sem\\), and be sure to provide the sample size as well. For everything else just report the \\(sd\\). Are you estimating the value of a physical constant, such as the mass of a boson, the \\(K_D\\) of a drug receptor interaction, the \\(K_M\\) of an enzyme for a substrate? Something that should have the same value no matter where in the universe it is estimated? Researchers who are in the business of estimating those values with high precision, such as when those values are under dispute, cannot be faulted for reporting the \\(sem\\). Particularly when the goal of the experiment was to get a better estimate of the physical constant or how a variable changes it. In those cases, precision is more important. The \\(sd\\) estimates the dispersion of the population while also providing insight into the variability of the sample. The illustrator above proves the \\(sem\\) does neither. The reader of your research is usually far better served by getting dispersion information. For example, they may want to replicate your results but need to do a power analysis first. When youre reporting relative transformations (percent of control, fold, etc) the \\(sem\\) is useless. It is hard to imagine cases where precision has much utility to understand effects written in those units. Another consequence of the so-called replication crisis is that journals are requiring authors to show all their data more openly. For example, they ask for users to report data as scatter plots to show all data points rather than bar graphs with error bars. Most people who are honest about why they use \\(sem\\) error bars in graphs admit it makes the data look better. Although honest about their reasons, and naive because they dont really understand the difference between \\(sd\\) and \\(sem\\), that motivation is basically deception. Finally, if you do use SEM in bar graphs, then you must report the exact value of \\(n\\) for each group so a reader can back calculate out the \\(sd\\). This can get messy fast for all but the simplest graphs and figure legends. Do your reader a favor and just report the \\(sd\\). 22.10 Running the analysis in R In R t.test represents a single function by which each of the three t test experimental designs can be analyzed. 22.10.1 One sample t test Lets say a standard to measure against is the value of 100. We can ask if a random sample that is 2-fold greater is different than 100, less than 100, or greater than 100: # this makes the sample reproducible set.seed(1234) # here&#39;s a random sample a &lt;- rnorm(3, mean=200, sd=25) # you can run t.test without naming it as an object one2 &lt;- t.test(a, mu=100, alternative=&quot;two.sided&quot;, conf.level=0.95 ) oneL &lt;-t.test(a, mu=100, alternative=&quot;less&quot;, conf.level=0.95) oneG &lt;- t.test(a, mu=100, alternative =&quot;greater&quot;, conf.level=0.95) # the following line makes jabstb more readable # broom::tidy is a function to clean up R test output # not a bad trick to have for functions or writing clean reports knitr::kable(bind_rows(tidy(one2), tidy(oneL), tidy(oneG)), caption=&quot;One-sample t-test output. Read text for interpretation.&quot;) Table 22.1: One-sample t-test output. Read text for interpretation. estimate statistic p.value parameter conf.low conf.high method alternative 201.29 6.037536 0.0263538 2 129.1056 273.4744 One Sample t-test two.sided 201.29 6.037536 0.9868231 2 -Inf 250.2778 One Sample t-test less 201.29 6.037536 0.0131769 2 152.3023 Inf One Sample t-test greater 22.10.1.1 Interpretation of one-sample output \\(t\\) is a descriptive statistic, calculated from the sample values as described above. The test uses ordinary least squares. See the chapter on dispersion. Notice how the t-value is the same for all 3 tests. The same sample will give the same signal to noise ratio, irrespective of the hypothesis tested. You may be disappointed by no output for the \\(sd\\) or \\(sem\\) of the sample. But these are easy enough to calculate. From the equation as described above: \\(sem=\\frac{\\bar y}{t}\\), and \\(sd=sem \\times \\sqrt n\\) Parameter is value for the degrees of freedom. This sample with \\(n=3\\) has \\(df=2\\). A degree of freedom was lost in calculating \\(\\bar y\\) The p-value is a cumulative probability from a null \\(t_{df=2}\\) distribution. It is the probability of the observed value for \\(t\\) or a value more extreme, if the null hypothesis is true. The p-values do differ between the 3 tests because the hypotheses differ, and these hypotheses affect the calculation of \\(p\\) even though \\(t\\) is the same for each test. The one-sided less hypothesis argument predicts the sample mean will be less than \\(\\mu\\). Its p-value calculation is pt(q=6.037536, df=2, lower.tail=T). It is large because it is the wrong hypothesis for these data. If the sign of the \\(t\\) were negative, this would be the correct hypothesis. The one-sided greater hypothesis greater predicts the sample mean will be greater than \\(\\mu\\). Its p-value calculation is pt(q=6.037536, df=2, lower.tail=F). The two.sided hypothesis is the sample mean does not equal \\(\\mu\\). Its p-value is pt(t=6.037536, df=2, lower.tail=T)/2 The 95% confidence level is the default. If you want a different confidence level, for example, an 86.75309% CI, simply enter an argument for it. The upper and lower confidence limits differ between the 3 tests. The one-sided tests give you an infinite limit on one side. These are hard to interpret and not particularly useful. If you are tempted to to combine the upper of less with the lower of greater, youve created a 90% confidence interval, so dont. The simplest way to get a typical 95% CI in R that has both upper and lower limits is to run a two.sided t.test and pull it out of that. 22.10.2 Unpaired t test Now we will pull two random samples: one from a normal distribution that has a mean of 200, and the second from a distribution that has a mean of 100. Both have standard deviations of 25. We have the option to run three different hypotheses. set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25) b &lt;- rnorm(3, mean=100, sd=25) up2 &lt;- t.test(a, b, alternative=&quot;two.sided&quot;, var.equal = F ) upL &lt;- t.test(a, b, alternative=&quot;less&quot;, var.equal = F) upG &lt;- t.test(a,b, alternative =&quot;greater&quot;, var.equal = F) knitr::kable(bind_rows(tidy(up2), tidy(upL), tidy(upG)), caption=&quot;Unpaired t-test output. See text for interpretation. See interpretation of one-sample tests for details omitted here common to all tests.&quot;) Table 22.2: Unpaired t-test output. See text for interpretation. See interpretation of one-sample tests for details omitted here common to all tests. estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative 113.0443 201.29 88.24569 3.920543 0.0208179 3.622275 29.58648 196.5022 Welch Two Sample t-test two.sided 113.0443 201.29 88.24569 3.920543 0.9895910 3.622275 -Inf 176.4025 Welch Two Sample t-test less 113.0443 201.29 88.24569 3.920543 0.0104090 3.622275 49.68623 Inf Welch Two Sample t-test greater 22.10.2.1 Interpretation ot unpaired output The interpretation is is similar to the one sample case, so I wont repeat a lot of that, but there are some evident differences. The estimate now is the difference between the means of each group. Estimate1 and estimate2 are the means of each group. Statistic is the value of \\(t\\), in this example calculated by the default Welch method. That arises from the default var.equal=F argument in the test function. Parameter is the value for degrees of freedom \\(df\\),in this example also calculated by the Welch method. These are almost always fractional, which the Welch formula for calculating \\(df\\) predicts, whereas \\(df\\) for the Students t-test is always an integer value. Although you can calculate the standard error for the test, \\(se = \\frac{estimate}{statistic}\\), because there are two means, note how this value is not a standard error of the mean. In fact, each group will have different \\(sem\\) values. Best use R to calculate sem per group: sem &lt;- sd(a)/sqrt(length(a)) The mean of x corresponds to the a sample, or the first group in the argument. In the unpaired t test, good practice is to put the treatment as the first data argument, and the control as the second argument. That will make interpretation of one-sided test output a LOT easier. Welchs test is only for unpaired tests, because it serves as an adjustment for when the sample sizes and variances of the two groups differ. The one-sample doesnt have two groups, whereas the group variances are completely irrelevant in the paired t-tests. 22.10.3 Paired t Test The paired test calculates the differences between paired measurements. Then it asks how the mean of these differences differs from zero. In fact, although two levels of a predictor variable are involved, there are not two groups of replicates. There is only one group. The test calculates the mean and its standard error for the differences between matched measurements within each replicate. The test asks, what is the relationship of these differences to zero? Thus. it is much more like the one-sample t-test than it is like the unpaired t-test. The structure of the data input matters strictly. If entered as two vectors (like below), the first value of the first vector will be paired with the first value of the second vector, and so on. Alternately, like all t-tests, you can run it using the formula method, which reads columns from a tidy dataframe. Good practice is to include replicate ids in these data frames. Though not necessary to run the t-test, replicate ids are necessary to plot the results correctly. set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25) b &lt;- rnorm(3, mean=100, sd=25) p2 &lt;- t.test(a, b, alternative=&quot;two.sided&quot;, paired=T) pL &lt;- t.test(a, b, alternative=&quot;less&quot;, paired=T) pG &lt;- t.test(a, b, alternative =&quot;greater&quot;, paired=T) #make a data frame suitable for the formula argument long &lt;- tibble(replicate_id=1:3, a, b) %&gt;% pivot_longer(-replicate_id, names_to=&quot;predictor&quot;, values_to=&quot;response&quot;) #running the formula argument l2 &lt;- t.test(response ~ predictor, data=long, alternative=&quot;two.sided&quot;, paired=T) knitr::kable(bind_rows(tidy(l2), tidy(p2), tidy(pL), tidy(pG)), caption=&quot;Paired t-test output. See text for interpretation. See interpretation of one-sample tests for details omitted here common in all tests.&quot;) Table 22.3: Paired t-test output. See text for interpretation. See interpretation of one-sample tests for details omitted here common in all tests. estimate statistic p.value parameter conf.low conf.high method alternative 113.0443 12.10457 0.0067559 2 72.86194 153.2268 Paired t-test two.sided 113.0443 12.10457 0.0067559 2 72.86194 153.2268 Paired t-test two.sided 113.0443 12.10457 0.9966221 2 -Inf 140.3140 Paired t-test less 113.0443 12.10457 0.0033779 2 85.77465 Inf Paired t-test greater 22.10.3.1 Interpretation of paired output The first row of the output is from the formula test. It should have the same everything as the second row. The formula method is a less old school data input method, and has no other effect on the test. The estimate is the mean of the differences between pairs. Group means are irrelevant. What matters in a paired t-test is whether the differences are reliably different from zero. Notice how, given the same data, the paired computes a higher value for \\(t\\) compared to the unpaired. \\(t\\) is the ratio of the mean of the differences divided by the standard error of the mean of the differences. Notice how, given the same data, the paired test computes a lower p-value compared to the unpaired. The 95% CI is for the mean of the differences. There is 95% confidence the true mean of the differences in the population sampled is included within the range bounded by conf.low and conf.high. The parameter is the degrees of freedom. In this sample \\(df=2\\) because there are 3 pairs. A degree of freedom was lost by calculating the mean of the differences. The paired test has the same number of measurements as the unpaired test, but fewer independent replicates. This is due to the experimental design, which called for paired measurements from each replicate. Which is based entirely upon scientific judgment. There is no Welchs test for paired measurements. Welch is all about two groups. There is only one group of replicates in a paired t-test design. Good statistical judgment is to not conflate the paired and unpaired tests, or to assume they differ trivially, or to assume one is a simple analytic difference from the other. They differ dramatically. 22.11 Plotting t Tests Plot t-tests in a way that illustrates the experimental design. 22.11.1 One-sample The one-sample t-test compares a group mean to a reference value. So the plot should only show one group, ideally as scatter points instead of a bar graph. The reference value should be highlighted. A geom to illustrate the mean and standard deviation or confidence interval is a nice touch. # the values for a were created in a code chunk above data.o &lt;- tibble(treatment=a) The main trick here is to code a dummy factorial variable for the x-axis within the ggplot aesthetic. # note dummy x axis variable # reference value is 100 ggplot(data.o, aes(x=factor(1), y=treatment))+ stat_summary(fun.data = mean_sdl, fun.args = list(mult=1), geom=&quot;crossbar&quot;, width=0.1, color=&quot;red&quot; )+ geom_point(size=4)+ geom_hline(aes(yintercept=100), color=&quot;blue&quot;, size=2, linetype=&quot;dashed&quot;)+ scale_y_continuous(limits= c(50, 250))+ labs(x=&quot;treatment&quot;) Figure 22.4: Plotting one-sample t-test data 22.11.2 Unpaired An unpaired t-test compares two means. So the plot should show two means. Tidy the data, which makes it simple to create a descriptive statistics summary table: # munge data that was simulated above data.u &lt;- data.frame(replicate=1:6, control=b, treatment=a) %&gt;% pivot_longer(cols=c(control, treatment), names_to=&quot;Predictor&quot;, values_to=&quot;Response&quot;) #summary statistics table to have data.y &lt;- data.u%&gt;% group_by(Predictor) %&gt;% summarise( mean=mean(Response), sd=sd(Response), n=length(Response), sem=mean(Response)/sqrt(n), .groups = &quot;drop&quot; ) knitr::kable(data.y, caption=&quot;Summary statistics for a two group sample.&quot;) (#tab:summary stats for unpaired t)Summary statistics for a two group sample. Predictor mean sd n sem control 88.24569 36.32957 6 36.02615 treatment 201.29004 25.99038 6 82.17631 Unless there is a scientific reason to report precision (and there rarely is a good reason to show the standard errors of the means) plot the data points with mean +/- standard deviation. A straight forward ways to add error bars is by use of the stat_summary function. A few ways exist to do this. I use the crossbar geom because it is a bit quicker. Note the mean_sdl is from the Hmiscpackage. The package has related funtions for standard error and confidence intervals.12 ggplot(data.u, aes(Predictor, Response)) + geom_jitter(width=0.15, size=4) + stat_summary(fun.data = mean_sdl, fun.args = list(mult=1), geom=&quot;crossbar&quot;, width=0.2, color=&quot;red&quot; ) Figure 22.5: Journals want data as scatter plots. Stat_summary geoms add error bars. The argument mult=1 coupled with mean_sdl function draws +/- 1 SD # or if you prefer # stat_summary(fun.data=&quot;mean_sdl&quot;, fun.args=list(mult=1), # geom=&quot;errorbar&quot;, color=&quot;red&quot;, width=0.2) + # stat_summary(fun.y=mean, geom=&quot;point&quot;, color=&quot;red&quot;, shape=&quot;plus&quot;, size=5) 22.11.3 Paired In the paired experimental design, each replicate is a pair of observations. Plots with point-to-point lines will illustrate the paired relationship of the measurements. The means of the two groups of observations are irrelevant. The difference between each pair of observations is relevant. The mean of these differences is also relevant The design tests for the difference the treatment causes within each pair, which the slope of the line illustrates. For example, a horizontal line connects a replicate pair would indicate no effect of the predictor variable! Munge the data into a long data frame format. Adding an ID for each replicate is necessary as a grouping variable. This grouping variable is used to connect point pairs with a line and to calculate differences between pairs. # the response values a and b were simulated above data.w &lt;- tibble(control=b, treatment=a, id=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) %&gt;% pivot_longer(cols=c(control, treatment), names_to=&quot;Predictor&quot;, values_to=&quot;Response&quot;) The grouping variable is also used for creating a second plot to illustrate the computed values of the differences. data.wd &lt;- data.w %&gt;% select(id, Predictor, Response) %&gt;% group_by(id) %&gt;% summarise(dif=diff(Response), .groups=&quot;drop&quot;) data.wd ## # A tibble: 3 x 2 ## id dif ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 128. ## 2 B 96.2 ## 3 C 114. The code and the proper way(s) to plot the results of a paired experimental design are shown below. Most people fail to plot paired data correctly. The most commone error is to plot paired data as if it were unpaired. p1 &lt;- ggplot(data.w, aes(Predictor, Response, group=id)) + geom_point(size=4) + geom_line(color=&quot;red&quot;) p2 &lt;- ggplot(data.wd, aes(x=factor(&quot;Treatment - Control&quot;), y=dif))+ geom_point(size=4) + stat_summary(fun.data = mean_cl_normal, fun.args = list(conf.int=0.99), geom=&quot;crossbar&quot;, width=0.2, color=&quot;red&quot; )+ geom_hline(aes(yintercept=0), color=&quot;blue&quot;, size=2, linetype=&quot;dashed&quot;)+ scale_y_continuous(limits=c(0, 300)) + labs(y=&quot;Response difference&quot;, x=&quot;Predictor&quot;) Think of the plot in A as showing how each replicate performed. The plot in B illustrates a summary for a paired t-test. It shows the difference between each replicate pair, the mean of those differences, and the confidence interval for the mean of the differences. As long as the confidence interval doesnt include the value of zero, the plot illustrates a statistical difference from the null of zero. And no asterisks were consumed to convey this vital information! plot_grid(p1, p2, labels=&quot;AUTO&quot;) Figure 22.6: Paired responses (A) and within pair differences with 99% confidence interval (B). 22.12 t Test Power The principal output of any power function is the sample size that would be necessary in an experiment to conduct a severe test of an hypothesis. There are two ways to make these calculations, using custom Monte Carlos or package functions. If they confuse you, see Chapter 23 for extensive detail on performing Monte Carlo-based power calculations. Below is a streamlined version of a Monte Carlo function, t.pwr. The R pwr package has the pwr.t.test function. Given the same effect size you should find that a properly executed Monte Carlo will give you roughly the same result as a pwr.t.test function. 22.12.1 pwr.t.test It is very simple to execute pwr.t.test. Using this function requires that several decisions be made in advance. Some of these are not necessarily simple. First, it takes arguments for acceptable type1 error and for intended power. The standard for these in basic biomedical research is 5% and 80%, respectively, but you can use any level you deem appropriate. If you want to run at 86.75309% power and at 1% alpha, go for it. Thats all simple. Two other arguments are type, which is the experimental design, and alternative, which is the hypothesis. These two selections are scientific decisions, but clearly impact the statistical output. A common error is to get type wrong, when the researcher doesnt understand the differences between the t-tests. Hopefully what is written in this chapter helps to clarify. Chapter 10 covers what are two-sided and one-sided hypotheses. Most researchers struggle with that concept, too. Finally, there is the d argument, which is Cohens \\(\\delta\\). This is less obvious, but very important and simple to understand: Cohens \\(\\delta\\) is the signal to noise for the effect size you anticipate. Yet, Cohens \\(\\delta\\) is very opaque. I strongly, strongly recommend AGAINST defaulting to Cohens values for large, medium and small effects, which apply to common standards in psychological research. Their values are much too ambiguous to be useful. Instead, calculate your own value for d. Imagine you anticipate measurements that will have a standard deviation of 25 units. You know this because of some familiarity with the assay and the model system. You estimate this value on the basis of your own preliminary data or on the basis of published information. It takes scientific judgment to settle on the expected standard deviation value. You also believe the average response to a negative control or basal level of the independent variable will be around 50 units. Also, based upon your scientific understanding of the system, in your mind, a minimally scientifically valid treatment effect will have a 100 unit response, or 2-fold above the negative control. The signal will be 100-50=50, and the noise will be 25. Cohens \\(\\delta\\) will therefore be 50/25=2. A power calculation for a one-sided hypothesis at a 95% confidence level is illustrated for an unpaired design for a Cohens \\(\\delta=2\\) : pwr.t.test(d=2, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;, alternative=&quot;greater&quot;) ## ## Two-sample t test power calculation ## ## n = 3.986998 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater ## ## NOTE: n is number in *each* group 22.12.1.1 Interpretation of pwr.t.test output The experiment should have a total sample size total of at least 8, 4 in each of two groups. Randomly allocate each of two levels of the predictor variable to 4 replicates each. Notice how this function produces a bizarre, fractional sample size. Obviously, there is no such critter as a partial replicate. So always round up if conservative, or round down if you like to live on the wild side. People seem to have difficulty with Cohens delta. Its nothing more complicated than a simple signal-to-noise estimate, just like is the t-statistic. You need to be able to calculate the delta value based upon outside knowledge. 22.12.1.2 Minimal scientifically relevant effect When designing an experiment temper your hoped for effect size with a sense of what you consider to be a minimally scientifically relevant effect size. In some systems, 2-fold is huge, whereas in other systems (with a lot of variation) a 2-fold response might be deemed insignificant scientifically. It is crucial to give the standard deviation a lot of thought. In my experience, students tend to under-estimate standard deviations. Perhaps because they conflate SD for SEM? The right SD estimate is important. For example, here is a power calculation for when all else is equal to the example above but the the standard deviation is twice what was estimated above. It has a dramatic effect on sample size. pwr.t.test(d=1, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;, alternative=&quot;greater&quot;) ## ## Two-sample t test power calculation ## ## n = 13.09778 ## d = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater ## ## NOTE: n is number in *each* group I strongly caution against using Cohens delta values recommended for small, medium, and large effects. Theres a good chance his idea of large effects is a lot smaller probably closer to your idea of small effects. 22.12.2 t-pwr: A Monte Carlo method Below I show a lean custom Monte Carlo function for t-test power. This is designed to assess power and sample size relationships prior to running an experiment. I find it to be much more intuitive to use than the pwr.t.test function.. YMMV on that point. The real advantage is the ability to customize virtually any expected outcome and to see which test performs best under those unique conditions. For more expansive versions with more explanation, see Chapter 23. The t.pwr() below takes sample size n as an argument, and returns the power of a t-test. Feed t.pwr different values of n until it finds the power you need. Or apply the t.pwr function over a vector of n values. Feed some expected experimental results into the initializer for it to behave as the population being sampled. The function will simulate a long run of random samples, conduct a t-test on each. For each t-test a p-value is collected. If you set a confidence level of 95%, every p-value less than 0.05 will be scored as a hit. Power = hits/total number of tests. To customize for a specific application, modify the initializer values and the t.test arguments. What means and standard deviations should you estimate? Either the values you predict will happen, OR values that you believe would be the minimal scientifically meaningful effect size. t.pwr &lt;- function(n){ # Intitializers. Place expected values for the means and # standard deviations of two groups to be compared here. m1=50 #mean of group 1 sd1=25 #standard deviation of group 1 m2= 100 #mean of group 2 sd2=25 #standard deviation of group 2 alpha=0.05 #type1 error tolerance ssims=100 #number of function cycles p.values &lt;- c() i &lt;- 1 repeat{ x=rnorm(n, m1, sd1); y=rnorm(n, m2, sd2); # optional for paired, we must account for # correlation of measurements within pairs # transform y from above given r coefficient # y=r*x + y*sqrt(1-r^2) # and change test to paired=T p &lt;- t.test(x, y, paired=F, alternative=&quot;two.sided&quot;, var.equal=T, conf.level=1-alpha)$p.value p.values[i] &lt;- p if (i==ssims) break i = i+1 pwr &lt;- length(which(p.values&lt;alpha))/ssims } return(pwr) } 22.12.2.1 How to use What is the power for a two-sided, unpaired experiment with those initializer estimates when run at a sample size of 4? After running the t.pwr function into the environment, do this: t.pwr(4) ## [1] 0.68 Or run t.pwr over a range of sample sizes and plot a power vs sample size curve: frame &lt;- data.frame(n=2:50) data &lt;- bind_cols(frame, power=apply(frame, 1, t.pwr)) #plot ggplot(data, aes(n, power))+ geom_point() + scale_y_continuous(breaks=c(seq(0, 1, 0.1)))+ scale_x_continuous(breaks=c(seq(0,50,2))) Figure 22.7: Power curve for two-sided, unpaired t-test, delta=2 22.12.2.2 Interpret t-pwr Monte Carlo For unpaired design, n represents the number of replicates per group. For paired design, n represents the number of replicates, or pairs. From the power graph, it looks like a sample size of 5 or 6 per group yields 80% power. This result is not exactly identical to the pwr.t.test prediction of sample size, but it is in the same ballpark. There are a few differences, the first being the Monte Carlo is a simulated series and the test is Students (var.equal=T). Since the Monte Carlo is comprised of the t-test configuration that Ill run after the experiment, and is more conservative, Ill go with that. 22.13 Summary Use t-tests for factorial experiments involving measured dependent variables. Assumptions about normality and homoscedasticity are difficult to validate with small samples. Although people commonly use t-tests on data transformed from proportions to percents, or on ordered data, proportion and nonparametric tests can be better options. The t-test is actually 3 different types of t-tests, only one of which compares the means of two groups. Which t-test to use depends entirely on the experimental design and requires matching scientific judgment with statistical judgment. Plot your data based on the experimental design. Configuration of the test depends upon the statistical hypothesis. The t-test permeates statistical analysis of biomedical experiments, from simple stand alone experiments to posthoc testing testing of ANOVA and regression coefficients. For this reason, you are advised to embrace a spiritual relationship with the t-test. In fact, the sampler function basically simulates the central limit theorem. Each blue point is a mean. Imagine calculating the mean and standard deviation of the 50 blue means at each value of \\(n\\). The standard deviation of the 50 means at each \\(n\\) is the \\(sem\\)! Ive seen several instances where Hmisc functions throw errors on the machines of some students. These errors have not been reproduced by the package author, so they remain unresolved. When mean_sdl doesnt work the parameter values can be computed readily. "],["ttestmc.html", "Chapter 23 Statistical design of t-tests 23.1 About this chapter 23.2 Overview of the Monte Carlo 23.3 Changing Sample Size 23.4 Diabetes drug scenario 23.5 One sample t-test Monte Carlo 23.6 Unpaired t-test Monte Carlo 23.7 Paired t-test Monte Carlo 23.8 Step 4 23.9 Comparison to pwr tests 23.10 Summary", " Chapter 23 Statistical design of t-tests library(datapasta) library(tidyverse) 23.1 About this chapter This chapter walks through a statistical design for each of three different types of t-test experiments. Each section focuses on illustrating how to plan a sample size by using Monte Carlo simulation. To do that this method calls on the researcher to make decisions about several statistical issues in advance of running the experiment: Explicitly define the dependent and independent variables Define the properties of the dependent variable Declare a scientifically-meaningful effect size Declare tolerance for error Determine an appropriate sample size Which is why I call this statistical design, if that is not obvious. The huge advantage of statistical design is the thinking-through of the problem with only cognitive investment. We get a well-designed experiment out of it. We get a sense of whether running that test is worth it. We might even detect flaws before it is two latebefore real-life time and treasure has been spent on the experiment. To illustrate how existing data is used to define the properties of the dependent variable we use information in a mouse diabetes study conducted by the Jackson Labs in the examples below. 23.2 Overview of the Monte Carlo In statistics, Monte Carlo is just jargon for a repetitive simulation. All the cool people do Monte Carlo these days, but it has been around forever. In just a few minutes we can simulate thousands of experiments. In real life any one of these experiments might take weeks or months to conduct. A Monte Carlo t-test is therefore simply a repetitive simulation a random sample and statistical testing performed on it. In one cycle a random sample is generated, a t-test is performed, and in our examples below, a p-value is extracted from the output. Hundreds or thousands of these cycles are run for a given simulation. The long-run success of an experiment based upon a set of assumptions is calculated. The most practical use for the Monte Carlo is to determine the sample size for an experiment with the technique. The fraction of p-values from those tests that are less than our chosen type1 error threshold serves as the experimental power. The script can be run on range of sample sizes to hone in on the sample size that best yields an acceptable experimental power. Tweaking the parameter arguments of the functions can generate random replicate values that closely approximate what are likely to be collected in a real life experiment. The key distinction between real life and simulated samples is that the true parameters of the real life population to be sampled are unknown. In the simulation we pretend as if these population parameters are known and then code them into the function. Statistical design using Monte Carlo helps the researcher plan an experiment explicitly, to visualize the outcome, and to even think about any thing theyve missed. Ultimately it can translate to saved time and cost. 23.2.1 Data simulation In the cases below we will use the rnorm function to generate sample replicates. We should simulate the type of data we expect the real-life experiment to generate. For example, when expecting skewed data, lognormal shaped data can be simulated using the rlnorm function. If we expect unequal variances, we can simulate groups that have unequal variances with rnorm or even with rlnorm. We can even use this technique to simulate ordered and sorted data and see how well the t-test performs on that. 23.2.2 The test A t-test is configured to test each simulated sample. We could collect any test parameter we wished from that t-test, but for the analyses below well just capture the p-values. When testing at a 95% confidence level, a p-value less than 0.05 would be counted as a hit indicative of a positive treatment effect. You can adjust your confidence level and hit threshold to whatever value you wish. Cycles of random sampling, t-testing, and p-value collection is repeated anywhere from 100 to 1000 times or more. The number of hits relative to the number of simulated tests is the power of the test. How many cycles to run? It depends upon how accurate youd like your power estimate to be. Reliable estimates can be had with only a few hundred simulations. More simulations will be more accurate. 23.3 Changing Sample Size The examples below are designed for running chunk by chunk, then changing n, then re-running chunk by chunk. Thats a bit clunky, but their purpose here is to illustrate the modular structure of a Monte Carlo. My t.pwr function, a streamlined version of this code which can be applied over a vector of n values, can be seen in the t-test power section of Chapter 22. After the first run through the chunks, if the power value is not ideal, whether too hi or too low, change the sample size n in the rnorm function. Then re-run all the chunks. Repeat this until a Monte Carlo simulation run gives you an acceptable fraction of hits and power. Theres your sample size. Importantly, not just \\(n\\), but any other assumptions or conditions can be changed, too. Need to re-evaluate the predicted standard deviation? Change it! Will the effect size be larger or smaller than you think? Simulate that! Want to compare a one-tailed to a two-tailed hypothesis? Switch it up! The time to do p-hacking and HARKing is during a Monte Carlo power analysis, before running the real life experiment. 23.4 Diabetes drug scenario Lets imagine we have developed a new drug we hope will be useful for treating type II diabetes. Pretend our role is to generate pre-clinical data in support of an FDA application. The planning for this scenario is based on some mouse phenome data in a diet-induced obesity study using a fancy mouse, which is a common preclinical model for type II diabetes. 23.4.1 What would be a scientifically meaningful response? Most of the really important effort in a Monte Carlo for t-test power is in figuring out the mean and standard deviations of the population being sampled. These are used to predicted an expected effect size of the treatment or to define a minimally scientifically relevant treatment effect size. Beyond understanding the relationship of these parameters to data, there isnt a lot of statistics involved in deciding what would be a meaningful effect size. Because that mostly involves the researcher making scientific judgments. On the basis of expertise in the field, and familiarity with the preclinical model, we make the judgment that a 50% reduction in blood glucose caused by the drug in this diet-induced obesity model would be a scientifically meaningful outcome. Therefore, we should design an experiment capable of detecting that 50% effect size. If we go to Chapter 13 weve already compiled the phenome data into a summary format useful for our planning. These data show that under diabetogenic conditions the animals have an average blood glucose of 368 and SD = 119 (mg glucose/dl). Since this is an exercise in prediction and estimation, well round those values to 370 and 120. Any more precision misses the point of what this prediction hopes to accomplish. A 50% reduction would therefore yield a target glucose value of about 185 mg/dl. Finally, wed like to run the real life experiments at 90% power. Why? Lets imagine that these are a pretty important test: they represent a go vs no go inflection point for a novel drug candidate. When the stakes are higher the experimental test should be more severe. Therefore, well run through the code chunks, changing sample size each time, until they return a power value of about 90%. 23.5 One sample t-test Monte Carlo In this single arm design, each C57Bl/6J mouse enrolled in an diabetogenic protocol would receive the drug treatment. Blood glucose levels are taken once, at the end of a preset drug treatment period. The statistical test evaluates the null hypothesis that mean blood glucose with drug treatment is the same as the mean blood glucose in animals that undergo the diabetogenic protocol. There is no formal placebo control group. That may shock someone used to having control groups. When we have strong experience measuring diabetic blood glucose in this specific model this can be a good design. For example, we have performed lots of control group measurements in the past. So many that we have with high confidence in that this sample would, on average, have the value we set for \\(\\mu\\) in this test. This can be a useful experimental design particularly when the cost for securing a control group is high, such as when non-human primates are involved. 23.5.1 Step 1 Initialize the sampling function by entering these mean and sd parameter estimates for the expected values of the new drug. The sd estimate is a judgment call to think through and to model out. The entry below is conservative. It assumes the drug-treated group has the same sd as an untreated group. The Jaxwest7 data suggest a lower sd might happen with drug (the sd was 80 for the rosiglitazone group vs 120 for the control group). Also enter an estimate for the theoretical mean of the population it will be evaluated against. Finally, enter a best guess value for the sample size of the drug-treated group. Well come back to change this \\(n\\) to hone in on the right power. meanDrug &lt;- 185 sdDrug &lt;- 120 muCon &lt;- 370 nDrug &lt;- 3 23.5.2 Step 2 Initialize with relevant arguments for the t-test function: alt =&quot;two.sided&quot; pairing = FALSE var = FALSE alpha=0.05 23.5.3 Step 3 Declare the number of simulations for the Monte Carlo. Also set up an empty vector in the environment which will be used to capture a p-value each time one is generated by the function. nSims &lt;- 1000 p &lt;- c() 23.5.4 Step 4 Run the simulation function. Notice how with each loop it simulates a new random sample based upon the Step 1 initializer above. This also runs a one-sample t-test on that sample based upon the Step 2 initializer. Next it grabs the p-value from the t.test object. Then it stores the p-value in the formally empty vector from step 3. After it hits the bottom it goes back to the top to run another cycle, only stopping after \\(i == nSims\\). Therefore, the p-value vector grows with each loop. for(i in 1:nSims){ x&lt;-rnorm(n = nDrug, mean = meanDrug, sd = sdDrug) z&lt;-t.test(x, alternative = alt, paired = pairing, mu=muCon, var.equal = var, conf.level = 1-alpha) p[i]&lt;-z$p.value #get the p-value and store it } 23.5.5 Step 5 Calculate and show hits and power. A hit is a simulation with a p-value &lt; 0.05. Power is the fraction of all simulations that meet this hit critria. # the output hits &lt;- length(which(p &lt; alpha)); paste(&quot;hits=&quot;, hits) ## [1] &quot;hits= 304&quot; power &lt;- hits/nSims; paste(&quot;power=&quot;, power) ## [1] &quot;power= 0.304&quot; 23.5.6 Step 6 Visualize the p-value output with a histogram, with Emory colors of course. Because its pretty. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#012169&quot;, fill=&quot;#f2a900&quot;, binwidth=1/100)+ scale_x_continuous(breaks = seq(0, 1, 0.05)) 23.5.7 Step 7 Is the power too low or high? Go back top to change the sample size. The returned power for the estimates above should be about 40%. Thats a bit lower than a power of 90%, which wed like here. Change the value of the nDrug term in the Step 1 initializer to a higher sample size, before re-running all the code chunks. Iterate until a power of ~90% is achieved. Why 90%? Thats both a scientific and strategic call for this case. In this instance a positive result will have important implications for committing further to a costly drug development process. For that reason, the study should be run at a higher power than what might be chosen for a test with less riding upon it. Next steps: Now is the time to think about changing all of the other parameters. Is rnorm the right distribution to model the sampled population? Are the mean and sd parameters reasonable? Should I consider a smaller effect size? Confidence level? Alternative? The Monte Carlo is useful to test out a number of different scenarios. Once we are satisfied we have the right experimental design (sample size, predicted effect size, power, hypothesis and confidence level), we write all of these choices down in our notebookor register the study online. Save the power script for record keeping or to share with collaborators or with a consulting statistician. Add the script as a supplement to a manuscript and point to it by describing in methods how sample sizes were chosen. 23.6 Unpaired t-test Monte Carlo This is an alternative experimental design to the one above. This design involves two groups of animals. All animals would be subjected to the diabetes-inducing diet. In the control arm, the group would receive a placebo. In the experimental arm, the group would receive the new drug. Each animal would be assumed as statistically independent of every other animal. The objective is to test the null hypothesis that the means of the blood glucose concentrations do not differ between the two groups. 23.6.1 Step 1 Lets call the A group the placebo, and the B group the drug treated. Well use standard deviation and the mean estimates for blood glucose levels as described above. Well design for equal sample sizes, though this test can tolerate differences. #Sampled population paramemters # sample A placebo meanA &lt;- 380 sdA &lt;- 120 nA &lt;- 5 # sample B new drug meanB &lt;- 190 sdB &lt;- 120 nB &lt;- 5 23.6.2 Step 2 Set the t-test function arguments as initializers, rather than down in the loop function, so they are easy to read and to modify. #t-test function arguments alt&lt;- &quot;two.sided&quot; pairing &lt;- FALSE var &lt;- TRUE alpha &lt;- 0.05 23.6.3 Step 3 Declare the number of simulations. The larger the number of simulations, the more accurate will be the power calculation. Also set up an empty vector for the following function to fill as it cycles through simulations and generates p-values. nSims &lt;- 10000 #number of simulated experiments p &lt;- c() 23.6.4 Step 4 Run the simulation function. This for loop will run nSims cycles. In each cycle is simulates both an A group and a B group, which it t-tests, and then the p-value of that test is collected. # the monte carlo function for(i in 1:nSims){ #for each simulated experiment x&lt;-rnorm(n = nA, mean = meanA, sd = sdA) #produce n simulated participants #with mean and SD y&lt;-rnorm(n = nB, mean = meanB, sd = sdB) #produce n simulated participants #with mean and SD z&lt;-t.test(x,y, alternative = alt, paired = pairing, var.equal = var, conf.level = 1-alpha) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } 23.6.5 Step 5 Print out the power, which is the number of significant results (hits) divided by the total number of simulations. # the output hits &lt;- length(which(p &lt; alpha)); paste(&quot;hits=&quot;, hits) ## [1] &quot;hits= 5919&quot; power &lt;- hits/nSims; paste(&quot;power=&quot;, power) ## [1] &quot;power= 0.5919&quot; 23.6.6 Step 6 Plot out the distribution of p-values. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) Figure 23.1: P-value distribution of an unpaired t-test Monte Carlo power analysis. 23.6.7 Step 7 This configuration with a sample size of 5 in each group is a bit underpowered. Adjust these sample sizes to derive a power of about 90%. Also experiment with adjusting other features of the test. What happens if the SD for the drug-treated group is lower? What about a one-tailed hypothesis instead of a two-sided? Monte Carlo is the time for p-hacking and harking. The Monte Carlo is useful to test out a number of different scenarios. Once we are satisfied we have the right experimental design, we write all of these joices down in our notebookor register the study online. Save the power script for record keeping or to share with collaborators or with a consulting statistician. 23.7 Paired t-test Monte Carlo Paired experimental designs tend to be very efficient because they take advantage of the intrinsic relationships between measurements derived from a common replicate. For example, individuals may differ a lot in their absolute blood glucose concentrations, but the proportional change due to a drug treatment from one subject to another might be fairly consistent. In a paired design glucose is measured twice in each subject: after a placebo and once again after drug treatment. The important response variable per replicate is the difference between these two measurements. Given several independent replicates, the paired t-test challenges the null hypothesis that the mean change in blood glucose caused by the drug is zero. 23.7.1 Step 1 Estimating distribution parameters for groups and deciding upon a minimal valid effect size for a paired experiment is similiar to that for an unpaired test and one-sample tests, but with one crucial distinction. The power in paired experiments rests in the fact that the two intrinsically-linked measurements are correlated. When simulating data for a paired design accounting for the expected correlation between the response values better mimics the instrinsic relationship between measurements. Deriving a correlation coefficient to use for these simulations is best accomplished on the basis of some pre-existing data. We can use the serial glucose measurements from individual subjects in the Jaxwest7 data set to extract this important information. There are two daily blood glucose measurements taken on days 1, 3, 5, 7, 9, 11 and 12 of a study, from each of 16 different subjects. On each day there are two paired measures from each of 16 independent replicate measurements are taken. Across the blood collections we expect to see high correlation within the replicates. In other words, we would expect that animals with high values should be consistently high across the study period, and animals with low values should be consistently low across the same time frame. #Copying cells F14:S32 of the Jaxwest7 table using datapasta #the value at F21 was imputed as the average of its row before pasting #colnames a=am, p=pm bloodGlucose &lt;- data.frame( day01a = c(136L, 345L, 190L, 434L, 424L, 170L, 487L, 218L, 179L, 260L, 115L, 526L, 325L, 329L, 230L, 204L), day01p = c(270L, 518L, 301L, 504L, 486L, 208L, 449L, 273L, 184L, 381L, 191L, 517L, 252L, 296L, 414L, 120L), day03a = c(162L, 429L, 311L, 453L, 447L, 134L, 525L, 254L, 124L, 174L, 132L, 465L, 203L, 212L, 408L, 138L), day03p = c(165L, 413L, 361L, 392L, 417L, 129L, 419L, 265L, 107L, 140L, 132L, 394L, 158L, 159L, 179L, 139L), day05a = c(192L, 456L, 398L, 350L, 496L, 147L, 437L, 338L, 108L, 132L, 169L, 310L, 135L, 156L, 432L, 157L), day05p = c(397L, 487L, 465L, 400L, 484L, 141L, 476L, 386L, 149L, 138L, 158L, 269L, 162L, 200L, 288L, 122L), day07a = c(172L, 468L, 388L, 458L, 468L, 241L, 525L, 287L, 142L, 164L, 129L, 213L, 164L, 139L, 163L, 163L), day07p = c(148L, 419L, 392L, 387L, 423L, 128L, 499L, 236L, 143L, 137L, 120L, 185L, 181L, 143L, 240L, 168L), day09a = c(291L, 507L, 453L, 342L, 472L, 162L, 516L, 347L, 112L, 122L, 122L, 145L, 150L, 164L, 185L, 164L), day09p = c(239L, 559L, 421L, 368L, 507L, 163L, 485L, 235L, 233L, 140L, 157L, 201L, 177L, 150L, 208L, 128L), day11a = c(192L, 420L, 355L, 355L, 458L, 222L, 472L, 432L, 113L, 102L, 94L, 131L, 162L, 119L, 138L, 129L), day11p = c(172L, 415L, 381L, 429L, 456L, 438L, 535L, 450L, 137L, 174L, 141L, 258L, 192L, 193L, 208L, 218L), day12a = c(235L, 511L, 394L, 373L, 519L, 307L, 500L, 509L, 106L, 120L, 120L, 114L, 170L, 148L, 153L, 135L), day12p = c(153L, 464L, 444L, 501L, 570L, 252L, 497L, 326L, 150L, 135L, 166L, 160L, 162L, 188L, 140L, 182L) ) We calculate the correlation between any two daily sets of values. In fact, we can calculate the correlation between all possible pairs of daily values. This leaves us with a large number of unique correlation coefficients. An overall average correlation coefficient will be used in the Monte Carlo function. #create a full correlation matrix cormat &lt;- cor(bloodGlucose) #remove lower half of matrix due to duplication cormat[lower.tri(cormat)] &lt;- NA #remove matrix diagonal as uninformative cormat[cormat==1.0000000] &lt;- NA How correlated are the glucose levels in the Jaxwest7 data set? #calculate the average correlation coefficient among all the correlations in the Jaxwest7 glucose level data set mean(cormat, na.rm=T) ## [1] 0.7665732 #phew! What does this value mean and how to use it in a Monte Carlo? First, it can be shown that when the value of the correlation coefficient between the variables \\(X,Y\\) is \\(r\\), then the relationship between each pair of \\(x_i, y_i\\) values is \\[\\begin{equation} y_i=x_i\\times r+y_i\\sqrt{1-r^2} \\tag{23.1} \\end{equation}\\] Intuitively, this should make sense. When \\(r=0\\), then \\(x_i\\) does not predict \\(y_i\\), thus \\(y_i=y_i\\). When \\(r=1\\) the prediction is perfect so \\(y_i=x_i\\). Oh what the hell, heres a proof: x &lt;- rnorm(100000, 0, 1) y &lt;- rnorm(100000, 1, 1) paste(&quot;Random x and y are uncorrelated: r=&quot;, cor(x, y)) ## [1] &quot;Random x and y are uncorrelated: r= 0.00029079959446801&quot; r &lt;- 0.76 yc &lt;- x*r+y*sqrt(1-r^2) paste(&quot;Now they are: r=&quot;, cor(x, yc) ) ## [1] &quot;Now they are: r= 0.761306797196214&quot; The average correlation coefficiennt from the Jaxwest7 data set means that within each subject in our experiment the expected correlation between pre-drug glucose concentrations and post-drug glucose concentrations is about 0.7666. We take advantage of this deriving this estimate to simulate a group values that will be more life-like. 23.7.2 Step 2 Initialize the Monte Carlo with estimates for the measurement values. We start with the mean and sd values for the pre-drug blood glucose measurements. Their estimates are derived from the placebo group in the Jaxwest7 data set, rounded to 380 and 120, respectively. A scientifically-meaningful effect of the drug would be a 50% reduction in glucose. We want to set up an experiment that can detect that effect. The expected correlation between pairs of measures is 0.7666, rounded to 0.75. #Sampled population paramemters # pre-drug measurements mean1 &lt;- 185 sd1 &lt;- 120 # post-drug response mean2 &lt;- 380 sd2 &lt;- 120 r &lt;- 0.75 k &lt;- sqrt(1-r^2) # number of paired measures pairs &lt;- 4 23.7.3 Step 3 This step sets the arguments in the t-test function. Even though we predict a reduction in glucose, well test this as a two-tailed hypothesis. Its a little more stringent. The t.test function needs to be set for paired=TRUE so that it runs the appropriate test. #t-test function arguments alt&lt;- &quot;two.sided&quot; pairing &lt;- TRUE var &lt;- TRUE alpha &lt;- 0.05 23.8 Step 4 Declare the number of simulations. The larger the number of simulations, the more accurate will be the power calculation. Also set up an empty vector to fill with p-values, as they are generated each cycle. nSims &lt;- 10000 #number of simulated experiments p &lt;- c() 23.8.1 Step 5 Re-simulate and re-run the t-test nSims times. The y1 and y2 vectors are each a set of randomly generated values for the post- and pre-drug measurements, respectively. They are initially uncorrelated. But in a second step the y2 vector is corrected by the relatonship in Equation (23.1) aboves so that cor(y1, y2)=r. for(i in 1:nSims){ #for each simulated experiment y1&lt;-rnorm(n = pairs, mean = mean1, sd = sd1) #produce n simulated participants #with mean and SD y2&lt;-rnorm(n = pairs, mean = mean2, sd = sd2) #produce n simulated participants #with mean and SD #correlated y2 &lt;- r*y1+k*y2 z&lt;-t.test(y1,y2, alternative=alt, paired=pairing, var.equal=var, conf.level=1-alpha) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } 23.8.2 Step 6 Calculate power as the fraction of p-values less than 0.05. 1@ref(### Step 5) # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 8793 power &lt;- hits/nSims; power ## [1] 0.8793 Visualize the p-value distribution. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) 23.8.3 Step 7 Once the sample size is dialed in it is useful to re-evaluate assumptions about the effect size and the parameters of the sample. The analysis can also be re-run by changing the hypothesis, or the confidence level. The Monte Carlo is useful to test out a number of different scenarios. Once we are satisfied we have the right experimental design, we write all of these joices down in our notebookor register the study online. Save the power script for record keeping or to share with collaborators or with a consulting statistician. 23.9 Comparison to pwr tests Its natural not to trust Monte Carlo output when you dont fully understand the details. library(pwr) pwr.t.test(n=8, d=1.58, sig.level=0.05, type=&quot;two.sample&quot;, alternative=&quot;two.sided&quot;) ## ## Two-sample t test power calculation ## ## n = 8 ## d = 1.58 ## sig.level = 0.05 ## power = 0.8358049 ## alternative = two.sided ## ## NOTE: n is number in *each* group pwr.t.test(n=6, d=1.58, sig.level=0.05, type=&quot;paired&quot;, alternative=&quot;two.sided&quot;) ## ## Paired t test power calculation ## ## n = 6 ## d = 1.58 ## sig.level = 0.05 ## power = 0.867253 ## alternative = two.sided ## ## NOTE: n is number of *pairs* 23.10 Summary Monte Carlo methods are a great way to conduct a statistical design for an experiment. Writing a Monte Carlo method is also a great way to learn how to write custom functions. The result, which is either the statistical power of the sampling or a sample size necessary to achieve a specific level of power, should give results similar to other power functions in R. "],["tdist.html", "Chapter 24 t Distributions 24.1 dt 24.2 pt 24.3 qt 24.4 rt", " Chapter 24 t Distributions Sample means are a statistical model most appropriate when applied to groups of measured continuous data. Students t statistic is a transformation of measured data as a ratio of a sample mean to its standard error. Therefore, Students t distributions are continuous probability models used for comparing the signal to noise ratios of sample means. The t-distribution is used widely in experimental statistics, a) for experiments that compare one or two variables with t-tests, b) for post hoc tests following ANOVA, c) for confidence intervals and d) for testing regression coefficients. A sample for the variable \\(Y\\) with values \\(y_1, y_2,...,y_n\\), has a sample mean: \\(\\bar y =\\sum_{i=1}^ny_i\\). The degrees of freedom for the sample mean is \\(df=n-1\\). The sample standard deviation is \\(s=\\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{df}}\\) and the standard error of the mean for a sample is \\(sem=\\frac{s}{\\sqrt n}\\) The t-distribution can be scaled in three different ways, depending upon the experimental design: The t scale units are \\(sem\\) for a one sample t test: \\(t=\\frac{(\\bar y-\\mu)}{sem}\\) where \\(\\mu\\) is a hypothetical or population mean for comparison. \\(sedm\\) for a two sample unpaired t test: \\(t=\\frac{\\bar y_A-\\bar y_B}{sedm}\\) where \\(\\bar y_A\\) and \\(\\bar y_B\\) are the means the uncorrelated groups A and B comparison, \\(s_p^{2}\\) is the pooled variance and \\(sedm=\\sqrt{\\frac{s_p{^2}}{n_A}+\\frac{s_p{^2}}{n_B}}\\) is the standard error for the difference between the two means. \\(sem_d\\) for a two sample paired t test: \\(t=\\frac{\\bar d}{sem_d}\\), where \\(\\bar d\\) is the mean of the treatment differences between correlatd pairs whose variance is \\(s_d^{2}\\), and \\(sem_d=\\sqrt\\frac{s_p^{2}}{n}\\). 24.1 dt dt is a continous probability density function of the \\(t\\) test statistic. \\[p(t)=\\frac{\\Gamma(\\frac{df+1}{2})}{\\sqrt{df\\pi}\\Gamma(\\frac{df}{2})}(1+\\frac{t^2}{df})^{-(\\frac{df+1}{2})}\\] Thedtfunction takes two arguments, a value of \\(t\\) derived from an experimental dataset, and also a value for the \\(df\\) of the sample. Lets assume a simple one-sample t test was performed. The sample had 3 independent replicates, and thus 2 degrees of freedom. The value for \\(t\\) calculated from the test is 3.3. The exact probability for that value of \\(t\\) is: dt(3.3, 2) ## [1] 0.0216083 That is not a p-value. Alone, a single probability value from a continuous distribution such as this is not particularly useful. But a range of \\(t\\) values can be interesting to model. Note how this is a continuous function, thus we draw a line graph rather than columns. df &lt;- 2 t &lt;- seq(-5, 5, 0.001) data &lt;- data.frame(dt=dt(t, df)) g &lt;- ggplot(data, aes(x=t, y=dt))+ geom_line()+ scale_x_continuous(breaks=seq(-5,5,1)) + xlab(&quot;t&quot;) +ylab(&quot;p(t)&quot;); g A couple of important features of the \\(t\\) probability density function: 1) there is a unique \\(t\\) distribution for every sample size, 2) the t distribution approaches the normal distribution with larger sample sizes. Heres a plot comparing a sample size of 3 (\\(df=2\\)), 6 (\\(df=5\\)), 51 (\\(df=50\\)) and the normal distribution. Relative to the normal distribution, the \\(t\\) distributions at these \\(df\\) are heavy shouldered. Its as if a finger is pressing down from the top, spreading the distribution on the sides. This has the effect of increasing the area under the curves, relative to the normal distribution, at more extreme values on the x-axis. Increase the \\(df\\) for the blue-colored plot. At what values do you think it best approximates the normal distribution? g + stat_function(fun=dnorm, args=list(mean=0, sd=1), color=&quot;red&quot;) + stat_function(fun=dt, args=list(df=5), color=&quot;blue&quot;)+ stat_function(fun=dt, args=list(df=50), color=&quot;green&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.35, label=&quot;N(0,1)&quot;, color=&quot;red&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.3, label=&quot;t(df=50)&quot;, color=&quot;green&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.25, label=&quot;t(df=5)&quot;, color=&quot;blue&quot;)+ annotate(&quot;text&quot;, x=2.5, y=0.2, label=&quot;t(df=2)&quot;, color=&quot;black&quot;)+ labs(x=&quot;t or z&quot;)+ theme_bw() One additional feature of \\(t\\) distributions is the \\(ncp\\) argument, the non-centrality parameter. Full treatment of non-centrality is quite involved and beyond the scope here. Suffice to say that a distribution with \\(ncp&gt;0\\) would differ from a null distribution. Thus, \\(ncp\\) is used when simulating alternative distributions, for example, for the expectation of skewed data in power analysis. df &lt;- 2 ncp &lt;- 1 t &lt;- seq(-5, 5, 0.001) data &lt;- data.frame(dt=dt(t, df, ncp)) g &lt;- ggplot(data, aes(x=t, y=dt))+ geom_line()+ scale_x_continuous(breaks=seq(-5,5,1)) + xlab(&quot;t&quot;) +ylab(&quot;p(t)&quot;); g 24.2 pt If given a \\(t\\) ratio from a comparison and also the \\(df\\) for the test, pt can be used to generate a p-value. As the cumulative probability function for the \\(t\\) distribution pt returns the area under the curve when given these arguments. Thus, about 96% of the area under the curve is to the left of a \\(t\\) value of 3.3 at \\(df\\)=2, and about 4% of the AUC is to the right of that value. pt(q=3.3, df=2, lower.tail =T) ## [1] 0.9595762 pt(q=3.3, df=2, lower.tail=F) ## [1] 0.04042385 Thats precisely what is depicted graphically here, with navy representing the lower tail and green the upper tail of the cumulative function on either side of \\(t_{df2}=3.3\\): ggplot(data.frame(x=c(-5,5)), aes(x)) + stat_function(fun=dt, args=list(df=2), geom=&quot;line&quot;, color=&quot;black&quot;) + stat_function(fun=dt, args=list(df=2), xlim = c(-5, 3.3), geom = &quot;area&quot;, fill= &quot;navy&quot;) + stat_function(fun=dt, args=list(df=2), xlim = c(3.3, 5), geom = &quot;area&quot;, fill= &quot;green&quot;) + scale_x_continuous(breaks=seq(-5,5,1)) + xlab(&quot;t&quot;) +ylab(&quot;p(t)&quot;) 24.3 qt The inverse cumulative function qt is most useful as a tool to generate critical value limits. This is a particularly important function given its use in constructing confidence intervals. For example, the two sided, 95% critical limits for \\(t_{df2}\\) are: qt(.025, 2) ## [1] -4.302653 qt(0.025, 2, lower.tail=F) ## [1] 4.302653 Whereas each of the one-sided 95% critical limits \\(t_{df2}\\) are: qt(0.05, 2) ## [1] -2.919986 qt(0.05, 2, lower.tail=F) ## [1] 2.919986 The inverse cumulative distribution is shown here: df &lt;- 2 x &lt;- seq(0, 1, 0.01) data &lt;- data.frame(qt=qt(x, df)) g &lt;- ggplot(data, aes(x=x, y=qt))+ geom_line()+ scale_x_continuous(breaks=seq(0,1,.1)) + xlab(&quot;p&quot;) +ylab(&quot;t&quot;); g 24.4 rt Finally, the rt function can be used to simulate a random sample of t values for a distribution with \\(df\\) degrees of freedom. For example, here are 5 t values from a \\(df2\\) and another 4 from \\(df20\\). set.seed(12345) rt(5, 2) ## [1] 0.5514286 -0.2275454 4.1377301 -1.1483718 -0.4579998 rt(5, 20) ## [1] -0.28197205 -0.06705986 -0.28785784 0.27207442 1.66794394 "],["simcorrelation.html", "Chapter 25 Simulating correlated variables 25.1 Estimating correlation between two variables 25.2 Simulating correlated variables 25.3 Monte Carlo simulation", " Chapter 25 Simulating correlated variables library(pwr) library(tidyverse) Experimental designs involving paired (or related/repeated) measures are executed when two or more groups of measurements are expected to be intrinsically-linked. Take for example, a before and after design. A measure is taken before the imposition of some level of a predictor variable. Then the measure is taken afterwards. The difference between those two measures is the size of the effect. Those two measures are intrinsically-linked because they arise from a common subject. Subjects are tuned differently. You can imagine a subject who displays a low basal level of the measure will generate a low-end response after some inducer, whereas one with a high basal level will generate a high-end response. Statistically, these intrinsically-linked measurements within such designs are said to be correlated. Monte Carlo simulations of experimental power afford the opportunity to account for the level of correlation within a variable. Building in an expectation for correlation can dramatically impact the expected power, and thus the sample size to plan for. 25.1 Estimating correlation between two variables How to estimate correlation? Inevitably youll run an experiment where the actual values of the dependent variables, at first blush, differ wildly from replicate to replicate. But on closer inspection, a more consistent pattern emerges. For example, an inducer seems to always elicits close to a 2-fold response relative to a control, and this response is consistently inhibited by about a half by a suppressor. That consistency in the fold-response, irrespective of the absolute values of the variable, is the mark of high correlation! Here are some data to illustrate this problem. Four independent replicates of the same experiment that measures NFAT-driven luciferase reporter gene output, on each of 4 different passages of a cultured cell line. The data have several other treatment levels, but those corresponding to vehicle and drug represent negative and positive responses, respectively. Luciferase reacts with luciferin to produce light. The values here are in arbitrary light units on a continuous scale beginning at zero and linear for up to at least 5 orders of magnitude higher. Thus, values of the variable can be assumed to be normally-distributed. Heres the experimental data: Table 25.1: NFAT-Luciferase reporter values of different replicates. P11, P12 represent different passages of a cell line. id vehicle drug P11 20.2 38.3 P12 5.7 9.1 P13 2.1 3.6 P14 9.9 15.5 The data show that the luciferase values in response to vehicle wanders substantially across passages over a 10-fold range. Yet the drug response as a ratio to the vehicle is more consistent from passage to passage. In fact, the two variables, vehicle and drug, are actually very highly correlated: cor(df$vehicle, df$drug) ## [1] 0.9955158 ggplot(df, aes(vehicle, drug))+ geom_point(size=4, color=&quot;#012169&quot;) This example points to how you can derive an estimate for the correlation coefficient between two variables. Simply plot out their replicates as \\(XY\\) pairs and calculate their correlation coefficient using Rs cor function. Where do you find values for these variables? They can come from pilot or from published data. 25.2 Simulating correlated variables It can be shown that when the correlation coefficient between a pair of random variables \\(X, Y\\) is \\(r\\), then for each \\(x_i, y_i\\) pair, a correlatd value of \\(y_i\\) can be calculated as \\(z_i\\) by \\[z_i=x_ir+y_i\\sqrt{1-r^2}\\] Thus, we can first simulate a random pair of \\(X,Y\\) values, then convert the values of \\(Y\\) into \\(Z\\), such that the \\(X,Z\\) values are correlated. Using the luciferase example above, heres some code to accomplish that. Each pair is initially uncorrelated, but then becomes correlated after using the relationship above. There is a slight twist in this. When using an rnorm function with the means and sd estimates from the table above, negative values will be produced. However, the luciferase values are a ratio scale, with an absolute 0 value. The code below uses the absto simulate only positive values. This generates a skewed normal distribution #first simulate and view uncorrelated random variables set.seed(1234) x &lt;- abs(rnorm(10000, 10, 8)) y &lt;- abs(rnorm(10000, 17, 15)) cor(x,y) ## [1] 0.004357573 #scatter plot the simulated vectors ggplot(data.frame(x,y), aes(x,y))+ geom_point() #now convert y to z, so that it correlates to x r=0.99 k&lt;- sqrt(1-r^2) z &lt;- r*x+k*y #confirm the correlation ggplot(data.frame(x,z), aes(x,z))+ geom_point() cor(x, z) ## [1] 0.9673284 #explore the distribution of z ggplot(data.frame(x,z))+ geom_histogram(aes(z))+ geom_histogram(aes(x)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 25.3 Monte Carlo simulation Heres a Monte Carlo simulation of a paired t-test between an A and a B group. The true effect size programmed to be very modest. The code also factors in a fairly strong correlation between the two measures of the variable. #Initial Parameters # sample A intial true parameters nA &lt;- 3 meanA &lt;- 1.5 sdA &lt;- 0.5 # sample B intial true parameters nB &lt;- 3 meanB &lt;- 2.0 sdB &lt;- 0.5 alpha &lt;- 0.05 nSims &lt;- 10000 #number of simulated experiments p &lt;-numeric(nSims) #set up empty container for all simulated p-values # correlation coefficient r &lt;- 0.8 # the monte carlo function for(i in 1:nSims){ #for each simulated experiment x&lt;-rnorm(n = nA, mean = meanA, sd = sdA) #produce n simulated participants #with mean and SD y&lt;-rnorm(n = nB, mean = meanB, sd = sdB) #produce n simulated participants #with mean and SD #correlated w &lt;- r*x+sqrt(1-r^2)*y z&lt;-t.test(x,w, paired=T) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } # the output hits &lt;- length(which(p &lt; alpha));hits ## [1] 7027 power &lt;- hits/nSims;power ## [1] 0.7027 #now plot the histogram #main=&quot;Histogram of p-values under the null&quot;, hist(p, col = &quot;blue&quot;, ylim = c(0, 10000), xlim = c(0.0, 1.0), main =&quot;Histogram of simulated p-values&quot;, xlab=(&quot;Observed p-value&quot;)) The result as written above is a bit underpowered, but not too shabby. Now run the code by dialing down the correlation to an r = 0. How much more underpowered is the planned experiment? Factoring in the correlation between variables makes a huge difference. "],["introanova.html", "Chapter 26 Introduction to ANOVA 26.1 Assumptions 26.2 Types of designs 26.3 Jargon: Factors and levels 26.4 ANOVA models: One-, Two-, and Three-way 26.5 Inference 26.6 ANOVA calculations 26.7 Completely randomized or related measures 26.8 Two-way ANOVA 26.9 Other ANOVA models 26.10 Alternatives to ANOVA 26.11 Summary", " Chapter 26 Introduction to ANOVA library(tidyverse) library(RColorBrewer) The choice of any statistical design and analysis is always driven by the type of outcome and predictor variables involved. Recall that all variables are either continuous or discrete. Furthermore, its helpful to think of outcome variables further classified as either measured, ordered or sorted, where measured variables are continuous, and ordered and sorted are discrete. Once the dependent variable is deemed to be continuous measured, the model for the experiment is determined by the number of levels of the independent variables. Figure 26.1: ANOVA heuristic The analysis of variance (ANOVA) is a method to design and evaluate experiments in which the predictor variable(s) are discrete factors with three or more levels and when the outcome variable is on some continuous measured scale. ANOVA is also univariate, in so far as the analysis involves only a single outcome variable. 26.1 Assumptions The validity of an ANOVA depends upon fulfilling the following assumptions. These are basically the same as we discussed for t-testing. Strong assumptions, do not use ANOVA when violated Every replicate is independent of all others. Some random process is used when generating measurements. Weaker assumptions, ANOVA is said to be robust to violations of the following The distribution for the residuals from which the outcome variable is derived is continuous random normal. The variances of the groups are approximately equal. With small samples it is usually difficult to conclude that these weaker assumptions are met. Both affirmative and negative results of tests of normality, tests for homogeneity of variance and outlier tests should be taken with a grain of salt. The smaller the sample size, the larger the grain of salt. Use your judgment. Do you have any reason to believe the variable is not normally distributed? Are you not up against a zero boundary? Are you confident the variable is measured in a linear range of your assay procedure? If in doubt, analyze the data with some other procedure. Data that appears markedly skewed can be transformed using log or reciprocal functions. The resulting distributions are ~ normal. ANOVA testing can be performed on those transformed values. As we discussed previously, the nonparametric analogs of ANOVA (Kruskal-Wallis and Friedman tests) are suitable options when normality is violated. There are alternative analytic options as will be detailed below. In particular, there is no need to use ANOVA for discrete types of outcome data. ANOVA is not designed to analyze non-continuous data. Additionally, regression, either linear or nonlinear, is often a preferred method of analysis when the predictor variable is continuous rather than discrete. 26.2 Types of designs tl;dr: ANOVA represents a family of about a dozen different statistical models. These differ by how many predictor factors are involved and whether or not the measurements are intrinsically-related. A given factor can be applied in either a completely randomized or repeated/related measures format. Experiments can involve one, two or even three factors. ANOVA experiments tend to design themselves since they are fairly intuitive way of asking questions. In fact, ANOVA is probably the most widely used experimental design in the biomedical sciences. If we randomly access any article in our favorite journal, chances are it will have a figure or table depicting an ANOVA design. They tend to look something like these. ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.y` is deprecated. Use `fun` instead. Figure 26.2: Typical graphs depicting some of the different ANOVA designs. Why is ANOVA so common? First, ANOVA is versatile. One or more factors, each at several levels, can be tested simultaneously. We can test for the main effect of each factor, or for interactions between factors. We can also test for differences between specific individual groups within a context of many other groups, using post hoc analysis. Designs involving multiple instrinsically-related measurements from a common replicate are readily accommodated. We can have mixed designs where one factor is completely randomized and another is related measure within a single multi-factor experiments. Second, ANOVA is efficient. As a general rule, fewer replicates are needed to make the same number of pairwise group comparisons than would otherwise be necessary using a t-test-based experimental design. That efficiency can improve modestly as the number of groups increases. Third, we can test many hypotheses simultaneously or a grand hypothesis. Since each individual hypothesis test carries a risk of type1 error, ANOVA serves as a protocol to detect differences between many groups while ensuring that the overall type1 error, the so-called experimentwise error or the family-wise error rate (FWER), wont inflate to intolerable levels. 26.3 Jargon: Factors and levels In jargon, ANOVA predictor variables are classified as factors. ANOVA designs are said to be factorial. They are multifactorial if more than a single factor is involved. In other corners, ANOVA is referred to as factorial analysis (which should not be confused with factor analysis). Where some people describe an ANOVA experiment as a one-way ANOVA others might describe it as one-factor ANOVA. Yet elsewhere it is called multiple regression. Its all the same. The factors of ANOVA represent categorical, discrete variables that are each applied at two or more levels. For example, a factor at three levels is a predictor variable that has three discrete values. Imagine an experiment to explore how a particular gene influences blood glucose levels. Blood glucose levels, a continuous variable, are measured in subjects that have three different genotypes for the gene: either wild-type, or heterozygous knockouts, or homozygous knockouts. Here, genotype is a discrete predictor variable, a factor, which has three levels. 26.3.1 Woring with factors in R R requires that ANOVA predictor variables are classified as factors in data sets. The following script creates a vector object called genotype. The object is a representation of the genotype variable. The data class for that vector is character because it is comprised of character strings. But look what happens when it is packaged into a data frame called my.factors using the data.frame. R coerces genotype into a factor variable with 3 levels. Which is nice. genotype &lt;- c(&quot;wild-type&quot;, &quot;heterozygote&quot;, &quot;homozygote&quot;) class(genotype) ## [1] &quot;character&quot; my.factors &lt;- data.frame(genotype) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 1 variable: ## $ genotype: chr &quot;wild-type&quot; &quot;heterozygote&quot; &quot;homozygote&quot; But beware. The tidyverse tibble function is an analog of data.frame. Yet tibble does not coerce factors from character vectors. If using tibble or functions that create tibbles, such as read_csv, well need to remember to convert by hand using functions like as_factor. genotype &lt;- c(&quot;wild-type&quot;, &quot;heterozygote&quot;, &quot;homozygote&quot;) class(genotype) ## [1] &quot;character&quot; my.factors &lt;- tibble(genotype) str(my.factors) ## tibble [3 x 1] (S3: tbl_df/tbl/data.frame) ## $ genotype: chr [1:3] &quot;wild-type&quot; &quot;heterozygote&quot; &quot;homozygote&quot; as_factor(genotype) ## [1] wild-type heterozygote homozygote ## Levels: wild-type heterozygote homozygote This coercion wont occur if a factor in an experiment represents a variable with numeric values. To illustrate what I mean by this, imagine adding a second factor to the genotype experiment. We wish to test for the effect of an antidiabetic drug on blood glucose at 0, 10 and 30 microgram/kg. These effects would be measured at each level of the genotype factor, leading to a nifty 3 X 3 factorial design. We would create the vector drug as an object representing the drug variable and its three levels as follows. Note however that here, R does not coerce numeric values as factors: drug &lt;- c(0, 10, 30) my.factors &lt;- data.frame(genotype, drug) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ genotype: chr &quot;wild-type&quot; &quot;heterozygote&quot; &quot;homozygote&quot; ## $ drug : num 0 10 30 class(drug) ## [1] &quot;numeric&quot; Thats easily fixed using the as.factor or as_factor function. Whatev. drug &lt;- as_factor(c(0, 10, 30)) my.factors &lt;- data.frame(genotype, drug) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ genotype: chr &quot;wild-type&quot; &quot;heterozygote&quot; &quot;homozygote&quot; ## $ drug : Factor w/ 3 levels &quot;0&quot;,&quot;10&quot;,&quot;30&quot;: 1 2 3 class(drug) ## [1] &quot;factor&quot; Alternately, we would enter the drug levels as character strings (tossing in some other random character value), which would be coerced into a factor at two levels when added to a data frame, like for the first example above: drug &lt;- c(&quot;zero&quot;, &quot;dark&quot;, &quot;thirty&quot;) When you forget to convert a variable to a factor, Rs anova functions will bark at you and you wont know what the problem is because the error messages are confusing. R doesnt know that you dont know you should be using factors. You can do ANOVA with continuous predictor variables. You just have to treat them as factors. When a continuous predictor variable is used at many levels (eg, a time series, a dose series, etc), regression per se can be a better alternative to ANOVA. Regression allows for capturing additional information encoded within continuous variables (eg, rate constants, half-lives, etc). But that decision is more scientific than it is statistical. 26.4 ANOVA models: One-, Two-, and Three-way If an experiment has only one factor, it is a one-way ANOVA design (alternately, one-factor ANOVA). If an experiment has two factors, it is a two-way ANOVA or two factor design. Three factors? Three-way ANOVA. All of these models can also be either completely randomized, repeated/related measures, or mixed. In a completely randomized structure, every level of the factor(s) is randomly assigned to replicates. Every replicate measurement is independent from all others. The two group analog is the unpaired t-test. In a related measures structure, measurements within an experimental unit are intrinsically-linked. Every replicate receives all levels of a factor (eg, before-after, stepped dosing, or when subjects are highly homogeneous, such as cultured cells or inbred animals, or repeated recording from the same cell, etc). Thus, the possible ANOVA models are quite diverse: One-way ANOVA -completely randomized One-way ANOVA -related measures Two-way ANOVA -completely randomized on both factors Two-way ANOVA -related measures on both factors Two-way ANOVA -mixed, one factor completely randomized, the other factor related measures Three-way ANOVA -can be purely CR, ourely RM or mixed I should mention that ANOVA for even more than three factors is conceptually possible. However, such large, complex designs have considerable downside where, by ANOVA analysis, it is difficult to tease out which factor and level is responsible for what. Three-way ANOVA, for example, allows for such a large number of hypotheses to be tested (three different main effects and four possible interaction effects, not to mention the large number of post hoc group comparisons) that it can be difficult to conclude exactly which factor at what level is responsible for any observed effects. These larger designs also tend to break the efficiency rule, its fair to say that three way ANOVA designs tend to be over-ambitious experimentsover-designed and usually under powered for the large number of hypotheses they can test. As a general rule, I advise against doing them. Designing an experiment as completely randomized or related measures is largely a scientific, not a statistical, decision. Measurements are either intrinsically-linked or they are not based upon the nature of the biological material that we have at hand. Or because of the way the experiment is being conducted. When weve concluded that measurements are intrinsically-linked, then the choice must be a related measures design and analysis. Similarly, choosing to run experiments as one-way or two-way ANOVA designs is also scientific. In fact, intuitively, we choose a two-way design mostly to test whether two factors interact. When not interested in an interaction, it is usually best not to combine them in an experiment. The main effect questions may be more safely answered using separate one-way ANOVAs, instead. Well discuss interaction hypotheses and effects in more detail when we arrive at our two-way ANOVA discussion in the course. 26.5 Inference ANOVA can be used inferentially as stand alone test for the main effect of a factor. More commonly, it is used as an omnibus test for permission to explore group differences. Most of the time we see it used as the latter but there is no rule that this must be so. A positive ANOVA test result can be used to infer whether a factor, or an interaction between factors, is effective. Continuing with the example, I can use a positive F-test to conclude that genotype at a given locus influences blood glucose. And just leave it at that, without demonstrating which levels of the genotype differ from each other. I would write it up as follows: The genotype influences blood glucose (one-way related measures ANOVA, F(2,15)=11.48, p=0.0009). Then I will point to the data, which will speak for itself revealing which groups actually differ. Alternately, a positive F-test result can be used as an omnibus. Here, a positive F-test implies that at least two levels of a factor differ from each other. The positive F-test grants us access to finding exactly where those differences exist. This is done by making posthoc pairwise comparisons. We would, perhaps, compare each of the responses to the three genotypes to each other; for a total of 3 t-test like comparisons. I would adjust the p-values of each of those outcomes to control the FWER for those multiple comparisons. The decision to use the F-test as a stand alone or as an omnibus is driven by our scientific objectives. Figure 26.3: ANOVA work flow at a 5% type1 error threshold. 26.5.1 A quick peek at posthoc testing These post hoc comparisons are, essentially, any of several variations on the t-test that are performed simultaneously. See Chapter 33 for more information about these. These are designed in a way to limit the FWER to the predetermined type1 error threshold. Instead of all 5% error being devoted to compare two groups, the 5% is distrubuted across many comparisions. Sometimes dozens or more. The choices for comparisons after the F-test can be either planned or unplanned. They should be driven by scientific, rather than statistical, reasoning. Unplanned is comparing all levels of all factors to each other. We might do that if we are looking for any and all differences on a project that is more exploratory in scope, such as in a hit screen. Planned comparisons focus in on a much more limited subset of groups. We do this when the experiment is designed to test a fairly specific hypothesis or group of hypotheses. What is important, statistically, is to make adjustments to the p-value threshold given all the comparisons made, so that the overall experimentwise type1 error does not exceed our declared threshold (usually 5%). In an experiment whose number of groups equals \\(k\\), there are a total of \\(m=\\frac{k(k-1)}{2}\\) possible comparisons to make. Lets use the simplest case of a \\(k=3\\) groups ANOVA as an example, such as the glucose genotype discussed above. There are \\(m=\\frac{3(3-1)}{2}=3\\) comparisons that can be made an imagine using the Bonferroni correction (\\(\\alpha_{adjust}=\\frac{0.05}{m}=0.01667\\)) the type1 error threshold is now 0.01667. Thus, our adjusted threshold for declaring a statistical difference between 2 groups is now p &lt; 0.01667, rather than p &lt; 0.05. If doing the 3 X 3 two-way ANOVA with genotype and drug as two factors, we now have \\(k=9\\) groups, yielding 36 possible comparisons and the threshold \\(\\alpha_{adjust}=\\frac{0.05}{m}=0.001389\\) for each comparison. 26.6 ANOVA calculations The simplest way to think about ANOVA is that it operates like a variance budgeting tool. In the final analysis, the more variance associated with the grouping factor(s) compared to the residual variance, the more likely that some group means will differ. ANOVA uses the least squares method to derive and account for sources of variation within a data set. Recall, our measurements of the random variable \\(Y\\) have values of \\(y_i\\) for \\(i=1, 2,..n\\) and a sample mean \\(\\bar y\\). Variance is calculated by dividing the sum of its squared deviates, \\(SS\\), by the sample degrees of freedom (df). \\[var(Y)=\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{n-1}=\\frac{SS}{df}=MS\\] In ANOVA jargon the variance is also commonly referred to as the mean square \\(MS\\). This jargon emphasizes that variance can be thought of as an averaged deviation for sample measurements from their mean values. That formula only illustrates how variance is calculated for a single sample group. What about multiple groups? As you might imagine, we have to incorporate information from all of the groups. To begin to understand that, recognize that all ANOVAs, irrespective of the specific design are assumed to have two fundamental sources of variation: Variation due to the experimental model, which is determined by the nature of responses to the predictor variables. Residual variation, which is variation that cannot be explained by the predictor variables. Law of Conservation of SS: That total variation within an experiment can be expressed as the sum of the squared deviates for these two sources. \\[SS_{total}=SS_{model}+SS_{residual} \\] Lets imagine a simple one-way completely randomized ANOVA data set that looks like the graph below. There is only one factor, which has three levels. Each group has five independent replicates, for a total of 15 replicates within the entire experiment. Our model seeks to account for the effect of genotype \\[SS_{model}=SS_{genotype} \\] Models are perfect but data are not. Not every data point is equal to the value of the black bar it is near. The residual variation are these differences and thus \\[SS_{total}=SS_{genotype}+SS_{residual}\\] The graph illustrates each data point, the means for each group (black bars) and the grand mean of the sample (gold bars). We can readily imagine the distances from the data points to the grand mean. We can readily imagine the distances from the data points to the group means. One can also appreciate and the distances from the group means to the grand mean. We probably have a harder time visualizing the squares of those distances. My mind sees it Euclidean (geometrically). Larger distances, squared, lead to bigger boxes! The bigger the boxes, the greater that replicate contributes to the variance. With that picture in mind, think of the variances within the experiment as follows: Total variance: average squared distances of the all the points to the grand mean Model variance: weighted average squared distances of group means to the grand mean Residual variance: average squared distances of the points to the group means Figure 26.4: A completely randomized one way ANOVA, the genotype factor has three levels. Gold bar = grand mean, black bar = group means 26.6.1 Sums of Squares partitioning The first step in an ANOVA involves partitioning the variation in a data set using sums of squares. In this experiment, there are \\(i=1, 2,...n\\) independent replicates. There are also \\(j=1, 2,...k\\) groups. The total sum of squares is the sum of the squared deviation from all data points to \\(\\hat y\\), which represents the grand mean of the sample. \\[SS_{total}=\\sum_{j=1}^k\\sum_{i=1}^n(y_i-\\hat y)^2\\] The sum of squares for the genotype effect is sum of the weighted squared deviation between the group means, \\(\\bar y_j\\) and the grand mean. Here, \\(n_j\\) is the sample size within the \\(j^{th}\\) group. Thus, the group sample sizes are creating that weight. \\[SS_{genotype}=\\sum_{j=1}^kn_j(\\bar y_j-\\hat y)^2\\] Anova software usually refers to this variation as the treatment sum of squares. Finally, the residual sum of squares is calculated as the sum of the squared deviation between replicate values and group means, \\[SS_{residual}=\\sum_{j=1}^k\\sum_{i=1}^n(y_i-\\bar y_j)^2\\] which, because the total variation amount of is fixed, can also be calculated as follows: \\[SS_{residual}=SS_{total}-SS_{genotype}\\] Software will tend to label this as either residual variation or as error. The term error arises from ANOVA theory, which holds that the true population means represented by these sample groups are fixed in the population. Thus, any deviation from the means is explained by random error. Residual is jargon for the measurements of that error. Perhaps we can intuit a few things. First, the residual variation is the variation unaccounted for by the model. Meaning that whatever its causes, they are not under experimental control. If the data were perfectly explained by the model, every data point would be identical to a value of a group mean. Second, if the variation around each group mean remains the same, but as the group means differ from each other more and more, the greater the fraction of the overall variation that will be associated with the model, and the less that will be associated with the residual. Third, when the noise around those group means increases, less of the total variation will be associated with the model of group means, and the more with the residual. In other words, big effect sizes are easier to detect, while noisy experiments tend to hide detectable differences between means, whereas clean experiments favor detecting these differences. If this seems bloody obvious, and it is simple, then we should not have any problem processing how ANOVA works. The following two graphs emphasize these observations. In the null graph, the group means are roughly equivalent and very nearly the same as the grand mean. Theres very little model variation relative to the grand mean. Most of the variation is in the residuals, relative to each group mean. In the effective treatment graph, where the means truly differ because I coded them to differ, the residual variation is about the same as the null. The distance from each point to its group mean is about the same as in the null case. But we can see there is a lot more model variation, at least compared to the null graph: The group means (black bars) really separate from the grand mean (gold line). ANOVA tests become very simple when we understand this. The variation associated with the model is the signal and the residual variation is the noise. F tests are simple ratios between the model variance (or elements of the model) and residual variance. F tests are simple signal-to-noise ratios, just like every other statistical test. 26.6.2 The ANOVA table The typical ANOVA table lists the following: source of variation, its \\(df\\), its \\(SS\\), its \\(MS\\) an F-test, where appropriate a p-value from the F-test ANOVA functions in R vary in their output. But heres the ANOVA table output for the data in the last previous figure: anova(lm(blood_glucose ~ genotype, data)) ## Analysis of Variance Table ## ## Response: blood_glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 2 21176.9 10588.4 22.979 7.877e-05 *** ## Residuals 12 5529.4 460.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 26.6.2.1 Sums of squares As described above. These are the absolute measures of variation or deviation. Their values are in squared units of the response variable. Weird. But all \\(SS\\) sources will sum to \\(SS_{total}\\) and that is helpful. 26.6.2.2 Degrees of freedom Variance is averaged deviation. The \\(SS\\) averaged. To calculate a variance well need to divide the sum of squares by the degrees of freedom for that sum of squares. Just as sum of squares are calculated differently depending on whether it is total, or model or residual, so too are degrees of freedom. The mathematical theory behind \\(df\\) is a bit more complicated than this, but as a general rule, we lose a degree of freedom every time the calculation of a mean is involved in determination of a given \\(SS\\). The basic idea is this: For that mean value to be true, one of the values used in the calculation must remain fixed, while all the others are free to vary. The degrees of freedom for total variance are \\(df_{total}=N-1\\). We use all 15 replicates, \\(N\\), to calculate the grand mean \\(\\hat y\\). One degree of freedom is lost for that grand mean to be true, one of those replicate values must be fixed while the others are free to vary. We have \\(k\\) predictor levels. The degrees of freedom for the genotype model variance are \\(df_{genotype}=k-1\\), because the \\(SS_{model}\\) calculation is based upon the group means, the value of one group mean must be fixed while two are free to vary. The residual degrees of freedom are \\(df_{residual}=N-k\\) because although residual values are calculated for each data point, one degree of freedom is lost to calculate each group mean. 26.6.2.3 The mean squares The mean squares are ANOVA jargon to represent variances. Think of variance as averaged variation or averaged deviation. Total variance is \\[MS_{total}=\\frac{SS_{total}}{df_{total}} \\] The variance associated with the model is \\[MS_{model}=\\frac{SS_{model}}{df_{model}} \\] And the residual variance is \\[MS_{residual}=\\frac{SS_{residual}}{df_{residual}} \\] 26.6.2.4 The F statistic and p-value The null distributions of the F statistic are explained in Chapter 27. The value of F is the ratio of two variances. For the result in the ANOVA table above \\(F_{2,12}=\\frac{10588.4}{460.8}=22.979\\). We can interpret that as saying the variance associated with the genotype model is 22.979-fold greater than the residual variance. But is an F-statistic value of 22.979 with 2 and 12 degrees of freedom extreme? The p-value is derived from an null F probability distribution that has 2 and 12 degrees of freedom. The p-value result in the table (Pr(&gt;F)) is obtained using Rs pf function: pf(22.979, 2, 12, lower.tail=F) ## [1] 7.87784e-05 The example above is a simple one factor completely randomized model for which there is only one F test, because it has only one component. Experimental designs that have more components, such as two or more factors with our without related measures, will have more F tests if completely randomized. A three-way ANOVA performed a certain way can have about a dozen F tests. 26.6.3 Post-hoc group comparisons A positive ANOVA F test only tells you that the model (or its component) explains the response better than null. The final stage of ANOVA, which is optional but done very commonly, is to run what are essentially t-test-like comparisons between groups. These posthoc tests help identify the specific factor levels that actually differ from each other. The total number of comparisons that could be made are \\(\\frac{k(k-1)}{2}\\). Sometimes we are fishing and want to do just that. Other timest, for scientific reasons, only a specific subset are of any interest. For example, we would only want to compare test groups to a negative control group. There are several ways to run these post-hoc group comparisons. See Chapter 33 for more information about these various options. What is most important, however, is these be done in a way that keeps the family-wise error rate (FWER) below the pre-set type1 error threshold. That threshold is commonly 5% in biomedical research, but its value can be whatever you decide. When would we change that threshold? Consider when we are conducting an exploratory hit screen involving large numbrers of groups. For example, we have a lot of RNAseq data. Scientifically the mistake of throwing away false negatives might be worse than the mistake of accepting false positives. In that case, raising the type1 error threshold to 10% has merit. 26.7 Completely randomized or related measures Related measures (RM) ANOVA is done when the measurements are not completely independent, but instead are intrinsically-linked. Examples of intrinsically-linked subjects include two measurements, one from each of a pair of identical human twins, before and after on a single subject, all plates and wells from a single batch or passage of a cell culture, a protein preparation from a single batch, a single cell in culture, split tissues from one animal subject, and litter mates of isogenic animal strains. There are certainly others. An RM design involves taking multiple measurements from each independent replicate. Each replicate may vary randomly. That random variation between replicates can be accounted for, too. Because we can account for the variation within each replicate, that source of variation is no longer in the residual error term, but can be taken right into the model. 26.7.1 The problem of lost data in related measures designs The CR vs RM design decision has a few important consequences. First, when within-subject correlation is high, RM are much more efficient and less costly to produce. How much more? You can run Monte Carlo simulations to establish this for virtually any set of conditions you can imagine. Second, over the course of any experiment it is possible to lose specific response values here and there. For example, a data value may be lost due to a bad lane in a replicate western blot, or we accidentally throw away a tube from a series, or we have a spill, or any of a number of such primitive errors. CR ANOVA is tolerant of such losses. That leads to a missing replicate, and an unbalanced data sets, but its only one or a few values out of many. That is not always the case with related measures designs. All of the values for every level of every factor for every replicate must be included for the analysis. If any values are missing for a given replicate, all of the remaining values for that replicate either have to be censored, or the missing values should be imputed. The missing data problem becomes amplified in two way and three way related measures ANOVA. Those experiments tend to have more groups, meaning more data is at risk of being censored. Researchers often ask if it is reasonable to flip to a completely randomized analysis when they notice too many values are missing from their data set. No, it is not reasonable. In fact, thats a horrible p-value hack. The type of experimental design is scientifically-driven. Intrinsically-linked measurements are not independent, and should not be analyzed as if they are independent. To do so violates one of the two primary assumptions of the statistical analysis. How to avoid this problem? First, where possible, include an extra replicate or two as a hedge over what the power analysis suggests is necessary. Second, dont make a RM design too over-ambitious. Limit the number of levels to those that are scientifically important. Third, be aware of the risk of lost values. Is the experimental protocol difficult? Are any protocol steps at high risk of failure? Are there any intrinsic barriers to efficiently collecting the data? 26.8 Two-way ANOVA This is a method to investigate the effects of two factors simultaneously. Thus, the variation associated with each factor can be partitioned. This also allows for assessing the variation associated with an interaction between the two factors. What is an interaction? Simply, it is a response that is greater (or lesser) than the sum of the two factors combined. Lets go back to the genotype blood_glucose problem. Well add a factor, and simplify the study a bit. Were interested in a gene that, when absent, raises blood glucose. Were also interested in a drug that, when present, lowers blood glucose. We have reason to hypothesize that a genotype:drug interaction might exist. For example, the gene might encode a protein that metabolizes our drug, thus impairing the drugs ability to lower blood glucose. Well simulate a \\(2\\times 2\\) experiment that has only the presence or absence of each of these factors. Heres the data: set.seed(12345) blood_glucose &lt;- round(c(rnorm(5, 100, 20), rnorm(5, 75, 20), rnorm(5, 200, 20), rnorm(5, 100, 20)), 1) drug &lt;- as.factor(rep(rep(c(0, 30),each=5),2)) genotype &lt;- rep(c(&quot;WT&quot;, &quot;KO&quot;), each=10) test &lt;- data.frame(genotype, drug, blood_glucose) y0 &lt;- mean(subset(test, genotype==&quot;WT&quot; &amp; drug==0)$blood_glucose) y30 &lt;- mean(subset(test, genotype==&quot;WT&quot; &amp; drug==30)$blood_glucose) yend0 &lt;- mean(subset(test, genotype==&quot;KO&quot; &amp; drug==0)$blood_glucose) yend30 &lt;- mean(subset(test, genotype==&quot;KO&quot; &amp; drug==30)$blood_glucose) ggplot(test, aes(genotype, blood_glucose, color=drug))+ geom_jitter(size=6, width =0.3)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ stat_summary(fun.y=mean, geom=&quot;point&quot;, shape = 95, size= 15)+ scale_x_discrete(limits=c(&quot;WT&quot;, &quot;KO&quot;))+ labs(y=&quot;blood glucose&quot;)+ geom_segment(aes(x=&quot;WT&quot;, y=y30, xend=&quot;KO&quot;, yend=yend30))+ geom_segment(aes(x=&quot;WT&quot;, y=y0, xend=&quot;KO&quot;, yend=yend0)) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function An interaction effect can be represented by the differing slopes of those two lines. If the lines are not parallel, it means that the effect of the drug is not the same at both levels of genotype. Or you could say the effect of the genotype is not the same at both levels of the drug. Whatever. The statistical term used to describe such phenomena is that the two factors interacted. Thus, a statistical interaction occurs when the effects of two factors are not the same across all of their levels. Heres a quick ANOVA table for those data. It has an F test for each of the factors genotype and drug, and an F test for the genotype:drug interaction. All three F-tests are extreme. anova(lm(blood_glucose ~ genotype + drug +genotype*drug, test)) ## Analysis of Variance Table ## ## Response: blood_glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 1 25127.0 25127.0 94.333 4.116e-08 *** ## drug 1 26028.1 26028.1 97.716 3.225e-08 *** ## genotype:drug 1 4845.4 4845.4 18.191 0.0005923 *** ## Residuals 16 4261.8 266.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since the F-test for the genotype:drug interaction is extreme, we can reject the null that there is no interaction between them. Furthermore, the existence of the interaction effect complicates the interpretation of the effects of the genotype and drug factors. Generally, an interaction effect supersedes an effect of either factor, alone. Although genotype seems to have a strong effect on glucose levels, thats really blunted in the presence of drug. And although the drug seems to reduce glucose, that effect becomes remarkably prominent when the genotype changes. When interactions occur, the effect of one factor cannot be interpreted without condition on the other factor! It might be useful to see the data for the discussion that follows. genotype drug blood_glucose WT 0 111.7 WT 0 114.2 WT 0 97.8 WT 0 90.9 WT 0 112.1 WT 30 38.6 WT 30 87.6 WT 30 69.5 WT 30 69.3 WT 30 56.6 KO 0 197.7 KO 0 236.3 KO 0 207.4 KO 0 210.4 KO 0 185.0 KO 30 116.3 KO 30 82.3 KO 30 93.4 KO 30 122.4 KO 30 106.0 Four sources of variation are accounted for: the main effect of genotype, the main effect of drug, the interaction between drug and genotype, and the residual error. Three F tests were performed. Each of these used \\(MS_{residual}\\) in the denominator and the \\(MS\\) for the respective source in the numerator. How has this variation been determined? The experimental model is as follows: \\[SS_{model}=SS_{genotype}+SS_{drug}+SS_{genotype\\times drug} \\] The grand mean of all the data is computed as before: \\[\\hat y=\\frac{1}{n}\\sum_{i=1}^ny_i \\] There are two factors, each at two levels. Thus, there are a total of \\(j=4\\) experimental groups. Each group has a sample size of \\(n_j=5\\) replicates and represent a combination of predictor variables: WT/0, WT/30, KO/0, KO/30. The mean of each group is \\(\\bar_j\\). Therefore, \\[SS_{model}= \\sum_{j=1}^kn_j(\\bar y_j-\\hat y)^2 \\] represents the total model variation. We can also artificially group these factors and levels further. Two groups correspond to the levels of the genotype factors and two correspond to the levels of the drug factor. Their means are \\(\\bar y_{wt}, \\bar y_{ko}\\) and \\(\\bar y_0, \\bar y_{30}\\), respectively. Each of these groups has a sample size of \\(2n_j\\). These contrived means are used to isolate for the variation of each of the two factors: \\[SS_{genotype}= \\sum n_{wt}(\\bar y_{wt}-\\hat y)^2+n_{ko}(\\bar y_{ko}-\\hat y)^2 \\] \\[SS_{drug}= \\sum n_{0}(\\bar y_{0}-\\hat y)^2+n_{30}(\\bar y_{30}-\\hat y)^2 \\] All that remains is to account for the variation associated with the interaction effect. That can be solved for algebraically: \\[SS_{genotype\\times drug}=SS_{model}-SS_{genotype}-SS_{drug} \\] Because two way ANOVAs have two factors, one of the factors can be applied completely randomized, and the other can be applied as related measures. Or both factors can be completely randomized, or both can be related measures. 26.9 Other ANOVA models For all ANOVA experiments, irrespective of the design, the total amount of deviation in the data can be partitioned into model and residual terms: \\[SS_{total}=SS_{model}+ SS_{residual}\\] Whats interesting is that different ANOVA experimental designs have different models. Were interested in two factors, factorA and factorB, and have the ability to study each at multiple levels. The interaction between factorA and factorB is \\(A\\times B\\) One way CR: \\(SS_{model}=SS_{factorA}\\) One way RM: \\(SS_{model}=SS_{factorA}+SS_{subj}\\) Two way CR: \\(SS_{model}=SS_{factorA}+SS_{factorB}+SS_{A\\times B}\\) Two way RM on A factor: \\(SS_{model}=SS_A+SS_B+SS_{A\\times B}+SS_{subj\\times A}\\) Two way RM on both factors: \\(SS_{model}=SS_A+SS_B+SS_{A\\times B}+SS_{subj\\times A}+SS_{subj\\times B}+SS_{subj\\times A\\times B}\\) The big difference between completely randomized and related measure designs is that in the latter, were now accounting for the deviation associated with each replicate in the model! Otherwise, that replicate deviation would have been blended into the residuals. This turns out to be a pretty big deal. When that deviation due to the subjects is pulled out of the residual, it lowers the value of the denominator of the F statistic. Thus making the F statistic larger! 26.9.1 R and ANOVA There are a handful of ways to conduct ANOVA analysis on R. These are not necessarily more right or wrong than the others. What is important to know, however, is that they do perform calculations differently under certain circumstances (eg, Type 1 v Type 2 v Type 3 SS calculations). Therefore, they produce distinct results. This always confuses researchers, particularly when comparing Rs results to other software we might be more comfortable with. We wonder which output is right. Chances are they are all right. We have that output because of the way we argued the analysis. This again emphasizes the need to share specific details of the analysis in our publications. In this case, specify using R, specify the package version and the R function used, and even specify the arguments. Or append an R script file as supplemental information to illustrate exactly how the analysis is performed. Given data and a group of arguments well call foo, Rs ANOVA function options are as follows: anova - A function in Rs base. eg, anova(lm(foo)) aov - A function in Rs base. eg, `aov(foo) Anova - A function in Rs car package. eg, Anova(lm(foo)) ezAnova - A function in Rs ez package, ezAnova(foo) There are others. For example, since ANOVA analyses are also general linear models the same basic problem can also be solved using lm(foo)without ANOVA. Passing an lm(foo) into an ANOVA function mostly just provides output in the familiar ANOVA notation. The statistical fundamentals of lm(foo) and ezAnova(foo)are identical. For the ANOVA part of this course, well use ezANOVA from the ez package. In particular, it is a bit more straightforward to use (and teach) in terms of arguing the experiments ANOVA model. Key Jargon to understand to do ezANOVA in R Specify a completely random design by defining the between variable as your factor name. The between here is meant to imply comparisons between groups. Specify a related measures design by defining the within variable as your factor name. The within here is meant to imply comparisons within replicates. 26.9.1.1 Type of calculation When analyzing 2-way and 3-way ANOVAs there are three different methods to calculate, which are referred to as type I, type II and type III. When an experiment is balanced, which is to say it has equal sample sizes per group, the type of calculation is immaterial. In that case,type I, II and III yield exactly the same output. Unbalanced experiments are those in which the sample sizes of groups are not the same. As long as the differences are not too large, the presence of unbalance is usually not a problem. But it will always impact the precise output of different ANOVA functions, depending upon whether they perform type I, II or III calculations. This is what really causes confusion when we notice different output from different functions. This is particularly confusing when comparing the output of different ANOVA functions in R (eg, Anova vs aov vs anova vs ezAnova) and/or commercial software (eg, SAS, SPSS, Prism). The researcher scratches her head, wonders which is correct. In one sense, they are all correct. Type I, II and II sum of squares calculations are explained here. Suffice to say this is important to not overlook. This serves to illustrate how providing good detail about the software used to analyze data is important for reproducibility. The most significant point to understand is that some commercial software uses type 3 calculations by default. As a consequence, given the same data set, the results from those packages may not coincide perfectly with those of ezANOVA unless using a type = 3 argument in the function. My recommendation is to use type = 2 when interested in only testing hypotheses about the main effects of factors, and there is no interest in an interaction if working on a two- or three-way ANOVA data set. Thats because type = 2 is purported to yield consistently higher power for main effects. Use type = 3 when, instead, the experiment is designed to test whether an interaction occurs between factors. When an interaction occurs, the main effects are not interpretable. Type I sum of squares are calculated when using the anova and aov functions of base R. This is otherwise known as sequential sum of squares calculation. On multifactor data with those functions, the results can differ given the order by which the factors are argued. Thus, aov(lm(outcome~factorA + factorB)) might yield slightly different results compared to aov(lm(outcome~factorB + factorA)). The idea is to calculate the effect on a factor that is most interesting to you scientifically, while controlling for the effect of the other factor. When using ezANOVA we can explicitly argue which calculation we wish to run, removing some ambiguity about what actually happened. 26.10 Alternatives to ANOVA When the outcome variable is measured and the design is completely randomized, the data can be analyzed using the general linear model using Rs lm function, rather than by ANOVA. This allows for analyzing interaction effects between factors. The results will be the same as ANOVA. If the design has a related measures component, then a linear mixed effects model should be run instead. In that case, use lmer in the lme4 package. Alternately a nonparametric analysis can be performed using either the Kruskal-Wallis (completely randomized) for the Friedman (related measures) test. Bear in mind that no nonparametric analog for the two-way ANOVA exists. Thus, hypotheses related to interaction effects are not testable using nonparametric statistics. Finally there is the generalized linear model (glm) for completely randomized designs or the generalized linear mixed model (glmer) for designs that incorporate related measures, respectively. Each of these allow for testing interactions between factors. The generalized linear models also allow for a flexible array of outcome variables. These should be used, rather than ANOVA, when the outcome variable is non-normal or is discrete. For example, these are the tools of choice when the outcome variable is binomial (logistic regression) or frequency data (Poisson regression) and there are 3 or more groups to compare. Additional families are possible. 26.10.1 Screw ANOVA, Just Tell Me How to t-Test Everything OK, fine. This is far from ideal because an ANOVA-free approach risks introducing bias through cherry-picking comparisons after unpacking the data (the HARK fallacy). You dont have to do ANOVA for an experiment with 3 or more groups (or anything else for that matteryou just have to be able to defend your choices). A major purpose of ANOVA is to maintain an experiment-wise type1 error of 5%. But there are other ways to accomplish this objective. For example, you might skip the ANOVA step and simply run serial t-tests comparing all of the groups in an experiment. Or run t-tests to compare a pre-planned sublist of all possible comparisons. The emphasis here on pre-planning is important. Make decisions ahead of time about what is to be compared, then make only those comparisons. No more and no less. Otherwise, youre snooping. Once youre in snooping mode, youre deeply biased towards opportunistic outcomes. For example, you may run multiple control groups within your experiment to signal that some important aspect of the protocol is working properly, but these controls are otherwise not scientifically interesting (with respect to testing new hypotheses). You may not wish to expend any of your type1 error budget doing comparisons on these controls. With those reservations noted, what follows are two ways to go about this. To begin, if we have \\(k\\) levels of predictor variables in our experiment it has a total of \\(m=k(k-1)/2\\) possible comparisons that could be made. The pairwise.t.test is the function to use for this purpose. Use it to make the group comparisons that interest you. choosing the p.adjust.method that strikes your fancy. Two of the latter are listed below. 26.11 Summary We conduct ANOVA designed experiments intuitively, any time we have 3 or more groups. In R make sure that variable you want to be a factor is coded as a factor. Accounting for combinations of factors and whether the factor is applied randomized or related measures, We have at least 12 different ANOVA designs at our disposal. F-tests compare model variance to residual variance; if F is large, the model is extreme. ANOVA test results can infer effects of factors, or be permission for posthoc testing Several ANOVA methods exist in R. When configured the same, they should give the same output. "],["fdistr.html", "Chapter 27 The F distribution 27.1 Background 27.2 df 27.3 pf 27.4 qf 27.5 rf", " Chapter 27 The F distribution library(tidyverse) 27.1 Background George Snedecor once said that Students t distribution is the distribution that revolutionized the statistics of small samples. As the inventor of the F distribution, nobody would have faulted Snedecor for praising himself the same way. The F distribution is heavily used in parametric statistics. The most common use of the F statistic is to examine ratios for two estimators of population variance. In particular, F-tests in ANOVA are ratios of model variances, at given degrees of freedom, to residual variances, a some other given degrees of freedom. Another common use is to F-test nested regression models to determine if the more complex model provides a better fit for the data. When the value of this ratio is extreme, the variances are not equivalent, meaning the variance in an experiment is better explained by factor effects than by residual error. An F distribution represents the null probability distribution for such ratios and is therefore used to test hypotheses involving variances. F is used widely in statistics to answer a simple question: Are two samples drawn from populations that have the same variance? For example, the F statistic serves as an omnibus test for ANOVA, and is used in regression to determine which of two models best fit a dataset, and is also used for normality tests. More generally, the F statistic can be used to analyze a proportion of two random, \\(\\chi^2\\)-distributed variables. 27.1.1 Sample Variance and Fs PDF A sample of size \\(i=n\\) for a random, independent variable \\(X\\) can take on the values \\(x_1, x_2,...x_i\\). The sample mean is \\(\\bar x=\\frac{1}{n}\\sum\\limits_{i=1}^{n}x_i\\) with \\(df=n-1\\) degrees of freedom. The sum of the residual deviation from the sample mean, also known as the sample sum of squares, (\\(SS\\)) is: \\[SS=\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2\\] The sample variance is: \\[s^2=\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{df}\\] The sample variance is otherwise known as the mean square in jargon commonly associated with ANOVA: \\[MS=\\frac{\\sum\\limits_{i=1}^{n}(x_i-\\bar x)^2}{df}\\] The \\(MS_{df}\\) of a normally distributed random variable \\(X\\) with \\(df\\) will have a \\(\\chi^2_{df}\\) distribution. Now let the normally distributed variables \\(X1\\) and \\(X2\\) have \\(df_1\\) and \\(df_2\\) degrees of freedom, and also variances of \\(MS_{df1}\\) and \\(MS_{df2}\\), respectively. The F statistic is: \\[F=\\frac{MS_{df1}}{MS_{df2}}\\] The probability density function for the F statistic is: \\[f(x)=\\frac{(\\frac{df_1}{df_2})^\\frac{df_1}{2}\\Gamma[\\frac{(df_1+df_2)}{2}]x^{\\frac{df_1}{2}-1}}{\\Gamma[\\frac{df_1}{2}]\\Gamma[\\frac{df_2}{2}][1+(\\frac{df_1x}{df_2})]^{\\frac{(df_1+df_2)}{2}}}\\] 27.2 df Rs function for the F PDF is df and returns a value for the probability of F, given its degrees of freedom. It takes as arguments a value for x, which represents F. x can either be unique value or represent a range of values. Other arguments include df1 and df2, which represent values for the degrees of freedom represented in the numerator and denominator, respectively. There is a unique F distribution for any combination of df1 and df2. The exact probability when F has a value of 2.5 and 2 and 10 degrees of freedom (\\(F_{df_1,df_2=2.5}\\)) is: df(2.5, df1=2, df2=10) ## [1] 0.0877915 The distribution of the F statistic can vary quite markedly depending upon the combination of df1 and df2. For example, lets imagine the 3 curves below correspond to each of 3 different one-way ANOVA experimental designs. The red distribution represents a null distribution for F for an ANOVA experiment having 3 predictor groups with a sample size of 5 independent subjects per group. The blue distribution represents the null of F for an experiment of 10 groups with 3 replicates per group. The green distribution is the null of F for an experiment with 20 groups, each with 4 replicates. Thus, since the numerator and denominator of the F statistic represent two different populations, the F distribution is extraordinarily flexible in terms of the comparisons that can be made using it! ggplot(data.frame(x=c(0,6)), aes(x)) + stat_function(fun=&quot;df&quot;, args=list(df1=2, df2=12), color=&quot;red&quot;)+ stat_function(fun=&quot;df&quot;, args=list(df1=9, df2=20), color=&quot;blue&quot;) + stat_function(fun=&quot;df&quot;, args=list(df1=19, df2=60), color=&quot;green&quot;)+ labs(x =&quot;F&quot;, y=&quot;p(F)&quot;) 27.3 pf The cumulative distribution function for F returns the cumulative probability under the F distribution for a value of the F statistic and a given pair of degrees of freedom. pf(q=4, df1=2, df2=12) ## [1] 0.953344 A p-value is returned by using the following argument: lower.tail=F. Thus, the probability of an F statistic whose value is 4.0 or larger is: pf(q=4, df1=2, df2=12, lower.tail=F) ## [1] 0.046656 ggplot(data.frame(x=c(0,6)), aes(x)) + stat_function(fun=&quot;pf&quot;, args=list(df1=2, df2=12), color=&quot;red&quot;)+ stat_function(fun=&quot;pf&quot;, args=list(df1=9, df2=20), color=&quot;blue&quot;) + stat_function(fun=&quot;pf&quot;, args=list(df1=19, df2=40), color=&quot;green&quot;)+ labs(x =&quot;F&quot;, y=&quot;p(F)&quot;) 27.4 qf The inverse cumulative probability function for the F distribution is qf. This function will take a probability as an argument, and return the corresponding value of the F statistic for a given pair of degrees of freedom. qf(p=0.95, df1=2, df2=12) ## [1] 3.885294 An F statistic limit for a given p-value can be calcuated using the lower.tail=F argument. qf(p=0.05, df1=2, df2=12, lower.tail=F) ## [1] 3.885294 ggplot(data.frame(x=c(0,1)), aes(x)) + stat_function(fun=&quot;qf&quot;, args=list(df1=2, df2=12), color=&quot;red&quot;)+ stat_function(fun=&quot;qf&quot;, args=list(df1=9, df2=20), color=&quot;blue&quot;) + stat_function(fun=&quot;qf&quot;, args=list(df1=19, df2=40), color=&quot;green&quot;)+ labs(x =&quot;p(F)&quot;, y=&quot;F&quot;) 27.5 rf The rf function can be used to generate n random F statistic values for a given pair of degrees of freedom. rf(n=10, df1=2, df2=12) ## [1] 0.5597275 0.9402234 0.5752848 1.8002392 0.6221102 0.2859954 1.4767106 ## [8] 2.2144931 1.4721874 2.4914325 "],["onewayanova.html", "Chapter 28 One-way ANOVA Completely Randomized 28.1 Using ezANOVA 28.2 The chickwt data set 28.3 Run the ANOVA 28.4 Post hoc pairwise comparisons 28.5 Summary", " Chapter 28 One-way ANOVA Completely Randomized library(magrittr) library(tidyverse) library(ggformula) library(DescTools) library(ez) library(lsr) You probably should read about the ANOVA big picture). 28.1 Using ezANOVA R has several functions to run ANOVA. We are mostly going to use ezANOVA from the ez package in this course. The syntax for defining the ANOVA analysis is a bit more clear in ez, especially if we are new to ANOVA. This chapter runs through an analysis of a one-way completely randomized ANOVA data set as how to example. 28.2 The chickwt data set This data set is in Rs base. It compares the influence of 6 different types of food sources on chick weight. We have one predictor variable, the factor feed, which is tested at 6 different levels (the various food sources). There is one continuous outcome variable (weight). We assume the chicks are out bred, and thus not intrinsically-related, and that they have been randomly assigned to a level of feed, and have been weighed after a period of time consuming that feed. This is a classic one-way completely randomized ANOVA design. This chapter illustrates how to go through an analysis of the data. 28.2.1 Inspect the data The next few scripts involve inspecting the data set, which should always be done prior to running any statistical tests. data(chickwts) #take a look at the data structure, depending upon how you like to view data str(chickwts) ## &#39;data.frame&#39;: 71 obs. of 2 variables: ## $ weight: num 179 160 136 227 217 168 108 124 143 140 ... ## $ feed : Factor w/ 6 levels &quot;casein&quot;,&quot;horsebean&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... chickwts ## weight feed ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean ## 7 108 horsebean ## 8 124 horsebean ## 9 143 horsebean ## 10 140 horsebean ## 11 309 linseed ## 12 229 linseed ## 13 181 linseed ## 14 141 linseed ## 15 260 linseed ## 16 203 linseed ## 17 148 linseed ## 18 169 linseed ## 19 213 linseed ## 20 257 linseed ## 21 244 linseed ## 22 271 linseed ## 23 243 soybean ## 24 230 soybean ## 25 248 soybean ## 26 327 soybean ## 27 329 soybean ## 28 250 soybean ## 29 193 soybean ## 30 271 soybean ## 31 316 soybean ## 32 267 soybean ## 33 199 soybean ## 34 171 soybean ## 35 158 soybean ## 36 248 soybean ## 37 423 sunflower ## 38 340 sunflower ## 39 392 sunflower ## 40 339 sunflower ## 41 341 sunflower ## 42 226 sunflower ## 43 320 sunflower ## 44 295 sunflower ## 45 334 sunflower ## 46 322 sunflower ## 47 297 sunflower ## 48 318 sunflower ## 49 325 meatmeal ## 50 257 meatmeal ## 51 303 meatmeal ## 52 315 meatmeal ## 53 380 meatmeal ## 54 153 meatmeal ## 55 263 meatmeal ## 56 242 meatmeal ## 57 206 meatmeal ## 58 344 meatmeal ## 59 258 meatmeal ## 60 368 casein ## 61 390 casein ## 62 379 casein ## 63 260 casein ## 64 404 casein ## 65 318 casein ## 66 352 casein ## 67 359 casein ## 68 216 casein ## 69 222 casein ## 70 283 casein ## 71 332 casein It is also helpful to calculate some descriptive stats for inspection. Since the design is obviously about testing the different levels of feed, use the group_by function of tidyverse to summarize by feed level. The kable function from knitr makes a nicer output. cw1 &lt;- chickwts %&gt;% group_by(feed) %&gt;% summarise( mean= mean(weight), median=median(weight), sd= sd(weight), n = n(), var=var(weight) ) ## `summarise()` ungrouping output (override with `.groups` argument) knitr::kable(cw1, caption=&quot;Descriptive statistics for the chickwts dataset.&quot;) Table 28.1: Descriptive statistics for the chickwts dataset. feed mean median sd n var casein 323.5833 342.0 64.43384 12 4151.720 horsebean 160.2000 151.5 38.62584 10 1491.956 linseed 218.7500 221.0 52.23570 12 2728.568 meatmeal 276.9091 263.0 64.90062 11 4212.091 soybean 246.4286 248.0 54.12907 14 2929.956 sunflower 328.9167 328.0 48.83638 12 2384.992 We can see that the means and medians are about equal within each group. Thats a quick way to suggest no skew. Overall sample size is 71, distributed as 10-14 replicates per group, its a little unbalanced. But within acceptable limits. The variances are not equivalent, but are they unequal? Levenes test in the ANOVA will provide that answer. But this is a pretty small sample size. Whatever that output we will take it with salt. Plot the data for a look. Simply looking at the data graphically goes a long way to ensuring this is a one-way ANOVA design. Jitter plots are a great way to see group data like this. I like the crossbar geom to overlay some summary stats. ggplot(chickwts, aes(feed, weight))+ geom_jitter(width = 0.2, size=2) + stat_summary(fun.data = mean_sdl, fun.args = list(mult=1), geom=&quot;crossbar&quot;, width=0.2, color=&quot;red&quot; ) + theme_classic() 28.3 Run the ANOVA We can imagine designing an experiment like this with either of 3 experimental objectives in mind. Perhaps were interested mostly in whether any feeds are better than others for achieving weight gain? We could answer that by making all possible pairwise comparisons. Since there are 6 levels of the factor feed, that would involve \\(m=\\frac{6(6-1)}{2}=15\\) comparisons. Imagine casein is the standard feed, and we wish to know if any of the other feeds differ from this standard? We would compare casein to every feed. That would involve only 5 comparisons. Perhaps we just want to know if any of the feeds differ in causing weight gain, but we arent interested in which specific feeds differ? We could answer that question using the F-test result, and not comparing any groups post hoc. Each of those objectives are scientifically-driven. They should be declared before running an experiment so that an unbiased analysis is conducted after the data are in. Other than, perhaps, how we order our data in the data set, which of these objectives is true doesnt influence how we run the ezANOVA function per se. However, the objective will influence which post hoc analysis is performed. 28.3.1 Run the chickwts One Way ANOVA First, ezANOVA requires a wid, which is a unique ID variable for each independent replicate. We need to add one to the chickwts data set. Since all the measures are independent, well just do that by row number. At the same time well convert the integer to a factor so ezANOVA wont bark at us. chickwts$ID &lt;- as.factor(1:nrow(chickwts)) You should look at Rs help for ezANOVA ?ezANOVA to understand these test arguments. The help page is pretty clear for most of these. Since we dont use the term dependent variable much in this course, to be clear, dv is the outcome response variable..the dependent variable. We have to specify it in the ezANOVA arguments. If measurements for levels of the factor are not intrinsically-linked, if they are distributed to each replicate independently, the design is completely randomized. That factor should be listed in the function using a between argument. If measurements for levels of the factor are intrinsically-linked, it is a related/repeated measures design. List it as a within argument, rather than between. Here, the feed factor is between. Every chick was randomly assigned a level of feed. Notice that ezANOVA is a function. Use it to create a list object called my.ezaov, which has all of the output information. We can call all of the output at once, or we can call specific elements from the my.ezaov object to see the results. my.ezaov &lt;- ezANOVA( data = chickwts, wid = ID, dv = weight, between = feed, type = 2, return_aov = T, detailed = T) ## Warning: Data is unbalanced (unequal N per group). Make sure you specified a ## well-considered value for the type argument to ezANOVA(). ## Coefficient covariances computed by hccm() my.ezaov ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 feed 5 65 231129.2 195556 15.3648 5.93642e-10 * 0.5416855 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 5 65 4389.241 76154.92 0.7492639 0.5896095 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## feed Residuals ## Sum of Squares 231129.2 195556.0 ## Deg. of Freedom 5 65 ## ## Residual standard error: 54.85029 ## Estimated effects may be unbalanced # my.ezaov$ANOVA, this is a dataframe # my.ezaov$Levene, this is also a dataframe # my.ezaov$aov, this is an aov object that we can pass into posthoc functions. 28.3.2 Interpreting the One-Way CR ANOVA Output The ezANOVA output prints 3 list objects by default: $ANOVA (which is the first data frame) $Levene's Test for Homogeneity of Variance (which is the 2nd data frame) $aov (which is the end of the console output and is an important statistical object) In fact, there is a great deal more computed that is not printed, which you can visualize in the console by typing str(my.ezaov). 28.3.2.1 $ANOVA: The ANOVA table For a CR one way ANOVA design, the SS are partitioned as follows, in general: \\(SS_{total}=SS_{model}+SS_{residual}\\). In this example, \\(SS_{model}= SS_{feed}\\). Thus, the ANOVA table summarizes the feed model. The DFn = 5 corresponds to the 6 groups, less 1 degree of freedom (one is lost to calculate mean of groups (sort of)) for the model source of variance. The DFn = 65 corresponds to the degrees of freedom for the residuals (one df is lost per group to calculate group means). Therefore, this ANOVA tests a feed model against a null F distribution with 5 and 65 degrees of freedom. \\(F=MS_{feed}/MS_{residual}=15.3648\\), where \\(MS = SS/df\\). The SS can be found in the $aov output. ges = generalized eta-squared. ges is an effect size parameter for ANOVA. For this particular experimental design, \\(ges=\\frac{SS_n}{SS_n+SS_d}\\). In other words, ges summarizes the variation associated with the model as a fraction of the total variation in the data. Thus, 54.16% of the variation in weight is attributable to the different levels of feed in the experiment. In other words, the model explains 54.16% of the variation in the data. Think of eta-squared, partial eta-squared, and generalized eta-squared as all related to the more commonly understood \\(R^2\\), the so-called coefficient of regression. They are each calculated differently, but are related as estimates for how much of the variation is due to the model. ges takes on values from 0 to 1. Higher values indicate a greater degree of the overall variation is due to the factor tested in the experiment. Having said that, its a bit of a Goldilocks statistics by itself. It has more value as a way to describe fits of nested models. 28.3.2.2 $aov This table is an important object because it can be passed into certain posthoc tests, facilitating analysis. It provides the accounting for the sum of squares and degrees of freedom, while calculating the residual standard error. It is somewhat redundant with the $ANOVA table, though the residual standard error can come in handy. DFn=degrees freedom for numerator. k-1, where k = levels of factor. DFd=degrees freedom for denominator. n-k, where n = number of independent replicates. SSn &amp; SSd = sum of squares for model and residual, respectively Residual standard error is a parameter that estimates the precision by which the data fit the model, and is in units of the outcome variable, weight. \\(SE\\) is the square root of the residual variance: \\(S_{y.x}=\\sqrt{\\frac{SS_{residual}}{df_{residual}}}\\) If \\(S_{y.x}\\) were zero, there would be no residuals. The data points would all rest at the value of the group means. The data would fit perfectly to a model of 6 group means at their observed values. \\(S_{y.x}\\) therefore is a descriptive statistic that declares how much error, or the degree by which the data is unexplained by the model. It has some utility for calculating confidence intervals and power analysis as well. 28.3.2.3 The F test The scientific prediction for this experiment is that chick weights will vary depending upon the type of feed they are grown on. The null is that their weights will be roughly the same, irrespective of food source. ANOVA tests this hypothesis through the variance parameter. The question at hand is whether the variance associated with the model, one of 6 different feed group means, is fractionally greater than the residual variance in the sample. The null statistical hypothesis is that the variance associated with the different levels of feed is less than or equal to the residual variance. Therefore, the alternate hypothesis is the variance associated with feed is greater than residual variance. \\(H_0: MS_{feed}\\le MS_{residual}\\), \\(H_1: MS_{feed}&gt;MS_{residual}\\) Because of the relationship of group means to variance, it is just as valid to express the null hypothesis in terms of the group means, and that can be proven mathematically by a competent statistician (of which I am not): \\(H_0: \\mu_a=\\mu_b=\\mu_c=\\mu_d=\\mu_e=\\mu_f\\) Though, strictly, rejecting the null doesnt mean that all group means differ from each other, it just means that some of them differ. \\(H_1: \\mu_a\\ne\\mu_b\\ne\\mu_c\\ne\\mu_d\\ne\\mu_e\\ne\\mu_f\\) The F statistic of 15.3648 is extreme for a null F distribution of 5 and 65 degrees of freedom. The very low p-value illustrates this extremeness. We can reject the null and conclude that differences in effect on chick weights exist between this group of feeds. 28.3.2.4 Levenes test for homogeneity of variance Levenes test determines whether there is a substantial level of differences in variance between groups. Levenes test is run as a check to determine if the groups variance is homogeneous, as homoskedasticity is one of the validity assumptions of ANOVA. Levenes test statistic is calculated as follows: \\[W=\\frac{(n-k)}{(k-1)}\\frac{\\sum\\limits_{i=1}^{k}n_i(\\bar Z_i-\\bar Z)^2}{\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{n_i}(Z_{ij}-\\bar Z_i)^2}\\] where \\(Z_{ij}=|x_{ij}-\\bar x_i|\\) and \\(Z_i\\) are the group means and \\(\\bar Z\\) is the overall mean of \\(Z_{ij}\\). The null hypothesis of the Levene test is rejected when \\(W&gt;F_{(\\alpha,\\ k-1,\\ n-k)}\\), where the F is the critical value. Levenes test output is a 2nd ANOVA table, and can easily be confused with the ANOVA output. Levenes test lacks a \\(ges\\) parameter, nor does it have a column that lists the factor name. If the Levenes F value is low and the p-values is high, as is the case here, we cant reject the null that the variances are the same. In this way, the variance homogeneity assumption is validated. If this were not the case, we have two options. Option 1: Simply ignore the result. The luck of the draw with small samples can explain group differences in variance, where none really exists. It is hard to gauge the impact of any one violation on our inference. It may be very modest or it may be substantial. With data in hand, it is too late to come up with an on-the-fly solution if not specified in the planning stages. Option 2: Transform the data to homogenize outliers and variance, or switch the analysis to a Kruskal-Wallis nonparametric test. 28.4 Post hoc pairwise comparisons When the ANOVA F test for the factor is extreme we may be interested in knowing which treatments differ. Thats achieved by conducting post hoc analysis. These typically involves multiple group comparisons. There are two fundamental options for CR posthoc testing: p-value adjustment or a range test. Each are illustrated below, but only one method should be conducted in real life. That method is chosen in advance during the planning stages. For the adjusted p-value method, use the pairwise.t.test function set up a matrix of all possible group comparisons. The Bonferroni p-value adjustment procedure is selected for best possible control of type1 error. This may miss some true differences. For each comparison we are testing the null hypothesis that the two group means are the same: \\[H_0: \\bar y_i = \\bar y_j\\] allPairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, paired=FALSE, alternative=&quot;two.sided&quot;, pooled.sd=TRUE, p.adjust= &quot;bonf&quot;) allPairs ## ## Pairwise comparisons using t tests with pooled SD ## ## data: chickwts$weight and chickwts$feed ## ## casein horsebean linseed meatmeal soybean ## horsebean 3.1e-08 - - - - ## linseed 0.00022 0.22833 - - - ## meatmeal 0.68350 0.00011 0.20218 - - ## soybean 0.00998 0.00487 1.00000 1.00000 - ## sunflower 1.00000 1.2e-08 9.3e-05 0.39653 0.00447 ## ## P value adjustment method: bonferroni To quickly scan which comparisons are below the p &lt; 0.05 threshold we apply a simple custom extreme function across the matrix: extreme &lt;- function(x){ ifelse(x &lt; 0.05, TRUE, FALSE) } apply(allPairs$p.value, c(1, 2), extreme) ## casein horsebean linseed meatmeal soybean ## horsebean TRUE NA NA NA NA ## linseed TRUE FALSE NA NA NA ## meatmeal FALSE TRUE FALSE NA NA ## soybean TRUE TRUE FALSE FALSE NA ## sunflower FALSE TRUE TRUE FALSE TRUE With the Bonferroni correction we are able to reject 8 of the 15 null hypotheses. For each comparison corresponding to a value of TRUE we can reject the null and conclude that their means are not equivalent. Adjusting p-values for subsets of comparisons Often, we dont want to burn so much type1 error making scientifically uninteresting comparisons. In such cases, we instead want to compare subsets. For example, perhaps all we wanted to do was compare each of the feeds to casein. Heres a three step procedure for doing just that. Step1: First, run the pairwise.t.test function, setting the argument p.adjust=\"none\". The output includes a matrix of p-values well name allPairsn, providing all possible comparisons. #just repeating from above allPairsn &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, p.adjust= &quot;none&quot;) Step2: Select from the allPairs matrix only the p-values that correspond to the comparisons youd like to make. Name that vector of unadjusted p-values, selectPairs. This takes a bit of cleverness depending on what you want to grab from the matrix. For example, we only want to compare all of the diets to casein. The comparisons we want are all in the first column. Use your matrix indexing skillz to grab only the unadjusted p-values from that first column: selectPairsn &lt;- allPairsn$p.value[, 1] selectPairsn ## horsebean linseed meatmeal soybean sunflower ## 2.067997e-09 1.493344e-05 4.556672e-02 6.654079e-04 8.124949e-01 selectPairsn &lt; 0.05 ## horsebean linseed meatmeal soybean sunflower ## TRUE TRUE TRUE TRUE FALSE Step3: Now pass these unadjusted p-values in the selectPairs vector into the p.adjust function. The output of this step is a vector of adjusted p-values for the selected group of comparisons. adjustedPvalues &lt;- p.adjust(selectPairsn, method=&quot;bonferroni&quot;) adjustedPvalues ## horsebean linseed meatmeal soybean sunflower ## 1.033998e-08 7.466720e-05 2.278336e-01 3.327039e-03 1.000000e+00 Which of these are extreme? If its not clear by inspection (or too large), use a simple Boolean: adjustedPvalues &lt; 0.05 ## horsebean linseed meatmeal soybean sunflower ## TRUE TRUE FALSE TRUE FALSE Although the p-values differ in this selected group compared to the full matrix, the inference remains the same. We can conclude that chick weights on horsebean, linseed and soybean feeds differ from that on casein feed. 28.4.1 Range tests All range tests operate very differently from p-value adjustment methods. Range tests compare each of a data sets differences between group means to a critical value for the difference between two group means, which is calculated based upon the number of groups and their sample sizes. Any differences between group means that exceed the critical value difference are deemed significant. In addition, these functions calculated adjusted p-values and adjusted confidence intervals given the method selected. One-way completely randomized ANOVAs, as opposed to related measures ANOVA, lend themselves well to range tests because the posthoc questions revolve around the differences between group means. Range tests are nice because they compute confidence intervals adjusted for multiple comparisons, in addition to producing p-values adjusted for multiple comparisons. But these should be avoided on related measures ANOVA, due to the fact that range tests are designed for unpaired comparisons, not for paired comparisons. 28.4.1.1 Dunnetts test In some designs we are frequently interested in a comparison back to a control value. These are referred to as dependent comparisons, because every comparison is back to the same group mean. Dunnetts test was created for exactly this type of situation. Lets imagine the researcher is interested in knowing if any of the feeds cause difference in chick weights compared to the casein feed. DunnettTest(weight ~ feed, control=&quot;casein&quot;, data = chickwts) ## ## Dunnett&#39;s test for comparing several treatments with a control : ## 95% family-wise confidence level ## ## $casein ## diff lwr.ci upr.ci pval ## horsebean-casein -163.383333 -223.95852 -102.80815 6.3e-09 *** ## linseed-casein -104.833333 -162.58951 -47.07716 8.6e-05 *** ## meatmeal-casein -46.674242 -105.72847 12.37999 0.1670 ## soybean-casein -77.154762 -132.81000 -21.49952 0.0032 ** ## sunflower-casein 5.333333 -52.42284 63.08951 0.9995 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note how the R method is to subtract the casein control from the test groups. We can reject the null that chick weight on casein is the same as on horsebean, linseed and soybean. There is no evidence that chick weight differs on meatmeal and sunflower compared to casein. 28.4.1.2 Reporting the result If you have CIs, flaunt them. When using Dunnetts there is no need to report both the 95% CI and the p-value, since they effectively show the same thing (any adjusted p-value above 0.05 will also have a 95% CI that includes the value of zero). We use either for inference. Chick weights differ on feed type (one-way completely randomized ANOVA, F(5, 65)=15.36, p=5.9e-10). Specifically, posthoc Dunnetts test show group mean chick weights differ between casein compared to horsebean (-163, adjusted 95%CI -223 to -102), linseed(-104, adjusted 95%CI -162 to -42), and soybean (-77,adjusted 95%CI -132 to -21) 28.4.1.3 Tukey test and related Other times we are interested in all possible combinations of comparisons. The PostHocTest function in the DescTools package allows for running any of several distinct adjustments. Pass into PostHocTest the aov object produced by ezANOVA and all the work is done for you. PostHocTest(my.ezaov$aov, method=&quot;hsd&quot;, conf.level=0.95) ## ## Posthoc multiple comparisons of means : Tukey HSD ## 95% family-wise confidence level ## ## $feed ## diff lwr.ci upr.ci pval ## horsebean-casein -163.383333 -232.346876 -94.41979 3.1e-08 *** ## linseed-casein -104.833333 -170.587491 -39.07918 0.00021 *** ## meatmeal-casein -46.674242 -113.906207 20.55772 0.33246 ## soybean-casein -77.154762 -140.517054 -13.79247 0.00837 ** ## sunflower-casein 5.333333 -60.420825 71.08749 0.99989 ## linseed-horsebean 58.550000 -10.413543 127.51354 0.14133 ## meatmeal-horsebean 116.709091 46.335105 187.08308 0.00011 *** ## soybean-horsebean 86.228571 19.541684 152.91546 0.00422 ** ## sunflower-horsebean 168.716667 99.753124 237.68021 1.2e-08 *** ## meatmeal-linseed 58.159091 -9.072873 125.39106 0.12770 ## soybean-linseed 27.678571 -35.683721 91.04086 0.79329 ## sunflower-linseed 110.166667 44.412509 175.92082 8.8e-05 *** ## soybean-meatmeal -30.480519 -95.375109 34.41407 0.73914 ## sunflower-meatmeal 52.007576 -15.224388 119.23954 0.22070 ## sunflower-soybean 82.488095 19.125803 145.85039 0.00388 ** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 28.5 Summary One-way completely randomized ANOVA is for experiments where every measurement represents an independent replicate. The results of the F test for the main effect tell us whether or not your one factor, no matter how many levels, has an effect on the response. We choose a posthoc analysis only if interested in knowing which levels of the factor might explain the positive F test. We can use either range tests or p-value adjusted pairwise.t.tests in posthoc *The range test functions provide effect sizes and adjusted confidence intervals "],["onewayRM.html", "Chapter 29 One-way ANOVA Related Measures 29.1 Data prep 29.2 Data visualization 29.3 The ANOVA 29.4 Interpretation 29.5 Post-hoc analysis 29.6 Write up", " Chapter 29 One-way ANOVA Related Measures library(tidyverse) library(readxl) library(viridis) library(ez) library(lme4) The analysis, interpretation and presentation of a one-way ANOVA related/repeated measures experimental design is covered in this chapter. This design has one predictor variable that is imposed at three or more levels. But the essential feature of the RM design that distinguishes it from a CR design is that the measurements of the outcome variable are intrinsically related for all levels of the predictor within each independent replicate. A complete experiment is therefore comprised of many independent replicates, within each of which are many intrinsically-linked measurements. Well use data the Jaxwest2 study to illustrate this. In this example, growth of human tumors in immunodeficient mice is assessed by repeated measurements of tumor vol over a time period. Serial measurements in each of 11 independent replicates. The outcome or dependent variable is tumor volume, in \\(mm^3\\). Tumor volumes are calculated after the researchers measured the lengths and widths of tumors using calipers. The predictor variable is time, in units of days. Although time is usually a continuous/measured variable, well treat it as a discrete factorial variable for this analysis. The model organism is an immunodeficient mouse strain. Each mouse is an experimental unit that has been implanted with HT29 human colon tumor cells. We imagine the experiment is designed to determine whether these cells will grow (rather than be rejected by the host immune system). In truth, these are probably just control data from a contract test on an experimental drug or cancer treatment (the results of which are omitted). The overall scope of the experiment is to test whether the immunodeficient mouse strain is suitable to study the properties of human cancers. A meaningful effect of the time variable in the ANOVA analysis implies that, yes, human tumors can grow in this host. 29.1 Data prep The data are in a file called Jaxwest2.xls, which can be downloaded from the Jackson Labs here. That site offers more details about the study design than are listed here. The munge has already been done conducted. For clarity, the script wont be shown again here. However, it is used in this chapter to create a data frame object by the same name, jw2vol to be used for plotting and statistical analysis. jw2 &lt;-&quot;datasets/jaxwest2.xls&quot; %&gt;% read_excel( skip=1, sheet=1 ) # remove whitespace names(jw2) &lt;- str_remove_all(names(jw2),&quot; &quot;) # trim columns jw2vol &lt;- jw2 %&gt;% select( mouse_ID, test_group, contains(&quot;tumor_vol_&quot;) ) # trim cases jw2vol &lt;- jw2vol %&gt;% filter( test_group == &quot;Control (no vehicle)&quot; ) # convert to numeric jw2vol &lt;- jw2vol %&gt;% mutate_at(vars(tumor_vol_17:tumor_vol_44), as.numeric ) ## Warning: Problem with `mutate()` input `tumor_vol_17`. ## i NAs introduced by coercion ## i Input `tumor_vol_17` is `.Primitive(&quot;as.double&quot;)(tumor_vol_17)`. ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion # impute jw2vol &lt;- jw2vol %&gt;% replace_na( list(tumor_vol_17 = mean(jw2vol$tumor_vol_17, na.rm=T))) # pivot long jw2vol &lt;- jw2vol %&gt;% pivot_longer(cols=starts_with(&quot;tumor_vol_&quot;), names_to=&quot;day&quot;, names_prefix = &quot;tumor_vol_&quot;, values_to = &quot;vol&quot; ) # factorize jw2vol &lt;-jw2vol %&gt;% mutate( mouse_ID=as.factor(mouse_ID), test_group=as.factor(test_group) ) jw2vol ## # A tibble: 143 x 4 ## mouse_ID test_group day vol ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 38 Control (no vehicle) 17 27.2 ## 2 38 Control (no vehicle) 18 54.8 ## 3 38 Control (no vehicle) 19 77.6 ## 4 38 Control (no vehicle) 22 104. ## 5 38 Control (no vehicle) 24 89.8 ## 6 38 Control (no vehicle) 26 213. ## 7 38 Control (no vehicle) 29 306. ## 8 38 Control (no vehicle) 31 432. ## 9 38 Control (no vehicle) 33 743. ## 10 38 Control (no vehicle) 36 721. ## # ... with 133 more rows 29.2 Data visualization By the bloody obvious test it is clearly evident that tumor growth occurs in this model. But here is the difficulty. This is a repeated measures design. Tumor volume is measured on multiple days within each replicate. The spaghetti plot and grouping by color illustrates the design of the experiment. The measurements within each color are intrinsically-liked. Not every point is an independent replicate. The number of independent replicates is far fewer. The number of colors is the number of independent replicates. Plots like this show all of the data for an experiment. ggplot(jw2vol, aes(as.numeric(day), vol, color=mouse_ID, group=mouse_ID))+ scale_color_viridis(discrete=T)+ geom_point(size=2)+ geom_line()+ xlab(&quot;Day after implant&quot;)+ ylab(&quot;Tumor volume&quot;) Figure 29.1: Tumor volume in each mose by days post transplantation Statistically naive researchers would instead plot this as bar graphs. With group means by day, perhaps with bars with error. That visualization implies the means of the days matter, statistically. They do not. The means of the slopes of the connecting lines between any two days are what matters statistically. This looks swell. But not so fast. 29.2.1 Visualize the statistical design The repeated measures statistical analysis operates not on group means, but on the means of differences between levels of a factor, in the same way a paired t-test is about the difference between treatment effects. The following illustrates this. The code below calculates the difference in tumor volume within each mouse between successive days. For example, for mouse #38, the day 40 volume is subtracted from day 44, 38 from 40, 36 from 38, and so on. The function lag helps do this automagically. sum &lt;- jw2vol %&gt;% # ensure sequential mutate_at(vars(day), as.numeric) %&gt;% group_by(mouse_ID) %&gt;% mutate(diff=vol-lag(vol, default=first(vol))) We might be interested in those differences between two sequential measurement days for scientific reasons. For example, it might be useful for detecting a significant acceleration in tumor growth. ggplot(sum, aes(day, diff, color = as.factor(mouse_ID)))+ scale_color_viridis(discrete=T)+ geom_hline(aes(yintercept=0), color=&quot;blue&quot;, size=0.5, linetype=&quot;dashed&quot;)+ scale_y_continuous(limits=c(-250, 1000))+ stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom=&quot;errorbar&quot;, color=&quot;blue&quot;)+ stat_summary(fun.y=mean, geom=&quot;point&quot;, color=&quot;blue&quot;)+ geom_point(size=3)+ xlab(&quot;Lead day&quot;)+ ylab(&quot;Tumor vol difference \\n (diff::lead day - lag day)&quot;)+ labs(color =&quot;mouse_id&quot;) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function Figure 29.2: Differences in tumor volume from prior measurement in each mouse. Alternately, we might be interested in the earliest detectable difference in tumor growth relative to, for example, the first day of measurements. At what point is it clear the tumore is growing? Here are the mean differences within the mice from each day of measurement back to the volume measured on day 17. This is the type of analysis we would do for repeated measure data rather than the Dunnetts test, which is for unpaired comparisons. sum2 &lt;- jw2vol %&gt;% # ensure sequential mutate_at(vars(day), as.numeric) %&gt;% group_by(mouse_ID) %&gt;% mutate(diff=vol-first(vol)) ggplot(sum2, aes(day, diff, color = as.factor(mouse_ID)))+ scale_color_viridis(discrete=T)+ geom_hline(aes(yintercept=0), color=&quot;blue&quot;, size=0.5, linetype=&quot;dashed&quot;)+ scale_y_continuous(limits=c(-250, 2000))+ stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom=&quot;errorbar&quot;, color=&quot;blue&quot;)+ stat_summary(fun.y=mean, geom=&quot;point&quot;, color=&quot;blue&quot;)+ geom_point(size=3)+ xlab(&quot;Lead Day&quot;)+ ylab(&quot;Tumor vol difference \\n (diff::lead day - first day)&quot;)+ labs(color =&quot;mouse_id&quot;) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function Figure 29.3: Differences in tumor volume from day 17 measurement in each mouse. 29.3 The ANOVA Every ANOVA test is one-sided. They test whether the variance associated with the model is greater than the variance associated with the residual error. To test the null, \\[H_0: \\sigma^2_{model}\\le\\sigma^2{residual}\\] What is the model in this case? All of the variation in tumor_vol variable can be accounted for by this relationship: \\[SS_{total}=SS_{day}+SS_{mouseID}+SS_{residual}\\] We have 143 volume measurements in the data set. But they dont come, once each, from 143 different mice. Since we take repeated measures from each of 11 mice we can account for the variation associated within the mice. Thats basically variation that would otherwise have been in the residual term. Since we can account for it, we can put in our model term. \\[SS_{model}=SS_{day}+SS_{mouseID}\\] and so \\[SS_{total}=SS_{model}+SS_{residual}\\] Variance is just the \\(SS\\) averaged using the degrees of freedom. For our experiment, the F statistic is ratio of the model in the numerator to the residual variance in the denominator, \\[F=\\frac{\\frac{SS_{model}}{df_n}}{\\frac{SS_{residual}}{df_d}}\\] 29.3.1 Running ezANOVA Running the function is ezANOVA function straightforward. Configuring it as below ensures that the \\(SS\\) associated with the mouse_ID term gets partitioned out of the residual and into the model. The data have been munged previously into the data frame jw2vol. See above and here. Because this involves repeated measures for each mouse, the time variable day is argued as within. We might say, the tumor_vol measurements are repeated within the day variable. The combination of the wid=mouse_ID and the within = mouse_ID arguments are what ensures the function knows this is a RM on day design. Type 1 sum of squares is chosen for this calculation only because a type 2 calculation produced a computation error. This is not a concern since this is a one-way ANOVA. Strictly, we ask does day have any influence on tumor growth? A detailed ANOVA table is called. There are additional arguments that could be made for custom situations. Consult ?ezANOVA for more information. There are a few ways to output the analysis. Heres the simplest: one_wayRM &lt;- ezANOVA(data = jw2vol, dv = vol, wid = mouse_ID, within = day, detailed=T, type=1) one_wayRM ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 day 12 120 27138034 3115733 87.09999 2.830845e-53 * 0.8970134 29.4 Interpretation The effect reminds us that the model is day. The inclusion of the mouse_ID in the model is not reported from this particular function. But we can tell this is properly accounted by the degrees of freedom values. DFn is the degrees of freedom for the F test numerator. This is one less than number of levels (13) in the day variable. DFd is the degrees of freedom for the F test denominator. We have 143 measurements and begin with 143 degrees of freedom. We lost one degree of freedom for the grand mean. We also remove from the residuals the 12 degrees of freedom for the day variable. We also have 11 mouses, comprising 10 degrees of freedom that are also in the model. That leaves 120 df for the residual error. SSn is \\(SS_{model}\\). SSd is \\(SS_{residual}\\). Divide each of those by their respective degrees of freedom and you have two variances. F is the ratio of the variances, \\[F_{DFn,DFd}=\\frac{\\frac{SSn}{DFn}}{\\frac{SSd}{DFd}}\\] This F-statistic tests the null hypothesis, which is that the variation associated with day+mouse_id is less than or equal to or less than residual variation. The experimental F is tested against an F distribution with 12 and 120 degrees of freedom. The p-value is the probability of obtaining an F-statistic value this high or even more extreme if the null hypothesis were true. When this value is less than a predetermined type1 error threshold the null can be safely rejected. The ges is a regression coefficient general eta squared that can take on values between 0 and 1. It is analogous to the better known regression coefficient \\(R^2\\) GES is the ratio of the variation due to the effect of the model to the total variation in the data set: \\[ges=\\frac{SS_{model}}{SS_{residual}+SS_{model}}\\] The value of 0.897 can be interpreted as follows: 89.7% of the observed variation in the data is associated with the differences between days when controlled for mouse_ID. Scientifically, you can infer from this F-test result that the HT29 tumor volumes grow with time when implanted into this mouse strain. Sometimes, thats all you wish to conclude. Does the tumor injection model work with this strain and that tumor cell line? Yes. End of story. If we wish to further identify differences of scientific interest we could do a post-hoc analysis. We are under no obligation to do so. 29.5 Post-hoc analysis I have a fairly extensive discussion elsewhere about posthoc analysis. See Chapter 33. Note that since this is a related measures ANOVA I recommend not using range tests (eg, Dunnetts or Tukey, etc) since those operate on group means. Related measures deals with paired measures and the posthoc questions are whether the mean of the differences between paired measures is zero. Therefore, there is no discussion of range tests below. Perhaps wed like to dig a little deeper. For example, we might want to know on which days tumor growth differs from the first day in the recorded series of measurements. The approach taken below involves two steps. First, all pairwise comparisons are made using a paired t-test to generate a matrix of all unadjusted p-values. Second, a vector of select p-values will be collected from this matrix. These p-values will then passed into the p.adjust function so that they are adjusted for a fewer number of multiple comparisons. First, the pairwise t-test. Note the arguments. No adjustment is made (yet) and a two-sided paired t-test is called. The output of the function is stored in an object named m. jw2vol &lt;-jw2vol %&gt;% mutate( day=as.factor(day) ) m &lt;- pairwise.t.test(x = jw2vol$vol, g = jw2vol$day, p.adjust = &quot;none&quot;, paired = T, alternative = &quot;two.sided&quot; ) m ## ## Pairwise comparisons using paired t tests ## ## data: jw2vol$vol and jw2vol$day ## ## 17 18 19 22 24 26 29 31 33 ## 18 0.00181 - - - - - - - - ## 19 6.8e-05 0.02302 - - - - - - - ## 22 4.0e-05 3.7e-05 0.00026 - - - - - - ## 24 0.00042 0.00080 0.00235 0.02637 - - - - - ## 26 6.4e-05 0.00012 0.00019 0.00056 0.00985 - - - - ## 29 2.3e-05 4.8e-05 5.3e-05 0.00023 0.00489 0.06577 - - - ## 31 1.4e-05 1.6e-05 1.7e-05 3.7e-05 6.0e-05 0.00078 0.00083 - - ## 33 1.3e-05 1.7e-05 1.6e-05 2.4e-05 4.1e-05 3.6e-05 1.8e-05 0.00053 - ## 36 1.0e-06 1.1e-06 1.1e-06 1.6e-06 1.3e-06 1.3e-06 1.0e-06 8.6e-07 0.00058 ## 38 1.3e-06 1.5e-06 1.4e-06 1.6e-06 1.2e-06 1.1e-06 1.6e-06 7.1e-06 1.4e-05 ## 40 6.3e-06 6.9e-06 6.6e-06 7.3e-06 6.0e-06 8.0e-06 8.9e-06 1.4e-05 1.3e-05 ## 44 7.0e-07 7.0e-07 6.9e-07 7.6e-07 6.5e-07 7.8e-07 7.1e-07 1.1e-06 8.6e-07 ## 36 38 40 ## 18 - - - ## 19 - - - ## 22 - - - ## 24 - - - ## 26 - - - ## 29 - - - ## 31 - - - ## 33 - - - ## 36 - - - ## 38 0.00023 - - ## 40 0.00014 0.00247 - ## 44 4.5e-06 1.1e-05 0.00181 ## ## P value adjustment method: none The pairwise.t.test output m is a list of 4 elements. str(m) ## List of 4 ## $ method : chr &quot;paired t tests&quot; ## $ data.name : chr &quot;jw2vol$vol and jw2vol$day&quot; ## $ p.value : num [1:12, 1:12] 1.81e-03 6.84e-05 4.05e-05 4.18e-04 6.40e-05 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:12] &quot;18&quot; &quot;19&quot; &quot;22&quot; &quot;24&quot; ... ## .. ..$ : chr [1:12] &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;22&quot; ... ## $ p.adjust.method: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr &quot;pairwise.htest&quot; The most important of these is $p.value, which you can see is a matrix. class(m$p.value) ## [1] &quot;matrix&quot; &quot;array&quot; The matrix contains p-values that represent the outcome of paired t-tests for tumor_vol between all possible combinations of days. Thus, the p-value in the cell defined by the second row and second column of the matrix (m$p.value[2,2]=0.00182) reflects that for the mean difference in tumor_vol between the 17th and 18th days. (note: day 22 is out of order) The first column of p-values, m$p.value[,1], are paired t-test comparisons of tumor_vol between the 22th day and each of the days 18 through 44. pv &lt;- m$p.value[,1] pv ## 18 19 22 24 26 29 ## 1.813199e-03 6.836240e-05 4.047601e-05 4.184046e-04 6.397132e-05 2.285676e-05 ## 31 33 36 38 40 44 ## 1.434113e-05 1.320992e-05 1.030436e-06 1.298778e-06 6.341427e-06 7.020905e-07 To create the adjusted p-values, we pass the vector of p-values pv selected from the p-value matrix m into the p.adjust function. Use your judgement to select an adjustment method that you deem most appropriate. p.adjust(p = pv, method = &quot;bonferroni&quot;, n = length(pv) ) ## 18 19 22 24 26 29 ## 2.175839e-02 8.203488e-04 4.857121e-04 5.020856e-03 7.676558e-04 2.742811e-04 ## 31 33 36 38 40 44 ## 1.720936e-04 1.585190e-04 1.236523e-05 1.558534e-05 7.609713e-05 8.425086e-06 Since each of these adjusted p-values is less than the type 1 error threshold of 0.05, we can conclude that the mean difference in tumor volume changes on each day through the study. If we were one to to put asterisks on the figure, we would illustrate one for each of the days (other than day 17). 29.6 Write up Heres how we might write up the statistical methods. For the jaxwest2 experiment, each of 11 mice are treated as independent replicates. Repeated tumor volume measurements were collected beginning on day 17 post-implantation. The tumor volume value for day = 17 for the 3rd subject was lost. This was imputed using the average tumor volume value for day 17 of all other subjects. The effect of time was assessed by one-way repeated measures ANOVA with type 1 sums of squares calculation. For a post hoc analysis, the mean differences in tumor vol between study days were compared using two-sided paired t-tests (pairwise.t.test), with p-values adjusted using the Bonferroni method. In the figure legend, something like this: Tumor volume increases with days after implantation (one-way RM ANOVA, F(12,120)=87.1, p=2.8e-53). Asterisks = adjusted p &lt; 0.05. "],["twowayCR.html", "Chapter 30 Two-way ANOVA Completely Randomized 30.1 Effect of Strain and Diet on Liver 30.2 The test 30.3 Interpretation of 2 Way CR ANOVA Output 30.4 Post Hoc Multiple Comparisons 30.5 Summary", " Chapter 30 Two-way ANOVA Completely Randomized library(tidyverse) library(ez) library(knitr) library(kableExtra) library(viridis) library(DescTools) A two-way ANOVA experimental design is one that involves two predictor variables, where each predictor has 2 or more levels. There is only one outcome variable in a 2 way ANOVA and it is measured on an equal interval scale. The predictor variables are often referred to as factors, and so ANOVA designs are synonymous with factorial designs. The experimental designs can be as follows: Completely randomized (CR) on both factors Related measures (RM) on both factors Mixed, CR on one factor and RM on the other In one sense, a two-way ANOVA can be thought of as two one-way ANOVAs run simultaneously. The major difference, however, is the ability to test whether an interaction exists between the two factors. In this chapter well focus on the data structure and analysis of a two-way ANOVA CR experimental design. 30.1 Effect of Strain and Diet on Liver A (hypothetical) study was conducted to evaluate the influence of mouse background strain and diet on the accumulation of cholesterol in liver. Ten animals were selected randomly from each of the C57BL/6 and C57BL/10 strains. They were each split randomly onto either of two diets, normal and high fat. After about two months they were sacrificed to obtain cholesterol measurements in liver tissue. Three predictions, and the corresponding null hypotheses that each tests, can be evaluated here simultaneously: The two strains differ in liver cholesterol content. \\(H0:\\sigma^2_{strain}\\le\\sigma^2_{residual}\\) The diets differ in how they affect liver cholesterol content. \\(H0:\\sigma^2_{diet}\\le\\sigma^2_{residual}\\) The liver cholesterol content is influenced by both diet and strain. \\(H0:\\sigma^2_{strainXdiet}\\le\\sigma^2_{residual}\\) The first two of these are commonly referred to as the main effects of the factors, whereas the third is referred to as the interaction effect of the factors. Heres some simulated data. But they are guided guided using means and standard deviations from the Jackson Labs phenome database). along with a graph of the results: ID Strain Diet Cholesterol 1 C57BL/6 Normal 49 2 C57BL/6 Normal 43 3 C57BL/6 Normal 44 4 C57BL/6 Normal 37 5 C57BL/6 Normal 47 6 C57BL/6 High Fat 91 7 C57BL/6 High Fat 86 8 C57BL/6 High Fat 120 9 C57BL/6 High Fat 111 10 C57BL/6 High Fat 101 11 C57BL/10 Normal 100 12 C57BL/10 Normal 123 13 C57BL/10 Normal 125 14 C57BL/10 Normal 115 15 C57BL/10 Normal 88 16 C57BL/10 High Fat 207 17 C57BL/10 High Fat 228 18 C57BL/10 High Fat 217 19 C57BL/10 High Fat 220 20 C57BL/10 High Fat 217 ggplot( liver, aes(Diet, Cholesterol, fill=Strain) ) + stat_summary( fun.data=&quot;mean_sdl&quot;, fun.args=list(mult=1), geom= &quot;errorbar&quot;, position=position_dodge(width=1), width=0.2, size=1 ) + stat_summary( fun.y=&quot;mean&quot;, fun.args=list(mult=1), geom= &quot;bar&quot;, position=position_dodge(width=1), width=0.5, size=2 ) + scale_fill_viridis( discrete=T ) + theme( legend.position=(c(0.8, 0.8)) ) + labs( title=&quot;Strain and Diet Effects on Liver Cholesterol, mean +/- SD&quot;, x=&quot;Diet&quot;, y=&quot;Liver Cholesterol, mg/&quot; ) ## Warning: `fun.y` is deprecated. Use `fun` instead. Figure 30.1: Two-way completely randomized ANOVA results In particular, pay attention to the data structure. It has four columns: ID, Strain, Diet, Cholesterol. All of these are variables, including two columns for each of the predictor variables (Strain, Diet), and one for the response variable (Cholesterol). The ID can also be thought of as a variable. 30.2 The test We use ezANOVA in the ANOVA package to test the three null hypotheses. Here are the arguments: out.cr &lt;- ezANOVA(data = liver, dv = Cholesterol, wid = ID, between = c(Strain,Diet), type = 3, return_aov = T, detailed = T ) out.cr ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 (Intercept) 1 16 280608.05 2096.4 2141.63747 1.801635e-18 * 0.9925845 ## 2 Strain 1 16 41496.05 2096.4 316.70330 5.742386e-12 * 0.9519091 ## 3 Diet 1 16 34196.45 2096.4 260.99180 2.499051e-11 * 0.9422366 ## 4 Strain:Diet 1 16 3100.05 2096.4 23.65999 1.722945e-04 * 0.5965707 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 3 16 283.8 748.4 2.022448 0.1513253 ## ## $aov ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Terms: ## Strain Diet Strain:Diet Residuals ## Sum of Squares 41496.05 34196.45 3100.05 2096.40 ## Deg. of Freedom 1 1 1 16 ## ## Residual standard error: 11.44662 ## Estimated effects may be unbalanced 30.3 Interpretation of 2 Way CR ANOVA Output We get two F test tables, Levenes and the ANOVA. We also get the aov object, which is useful should range test post hoc functions be selected later. Specific statistical details about these tests are covered in the document Completely Randomized One Way ANOVA Analysis. The only difference in two-way CR ANOVA compared to a one-way CR ANOVA is the test of the nulls for the additional factor and for the interaction between the two factors. The model is a bit more complex. 30.3.1 Levenes Levenes tests the null that homogeneity of variance is equivalent across the groups. The p-value of 0.15 is higher than the 0.05 type1 error rejection threshold. Levenes is inconclusive, offering no evidence the homogeneity of variance assumption has been violated. 30.3.2 ANOVA Table The ANOVA table shows p-values that are below the 0.05 type1 error threshold for each factor and for their interaction. We can safely reject the interaction null hypothesis (p=0.00017). Scientifically, we would conclude that the effect of diet on liver cholesterol depends upon the strain of the animal. The partial eta-square indicates this interaction between diet and strain explains about 59.6% of the observed variation. Since the interaction is positive, its difficult to say much about diet and strain. Sure, diet affects liver cholesterol. Strain does as well. The ANOVA per se cannot parse those out from the interaction effect (we wait for regression analysis for that!). 30.4 Post Hoc Multiple Comparisons We could leave well enough alone and draw our inferences on the basis of the interaction effect alone. However, it would not be unreasonable to compare the effect of diet within each strain. at each level of diet. Nor is it unreasonable to compare the each strain across the diets. 30.4.1 Pairwise.t.tests Well do run a pairwise.t.test function to make all possible comparisons, and use a p-value adjustment method to keep the family-wise error rate within 5%. The following script yields every possible comparison between levels of the two factors. pairwise.t.test(liver$Cholesterol, interaction(liver$Strain, liver$Diet), paired=F, p.adj = &quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: liver$Cholesterol and interaction(liver$Strain, liver$Diet) ## ## C57BL/10.High Fat C57BL/6.High Fat C57BL/10.Normal ## C57BL/6.High Fat 1.4e-10 - - ## C57BL/10.Normal 3.5e-10 0.26 - ## C57BL/6.Normal 3.4e-13 1.1e-06 2.8e-07 ## ## P value adjustment method: holm Diet effect within each strain: 10.High v 10.Normal: p=3.5e-10 6.High v 6.Normal: p=1.1e-06 Strain effect across diets: * 10.High v 6.High p=1.4e-10 * 10.Normal v 6.Normal p=2.8e-7 Of the six possible comparisons, only one shows no difference (liver cholesterol in C57Bl/10 on normal diet is no different from C57Bl/6 on high diet, p=0.26) 30.4.2 Write Up The interaction between diet and strain accounts for nearly 60% of the variation liver cholesterol levels (2 way CR ANOVA, p=0.00017, n=20). Pairwise differences in the liver cholesterol response exist between levels of diet within strains, and across strains at each level of diet (Holms adjusted p&lt;0.05, pairwise t tests). 30.4.3 Range tests For completely randomized designs, range tests serve as an alternative to pairwise.t.tests. Range tests compare the difference between the means of any two groups against a critical value for the difference. They also compute the effect size, confidence intervals and p-values. The latter two are adjusted for multiple comparisons. Simply feed the $aov object generated by ezANOVA into the PostHocTest function from the DescTools package. Note how these pairwise differences are ordered on the basis of the ANOVA model. Thus, the posthoc comparisons associated with the main effects for each of the two factors, followed by all possible comparisons for the interactions between the two factors. PostHocTest(out.cr$aov) ## ## Posthoc multiple comparisons of means : Tukey HSD ## 95% family-wise confidence level ## ## $Strain ## diff lwr.ci upr.ci pval ## C57BL/6-C57BL/10 -91.1 -101.952 -80.24803 5.5e-12 *** ## ## $Diet ## diff lwr.ci upr.ci pval ## Normal-High Fat -82.7 -93.55197 -71.84803 2.4e-11 *** ## ## $`Strain:Diet` ## diff lwr.ci upr.ci pval ## C57BL/6:High Fat-C57BL/10:High Fat -116.0 -136.71228 -95.28772 1.5e-10 *** ## C57BL/10:Normal-C57BL/10:High Fat -107.6 -128.31228 -86.88772 4.9e-10 *** ## C57BL/6:Normal-C57BL/10:High Fat -173.8 -194.51228 -153.08772 4.2e-13 *** ## C57BL/10:Normal-C57BL/6:High Fat 8.4 -12.31228 29.11228 0.6592 ## C57BL/6:Normal-C57BL/6:High Fat -57.8 -78.51228 -37.08772 3.1e-06 *** ## C57BL/6:Normal-C57BL/10:Normal -66.2 -86.91228 -45.48772 5.2e-07 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 30.5 Summary Two-way CR ANOVA is for experiments where all measurements are independent of all other measurements. When an interaction effect is observed, it is difficult to interpret the main effects of each factor in isolation. Because both factors influence each other. We follow up the F test by performing posthoc comparisons that are important to us. We can either do pairwise.t.tests with p-value adjustments for multiple comparisons, or range tests that generate more interesting information than simple p-values. "],["twowayRM.html", "Chapter 31 Two-way ANOVA Related Measures 31.1 Cell culture 31.2 The test 31.3 Interpretation of the output 31.4 Post Hoc multiple comparisons 31.5 Write Up 31.6 Summary", " Chapter 31 Two-way ANOVA Related Measures library(tidyverse) library(ez) library(knitr) library(kableExtra) library(viridis) A two-way ANOVA experimental design is one that involves two predictor variables, where each predictor has 2 or more levels. There is only one outcome variable in a 2 way ANOVA and it should be continuous, measured on an equal interval scale, and ideally sampled from a normally-distributed population. The predictor variables are often referred to as factors, and so ANOVA designs are synonymous with factorial designs. The list of two-way ANOVA experimental designs can be as follows: Completely randomized (CR) on both factors Related measures (RM) on both factors Mixed, CR on one factor and RM on the other In one sense, a two-way ANOVA can be thought of as two one-way ANOVAs run simultaneously. So it is possible to assess the effects of two factors at once. The major difference, however, is the ability to also test whether an interaction exists between the two factors. In this chapter well focus on the data structure and analysis of a two-way ANOVA RM experimental design. These are particularly common in cell-culture and other in vitro-based experiments, owing to the homogeneity of the biological material. For an experiment such as the one described below, all of the biological material assayed within a single replicate comes from a common source. Every well in every plate or every tube in every rack prepared from that source is identical, for all intents and purposes. Therefore, any measurements taken from these wells are intrinsically-linked. For this reason, the data should be analyzed as related-measures. Figure 31.1: A two-way ANOVA RM design viewed from the bench. Each combination of the two factors is tested in technical duplicate. All measurements are intrinsically-related. An independent replicate occurs when the same protocol is followed, from start to finish, beginning with a new batch of plates or with a new batch of a purified protein or whatever. Even so, its hard to argue these represent true biological replicates. The cells are likely clonal, and probably arent changing much from batch to batch. The statistical analysis is therefore essentially focused on establishing whether the observations occur repeatedly or not. The replicates are therefore blocked, usually in time. For example, an assay on the material on week1 is treated differently than one on week2 because these are two different time blocks. 31.1 Cell culture Coronary artery atheromas are enriched in fast-growing mesenchymal-like cells, which have prominent inflammatory characteristics. They express the transcription factor NFAT, which is regulated by mitogenic receptor signaling to control inflammatory gene expression. Cells from atheroma explants are easy to grow in culture, often indefinitely as for cancer cells. Eventually, a fast growing cell population dominates these monocultures when passaged serially. But these probably mimic the bad cells in an atheroma pretty well, so are useful to study. An experiment was performed on cultured human coronary smooth muscle cells derived from an atheroma to determine if the bzip suppresser ICER attenuates NFAT and whether it is mitogen-selective. One predictor variable is mitogen at 3 levels: vehicle, ADP and PDGF. A second predictor variable is an expression vector, which is either empty or encodes a cDNA to express ICER. The output response, luminescence in cell extracts due to an NFAT-driven luciferase transcriptional reporter, is detected using a luminometer. Luminescence is linearly proportional to the amount of luciferase in the cells. The protocol involved parallel transfections of the CASM cell line with the expression vectors and luciferase reporter, followed a few days later by treatment with the mitogens for 4 hrs before measuring luminescence. All treatments are performed in technical duplicate, from which the average value is used for the response measure. Three predictions, and the corresponding null hypotheses that each tests, can be evaluated here simultaneously: Icer and its control differ in how they each affect NFAT-mediated transcription. \\(H0:\\sigma^2_{suppressor}\\le\\sigma^2_{residual}\\) The mitogens differ in how they affect NFAT-mediated transcription. \\(H0:\\sigma^2_{mitogen}\\le\\sigma^2_{residual}\\) NFAT-mediated transcription is influence by a combination of suppressor and mitogen. \\(H0:\\sigma^2_{suppressorXmitogen}\\le\\sigma^2_{residual}\\) Heres some data: replicate vector mitogen Nfatluc A empty Veh 23.2 B empty Veh 15.7 C empty Veh 17.2 A empty ADP 83.5 B empty ADP 95.9 C empty ADP 89.0 A empty PDGF 124.6 B empty PDGF 187.7 C empty PDGF 170.9 A ICER Veh 16.8 B ICER Veh 5.4 C ICER Veh 27.6 A ICER ADP 89.8 B ICER ADP 80.2 C ICER ADP 52.5 A ICER PDGF 57.0 B ICER PDGF 78.1 C ICER PDGF 67.4 ggplot(hcsmc, aes( mitogen, Nfatluc, group=replicate) ) + geom_line( ) + geom_point( aes( color=replicate), size=4 ) + facet_grid( ~vector, switch=&quot;both&quot; ) + scale_shape_manual( values=c(16, 16) ) + scale_color_viridis( discrete=T ) + theme( legend.position=&quot;top&quot; ) + scale_x_discrete( limits=c(&quot;Veh&quot;,&quot;ADP&quot;,&quot;PDGF&quot;) ) + labs( y=&quot;NFAT-luciferase, light units/mg&quot;, x=&quot;Mitogen&quot; ) Figure 31.2: A cell culture-based two-way ANOVA RM experiment 31.2 The test Scientific judgement dictates running this test as related measures. Each replicate was performed on a different passage of cells, approximately 1 week apart. Within each passage the cells are highly homogeneous. The transfections and treatment and biochemical measurements were all conducted in a side-by-side block. The measurements within a replicate are intrinsically-linked. Perhaps most importantly, each measure taken on a given day is NOT independent of all other measures that day. But we can assume they differ from week to week. There really is no other way to analyze these data. out.rm &lt;- ezANOVA(data = hcsmc , dv = Nfatluc , wid = replicate , within = c(vector, mitogen) , type = 3 , return_aov = T , detailed = T ); out.rm ## Warning: Converting &quot;replicate&quot; to factor for ANOVA. ## Warning: Converting &quot;vector&quot; to factor for ANOVA. ## Warning: Converting &quot;mitogen&quot; to factor for ANOVA. ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 (Intercept) 1 2 91378.125 388.5700 470.33031 0.002119408 * ## 2 vector 1 2 6156.801 471.0011 26.14347 0.036186976 * ## 3 mitogen 2 4 29018.893 1981.2567 29.29342 0.004084641 * ## 4 vector:mitogen 2 4 7333.031 623.0722 23.53830 0.006133042 * ## ges ## 1 0.9634772 ## 2 0.6399535 ## 3 0.8933620 ## 4 0.6791774 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 mitogen 0.6795372 0.8243404 ## 4 vector:mitogen 0.4963405 0.7045144 ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 3 mitogen 0.7573102 0.01101064 * 2.620487 0.004084641 * ## 4 vector:mitogen 0.6650442 0.02123339 * 1.485468 0.006133042 * ## ## $aov ## ## Call: ## aov(formula = formula(aov_formula), data = data) ## ## Grand Mean: 71.25 ## ## Stratum 1: replicate ## ## Terms: ## Residuals ## Sum of Squares 388.57 ## Deg. of Freedom 2 ## ## Residual standard error: 13.93862 ## ## Stratum 2: replicate:vector ## ## Terms: ## vector Residuals ## Sum of Squares 6156.801 471.001 ## Deg. of Freedom 1 2 ## ## Residual standard error: 15.34603 ## 2 out of 3 effects not estimable ## Estimated effects are balanced ## ## Stratum 3: replicate:mitogen ## ## Terms: ## mitogen Residuals ## Sum of Squares 29018.893 1981.257 ## Deg. of Freedom 2 4 ## ## Residual standard error: 22.25565 ## 2 out of 4 effects not estimable ## Estimated effects may be unbalanced ## ## Stratum 4: replicate:vector:mitogen ## ## Terms: ## vector:mitogen Residuals ## Sum of Squares 7333.031 623.072 ## Deg. of Freedom 2 4 ## ## Residual standard error: 12.48071 ## Estimated effects may be unbalanced 31.3 Interpretation of the output Note here that we are NOT capturing an aov object. The only use of the aov object we might have is for posthoc testing with range tests. But we should not use range tests on a pure related measures design, where the means of groups are irrelevant. 31.3.1 ANOVA Table Note just as for the 2-way ANOVA CR analysis, there are 3 F tests for the model (the intercept F test is inconsequential for now). The F value for the suppress:mitogen interaction is beyond the critical limit for a null F distribution with 2 and 4 degrees of freedom. The result is extreme (p = 0.00613). The interaction takes precedence, since the main effects are difficult to interpret if an interaction occurs. It is safe to reject the interaction null and conclude that variance for its effect exceeds that for its residual. Scientifically, this means that levels of NFAT-mediated transcription are influenced by both the suppressor and the mitogen stimuli, as designed in this experiment. About 68% of the variance in the data can be explained by this interaction effect. 31.3.2 Mauchlys Sphericity Test Sphericity is defined as uniform variance of the differences. Think of it as the RM analog to the CR homogeneity of variance assumption. Sphericity, as for homogeneity of variance, is an assumption ideally met in RM ANOVA for validity of the result. Mauchlys tests the null hypothesis that the variances among the differences are equal. If the test statistic were extreme, the null would be rejected, meaning these variances are unequal and sphericity is violated. If that were the case, wed conduct inference on the basis of a corrected p-value for the ANOVA..the p[GG], which is the Geisser-Greenhouse corrected p-value. In our analysis, the Mauchly test is not extreme. We have no reason to believe the sphericity assumption is not satisfied. We can use the p-value in the ANOVA table to guide our inference, without using the sphericity correction. 31.4 Post Hoc multiple comparisons The true scientific objective here is to know whether the suppressor selectively attenuate NFAT-mediated transcription by either of the mitogens. In other words, scientifically, it would be interesting to learn whether an ICER-suppressible factor participates in the pathway of one of the mitogens, but not the other. Given that objective, there are some fairly specific planned comparisons worth making. If we compare ICER to Empty for each level of mitogen, well have an answer to the question posed above. In other words, we only care about 3 of the 15 possible comparisons that could be made. Therefore, well run a pairwise.t.test without creating any adjusted p-values. After that, well create a small vector of p-values for the 3 comparisons we wish to make and use the p.adjust function to correct them for multiple comparisons. Step1: Run the pairwise.t.test but dont adjust p-values We use a paired=T argument given the intrinsically-linked relationship of the measures in the samples. Not all of the measures are independent so the unpaired t-test is inappropriate. m &lt;- pairwise.t.test(hcsmc$Nfatluc, interaction(hcsmc$vector, hcsmc$mitogen), paired=T, p.adj = &quot;none&quot;) m ## ## Pairwise comparisons using paired t tests ## ## data: hcsmc$Nfatluc and interaction(hcsmc$vector, hcsmc$mitogen) ## ## empty.ADP ICER.ADP empty.PDGF ICER.PDGF empty.Veh ## ICER.ADP 0.3413 - - - - ## empty.PDGF 0.0439 0.0803 - - - ## ICER.PDGF 0.0129 0.6800 0.0191 - - ## empty.Veh 0.0066 0.0316 0.0214 0.0276 - ## ICER.Veh 0.0147 0.0720 0.0215 0.0429 0.7723 ## ## P value adjustment method: none Step2: Collect the p-values only for the comparisons of scientific interest pv &lt;- c(m$p.value[1,1], m$p.value[3,3], m$p.value[5,5]) pv ## [1] 0.34127356 0.01905007 0.77231675 Thus, the three comparisons and their unadjusted p-values are: ICER.ADP to empty.ADP p=0.3413 ICER.PDGF to empty.PDGF p=0.0191 ICER.Veh to empty.Veh p=0.7723 Step3: Adjust those p-values for multiple comparisons p.adjust(p=pv, method=&quot;bonf&quot;, n=length(pv)) ## [1] 1.00000000 0.05715022 1.00000000 Each of these adjusted p-values is greater than the type1 error threshold of 5%. On this basis, we cannot reject the null hypothesis that these response are the same. This illustrates the occasional case where a positive result at the level of the omnibus test is coupled to a negative finding in post hoc testing. In fact, that shouldnt be a surprise. The F test sniffed out some decent sized effects within the collection of responses that are not of scientific interest. 31.5 Write Up Statistical analysis of the luciferase data indicates there is an interaction between levels of the suppressor and the mitogen variables (2 way RM ANOVA, p=0.0061, n=3). However, post hoc comparisons do not support the hypothesis that ICER selectively suppresses either of the mitogens (pairwise two-sided paired t-tests, Holm adjusted p-values for multiple comparisons). The experiment may have been underpowered. 31.6 Summary We use two-way repeated measures ANOVA (both arms RM) when all measurements within each replicate are intrinsically-linked. When the experiment involves in vitro biological material there is a very good chance this ANOVA is the right choice. Inference can be done on only the main effect(s). When the interaction is positive, the main effects are difficult to interpret. Follow-up with pairwise.t.tests (paired=T), not with range tests. "],["twowaymixed.html", "Chapter 32 Two-way ANOVA RM/CR 32.1 ChickWeight Data set 32.2 Munge ChickWeight data 32.3 The test 32.4 Interpreting the ANOVA output 32.5 Post hoc 32.6 Summary", " Chapter 32 Two-way ANOVA RM/CR #library(magrittr) library(tidyverse) library(DescTools) library(ez) library(nlme) Two-way ANOVA experiments allow for simultaneously testing the effects of two predictor variables. In a two way completely randomized ANOVA, all of the levels of both factors are randomly allocated to individual experimental units. Each experimental unit represents an independent replicate and each generates a single measurement for analysis. In a two-way related/repeated measures ANOVA, all of the levels for both factors are measured within a single experimental unit. The experimental unit represents the independent replicate, and therefore the data set contains many intrinsically-related measurements from each replicate. A hybrid of the two above is called a two-way mixed ANOVA experimental design. In a mixed ANOVA, one factor is completely randomized, whereas the other is structured as a related/repeated measures within each replicate. The ChickWeight study is a good example for visualizing how a two-way mixed ANOVA would operate. 32.1 ChickWeight Data set The ChickWeight dataset in base R is from a study testing the effect of 4 diets on the growth of chicks. You can learn more about it by typing ?ChickWeight in the console. As you might imagine, the principal goal of a study like this is to determine if any of the 4 diets differ from each other in terms of how they impact chick weight over time. This is a two way mixed ANOVA design. The two factors are diet and time. Chicks are randomly assigned to a diet. While on the diet each chick is weighed repeatedly at various time points. Therefore, Diet is a completely randomized (CR) factor, whereas the factor time (in units of days) is a related/repeated measure variable. If thats not entirely clear, it should become more obvious by looking at a plot of the data. 32.2 Munge ChickWeight data We have several things to do before running ANOVA. First, lets just look at the data graphically. That makes the study design a bit easier to understand. Then well clean it up for what we need to do. Step 1: Plot it for a quick visualization Note how each point-to-point line represents a unique chick replicate. You can get a sense from this visual how weight correlates with the replicates by time. Heavier chicks are heavy throughout the time course, lighter chicks are lighter throughout the time course. This serves as an interesting visualization of biological variation. Visually, can you tell if any of the diets differ from the others? ggplot(ChickWeight, aes(Time, weight, group=Chick, color=Chick) ) + geom_point( ) + geom_line( ) + scale_color_viridis_d( ) + theme_classic( ) + facet_grid( ~Diet) Step 2: First, check for missing data on the within factor, chicks. We cant have any for a RM design. ezDesign is a slick function in the ez package with which to check for missing values: ezDesign( data = ChickWeight, x = .(Time), y = .(Chick), row = NULL, col = NULL, cell_border_size = 1 ) Step 3: Remove Chicks for which full Time series is missing, while creating a repaired dataset. Heres a hemi dplyr-based way to do that: temp &lt;- as.data.frame( ChickWeight%&gt;% group_by(Chick)%&gt;% tally() ) # show only the Chick replicates that have incomplete measures on time temp[temp$n&lt;12, ] ## Chick n ## 1 18 2 ## 2 16 7 ## 3 15 8 ## 8 8 11 ## 41 44 10 # create new dataframe for ANOVA analysis in which the incomplete replicates are removed ChickWeight1 &lt;- ChickWeight[!(ChickWeight$Chick %in% temp$Chick[temp$n&lt;12]),] Step 4: Check to make sure the new dataframe from the previous step is clean, that incomplete replicates are out ezDesign( data = ChickWeight1 , x = .(Time) , y = .(Chick) , row = NULL , col = NULL , cell_border_size = 1 ) Step 5: The ChickWeight dataframe has Time as a numeric, which needs to be converted to a factor to get stuff to work. ChickWeight1$Time &lt;- as.factor(ChickWeight1$Time) One last check of the data structure: head(ChickWeight1) ## Grouped Data: weight ~ Time | Chick ## weight Time Chick Diet ## 1 42 0 1 1 ## 2 51 2 1 1 ## 3 59 4 1 1 ## 4 64 6 1 1 ## 5 76 8 1 1 ## 6 93 10 1 1 str(ChickWeight1) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 540 obs. of 4 variables: ## $ weight: num 42 51 59 64 76 93 106 125 149 171 ... ## $ Time : Factor w/ 12 levels &quot;0&quot;,&quot;2&quot;,&quot;4&quot;,&quot;6&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Chick : Ord.factor w/ 45 levels &quot;13&quot;&lt;&quot;9&quot;&lt;&quot;20&quot;&lt;..: 11 11 11 11 11 11 11 11 11 11 ... ## $ Diet : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language weight ~ Time | Chick ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time&quot; ## ..$ y: chr &quot;Body weight&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(gm)&quot; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Diet ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;FUN&quot;)=function (x) ## - attr(*, &quot;order.groups&quot;)= logi TRUE Done with the munge! Woot! 32.3 The test The main scientific objective of this study is to learn whether growth differs between the four diets. The study design calls for a two-way mixed ANOVA analysis as an omnibus test. If that passes, well have access to compare the diets post-hoc. Were not particularly interested in the effect of the Time factor, per se. We know the chicks will grow with time. In one sense, Time is something of a nuisance factor. Nor are we particularly interested in the interaction effect between Time and Diet. Which is to say that we dont want to know at what times the various levels of the diets differ. We just want to know if the diets differ. So the experiment, although pretty, is a bit over-designed. Having said that, the repeated measures of weight on time seems, intuitively, a solid way for designing the experiment, compared to a few conceivable alternatives. It collects 12 different weights per replicate. That seems better than a study that only weighs each critter once, say, on the 21st day. Thus, there is more thorough information on how each replicate responded to the diet. Which seems useful at some level. ANOVA will not allow us to extract that utility, unfortunately. But were not explicitly applying a rate model to the data to derive growth rate constants that might be compared. The growth rate is more implicitits the information buried within the repeated measures. We just want to know if the four diets differ from each other. The Diet variable is completely randomized. Every weight measurement for a chick on diet 1 is independent of every measurement for a chick on diet 2, and so on. No chick is receiving all four diets, yet every chick is receiving all timed measurements. To encode a two-way mixed ANOVA using ezANOVA, simply configure the arguments such that the repeated measure factor is within and the completely randomized factor is between. An object named two_wayMM, which could have been named foo, is used to store the ANOVA output. two_wayMM &lt;- ezANOVA(data = ChickWeight1, dv = weight, wid = Chick, between = Diet, within = Time, type = 3, detailed = T, return_aov=F ) two_wayMM ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ## 1 (Intercept) 1 41 8404690.44 313495.0 1099.195477 3.122264e-31 * ## 2 Diet 3 41 116403.57 313495.0 5.074559 4.428259e-03 * ## 3 Time 11 451 2023644.28 295322.5 280.945086 6.411563e-194 * ## 4 Diet:Time 33 451 81375.09 295322.5 3.765802 9.341051e-11 * ## ges ## 1 0.9324550 ## 2 0.1605077 ## 3 0.7687269 ## 4 0.1179020 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 3 Time 2.67541e-17 1.032609e-251 * ## 4 Diet:Time 2.67541e-17 1.032609e-251 * ## ## $`Sphericity Corrections` ## Effect GGe p[GG] p[GG]&lt;.05 HFe p[HF] p[HF]&lt;.05 ## 3 Time 0.114145 2.005482e-24 * 0.1160483 8.633944e-25 * ## 4 Diet:Time 0.114145 1.045740e-02 * 0.1160483 1.001674e-02 * 32.4 Interpreting the ANOVA output Here ezANOVA creates Four objects: * $ANOVA * $Mauchlys Test for Sphericity * $Sphericity Corrections` * $aov (not called) 32.4.1 $ANOVA: The ANOVA table Before partitioning for subjects, the total residual sums of squares in the sample would be the sum of the residual error terms, or \\(295322.5 + 313495.0 = 608817.5\\). Because weight measurements were repeated on each chick over time, this procedure allows us to account for some of the variation in the Time variable as that due to variation among the chicks. Pulling 313495 out from the overall residual provides a new (and lower than otherwise) residual term for the effect of Diet, while lowering the residual term for the effect of Time and the interaction. The plot above gives you a sense of whats going on. Whats really noticeable, I think, is a lot of chick-to-chick variation, irrespective of the diet. Theres also a lot of variation across the time frame. But the chick-to-chick variation grows really prominent near the end of the time course. What this does is help to isolate from the variation over the time course that which is due to the differences between the 45 chicks. It doesnt explain why that variation between the chicks occurs, its obviously biological, but it does allow us to partition it out from the Time effect. In the ANOVA table the intercept effect is not particularly important except for the SSd, which tabulates the SS that have been partitioned to the chicks. It is the subject effect. You can think of it as the average subject effect. An extreme value means that there is substantial variation within the chick replicates. Of course there is, theyre growing up! The second effect is Diet, which is important for all the reasons above. There are four diets and thus 3 degrees of freedom for the F test numerator. There are 45 chicks serving as replicates spread out into the four diet groups. The four diet group means cost a total of 4 degrees of freedom, leaving 41 degrees of freedom for the Diet residual variance (denominator). F has been calculated as \\[F=\\frac{\\frac{SSn}{df_n}}{\\frac{SSd}{df_d}}\\] The p-value is the probability of getting an F value as or more extreme from the null F(3,41) distribution. Given the p-value is less than a type 1 error threshold of 5%, we can reject that the F test statistic obtained in the sample belongs to the null. Normally we would conclude that this means the diets effect on weight differs. But thats hard to call to make given the interaction effect is also positive. The general eta-squared (ges) value indicates that 16% of the overall variation is attributable to the Diet effect. The second effect is that for the twelve levels of the variable Time. The interpretation of its table is much the same. There are seven time points per replicate, and one is lost to calculate the grand time mean, leaving 11 df for this test. The residual \\(df_d=(45-1)*(12-1)-(12-1)*(4-1)=451\\). Almost 77% of the total variation in the sample can be explained the effect of time. Although a large effect, it isnt very interesting scientifically. Theres no surprise that the chicks grew a lot! The third effect is the interaction between the Diet and Time variables. \\(df_n=(4-1)*(12-1)=33\\). The interaction between these two variables explains about 11.8% of the total variation in the sample. This suggests the various diets have some differential influence on the growth rates. It isnt just the diets, and it isnt just the time; the two factors together combine to change weights in a way that neither does alone. 32.4.2 Mauchlys Test and Corrections Youll recall one of the assumptions to be met for valid ANOVA analysis is that the response variances of the different levels of a factor should be about equivalent (homoskedasticity). This concept extends to the related measures scenarios. Sphericity is defined as uniform variance for the differences between factor levels. Sphericity is an assumption of RM ANOVA, and if violated, should be corrected. Quick inspection of the time course shows violation. Focus on the connecting lines. They have far greater slope near the end of the time course compared to early stages. Mauchlys test, with its extreme F-statistic value, indicates we can reject the null that sphericity is present. There are differences in variance of differences across the time scale. The Dfs for this test are DfnGGe &amp; DfdGGe. Because of that, we correct the Time result for sphericity using the Geiser-Greenhouse correction. We do this by using the value of p[GG] as a p-value for our test. The correction is dramatic, at least with respect to the p-value correction for the interaction effect. The p[GG]= 0.01045 for the interaction test is that which should be used for inference. 32.5 Post hoc A positive F-test opens the gate to perform post hoc analysis of any groups that interest us. The F tests for the two factors and the interaction shows that there are effects of time and diet on the growth of chicks. The effect of time is not particularly interesting since it is no surprise that chicks grow. What the F tests havent told us is which diets differ. Is one better than any other? Is one worse than any other? 32.5.1 Pairwise.t.tests As has been made clear, we only care whether any of the 4 diets differ from each other. Therefore, we test all differences between each of the diets, making a p-value adjustment for multiple comparisons. This test focuses in on the diets. By using the pooled SD we take into account all of the variation within the experiment. Note how a method for p-value adjustment pairwise.t.test(ChickWeight1$weight, ChickWeight1$Diet, paired=F, conf.level=0.95, p.adjust=&quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: ChickWeight1$weight and ChickWeight1$Diet ## ## 1 2 3 ## 2 0.20679 - - ## 3 0.00012 0.10451 - ## 4 0.00165 0.20679 0.62227 ## ## P value adjustment method: holm Thus we can conclude that the effect on chick weights of Diets 3 (Holm adjusted p=0.00012) and 4 (Holm adjusted p=0.00165) differ from Diet 1. There is no evidence for any other differences between diets. 32.5.2 Tukey test: A range test Since there is a completely randomized arm in this experiment (Diet) it is possible to a range test on that arm. However, it is a bit of a contraption to get there. Unfortunately, the aov object produced by ezANOVA is incompatible with PostHocTest. The protocol for doing a TukeyHSD test in this case runs in three steps. The first step is to create a linear model of the design. The second step is to create an anova object from that model. And the third step is the reason for the first two. The TukeyHSD test function takes an aov object as its data source before running the group comparisons. Because it has a related measures component, the two-way mixed ANOVA experimental design is also known as a linear mixed-effects model. Exactly what this means will be more clear when we get to discussing linear models. We use the lme function from the nlme package to regress the model on the ChickWeight1 dataset, generating a large table of coefficients for all the model elements. model &lt;- lme(weight ~ Diet + Time + Diet*Time, data=ChickWeight1, random = ~ 1|Chick) Next, that linear model is passed into the aov function, to create an aov object. Because R is cool, we do that inside the PostHocTest function from the DescTools package. We specify Diet so that this only performs comparisons between Diet groups. (If you remove the which argument it will return all possible combinations of every group in the experiment!) PostHocTest(aov(model,data=ChickWeight1), which=&quot;Diet&quot;, method=&quot;hsd&quot;, conf.level=0.95) ## ## Posthoc multiple comparisons of means : Tukey HSD ## 95% family-wise confidence level ## ## $Diet ## diff lwr.ci upr.ci pval ## 2-1 14.976042 4.423629 25.52845 0.0016 ** ## 3-1 35.309375 24.756962 45.86179 9.7e-11 *** ## 4-1 30.692708 19.785494 41.59992 1.1e-10 *** ## 3-2 20.333333 8.626482 32.04018 5.5e-05 *** ## 4-2 15.716667 3.689020 27.74431 0.0045 ** ## 4-3 -4.616667 -16.644313 7.41098 0.7555 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What I like about the TukeyHSD test run this way is that it yields effect size, adjusted confidence intervals, and adjusted p-values for each comparison. For example, the difference in mean weight between the first and the second diets is 14.9 grams (95%CI(adjusted): 4.4 to 25.5 grams, adjusted p-value=0.0016). 32.5.3 Heres whats been discovered by the TukeyHSD test Thus, in terms of how the 4 levels of the Diet factor differ in causing chick weight we can conclude: a) all diets are better than Diet 1, b) Diets 3 and 4 are better than Diet 2, c) Diets 3 and 4 do not differ. Reporting The data were analyzed by type 3 two-way mixed ANOVA with repeated measures on Time (R 3.5.2, ezANOVA). The diet, time interaction F test was positive following correction for a positive Mauchlys test (correction factor = 0.114145, GG-adjusted p-value = 0.01045). Post hoc comparisons using TukeyHSD adjustments for multiple comparisons indicate the mass of animals on Diet 1 is less than on all others, and that weight is greater on Diets 3 and 4 than on Diet 2. Ideally, report the last sentence in a table, showing the results. It is important to understand that we took the p-value of p[GG] from the sphericity correction table, rather than that for Time from the ANOVA table, as our inference for this experiment 32.5.4 Why is the TukeyHSD result so different than the pairwise.t.test?? In a word, because they are completely different tests. In statistics, completely different tests will usually generate different results. TukeyHSD is a range test that defines a critical difference between group means and declares any difference more than that an asterisk. P-value adjustment is an adjustment of a p-value. The two operate on different assumptions and formulas. The fact that Tukey yields so many more significant differences than a Holm p-value adjustment simply points to the fact that Tukey is much more liberal in this case. Presumably it does not protect against type1 error as well. Swap out hsd in the code above with scheffe and youll see how range tests can differ from each other, too. This emphasizes the need to decide on which posthoc strategy to take PRIOR to conducting the experiment. 32.6 Summary In mixed two way ANOVA one variable is completely randomized and the second is related/repeated measures. Notice how we just tossed out chicks with missing data. For thus with many missing time points that makes sense. For those with fewer missing time points, we might have been more cautious. Whatever. This an unbalanced data set, good for exploring differences in typeI,II, III SS calculations. Posthoc options include range tests for the completely randomized variable, but use lme to create a model to create an aov object for the posthoc function (whew!). *Great data set for illustrating the importance of preplanning. "],["posthoc.html", "Chapter 33 ANOVA Posthoc Comparisons 33.1 Overview of options 33.2 Reporting the result 33.3 Summary", " Chapter 33 ANOVA Posthoc Comparisons library(tidyverse) library(DescTools) library(ez) library(lsr) Where example data is shown below it is base upon the ANOVA analysis of the chickwts data set as shown Chapter28 . 33.1 Overview of options The major issue to address in ANOVA posthoc testing is the multiple comparison problem. We have a family of comparisons. Every comparison is a hypothesis test. Every hypothesis test carries type1 and type2 error risk. Testing too many hypotheses in a family at once leads to certain error. There are two fundamental ways to control the family-wise error rate (FWER) after ANOVA: 1) p-value adjustments, or 2) range tests. The menagerie of options within each of these is large and confusing. That largely stems from the fact that nobody really agrees on the best way to control FWER. When a fist fight breaks out at a international statistics conference you can almost be certain it is due to FWER. In fact, it would seem everyone who has ever had a strong opinion on the matter has contributed an eponymous test. I say this to make it clear that you have a judgment to make on what the best approach is for the problem you are trying to solve with your own experiment. Heres what is most important to keep in mind. Sometimes we do these multi-factorial experiments in an exploratory mindset, without precise planning for the groups we might want to compare. We may even compare every group to all other groups, and sometimes that is all right. Know that there are post hoc tests more suited for that hit seeking mindset compared to other tests. Other times weve set up a severe test of a crucial scientific prediction. We have a clear idea of the comparisons we want to make. We hope the result will be the foundation of the next few years of work. We (should) have very low tolerance for type1 error in a situation like that. Know there are post hoc tests more suited than others for protecting against type1 error in cases like that. 33.1.1 Pairwise.t.test with p-value adjustments I promote posthoc testing using the pairwise.t.test. There are a few reasons for this. We are already familiar with t-tests. We only need one function for many kinds of uses. It can be used for both completely randomized (unpaired) and for related measure (paired) designs. Several p-value adjustment options are available. The pairwise.t.test function is not the t.test function. The pairwise.t.test denominator uses standard error derived from the residual variance from the full experiment, rather than just that for each pair in a comparison. This is statistically appropriate because it takes into account the context in which all the data was collected. A downside of using the pairwise.t.test is that the function only produces adjusted p-values. It does not produce adjusted confidence intervals. Indeed, except for the straight Bonferroni, adjusted confidence intervals are not readily derived from these step procedures. If confidence intervals are sought for inferential purposes, using a range test, such as the TukeyHSD, is a better post hoc option. For that, DescTools::PostHocTest(method = \"hsd\") 33.1.1.1 P-value adjustment methods When using the pairwise.t.test we need to select a p-value adjustment method. Here are the options. The output of the test will be an array of adjusted p-values, corresponding to each of the comparisons made. We reject the null for any comparisons where the adjusted p-values are lower than the pre-determined type1 FWER threshold (eg, 5%). The table below compares the performance of these adjustment algorithms on a common vector of unadjusted p-values (see the none column). The pattern from left to right is clear. Both the Bonferroni and Holm offer the most severe type1 protection, the BY is balanced between type1 and type2, the Hommel and Hochberg are liberal and perform about the same, and the BH (aka FDR) is the most liberal. (#tab:compare p-value adjustments)Comparison of p-values adjustment methods available in the pairwise.t.test function; most liberal on left to most conservative on right . none BH_FDR hochberg hommel BY holm bonferroni 0.0028426 0.0142129 0.0142129 0.0142129 0.0324528 0.0142129 0.0142129 0.0155575 0.0194806 0.0215229 0.0215229 0.0444807 0.0609275 0.0777874 0.0152319 0.0194806 0.0215229 0.0215229 0.0444807 0.0609275 0.0761593 0.0155845 0.0194806 0.0215229 0.0215229 0.0444807 0.0609275 0.0779224 0.0215229 0.0215229 0.0215229 0.0215229 0.0491439 0.0609275 0.1076144 33.1.1.1.1 Bonferroni The simplest to understand. Multiplies each unadjusted p-value, \\(p\\), by \\(m\\) total comparisons made. \\[\\begin{equation} p_{adjust} = m \\times p \\end{equation}\\] Use the Bonferroni correction when seeking strong protection against type1 error. Of all of these p-adjustment methods, only the Bonferroni includes a pathway to calculate confidence intervals that are coherent with the adjusted p-values. 33.1.1.1.2 Sidak (unavailable) Although not an option in Rs p.adjust.methods it is easy enough to code. Sidak adjusts each p by the total number of comparisons, m, using the following relationship. \\begin(equation) p_{adjust}=1-(1-p)^m \\end(equation) Use the Sidak correction when you wish strong protection against type1 error, but perhaps not as strong as Bonferroni. 33.1.1.1.3 Holm Selecting the \"holm\" option in the p.adjust.methods function executes a step-based Bonferroni correction procedure otherwise known as the Holm-Bonferroni. The step procedure accounts for why the adjustments can share the same value. This operates after first ranking the unadjusted p-values from smallest to largest p-value, stepping up through them. At each step \\(i\\), the comparison value used in the Bonferroni is reduced to account for those already made. The cumulative maximum is reported. \\[\\begin{equation} p_{adjust}=(m+1-i)\\times p \\end{equation}\\] Use the Holm correction when you wish strong protection against type1 error, but perhaps a bit less strong than the Bonferroni. 33.1.1.1.4 Hochberg Selecting the \"hochberg\" option in the p.adjust.methods function also executes a step-based Bonferroni correction procedure. This differs markedly from the Holm procedure . The \"hochberg\" works by first ranking the unadjusted p-values from highest to lowest. And rather than return the cumulative maxima, it returns the minima. This will leave the highest original p-value unadjusted while the lowest p-value gets the maximal Bonferroni adjustment. The overall outcome of the \"hochberg\" is a liberal p-value adjustment on the input array compared to the Holm and Bonferroni. \\begin(equation) p_{adjust}=(m+1-i)p \\end(equation) Use the Hochberg correction on experiments where you want strong protection against type2 errors. For example, if the experiment is designed more exploratory, closer to a hit screen than to a true hypothesis test. 33.1.1.1.5 Hommel Everybody seems to agree the Hommel procedures is the most difficult to describe. Even if I showed the code here Im not sure I could describe it.13 As is evident from the result in the table of options, it performs along the lines of the Hochberg. As a general rule, I try to avoid using things I dont understand. 33.1.1.1.6 Benjamini-Hochberg (BM, FDR) Selecting the \"BH\" option executes the same step-down procedure as for the \"hochberg\" but with a different correction. Furthermore, the \"BH\" and the \"fdr\" selections run the same calculation, yielding identical output. \"BH\" and \"fdr\" are one and the same in the table above and for this description. To perform the step-down, the unadjusted p-values are re-ranked from highest to lowest and multiplied by a ratio of the total comparisons to their original low-to-high index value. The net effect is that the highest p-value is uncorrected, the lowest p-value gets the full Bonferroni, while the others are corrected between these two extremes. Cumulative minima are reported in the output. The net net outcome is that the Benjamini-Hochberg procedure is even more liberal than the Hochberg. \\begin(equation) p_{adjust}=p \\end(equation) Use the Benjamini-Hochberg correction on experiments where you want strong protection against type2 errors. For example, if the experiment is designed to be more exploratory, more like a hit screen than to a true hypothesis test. Indeed, youll find the BH/fdr used a lot in testing for hits in procedures on large -omics data sets. 33.1.1.1.7 Benjamini-Yekutieli Selecting the \"BY\" method applies a Hochberg-like step-down based p-value correction that is considerably more impactful on type1 error than the Benjamini-Hochberg (FDR) method. \\begin(equation) p_{adjust}=(_{m=1}^{m})p \\end(equation) This strikes an interesting balance between very strong control of type1 error while allowing for a more liberal evasion of type2 error. This is probably a good choice for experiments with objectives that straddle severe hypothesis testing along with hit screening. 33.1.1.2 Examples No p-value adjustment Sometimes its useful to generate p-values that are not corrected for multiple comparisons. For example, when we wish to create a p-value array that we will next subset to focus in on planned comparisons. The script below generates a matrix of p-values for t tests of all possible comparisons, none of which are adjusted for multiple comparisons. Parenthetically, these p-values are what would be observed if the Fisher LSD test were applied. The Fisher LSD test actually does not adjust p-values. allPairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, paired=FALSE, pooled.sd=TRUE, p.adjust= &quot;none&quot;) allPairs ## ## Pairwise comparisons using t tests with pooled SD ## ## data: chickwts$weight and chickwts$feed ## ## casein horsebean linseed meatmeal soybean ## horsebean 2.1e-09 - - - - ## linseed 1.5e-05 0.01522 - - - ## meatmeal 0.04557 7.5e-06 0.01348 - - ## soybean 0.00067 0.00032 0.20414 0.17255 - ## sunflower 0.81249 8.2e-10 6.2e-06 0.02644 0.00030 ## ## P value adjustment method: none The unadjusted p-value output from the pairwise.t.test function is a matrix, which is important to know when the need arises to pull out specific elements of the analysis. class(allPairs$p.value) ## [1] &quot;matrix&quot; &quot;array&quot; allPairs$p.value ## casein horsebean linseed meatmeal soybean ## horsebean 2.067997e-09 NA NA NA NA ## linseed 1.493344e-05 1.522197e-02 NA NA NA ## meatmeal 4.556672e-02 7.478012e-06 1.347894e-02 NA NA ## soybean 6.654079e-04 3.246269e-04 2.041446e-01 0.17255391 NA ## sunflower 8.124949e-01 8.203777e-10 6.211836e-06 0.02643548 0.0002980438 For example, to quickly scan which comparisons are below the p &lt; 0.05 threshold we apply a simple custom extreme function across the matrix: extreme &lt;- function(x){ ifelse(x &lt; 0.05, TRUE, FALSE) } apply(allPairs$p.value, c(1, 2), extreme) ## casein horsebean linseed meatmeal soybean ## horsebean TRUE NA NA NA NA ## linseed TRUE TRUE NA NA NA ## meatmeal TRUE TRUE TRUE NA NA ## soybean TRUE TRUE FALSE FALSE NA ## sunflower FALSE TRUE TRUE TRUE TRUE So we can easily see that without adjustment all but 3 of the 15 possible comparisons are statistically different. Subsets of comparisons Lets imagine weve just started a new postdoc position in a lab that studies chick weights. To set up a new line of research that will span the next few years we need to make a decision on the best way to feed the chicks. The standard chow protocol is casein. Are there any other chows that would be better? The experiment is a one-way completely randomized ANOVA design that measures chick weight on 6 different diets including the casein diet. Lets stipulate the experiment passes an omnibus F test. We now wish to conduct posthoc comparisons to test whether any of the 5 other diets differ from casein. Heres a three step procedure for doing just that. Step1: First, run the pairwise.t.test function, setting the argument p.adjust=\"none\". The output includes a matrix of p-values well name allPairs, providing all possible comparisons. #just repeating from above allPairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, alternative = &quot;two.sided&quot;, p.adjust= &quot;none&quot;) Step2: Select from the allPairs matrix only the p-values that correspond to the comparisons youd like to make. Name that vector of unadjusted p-values, selectPairs. This takes a bit of cleverness depending on what you want to grab from the matrix. For example, we only want to compare all of the diets to casein. The comparisons we want are all in the first column. Use your matrix indexing skillz to grab only the unadjusted p-values from that first column: selectPairs &lt;- allPairs$p.value[, 1] selectPairs ## horsebean linseed meatmeal soybean sunflower ## 2.067997e-09 1.493344e-05 4.556672e-02 6.654079e-04 8.124949e-01 selectPairs &lt; 0.05 ## horsebean linseed meatmeal soybean sunflower ## TRUE TRUE TRUE TRUE FALSE Step3: Now pass these unadjusted p-values in the selectPairs vector into the p.adjust function. We choose a stringent Bonferroni test because our foreseeable life depends on the outcome of this test. We have low tolerance for type1 error. The output of this step is a vector of adjusted p-values for the selected group of comparisons. adjustedPvalues &lt;- p.adjust(selectPairs, method=&quot;bonferroni&quot;) adjustedPvalues ## horsebean linseed meatmeal soybean sunflower ## 1.033998e-08 7.466720e-05 2.278336e-01 3.327039e-03 1.000000e+00 Which of these are extreme? If its not clear by inspection (or too large), use a simple Boolean: adjustedPvalues &lt; 0.05 ## horsebean linseed meatmeal soybean sunflower ## TRUE TRUE FALSE TRUE FALSE We reject the null that mean chick weights are the same between casein and horsebean, or linseed or soybean. Chick weights on meatmeal and sunflower diets show no statistical difference from casein. How would we act on this information? Thats a scientific judgment. Perhaps the sunflower diet is less expensive, or we are from Kansas. It is important to recognize that we have NOT run a test of equivalence. We cannot conclude that chick weights on casein, meatmeal and sunflower are equivalent. The statistical analysis only suggests that they dont differ, given these sampling conditions. That sounds like the same thing but it is not. For example, if the sample size were larger we might observe a statistical difference. Making all possible comparisons Imagine that we were just exploring different feeds and we were interested in comparing all feeds to all other feeds. bonf.adjustedAllpairs &lt;- pairwise.t.test(chickwts$weight, chickwts$feed, alternative = &quot;two.sided&quot;, p.adjust = &quot;bonferroni&quot;) bonf.adjustedAllpairs ## ## Pairwise comparisons using t tests with pooled SD ## ## data: chickwts$weight and chickwts$feed ## ## casein horsebean linseed meatmeal soybean ## horsebean 3.1e-08 - - - - ## linseed 0.00022 0.22833 - - - ## meatmeal 0.68350 0.00011 0.20218 - - ## soybean 0.00998 0.00487 1.00000 1.00000 - ## sunflower 1.00000 1.2e-08 9.3e-05 0.39653 0.00447 ## ## P value adjustment method: bonferroni And heres a quick scan for which of these are now below a FWER p &lt; 0.05. Note how a handful of comparisons that were scored extreme by this simple test above, without a p-value adjustment, are no longer extreme with the p-value adjustment. extreme &lt;- function(x){ ifelse(x &lt; 0.05, TRUE, FALSE) } apply(bonf.adjustedAllpairs$p.value, c(1, 2), extreme) ## casein horsebean linseed meatmeal soybean ## horsebean TRUE NA NA NA NA ## linseed TRUE FALSE NA NA NA ## meatmeal FALSE TRUE FALSE NA NA ## soybean TRUE TRUE FALSE FALSE NA ## sunflower FALSE TRUE TRUE FALSE TRUE In contrast to what we observed running this trick on the unadjusted p-values, with the Bonferroni we go from twelve statistically different comparisons to eight. 33.1.2 Range tests These range tests are designed to test group means. They do not test the means of differences between two levels of a factor. Group means are irrelevant in related measures designs. They should only be used as posthoc for completely randomized designs. When the design is related measures, the posthoc testing should be performed using the pairwise.t.test with p-value adjustments, as covered in the section above. These tests differ in a fundamental way from the p-value adjustments associated with pairwise.t.testing. They operate not on p-values, but on the differences between group means. Their adjusted p-values are derived from their unique Studentized probability distributions of these differences. Range tests first calculate a critical threshold for differences between group means, then compare every group mean to this critical difference. Statistically different comparisons are those that have differences that are greater than the critical difference. The adjustment for multiple comparisons comes from probability distribution of a statistic used to calculate these critical differences. youll recognize these procedures as analogous to calculating confidence intervals and for calculating the critical value for a test statistic value to define the boundary of statistical extremeness. For range tests, that boundary is called the critical difference. Any differences more extreme are ruled as statistically different. 33.1.2.1 How this works An array of group mean differences can be calculated from a vector of group means. Suppose we had a way of declaring some critical difference between group means. Any differences observed in our array above that critical difference value would be taken as statistically different. Imagine a set of ANOVA data comprised of \\(N\\) total replicates. There are \\(n\\) replicate values are in each of \\(k\\) groups. The largest group mean for the response is \\(\\bar y_{max}\\) and the smallest is \\(\\bar y_{min}\\). The pooled variance for all replicate values in the set is \\(s^2\\). Then the random variable \\[q =\\frac{\\bar y_{max}-\\bar y_{min}}{\\sqrt\\frac{s^2}{n}}\\] has what is called a Studentized range distribution. Given a confidence level \\(1-\\alpha\\), the number of groups \\(k\\), and the residual error degrees of freedom \\(df=N-k\\), a critical value of \\(q\\) can be derived from an appropriate studentized distribution. This critical \\(q\\) value is used to define the critical difference between group means in a data set where the residual variance is \\(MS_{residual}\\). For example, the critical difference for the TukeyHSD test is: \\[Tukey\\ critical\\ difference = q_{(conf.level, k, df)}\\times\\sqrt\\frac{MS_{residual}}{n}\\] 33.1.2.1.1 Example of Tukey by hand Imagine a one-way ANOVA analysis where there are 5 groups, each having 4 replicates within. The \\(MS_{residual}= 4.0333\\) and has \\(df = 15\\). From these values we can calculate a critical difference between group means as follows. In the old days, we would find a table for Tukeys HSD q values in the back of a statistics textbook. Rs qtukey function is useful to calculate a value for \\(q\\). q &lt;- qtukey(0.05, 5, 15, lower.tail=F);q ## [1] 4.366985 The critical difference for the TukeyHSD test would be critdiff &lt;- q*sqrt(4.0333/4); critdiff ## [1] 4.385125 With 5 groups there are \\(k(k-1)/2=10\\) differences between group means to calculate. For those differences that exceed the value \\(critdiff = 4.385125\\) we would reject the null that there is no difference between those group means. 33.1.2.1.2 Example doing Tukey with R Lets run through a one-way ANOVA using the chickwt data with a TukeyHSD posthoc as follow up. We do this to produce an aov object that we can pass into the PostHocTest function. chickwts$ID &lt;- as.factor(1:nrow(chickwts)) my.ezaov &lt;- ezANOVA( data = chickwts, wid = ID, dv = weight, between = feed, type = 2, return_aov = T, detailed = T) ## Warning: Data is unbalanced (unequal N per group). Make sure you specified a ## well-considered value for the type argument to ezANOVA(). ## Coefficient covariances computed by hccm() # my.ezaov$ANOVA, this is a dataframe # my.ezaov$Levene, this is also a dataframe # my.ezaov$aov, this is an aov object that we can pass into posthoc functions. Now run the TukeyHSD test: PostHocTest(my.ezaov$aov, conf.level = 0.95, method=&quot;hsd&quot;) ## ## Posthoc multiple comparisons of means : Tukey HSD ## 95% family-wise confidence level ## ## $feed ## diff lwr.ci upr.ci pval ## horsebean-casein -163.383333 -232.346876 -94.41979 3.1e-08 *** ## linseed-casein -104.833333 -170.587491 -39.07918 0.00021 *** ## meatmeal-casein -46.674242 -113.906207 20.55772 0.33246 ## soybean-casein -77.154762 -140.517054 -13.79247 0.00837 ** ## sunflower-casein 5.333333 -60.420825 71.08749 0.99989 ## linseed-horsebean 58.550000 -10.413543 127.51354 0.14133 ## meatmeal-horsebean 116.709091 46.335105 187.08308 0.00011 *** ## soybean-horsebean 86.228571 19.541684 152.91546 0.00422 ** ## sunflower-horsebean 168.716667 99.753124 237.68021 1.2e-08 *** ## meatmeal-linseed 58.159091 -9.072873 125.39106 0.12770 ## soybean-linseed 27.678571 -35.683721 91.04086 0.79329 ## sunflower-linseed 110.166667 44.412509 175.92082 8.8e-05 *** ## soybean-meatmeal -30.480519 -95.375109 34.41407 0.73914 ## sunflower-meatmeal 52.007576 -15.224388 119.23954 0.22070 ## sunflower-soybean 82.488095 19.125803 145.85039 0.00388 ** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The test calculates the measured difference between each group. Application of the range test formula to the dataset yields confidence intervals for each comparison of interest. Those that do not include the value for the critical difference are flagged with an asterisk as statistically different. Adjusted p-values are also produced. We see that the TukeyHSD posthoc test flags eight group mean differences as extreme. 33.1.2.2 Dunnetts test This post hoc method differs from those above because it is for conducting multiple dependent comparisons, on just a subset of the group means. For example, use Dunnetts to compare each of a group of test means back to the negative control mean. The fewer comparisons dont spread the allowed FWER as thin as the other options. The following script is configured to compare the means at each level of feed to the mean response to the horsebean feed (to illustrate how to define the control group). Dunnetts is nice because it gives you the effect size (diff) and the confidence interval limits for the difference, as well. Note: diff = the difference between diet means for the compared groups. The p-values are adjusted for multiple comparisons. DunnettTest(weight ~ feed, control = &quot;horsebean&quot;, data = chickwts) ## ## Dunnett&#39;s test for comparing several treatments with a control : ## 95% family-wise confidence level ## ## $horsebean ## diff lwr.ci upr.ci pval ## casein-horsebean 163.38333 103.203907 223.5628 5.8e-09 *** ## linseed-horsebean 58.55000 -1.629427 118.7294 0.0591 . ## meatmeal-horsebean 116.70909 55.298874 178.1193 4.2e-05 *** ## soybean-horsebean 86.22857 28.035815 144.4213 0.0016 ** ## sunflower-horsebean 168.71667 108.537240 228.8961 2.0e-09 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 33.1.2.3 Which range test is best? Just like for the p-value adjustment methods, the range tests operate over a range from fairly liberal to fairly conservative control over type1 error. The best one to use depends upon the overarching scope of the experiment. When it is exploratory and we want protection from type2 error we choose a liberal test. When testing a crucial hypothesis and needing protection from type1 error, we choose a more conservative test. Fishers LSD: Only use the Fisher LSD after a positive ANOVA for a small, 3 group experiment. A 3 group experiment has a positive control, a negative control, and the test group. A one-way ANOVA of the data yields an extreme F statistic and a null rejection. The ANOVA doesnt tell us whether thats due to the positive control or due to the test group yielding a good response. In fact, there is only one scientific question of interest: Is the test group different from the negative control? In this case, the Fisher LSD is used as a posthoc to answer that question under the type1 error expended in the ANOVA test. A p-value adjustment is unnecessary because the ANOVA test already serves to protect the FWER. But it does allow for testing whether the test group differs from a control group. Newman-Keuls: The most liberal of range tests that do make adjustments. This should be used in exploratory experiments seeking potential hits, where type1 error is less of a concern. Duncan and TukeyHSD These tests offer moderate protection from type1 and type2 error. As you can see below, although not identical they perform about the same. Scheffe and Dunn Almost has the ring of a Nashville recording duo, doesnt it? Or a trendy Beltline tapas place. These offer the most protection against type1 error, and we are also sure to miss some hits with these two tests. These two tests should be avoided when the experiment is more exploratory in nature. (#tab:range test comparisons)Adjusted p-values from range tests applied to the chickwts data; left to right, liberal to conservative. contrast LSD Newman_Keuls Duncan TukeyHSD Scheffe Dunn horsebean-casein 0.0000000 0.0000000 0.0000000 0.0000000 0.0000006 0.0000000 linseed-casein 0.0000149 0.0000866 0.0000289 0.0002100 0.0016835 0.0002240 meatmeal-casein 0.0455667 0.0455667 0.0455667 0.3324584 0.5322842 0.6835008 soybean-casein 0.0006654 0.0019010 0.0009509 0.0083653 0.0356963 0.0099811 sunflower-casein 0.8124949 0.8124949 0.8124949 0.9998902 0.9999578 1.0000000 linseed-horsebean 0.0152220 0.0152220 0.0152220 0.1413329 0.2994229 0.2283296 meatmeal-horsebean 0.0000075 0.0000436 0.0000145 0.0001062 0.0009357 0.0001122 soybean-horsebean 0.0003246 0.0009361 0.0004681 0.0042167 0.0206107 0.0048694 sunflower-horsebean 0.0000000 0.0000000 0.0000000 0.0000000 0.0000003 0.0000000 meatmeal-linseed 0.0134789 0.0354947 0.0179077 0.1276965 0.2788785 0.2021841 soybean-linseed 0.2041446 0.2041446 0.2041446 0.7932853 0.8936535 1.0000000 sunflower-linseed 0.0000062 0.0000597 0.0000149 0.0000884 0.0007984 0.0000932 soybean-meatmeal 0.1725539 0.1725539 0.1725539 0.7391356 0.8604074 1.0000000 sunflower-meatmeal 0.0264355 0.0670935 0.0341292 0.2206962 0.4064409 0.3965322 sunflower-soybean 0.0002980 0.0016597 0.0005535 0.0038845 0.0192857 0.0044707 33.2 Reporting the result It is imperative to state how FWER has been controlled when performing multiple comparisons as an ANOVA follow up. A surprising number of papers describe analyzing data as using ANOVA, then doing many group comparisons and speckling their figures with many asterisks, without ever mentioning the posthoc procedures. Did they do any? What exactly are these p-values and what exactly do the asterisks represent? Somewhere we MUST write, \"A some way ANOVA was followed by insert posthoc range test or p-value adjustment name here If true, when describing a set of p-values or confidence intervals in a figure or table legend, always use the phrase, adjusted for multiple comparisons. This makes it clear that we are reporting adjusted, rather than unadjusted p-values. A very common mistake occurs when someone runs the posthoc but acts upon the unadjusted p-values rather than the adjusted p-values. This is probably due to the fact that they find posthoc testing confusing. 33.3 Summary Yes, posthoc testing is confusing, given the many options and LOTs of output. Everybody agrees it is wise to control FWER, but nobody agrees on how best to do it. There are two basic procedures: p-value adjustments or range tests. Range tests compare group means, and group means are irrelevant in related measures designs. Dont use range tests on related measure designs. Otherwise, before choosing an option, ask yourself: Is this exploratory or is this a severe test? For exploratory experiments, choose the liberal procedures. They are the ones on the left in the tables above. For severe tests, choose conservative procedures. They are the ones on the right in the tables above. Hommel, Biometrika (1988) 75, 2, pp383-6 "],["jaxwest2.html", "Chapter 34 Reproducible Data Munging Mostly with Tidyverse 34.1 Look at the original data carefully 34.2 Our goal 34.3 Step 1: Read the data into R 34.4 Step 2: Select the variables 34.5 Step 3: Trim the cases 34.6 Step 4: We have a variable problem to fix 34.7 Step 5: Impute or Delete 34.8 Step 6: Go long 34.9 Step 7: Convert other variables to factor 34.10 Step 8: Plot 34.11 Step 9: Run the ANOVA", " Chapter 34 Reproducible Data Munging Mostly with Tidyverse library(tidyverse) library(readxl) library(viridis) Reproducibility is when someone who has your data can conduct the same analysis, arriving at the same parameter estimates and conclusions. The data processing steps of an analysis are perhaps the most critical determinant of reproducibility. Ideally, this is performed using a breadcrumbs process, where each step is traceable. Thats what R scripts do and why they are better than munging data in GUI software, such as excel or other stats packages. Heres an example of what I mean by an R script munge. I thought it would be interesting to try and pull this off using mostly tidyverse functions, if possible. The Jaxwest2 data represent an experiment to establish tumor xenograft growth in \\(NOD.CB17-Prkdc^{scid}/J\\), an immunodeficient mouse strain. Jaxwest2 is a nice data set to illustrate a one-way related measures ANOVA. They also provides an opportunity to illustrate some data wrangling technique. The latter is the focus here. In particular, Ill illustrate how a complete reproducible data munge can be accomplished using mostly just the tidyverse. The study design involved injecting HT29 human colon cancer cells into the mice. Over the next few weeks repeated daily measurements were collected from each mouse on a handful of outcome variables, including body weight and tumor lengths, widths, and heights. Tumor volume was calculated from length and width data. The multiple measures taken from individual subjects are intrinsically-linked. The day of measurement is the only factor, and it has multiple levels. This all fits a one-way repeated measures ANOVA experimental design model. In the study per se, three groups are compared: 1) Control (no vehicle), 2) Control (vehicle), and 3) Test Group (pretreatment). The latter is apparently proprietary, providing very little outcome data. The first two are similar and can be expected to generate the same outcomes. Im not interested in comparing these groups since the comparisons arent particularly interesting scientifically. So well take only the first of these groups to conduct an ANOVA analysis (later). Well pretend the other two treatments dont exist. (Had we compared these groups, too, it would be a two-way ANOVA repeated measures design.) Therefore, we can use this to study the effect of time on tumor cell growth. We can answer the following scientific question: Will tumor cells grow if injected into the animals? Well focus only on a subset of the data, the tumor volume measurements over time in the first group. This chapter illustrates how to wrangle that subset out from the rest of the data in the excel file. 34.1 Look at the original data carefully The data are in a file called Jaxwest2.xls, which is available for you on Canvas but can also be downloaded from the Jackson Labs here. Before starting the munge take a close look at the excel file. It is NOT a cvs file. A few things to note. First, there are two worksheets. One has the experimental data. The second is a variable key. Now look at that first worksheet. There are two header rows, which is problematic. The first header row is incomplete since it has no values over the first 7 columns. The label in the 8th column actually refers to header values in the remainder of the columns, not the data beneath it. Those values correspond to the day data were collected in a time series. The second header row nicely defines the variables for each column. Note how beginning with the 9th column, the variable name incorporates the day number. Thus, bw_1 is the variable body weight on the first day post injection. Thus, the information about the time series is embedded within each variable name. In other words, most of the variable names are hybrids of two variables, carrying information about both the measurement and the day. Thats actually helpful, but well need to deconvolute those names. The good news is that the first header row doesnt provide any information we cant get from the second header row, so when we read in the data well simply omit that first header row. It would only complicate the munge. Finally, below the header, every row is a case that corresponds to a unique mouse. The values for the variable mouse_ID illustrates as much. Heres the big picture. The column and row structure indicate that repeated measures of multiple outcome variables were collected for each of these mice on each of several days. 34.2 Our goal Stop me if Ive used this metaphor previously. But starting a munge is a lot like starting an organic chemistry synthesis. You have the reagents (the excel file, your laptop, and your growing knowledge of R). You know the final product (it needs to be a long table format, with one variable per column). The only question is how will you create the latter given the former. In this chapter, lets collect the time series only for the tumor_vol variable. Well ignore all the other outcome variables. The outputthe final goal for nowis to create a plot of the data. Eventually, well run a one-way related measures ANOVA analysis to test whether time has an effect on tumor growth (it does, by the bloody obvious test). To get there well read in all but the top row of the first sheet of the excel file, then simplify by selecting only the variables that we want from the Jaxwest2 data. We want a long format data frame where every column represents a unique variable. It will have 1) a numeric tumor volume variable, and 2) a day of measurement variable as a factor, and 3) a variable for the mouse ID also as a factor, and will have data corresponding to only one treatment group (Control (no vehicle)). 34.3 Step 1: Read the data into R Well read in all but the first header row. The function read_excel is from the readxl package, which is part of the tidyverse but you may need to install the package separately. Do so now. The script below creates the object jw2, which is a data frame of 103 variables. Look very carefully at those arguments in the read_excel function. They explain how, except for the first header row, jw2 contains all of the data in the first sheet of the source file. Skipping a useless, incomplete row helps us immensely. And R is stupid, we have tell it explicitly what sheet to read. Note that jaxwest2.xls is otherwise untouched. No changes have been made locally to the original source file. Thats important because it is good reproducible practice. jw2 &lt;-&quot;datasets/jaxwest2.xls&quot; %&gt;% read_excel( skip=1, sheet=1 ) # remove whitespace names(jw2) &lt;- str_remove_all(names(jw2),&quot; &quot;) Note: Here is an optional method to read in the data. In two parts. This forces the bulk of the numeric data to be read in as numeric. Youll see in a moment why this matters. right &lt;- read_excel(&quot;datasets/jaxwest2.xls&quot;, skip=1, sheet=1, range=&quot;I2:CY37&quot;, col_types=&quot;numeric&quot;) left &lt;- read_excel(&quot;datasets/jaxwest2.xls&quot;, skip=1, sheet=1, range=&quot;A2:H37&quot;) alt &lt;- bind_cols(left, right) names(alt) &lt;- str_remove_all(names(alt), &quot; &quot;) 34.4 Step 2: Select the variables We slim the jw2 data set considerably using the select function. We dont have to call it a new object, but thats done just to illustrate how much better things are compared to the excel file after two simple steps. We want only the mouse_ID, the test group, and all the columns that correspond to a tumor volume measurement on a given day. We get the latter using the contains function. We want the mouse_ID because the data are repeated measures. Well need it as a grouping variable for both ggplot and ezANOVA. The test group variable will initially serve as a check to know we grabbed the right data. We can omit it later. The function contains is super helpful because the tumor volume variables for each day of measurement have a slightly different name, yet each contain the characters tumor_vol_ as a common stem. Give it a shot. Try to grab a different set of variables using contains. Well create a new object, jw2vol to represent the data only for tumor volumes. Notice how in subsequent chunks jw2vol is modified as we successively munge the data into shape. jw2vol &lt;- jw2 %&gt;% select( mouse_ID, test_group, contains(&quot;tumor_vol_&quot;) ) 34.5 Step 3: Trim the cases As described in the preamble, we only want a subset of the test_groups. We can omit a lot of rows. Looking at jw2vol we can see those happen to be the first 11 cases in the data set. Well slice them out, throwing away the rest. jw2vol &lt;- jw2vol %&gt;% filter( test_group == &quot;Control (no vehicle)&quot; ) 34.6 Step 4: We have a variable problem to fix Now we have a very simple dataframe with 11 rows and 15 columns. Woot! But look carefully at the column headers. There is a big problem. Most of the tumor_vol_ columns are listed as class character variables. They should all be class numeric. Because the tumor_vol variable should be numeric. The only correctly classed numeric variable of them is tumor_vol_19. What happened?? In a word, *&amp;%!ing NA values. NAs caused character coercion: This problem is avoidable by forcing a column type to be numeric on read (see alt above). We didnt do that with jw2. If a column of numeric values contains just one non-numeric character, the read_excel function will classify that column as character variable. Weve trimmed away a lot of rows and columns since the read step, but the original jaxwest file is full of NAs. NA values present two problems to solve: a data munging problem and a scientific problem. And the principles of reproducible data handling demand that we fix both of these problems in R, not in the original data excel data file. Fix the Munge We need to convert every tumor_vol column from class_character to class numeric. Well get an NA warning but that is OK, we expect it. jw2vol &lt;- jw2vol %&gt;% mutate_at(vars(tumor_vol_17:tumor_vol_44), as.numeric) ## Warning: Problem with `mutate()` input `tumor_vol_17`. ## i NAs introduced by coercion ## i Input `tumor_vol_17` is `.Primitive(&quot;as.double&quot;)(tumor_vol_17)`. ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion jw2vol ## # A tibble: 11 x 15 ## mouse_ID test_group tumor_vol_17 tumor_vol_18 tumor_vol_19 tumor_vol_22 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 38 Control (~ 27.2 54.8 77.6 104. ## 2 39 Control (~ 12.6 15.8 24.8 36.5 ## 3 43 Control (~ NA 30.0 54.9 75.6 ## 4 48 Control (~ 7.88 30.2 31.0 53.6 ## 5 51 Control (~ 16.2 25.8 28.6 60.6 ## 6 58 Control (~ 61.5 105. 93.2 136. ## 7 62 Control (~ 20.1 44.1 60.1 70.3 ## 8 63 Control (~ 56.1 93.1 104. 164. ## 9 64 Control (~ 17.1 78.1 79.8 120. ## 10 66 Control (~ 15.1 32.3 35.4 40.3 ## 11 67 Control (~ 21.7 27.0 44.6 68.8 ## # ... with 9 more variables: tumor_vol_24 &lt;dbl&gt;, tumor_vol_26 &lt;dbl&gt;, ## # tumor_vol_29 &lt;dbl&gt;, tumor_vol_31 &lt;dbl&gt;, tumor_vol_33 &lt;dbl&gt;, ## # tumor_vol_36 &lt;dbl&gt;, tumor_vol_38 &lt;dbl&gt;, tumor_vol_40 &lt;dbl&gt;, ## # tumor_vol_44 &lt;dbl&gt; 34.7 Step 5: Impute or Delete Now that we have numeric variables, we can hunt for NA values within and then do something about it. First, where are they? # We couldn&#39;t do is.na on character vectors. # well, we can but it doesn&#39;t work, it would yield false negatives map_df(jw2vol, function(x) sum(is.na(x))) ## # A tibble: 1 x 15 ## mouse_ID test_group tumor_vol_17 tumor_vol_18 tumor_vol_19 tumor_vol_22 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 1 0 0 0 ## # ... with 9 more variables: tumor_vol_24 &lt;int&gt;, tumor_vol_26 &lt;int&gt;, ## # tumor_vol_29 &lt;int&gt;, tumor_vol_31 &lt;int&gt;, tumor_vol_33 &lt;int&gt;, ## # tumor_vol_36 &lt;int&gt;, tumor_vol_38 &lt;int&gt;, tumor_vol_40 &lt;int&gt;, ## # tumor_vol_44 &lt;int&gt; Great! We have only 1 NA. A missing measurement for mouse_ID 43 on day 17. Impute v Delete Think of this as a scientific judgment informed by statistical knowledge. Or vice versa. Leaving the NA value as is will fail the repeated measures statistical analysis. It cant stand. The only deletion option involves deleting not just the NA (which is already deleted, but whatev) along with all other values for mouse_ID 43. That throws away 12 other pieces of information just because 1/13 of the information is missing. Seems extreme. The other option is to impute. Which is to replace the NA value with a number. Imputing one value in this decent sized data set is not going to cause much bias. But what number do we use? One option is to replace the NA with the mean of all other tumor_vols for mouse_ID 43. The other option is to replace using the mean of all other tumor_vols on day 17. In this case, since all later volume measures inflate, the mean of the ID will be biased high. So well use the mean of the grouped variable instead. jw2vol &lt;- jw2vol %&gt;% replace_na( list(tumor_vol_17 = mean(jw2vol$tumor_vol_17, na.rm=T))) 34.8 Step 6: Go long The iteration above is 15 columns wide. Next we use the pivot_longer function to make it long. Go here and carefully look at the examples. What happens in this step is just a copy of what they do for their billboard example. We want a column each for mouse ID, the test group, the day of measurement, and the tumor vol measurement values. The first two of these are done. The pivot_longer function lets us make the latter two. jw2vol &lt;- jw2vol %&gt;% pivot_longer(cols=starts_with(&quot;tumor_vol_&quot;), names_to=&quot;day&quot;, names_prefix = &quot;tumor_vol_&quot;, values_to = &quot;vol&quot;, values_drop_na = TRUE ) jw2vol ## # A tibble: 143 x 4 ## mouse_ID test_group day vol ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 38 Control (no vehicle) 17 27.2 ## 2 38 Control (no vehicle) 18 54.8 ## 3 38 Control (no vehicle) 19 77.6 ## 4 38 Control (no vehicle) 22 104. ## 5 38 Control (no vehicle) 24 89.8 ## 6 38 Control (no vehicle) 26 213. ## 7 38 Control (no vehicle) 29 306. ## 8 38 Control (no vehicle) 31 432. ## 9 38 Control (no vehicle) 33 743. ## 10 38 Control (no vehicle) 36 721. ## # ... with 133 more rows 34.9 Step 7: Convert other variables to factor ANOVA are called factorial analyses. The predictor variable must be a factor. Here, the predictor is day jw2vol &lt;- jw2vol %&gt;% mutate( mouse_ID=as.factor(mouse_ID), test_group=as.factor(test_group) ) jw2vol ## # A tibble: 143 x 4 ## mouse_ID test_group day vol ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 38 Control (no vehicle) 17 27.2 ## 2 38 Control (no vehicle) 18 54.8 ## 3 38 Control (no vehicle) 19 77.6 ## 4 38 Control (no vehicle) 22 104. ## 5 38 Control (no vehicle) 24 89.8 ## 6 38 Control (no vehicle) 26 213. ## 7 38 Control (no vehicle) 29 306. ## 8 38 Control (no vehicle) 31 432. ## 9 38 Control (no vehicle) 33 743. ## 10 38 Control (no vehicle) 36 721. ## # ... with 133 more rows 34.10 Step 8: Plot Repeated measures on subjects is the primary feature of this data set. Within each mouse_ID, every measurement is intrinsically-related to every other measurement. Point-to-point graphing illustrates this. This calls for a spaghetti plot. Heres all the data! Its beautiful. ggplot(jw2vol, aes(as.numeric(day), vol, color=mouse_ID, group=mouse_ID))+ scale_color_viridis(discrete=T)+ geom_point(size=2)+ geom_line()+ xlab(&quot;Day after implant&quot;)+ ylab(&quot;Tumor volume&quot;) 34.11 Step 9: Run the ANOVA See Chapter 29. "],["anovamc.html", "Chapter 35 ANOVA power using Monte Carlo 35.1 Alternatives to Monte Carlo 35.2 What is Monte Carlo 35.3 One-way completely randomized ANOVA Monte Carlo 35.4 One-way related measures ANOVA Monte Carlo 35.5 Directions", " Chapter 35 ANOVA power using Monte Carlo library(ez) library(tidyverse) library(viridis) Monte Carlo simulation is a great tool for statistical design of experiments. Monte Carlo forces us to do several things that, taken together, are characterize as good statistical hygiene. Explicitly define predictor variables and their levels Explicitly consider expected values and variation of the response variable Select scientifically meaningful groups to compare Consider which hypotheses is to be tested in advance Define a minimal scientifically meaningful effect size Establish a sample size a priori Establish the data munging and statistical analysis a priori Establish feasibility of the experiment a priori If we want to do unbiased research (and we should) Monte Carlo simulation helps do exactly that. Design the experiment in advance. Analyze a simulation of the experiment, in advance. Figure out what we are going to do, in advance. Then we go and conduct and analyze the experiment we designed. The primary output of the Monte Carlo script below is experimental power at given sample sizes. We run the script primarily to know the number of independent replicates that are necessary to observe the effect size that we anticipate. But conducting a Monte Carlo offers much more than just sample size. An ability to see experimental outcomes prior to running an experiment is the heartbeat of [turning predictions into strong testable hypotheses]. See Chapter 10. Poking around with simulated data before running an experiment helps to critically evaluate our assumptions. This helps us see what might be missing before investing the time and expense in an experiment. It also helps practice a statistical analysis. There is nothing unethical about p-hacking, harking and data snooping when playing with simulated data. All of tat helps design the analysis in a way that gives the best chance of observing the expected result! Figuring that out before the real data set is in front of us helps avoid ethical gray areas after the real data set is in front of us. 35.1 Alternatives to Monte Carlo Using R we can run the power.anova.testfunction in the base stats package or pwr.anova.test in the pwr package. To produce a sample size output, both of these require an entry for number of groups, type1 error and type2 error tolerance. The former requires estimates for model (between) and residual (within) variance. These can be entered as relative values. The latter requires an entry for Cohens \\(f\\). \\[f=\\sqrt{\\frac{\\eta^2}{1-\\eta^2}}\\] where \\(\\eta^2=\\frac{SS_{model}}{SS_{total}}\\) for a completely randomized factor, whereas for a related measures factor it is \\(\\eta_p^2=\\frac{SS_{model}}{SS_{model}+SS_{residual}}\\) Thats great. But without a lot of experience these can be difficult values to estimate because they are not very intuitive. 35.2 What is Monte Carlo As the name implies, Monte Carlo is based on random chance. Monte Carlo is a simulation tool to mimic random sampling. In a single cycle of a Monte Carlo we simulate a random sample for an experiment, and run a statistical analysis to decide whether the experiment worked or not. This is repeated many times. Hundreds or even thousands of times. What fraction of these repetes worked? That fraction is the experimental power. If the power is too low, we can adjust the experimental parameters and re-run. Or we can re-design the experiment entirely. Once the Monte Carlo simulations yield an acceptable power, we have a sample size for conducting the real life experiment. The process of creating a Monte Carlo script forces us to consider, explicitly, the variables in the experiment. What are their expected values? What effect size do we predict or think will be minimally scientifically significant? How many groups are in the experiment? Do these groups help us answer the question we really want answered? Should we add more groups or fewer groups? Is the experiment under- or over-designed, or even feasible? Is there a better way to answer the question? Spending an hour or two going through this process in silico, optimizing and validating the experimental design, can save months of futile effort doing the wrong experiment in real life. The first series of scripts below are for a one-way completely randomized ANOVA design. After that, a one-way related measures ANOVA Monte Carlo is shown. Their simulations differ in a couple of notable respects. 35.3 One-way completely randomized ANOVA Monte Carlo The Outcome variable is random, continuous, and measured on an equal, or nearly equal interval, scale. The experiment is comprised of \\(k\\ge3\\) groups representing different levels of the Predictor variable. The group variances are equivalent. Every replicate (n) is independent of every other replicate. 35.3.1 Directions Create initial values. Visualize one random sample. Run the Power Simulator Change n and repeat until a suitable power is achieved. 35.3.2 Step 1: Create initial values This is where we predict results. I like making these Monte Carlo projects using a stand-alone initializer section. The whole point of the script is to play around with different initial parameters! Using an initializer section means we change these in only one place. Heres the logic: First, provide an estimate for the expected mean of the outcome variable under basal conditions. Next, predict the fold-to-basal effect sizes for the positive control and also the treatment were planning to test. Next, estimate the value of the standard deviation of the Outcome variable. Finally, select the number of replicates per group (n per k) that we might run. b = 100 #expected basal outcome value a = 1.5 #expected fold-to-basal effect of our positive control f = 1.25 #minimal scientifically relevant fold-to-basal effect of our treatment sd = 25 #expected standard deviation of Outcome variable n = 3 # number of independent replicates per group sims = 100 #number of Monte Carlo simulations to run. 35.3.3 Step 2: Visualize one random sample This produces one graph illustrating an example of the type of random samples were generating in the Monte Carlo. No two samples will be the same, because theyre each random! So this figure will change every time it is run, even with the same initial parameter values. And because variances can differ a lot between groups with small sample sizes (n&lt;35). # Use this script to simulate our custom sample. CRdataMaker &lt;- function(n, b, a, f, sd) { a1 &lt;- rnorm(n, b, sd) #basal or negative ctrl a2 &lt;- rnorm(n, (b*a), sd) #positive control or some other treatment a3 &lt;- rnorm(n, (b*f), sd) #treatment effect Outcome &lt;- c(a1, a2, a3) Predictor &lt;- as.factor(c(rep(c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;), each = n))) ID &lt;- as.factor(c(1:length(Predictor))) df &lt;-data.frame(ID, Predictor, Outcome) } dat &lt;- CRdataMaker(n,b,a,f,sd) ggplot(dat, aes(Predictor, Outcome))+ geom_jitter(width=0.1,size = 4, alpha=0.5) Figure 35.1: Representative simulated random sample for a one-way completely randomized ANOVA. Is that what we expect to see? Should there be more variation? Is the effect size larger or smaller than what we anticipate? Should we add more or fewer groups? Edit the code in the initializer to make it as real life as possible. 35.3.4 Step 3: Run The Power Simulator Returns power (as a percentage value) for an experiment conducted given the values of the initializer variables. Change n in the initializer above to see what it takes to get 80% or better power. Carefully look at this code. It creates an ezANOVA output object, and then goes in to collect the p-value of the ANOVA F test from the ANOVA table. pval &lt;- replicate( sims, { sample.df &lt;- CRdataMaker(n, b, a, f, sd) sim.ezaov &lt;- ezANOVA( data = sample.df, wid = ID, dv = Outcome, between = Predictor, type = 2 ) pval &lt;- sim.ezaov$ANOVA[1,5] } ) pwr.pct &lt;- sum(pval&lt;0.05)/sims*100 paste(pwr.pct, sep=&quot;&quot;, &quot;% power. Change &#39;n&#39; in our initializer for higher or lower power.&quot;) ## [1] &quot;37% power. Change &#39;n&#39; in our initializer for higher or lower power.&quot; ggplot(data.frame(pval))+ geom_histogram(aes(pval), color=&quot;#d28e00&quot;)+ labs(x=&quot;p-value&quot;) Figure 35.2: P-value distribution for a one-way repeated measures ANOVA Monte Carlo. This could be modified. For example, here it collects the post hoc comparison between the test group and the negative control. Why is the power so much lower for this comparison compared to the power for a positive ANOVA? pval &lt;- replicate( sims, { sample.df &lt;- CRdataMaker(n, b, a, f, sd) pval &lt;- pairwise.t.test(sample.df$Outcome, sample.df$Predictor)$p.value[2,1] } ) pwr.pct &lt;- sum(pval&lt;0.05)/sims*100 paste(pwr.pct, sep=&quot;&quot;, &quot;% power. Change &#39;n&#39; in our initializer for higher or lower power.&quot;) ## [1] &quot;14% power. Change &#39;n&#39; in our initializer for higher or lower power.&quot; ggplot(data.frame(pval))+ geom_histogram(aes(pval), color=&quot;#d28e00&quot;)+ labs(x=&quot;p-value&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 35.3: P-value distribution for a one-way completely randomized ANOVA Monte Carlo 35.3.5 Step 4: Optimize for suitable power The initializer is preconfigured with a fairly crappy experiment, comprised of a low sample size and not particularly strong effect size. As we can see the power calculation is also pretty crappy. About a third of experiments run in this way are likely to yield positive results. Iteratively increase n until we get a more defensible level of power. Or maybe we need to change the estimates for the effect sizes? 35.3.6 Notes And Considerations At k = 3 this is the minimal ANOVA design (any fewer groups and wed just do a t test). This k = 3 experiment represents a minimal defensible scientific experiment: It has a negative control, a positive control, and the experimental. The example above is also a simple one-way ANOVA completely randomized. Modifying the code to two-way and even three-way ANOVA designs is fairly trivial. we just add more groups. For example, if we have a second factor, Factor B and two levels, estimate the b1 and b2 results in much the same way the a1, a2 and a3 results are estimated. well need to adjust how the data frame is created, adding a column for the levels of Factor B. Further modification will likely be necessary to collect a run of p-values for a specific group comparison. we may wish to grab the p-value for the interaction F test. Alternately, we may wish to go for a key posthoc comparison. 35.4 One-way related measures ANOVA Monte Carlo The script below is a modification of that above, so that one way related measures ANOVA experiments can be simulated. There are two key differences. The first is adding a correlation coefficient to the initializer. The second is in the arguments for the ANOVA test function (from classifying the predictor variable from between to within). The intrinsically-linked measurements that are a hallmark of related measures ANOVA tend to have high inherent correlation. This correlation should be taken into account when simulating related/repeated measure ANOVA experiments. 35.5 Directions Create initial values. Visualize one random sample. Run the Power Simulator Change n and repeat until a suitable power is achieved. 35.5.1 Step 1: Initial values Heres our initializers. Note the added terms r and k. Read Chapter 25 to understand what they represent and how we can estimate them for our system. b &lt;- 100 #expected basal outcome value a &lt;- 1.25 #expected fold-to-basal effect of our positive control f &lt;- 1.4 #minimal scientifically relevant fold-to-basal effect of our treatment sd &lt;- 25 #expected standard deviation of Outcome variable n &lt;- 3 # number of independent replicates per group r &lt;- 0.9 #correlation between groups of outcome values across replicates k &lt;- sqrt(1-r^2) #a conversion factor sims = 100 #number of Monte Carlo simulations to run. Keep it low to start. 35.5.2 Step 2: Visualize one sample Make sure we understand how the a2 and a3 variables were each correlated to the a1 variable. Lets call a1 the pivot variable. When simulating correlated values, our pivot variable should be the lowest expected outcome value in the data set. # Use this script to simulate a sample. RMdataMaker &lt;- function(n, b, a, f, sd, r, k) { a1 &lt;- rnorm(n, b, sd) #basal or negative ctrl a2 &lt;- a1*r+k*rnorm(n, (b*a), sd) #positive control or some other treatment a3 &lt;- a1*r+k*rnorm(n, (b*f), sd) #treatment effect Outcome &lt;- c(a1, a2, a3) Predictor &lt;- as.factor(c(rep(c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;), each = n))) ID &lt;- rep(as.factor(c(1:n))) df &lt;-data.frame(ID, Predictor, Outcome) } dat &lt;- RMdataMaker(n,b,a,f,sd,r,k) ggplot(dat, aes(Predictor, Outcome, color=ID, group=ID) )+ geom_line()+ geom_point(size = 4, shape=5)+ scale_color_viridis(discrete=T)+ theme_classic() Figure 35.4: Representative simulated results for a one-way related measures ANOVA. 35.5.3 Step 3: Run the power simulator Now we can simulate a long run of random samples like above, capturing how many of them yielded a positive result! pval &lt;- replicate( sims, { sample.df &lt;- RMdataMaker(n, b, a, f, sd, r, k) sim.ezaov &lt;- ezANOVA( data = sample.df, wid = ID, dv = Outcome, within = Predictor, type = 2 ) pval &lt;- sim.ezaov$ANOVA[1,5] } ) pwr.pct &lt;- sum(pval&lt;0.05)/sims*100 paste(pwr.pct, sep=&quot;&quot;, &quot;% power. Change &#39;n&#39; in the initializer for higher or lower power.&quot;) ## [1] &quot;99% power. Change &#39;n&#39; in the initializer for higher or lower power.&quot; ggplot(data.frame(pval))+ geom_histogram(aes(pval), color=&quot;#d28e00&quot;)+ labs(x=&quot;p-value&quot;, title=&quot;p-value distribution&quot;) Figure 35.5: P-value distribution for a one-way repeated measures ANOVA Monte Carlo 35.5.4 Step 4: Should anything be changed? The experiment simulated above has an n=3 and power of 99%! Notice how it has the same estimated group values as for the completely randomized simulation above, which yields only ~30% power. Thats the efficiency of related measures! But we might want to experiment with changing correlation coefficient and sample size to see how the results are affected. As for the completely randomized scripts, this one can be easily extended by adding more groups, or even more factors. well need to make changes in all three script stages to customize our needs. "],["manova.html", "Chapter 36 Multivariate Analysis of Variance (MANOVA) 36.1 What does MANOVA test? 36.2 Types of experiments 36.3 MANOVA is not many ANOVAs 36.4 Assumptions 36.5 Calculating variation 36.6 Drug clearance example 36.7 Planning MANOVA experiments 36.8 Summary", " Chapter 36 Multivariate Analysis of Variance (MANOVA) library(tidyverse) library(car) library(broom) library(mvtnorm) library(datapasta) MANOVA is a procedure to analyze experimental data involving simultaneous measurements of two or more dependent variables in response to two or more predictor groups. The basic MANOVA designs are no different than the various t-test or ANOVA designs. Paired, unpaired, one-way (one-factor), two-way (two-factor) and even three-way (three-factor) (or more way) MANOVA experiments are possible. They can be structured either as independent (completely randomized) or intrinsically-linked (related/repeated measures) or a mixture of the two. What differs is MANOVA designs collect measurements for more than one dependent variable from each replicate. The statistical jargon for such experiments is they are multivariate, whereas ANOVA is univariate. This term is owed to a mathematical feature of underlying MANOVA which involves calculating linear combinations of these dependent variables to uncover latent variates. The statistical test is actually performed on these latent variates. Statistical jargon is very confusing most of the time. When we have experiments that involve more than one treatment variable (each at many levels) and only one outcome variable, such as a two-way ANOVA, we call these multivariable experiments. But they are also known as univariate tests because there is only one dependent variable. When we have experiments that have one or more treatment variables and multiple outcome variables, such as one- or two-way MANOVA, we call these multivariate tests. Im sorry. 36.1 What does MANOVA test? Exactly what is that null hypothesis? Like ANOVA, MANOVA experiments involve groups of factorial predictor variables (it is possible to run MANOVA on only two groups). The null hypothesis addresses whether there are any differences between groups of means. As in ANOVA, this is accomplished by partitioning variance. Therefore, as for ANOVA, the test is whether the variance in the MANOVA model exceeds the residual variance. However, in MANOVA this operates on the basis of statistical parameters for a composite dependent variable, which is calculated from the array of dependent variables in the experiment. Youll see that the MANOVA test output per se generates an F statistic. In that regard the inferential procedure is very much like ANOVA. If the F value is extreme, the model variance exceeds the residual variance, and it has a low p-value. A p-value falling below a preset type1 error threshold favors a rejection of null hypotheses. However, this is a funny F statistic. First it is an approximated F, which is mapped to the F test statistic distribution. MANOVA data sets have one column for each dependent variable, which creates a matrix when there are multiple dependent variables. The MANOVA test statistic operates on a matrix parameter called the eigenvalue. Eigenvalues are scaling factors for eigenvectors, which are also matrix parameters. Eigenvectors are ways of reducing the complexity of matrix data. Eigenvectors represent latent variates within these multivariate datasets. Note the plural. There can be more than one eigenvector in a multivariate dataset, the number for which is related to the number of dependent and independent variables. There are four different MANOVA test statistics: Wilks Lambda, Hotelling-Lawley Trace, Pillais Trace, and Roys Largest Root. Each are calculated differently. Given the same input data, each test statistic is then used to generate an F statistic. Like a lot of things in statistics, they represent different ways to achieve the same objective, but generate different results in the process. Their eponymous inventors believe theirs offers a better mousetrap. Choosing which of the MANOVA test statistics to use for inference is not unlike choosing which multiple correction method to use following ANOVA. It can be confusing. It is not uncommon for the test statistics to yield conflicting outcomes. Therefore, it is imperative for the test statistic to be chosen in advance. 36.2 Types of experiments We imagine a gene expression study that measures the transcripts for several different genes simultaneously within every replicate. Each transcript represents a unique dependent variable since they come from different genes, each with its own network of transcription factors. But we can also imagine underlying latent relationships between some of the genes. For example, some of the genes may be regulated by a common transcript. We might be interested in how different genes are expressed over time and in the absence or presence of certain stimuli. Time and stimuli are each predictor variables. The sample source informs whether the design is completely randomized or related/repeated measure. Or we are interested in quantifying several different proteins simultaneously by western blot technique. We might be interested in testing how different groups of stimuli or certain mutations affects each of their levels. Or we are able to measure multiple proteins simultaneously in cells using differentially colored fluorescent tags. Each protein (each color) is a unique dependent variable. Or weve placed a set of animals through some protocol comparing three or more groups of treatments. The animals are run through some behavioral assay, such as a latency test. Afterwards, specimens are collected for bloodwork, and also for quantitative histochemical and gene expression analysis. The latency test, the various measures from the blood, and the markers and genes assayed post -mortum are each a unique dependent variable. There might be a dozen or more of these outcome variables measured in each replicate! 36.3 MANOVA is not many ANOVAs In one respect, you can think of experiments involving multiple dependent variables as running a bunch of ANOVA experiments. In fact, thats the mistake most researchers make. They treat each dependent variable as if it came from a distinct experiment involving separate replicates, when in fact they all come from just one experiment. They subsequently run a series of ANOVAs in parallel, one each for all of the dependent variables. The most common mistake is holding each ANOVA at the standard 5% error threshold, following each with multiple post hoc comparisons. The family-wise error rate (FWER) for this single experiment explodes. A cacophony of undeserved asterisks are splattered all over the chart. It doesnt help that the data are plotted in a way to make the dependent variables look like they are predictor variables. Maybe they have been treated as predictors?! What a mess! Minimally, MANOVA provides a handy way to manage the FWER when probing several different dependent variables simultaneously. As an omnibus test MANOVA provides fairly strong type1 error protection in cases where many dependent variables are assessed simultaneously. But it is important to recall what was mentioned above. MANOVA is not really testing for the signal-to-noise for the effects of the independent variables on each of the dependent variables. MANOVA tests whether there are treatment effects on a combination of the outcome variables. The advantage is that this is performed in a way that maximizes the treatment group differences. 36.3.1 Why MANOVA? For experiments that measure multiple dependent variables the main alternative to MANOVA is to run separate ANOVAs, each with a multiple comparison correction to limit the FWER. For example, if there are five dependent variables, use the Bonferroni correction to run the ANOVA for each under at type1 error threshold of 1%, meaning we only reject the null when the F p-value &lt; 0.01. There are good reasons to run MANOVA instead. The first is as an omnibus test offering protection against inflated type1 error, as when running a series of ANOVAs for each dependent variable. Particularly when the latter are uncorrected for multiple comparison. The second is testing more dependent variables increases the chances of identifying treatment effects. For example, a given stimulus condition may not affect the expression of three genes, but it does affect that for a fourth. Had the fourth gene not been added to the analysis we might have concluded that stimulus condition is ineffective. Third, because it operates on a composite variable, sometimes MANOVA detects effects that ANOVA misses. Imagine studying the effect of diet on growth. We could measure just the weight (or length) of a critter and be done with it. However, with only a little extra effort measuring both weight and height, we have measures in two dimensions instead of just one. In some cases the effects of diet on weight alone or on height alone may be weak, but stronger when assessed in combination. Select variable to measure judiciously. MANOVA works best when dependent variables are negatively correlated or modestly correlated, and does not work well when they are uncorrelated or strongly positively correlated. 36.4 Assumptions Some of these should look very familiar by now: All replicates are independent of each other (of course, repeated/related measurements of one variable my be collected from a single replicate, but this must be accounted for). Data collection must involve some random process. Here are some proscriptions unique to MANOVA but with univariate congeners: The dependent variables are linearly related. The distribution of residuals are multivariate normal. *The residual variance-covariance matrices of all groups are approximately equal or homogeneous. As a general rule MANOVA is thought to be more sensitive to violations of these latter three assumptions than is ANOVA. Several adjustments can help prevent this: Strive for balanced data (have equal or near-equal sample sizes in each group). Transform variables with outliers or that have nonlinearity to lessen their impact (or establish other a priori rules to deal with them if they arise). *Avoid multicolinearity. 36.4.1 Multicolinearity Multicolinearity occurs in a couple of ways. When one dependent variable is derived from other variables in the set (or if they represent two ways to measure the same response) they may have a very high \\(R^2\\) value. More rarely, dependent variables may be highly correlated naturally. In these cases one of the offending variables should be removed. For example, multicolinearity is likely to happen if including a body mass index (BMI) as a dependent variable along with variables for height and weight, since the BMI is calculated from the others. Multicolinearity collapse will also occur when the number of replicates are fewer than the number of dependent variables being assessed. Dont skimp on replicates and avoid the urge to be too ambitious in terms of the numbers of dependent variables collected and the number of treatment groups. As a general rule, if the dataset rows are replicates and the columns are dependent variables, we want the dataset to be longer than it is wide. 36.5 Calculating variation There are many parallels between ANOVA and MANOVA. Both are based upon the general linear model \\[Y=\\beta X+\\epsilon\\] However, for MANOVA, \\(Y\\) is an \\(n \\times m\\) matrix of dependent variables, \\(X\\) is an \\(n \\times p\\) matrix of predictor variables, \\(\\beta\\) is an \\(p \\times m\\) matrix of regression coefficients and \\(\\epsilon\\) is a \\(n \\times m\\) matrix of residuals. Least squares regression for calculating the \\(SS\\) for each dependent variable is performed in MANOVA as for ANOVA. In addition, variation is also tabulated from cross products between all possible combinations of dependent variables. As for ANOVA, the conservation of variation law applies for cross products just as it does for \\(SS\\), \\[CP_{total}=CP_{model}\\ +CP_{residual}\\] For illustration, consider the simplest MANOVA experiment with only two dependent variables (\\(dv1, dv2\\)). The cross product for total variation is: \\[CP_{total}= \\sum_{i=1}^n(y_{i,dv1}-\\bar y_{grand, dv1})(y_{i,dv2}-\\bar y_{grand, dv2}) \\] The cross product for variation associated with the model (group means) is: \\[CP_{model}= \\sum_{j=1}^kn\\times(\\bar y_{group_j,dv1}-\\bar y_{grand, dv1})(\\bar y_{group_j,dv2}-\\bar y_{grand, dv2}) \\] And the cross product, \\(CP\\), for residual variation is: \\[CP_{residual}= \\sum_{i=1}^n(y_{i,dv1}-\\bar y_{group_j, dv1})(y_{i,dv2}-\\bar y_{group_j, dv2}) \\] For partitioning of the overall variation, these cross products, along with their related \\(SS\\) are assembled into \\(T\\), \\(H\\) and \\(E\\) matrices. These letters reflect a historical MANOVA jargon representing total, hypothesis and error variation. These correspond to the total, model and residual terms weve adopted in this course for discussing ANOVA. \\[T = \\begin{pmatrix} SS_{total,dv1} &amp; CP_{total} \\\\ CP_{total} &amp; SS_{total,dv2} \\end{pmatrix}\\] \\[H = \\begin{pmatrix} SS_{model,dv1} &amp; CP_{model} \\\\ CP_{model} &amp; SS_{model,dv2} \\end{pmatrix}\\] \\[E = \\begin{pmatrix} SS_{residual,dv1} &amp; CP_{residual} \\\\ CP_{residual} &amp; SS_{residual,dv2} \\end{pmatrix}\\] The most important take away is that MANOVA not only accounts for the variation within each dependent variable via \\(SS\\) in the usual way, the \\(CP\\) computes the variation associated with all possible relationships between each of the dependent variables. Note: When experiments have even more dependent variables, there is more variation to track. For example an experiment with three independent variables has a T matrix of 9 cells with 3 unique cross-product values, each duplicated: \\[T = \\begin{pmatrix} SS_{total,dv1} &amp; CP_{total,dv1\\times dv2} &amp; CP_{total,dv1\\times dv3} \\\\ CP_{total, dv2\\times dv1} &amp; SS_{total,dv2} &amp; CP_{total,dv2\\times dv3}\\\\ CP_{total,dv3\\times d1} &amp; CP_{total,dv3\\times dv2} &amp; SS_{total, dv3} \\end{pmatrix}\\] The conservation of variation rule applies to these matrices just as in univariate ANOVA. The total variation is equal to the sum of the model and the residual variation. The same applies in MANOVA, which is expressed by the simple matrix algebraic relationship: \\(T=H+E\\). 36.5.1 Eigenvectors and eigenvalues To deal with this mathematically all of the computations necessary to sort out whether anything is meaningful involve matrix algebra, where the focus is upon decomposing these matrices into their eigenvectors and eigenvalues. What is an eigenvector? The best way Ive been able to answer this is an eigenvector represents a base dimension in multivariate data, and the eigenvalues serve as the magnitude of that dimension. MANOVA datasets have multiple response variables, \\(p\\), and also multiple predictor groups, \\(k\\). How many dimensions can these datasets possess? They will have either \\(p\\) or \\(k-1\\) dimensions, whichever is smaller. The mathematics of these are beyond where I want to go on this topic. If interested in learning more the Kahn Academy has a nice primer. Here is a good place to start for an introduction to R applications. Heres a graphical approach that explains this further. It is not necessary to fully understand these fundamentals of matrix algebra in order to operate MANOVA for experimental data. However, it is worth understanding that the MANOVA test statistics operate on something that represents a dimension of the original data set. 36.5.2 MANOVA test statistics Recall in ANOVA the F statistic is derived from the ratio of the model variance with \\(df1\\) degrees of freedom to residual variance with \\(df2\\) degrees of freedom. In MANOVA these variances are essentially replaced by matrix determinants. The congeners to ANOVAs model and residual variances in MANOVA are the hypothesis \\(H\\) and error \\(E\\) matrices, which have \\(h\\) and \\(e\\) degrees of freedom, respectively. There are \\(p\\) dependent variables. Let the eigenvalues for the matrices \\(HE^{-1}\\), \\(H(H+E)^{-1}\\), and \\(E(H+E)^{-1}\\) be \\(\\phi\\), \\(\\theta\\), and \\(\\lambda\\), respectively. From these lets catch a glimpse of the four test statistic options available to the researcher when using MANOVA. When we read a MANOVA table in R the test stat column will have values calculated from the parameters below. 36.5.2.1 Pillai \\[V^{(s)}=\\sum_{j=1}^s\\theta_j \\] where \\(s=min(p,h)\\) $V^{(s)} is then used in the calculation of an adjusted F statistic, from which p-values are derived. Calculation not shown^. 36.5.2.2 Wilks \\[\\Lambda = \\prod_{j=1}^p(1-\\theta_j) \\] \\(\\Lambda\\) is then used to calculate an adjusted F statistic, from which p-values are derived. Calculation not shown. 36.5.2.3 Hotelling-Lawley \\[T_g^s = e\\sum_{j=1}^s\\theta_j \\] where \\(g=\\frac{ph-2}{2}\\) \\(T_g^s\\) is then used to calculate an adjusted F statistic, from which p-values are derived. Calculation not shown. 36.5.2.4 Roy \\[F_{(2v_1+2, 2v_2+2)}=\\frac{2v_1+2}{2v_2+2}\\phi_{max} \\] where \\(v1=(|p-h|-1)/2\\) and \\(v2=(|e-p|-1)/2\\) Unlike the other MANOVA test statistics, Roys greatest root (\\(\\phi_max\\)) is used in a fairly direct calculation of an adjusted F statistic, so that is shown here. Note how the degrees of freedom are calculated. One purpose of showing these test statistics is to illustrate that each are calculated differently. Why? The statisticians who created these worked from different assumptions and objectives, believing their statistic would perform well under certain conditions. A second reason to show these is so the researcher avoids freeze when looking at MANOVA output: OMG! What the heck??! Yes, the numbers in the MANOVA table can be intimidating at first, even if when we have a pretty good idea of what we are up to. There are three columns for degrees of freedom and two for test statistics. Then there is the p-value. And then there is a MANOVA table for each independent variable. And if we dont argue a specific test, we might get all four!! Yikes. 36.5.3 Which MANOVA test statistic is best? Thats actually very difficult to answer. In practice, the researcher should go into a MANOVA analysis with one test in mind, declared in a planning note, ideally based upon some simulation runs. This is no different than running a Monte Carlo before a t-test experiment, or an ANOVA experiment. Knowing in advance how to conduct the statistical analysis is always the most unbiased approach. Otherwise, the temptation will be to produce output with all four tests and choose the one that yields the lowest p-value. In most instances the four tests should lead to the same conclusion, though they will not always generate the same p-values. The most commonly used test seems to be Wilks lambda. The Pillai test is held to be more robust against violations of testing conditions assumptions listed above and is a reasonable choice. 36.6 Drug clearance example Mimche and colleagues developed a mouse model to test how drug metabolism is influenced by a malaria-like infection (Plasmodium chaubadi chaubadi AS, or PccAS). On the day of peak parisitaemia an experiment was performed to derive values for clearance (in units of volume/time) of four drugs. Clearance values are derived from a regression procedure on data for repeated measurements of drug levels in plasma at various time points following an injection. Any factor causing lower clearance indicates drug metabolism is reduced. Thus, the clearance values for each drug are the dependent variables in the study. All four drugs were assessed simultaneously within each replicate. The independent variable is treatment, which is at two levels, naive or infection with PccAS, a murine plasmodium parasite. Ten mice were randomly assigned to either of these two treatments. 36.6.1 Rationale for MANOVA The overarching scientific hypothesis is that parasite infection alters drug metabolism. The drug panel tested here surveys a range of drug metabolizing enzymes. Limiting the clearance study to only a single drug would preclude an opportunity to gain insight into the broader group of drug metabolizing systems that might be affected by malaria. Thus, there is good scientific reason to test multiple dependent variables. Together, they answer one question: Is drug metabolism influenced by the parasite? Clearance values are also collected in one experiment in which all four drugs are injected as a cocktail and measured simultaneously. This cannot be treated as four separate experiments statistically. The main reason to use MANOVA in this case is as an omnibus test to protect type1 error while considering all of the information for the experiment simultaneously. 36.6.2 Data structure Read in the data. Note that each dependent variable is its own column, just as for the predictor variable. An ID variable is good data hygiene. pkdata &lt;- read.csv(&quot;datasets/pkdata.csv&quot;) pkdata ## id treatment caffeine tolbutamide buproprion midazolam ## 1 M1 naÃ¯ve 0.08929834 0.008239704 1.2445632 0.005845354 ## 2 M2 naÃ¯ve 0.07622691 0.010257611 0.6566237 0.004149744 ## 3 M3 naÃ¯ve 0.23075655 0.010771581 0.9107329 0.001229257 ## 4 M4 naÃ¯ve 0.06696628 0.013519425 0.8779273 0.005178790 ## 5 M5 naÃ¯ve 0.08224076 0.008576711 0.9820787 0.003787842 ## 6 I1 pccas 0.03732863 0.007392392 0.2951946 0.001619670 ## 7 I2 pccas 0.04832825 0.002130560 0.4369309 0.002034126 ## 8 I3 pccas 0.02322334 0.000701000 0.1728363 0.000431000 ## 9 I4 pccas 0.02793257 0.004093938 0.2421983 0.001170104 ## 10 I5 pccas 0.04521240 0.006612965 0.2541623 0.001632887 36.6.3 MANOVA procedure The Manova function in the car package can take several types of models as an argument. In this case, we have a two group treatment variable so a linear model is defined using the base R lm. Recall the linear model, \\[Y=\\beta_o+\\beta_1X+\\epsilon\\] Here we have one model for each of the four dependent variables. In this experiment we have four \\(Y\\) variables, each corresponding to one of the drugs. The same \\(X\\) variable (at two levels) applies to all four dependent variables. \\(\\beta_0\\) is the y-intercept, and \\(\\beta_1\\) is the coefficient. Each dependent variable will have an intercept and a coefficient. \\(\\epsilon\\) is the residual error, accounting for the variation in the data unexplained by the model parameters for that dependent variable. By including each of the dependent variable names as arguments within a cbind function we effectively instruct lm to treat these columns as a matrix and run a linear model on each. Here is the omnibus test. The Pillai test statistic is chosen because it is more robust than the others to violations of the uniform variance assumptions: mod &lt;- lm(cbind(caffeine, tolbutamide, buproprion, midazolam) ~ treatment, pkdata) Manova(mod, test.statistic = &quot;Pillai&quot;) ## ## Type II MANOVA Tests: Pillai test statistic ## Df test stat approx F num Df den Df Pr(&gt;F) ## treatment 1 0.90525 11.943 4 5 0.009017 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The approximated F statistic of 11.943 is extreme on a null distribution of 4 and 5 degrees of freedom with a p-value of 0.009017. Reject the null hypothesis that the variance associated with the combination of linear models is less than or equal to the combined residual error variance. Since this is a linear model for independent group means this result indicates that there are difference between group means. Additional information about the Manova test can be obtained using the summary function. Note the matrices for the error and hypothesis, which are as described above. Also not the output for the four different test statistics. In this simple case of one independent variable with two groups, they all produce an identical approximate F and the p-values that flows from it. The test values do differ, however, although the Hotelling-Lawley and the Roy generate the same values. More complex experimental designs tend to shatter this uniform agreement between the tests. summary(Manova(mod)) ## ## Type II MANOVA Tests: ## ## Sum of squares and products for error: ## caffeine tolbutamide buproprion midazolam ## caffeine 1.923586e-02 5.674854e-05 0.0045996194 -3.979750e-04 ## tolbutamide 5.674854e-05 5.034943e-05 -0.0008619943 2.415136e-06 ## buproprion 4.599619e-03 -8.619943e-04 0.2178219304 7.290553e-04 ## midazolam -3.979750e-04 2.415136e-06 0.0007290553 1.402593e-05 ## ## ------------------------------------------ ## ## Term: treatment ## ## Sum of squares and products for the hypothesis: ## caffeine tolbutamide buproprion midazolam ## caffeine 0.013210583 1.106172e-03 0.118874544 4.835230e-04 ## tolbutamide 0.001106172 9.262391e-05 0.009953812 4.048719e-05 ## buproprion 0.118874544 9.953812e-03 1.069684582 4.350949e-03 ## midazolam 0.000483523 4.048719e-05 0.004350949 1.769751e-05 ## ## Multivariate Tests: treatment ## Df test stat approx F num Df den Df Pr(&gt;F) ## Pillai 1 0.905253 11.94302 4 5 0.0090167 ** ## Wilks 1 0.094747 11.94302 4 5 0.0090167 ** ## Hotelling-Lawley 1 9.554419 11.94302 4 5 0.0090167 ** ## Roy 1 9.554419 11.94302 4 5 0.0090167 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 36.6.3.1 Now what? Here are the data visualized. The red lines connect group means. This is presented as a way to illustrate the linear model output. ggplot(pkdata %&gt;% pivot_longer(cols=caffeine:midazolam, names_to =&quot;drug&quot;, values_to=&quot;clearance&quot;), aes(treatment, clearance))+ geom_jitter(width=0.1)+ stat_summary(fun.y=mean, colour=&quot;red&quot;, geom=&quot;line&quot;, aes(group = 1))+ facet_wrap(~drug, scales=&quot;free&quot;) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function ## Warning: Computation failed in `stat_summary()`: ## Can&#39;t convert a double vector to function Figure 36.1: Drug clearance in a mouse model for malaria. Print the linear model results to view the effect sizes as regression coefficients: mod ## ## Call: ## lm(formula = cbind(caffeine, tolbutamide, buproprion, midazolam) ~ ## treatment, data = pkdata) ## ## Coefficients: ## caffeine tolbutamide buproprion midazolam ## (Intercept) 0.109098 0.010273 0.934385 0.004038 ## treatmentpccas -0.072693 -0.006087 -0.654121 -0.002661 Considering the graph above it should be clear what the intercept values represent. They are the means for the naive treatment groups. The treatmentpccas values are the differences between the naive and pccas group means. Their values are equivalent to the slopes of the red lines in the graph. In other words, the treatmentpccas values are the means of the pccas groups subtracted from the means of the naive groups. Intercept and treatmentpccas correspond respectively to the general linear model parameters \\(\\beta_0\\) and \\(\\beta_1\\). Another way to think about this is the intercept (\\(\\beta_0\\)) is an estimate for the value of the dependent variable when the independent variable is without any effect, or \\(X=0\\). In this example, if pccas has no influence on metabolism, then drug clearance is simply \\(Y= \\beta_0\\) In regression the intercept can cause confusion. In part, this happens because R operates alphanumerically by default. It will always subtract the second group from the first. Scientifically, we want the pccas means to be subtracted from naive.because our hypothesis is that malaria will lower clearance. Fortunately, n comes before p in the alphabet, so the group means for p will be subtracted from group means for n and this happens automagically. For simplicity, it is always a good idea to name your variables with this default alphanumeric behavior in mind. 36.6.3.2 MANOVA post hoc In this the data passes and omnibus MANOVA. It tells us that somewhere among the four dependent variables there are differences between the naive and pccas group means. The question is which ones? As a follow on procedure we want to know which of the downward slopes are truly negative, and which are no different than zero. We answer that in the summary of the linear model. There is a lot of information here, but two things are most important. First, the estimate values for the treatmentpccas term. These are differences in group means between naive and pccas. Second, statistics and p-values associated with the treatmentpccas effect. These are a one-sample t-test. These each ask, Is the Estimate value, given its standard error, equal to zero? For example, the estimate for caffeine is -0.07269, which is the slope of the red line above. The t-statistic value is -2.344 and it has a p-value just under 0.05. Were usually not interested in the information on the intercept term or the statistical test for it. It is the group mean for the control condition, in this case, demonstrating the background level of clearance. Whether that differs from zero not important to know. tidy(mod) ## # A tibble: 8 x 6 ## response term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 caffeine (Intercept) 0.109 0.0219 4.97 0.00109 ## 2 caffeine treatmentpccas -0.0727 0.0310 -2.34 0.0471 ## 3 tolbutamide (Intercept) 0.0103 0.00112 9.16 0.0000163 ## 4 tolbutamide treatmentpccas -0.00609 0.00159 -3.84 0.00497 ## 5 buproprion (Intercept) 0.934 0.0738 12.7 0.00000142 ## 6 buproprion treatmentpccas -0.654 0.104 -6.27 0.000241 ## 7 midazolam (Intercept) 0.00404 0.000592 6.82 0.000135 ## 8 midazolam treatmentpccas -0.00266 0.000837 -3.18 0.0131 Therefore to make the p-value adjustment we should only be interested in the estimates for the treatmentpccas terms. We shouldnt waste precious alpha on the intercept values. Heres how we do that: tidy(mod) %&gt;% select(response, estimate, p.value) %&gt;% filter(row_number() %% 2 == 0) %&gt;% mutate(p.value_adjusted=p.adjust(p.value, method=&quot;bonferroni&quot;)) ## # A tibble: 4 x 4 ## response estimate p.value p.value_adjusted ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 caffeine -0.0727 0.0471 0.188 ## 2 tolbutamide -0.00609 0.00497 0.0199 ## 3 buproprion -0.654 0.000241 0.000964 ## 4 midazolam -0.00266 0.0131 0.0522 Given a family-wise type1 error threshold of 5%, we can conclude that PccAS infection changes tolbutamide and buproprion clearance relative to naive animals. There is no statistical difference for caffeine and midazolam clearance between PccAS-infected and naive animals. It should be noted that this is a very conservative adjustment. In fact, were the Holm adjustment used instead all four drugs would have shown statistical differences. 36.6.3.3 Write up The drug clearance experiment evaluates the effect of PccAS infection on the metabolism of caffeine, tolbutamide, buproprion and midazolam simultaneously in two treatment groups, naive and PccAS-infected, with five independent replicates within each group. Clearance data were analyzed by a multiple linear model using MANOVA as an omnibus test (Pillai test statistic = 0.905 with 1 df, F(4,5)=11.943, p=0.009). Posthoc analysis usig t-tests indicates PccAS the reductions in clearance relative to naive for tolbutamide (-0.006 clearance units) and buproprion (-0.654 clearance units) are statistically different than zero (Bonferroni adjusted p-values are 0.0199 and 0.0009, respectively). 36.7 Planning MANOVA experiments Collecting several types of measurements from every replicate in one experiment is a lot of work. In these cases, front end statistical planning can really pay off. We need to put some thought into choosing the dependent variables. MANOVA doesnt work well when too many dependent variables in the dataset are too highly correlated and all are pointing in the same general direction. For example, when several different mRNAs increase in the same way in response to a treatment. They will be statistically redundant, which can cause computational difficulties and the regression fails to converge to a solution. The only recovery from that is to add the variables back to a model one at a time until the offending variable is found. Which invariably causes the creepy bias feeling. Besides, it seems wasteful to measure the same thing many different ways. Therefore, omit some redundant variables. Be sure to offset positively correlated variables with negatively correlated variables in the dataset. Similarly, MANOVA uncorrelated dependent variables should be avoided. Use pilot studies, other information, or intuition to understand relationships between the variables. How are they correlated? Once we have that we can calculate covariances? 36.7.1 MANOVA Monte Carlo Power Analysis The Monte Carlo procedure is the same as for any other test: 1)Simulate a set of expected data. 2)Run an MANOVA analysis, defining and counting hits. 3)Repeat many times to get a long run performance average. 4)Change conditions (sample size, add/remove variables, remodel, etc) 5)Iterate through step 1 again. 36.7.1.1 Design an MVME Lets simulate a minimally viable MANOVA experiment (MVME) involving three dependent variables. For each there are a negative and positive control and a test group, for a total of 3 treatment groups. For all three dependent variables we have decent pilot data for the negative and positive controls. We have a general idea of what we would consider minimally meaningful scientific responses for the test group. One of our dependent variables represents a molecular measurement (ie, an mRNA), the second DV represents a biochemical measurement (ie, an enzyme activity), and the third DV represents a measurement at the organism level (ie, a behavior). Well make the organism measurement decrease with treatment so that it is negatively correlated with the other two. We must assume each of these dependent variables in normally distributed, \\(N(\\mu, \\sigma^2)\\). We have a decent idea of the values of these parameters. We have a decent idea of how correlated they may be. All three of the measurements are taken from each replicate. Replicates are randomized to receiving one and only one of the three treatment groups. The model is essentially one-way completely randomized MANOVA; in other words, one factorial independent variable at three levels, with three dependent variables. 36.7.1.2 Simulate multivariate normal data The rmvtnorm function in the mvtnorm package provides a nice way to simulate several vectors of correlated data simultaneously, the kind of data youd expect to generate and then analyze with MANOVA. But it takes a willingness to wrestle with effect size predictions, correlations, variances and covariances, and matrices to take full advantage. rmvtnorm takes as arguments the expected means of each variable, and sigma, which is a covariance matrix. First, estimate some expected means and then their standard deviations. Then simply square the latter to calculate variances. To predict standard deviations, it is easiest to think in terms of coefficient of variation: What is the noise to signal for the measurement? What is the typical ratio of standard deviation to mean that you see for that variable? 10%, 20%, more, less? For this example, were assuming homoscedasticity, which means that the variance is about the same no matter the treatment level. But when we know there will be heterscedasticity, we should code it in here. Table 36.1: Estimated statistical parameters for an experiment with three treatment groups and three dependent variables. treat param molec bioch organ neg mean 270 900 0 neg sd 60 100 5 neg var 3600 10000 25 pos mean 400 300 -20 pos sd 60 100 5 pos var 3600 10000 25 test mean 350 450 -10 test sd 60 100 5 test var 3600 10000 25 36.7.1.2.1 Building a covariance matrix For the next step we estimate the correlation coefficients between the dependent variables. We predict the biochem measurements and the organ will be reduced by the positive control and test treatments. That means they will be negatively correlated with the molec variables, where the positive and test are predicted to increase the response. Where do these correlation coefficients come from? They are estimates based upon pilot data. But they can be from some other source. Or they can be based upon scientific intuition. Estimating these coefficients is no different than estimating means, standard deviations and effect sizes. We use imagination and scientific judgment to make these predictions. We pop them into a matrix here. Well need this in a matrix form to create our covariance matrix. A &lt;- matrix(c(1, -0.8, -0.3, -0.8, 1, 0.6, -0.3, 0.6, 1 ), ncol=3, byrow=F, dimnames=list(c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;), c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;) ) ) A ## molec biochem organ ## molec 1.0 -0.8 -0.3 ## biochem -0.8 1.0 0.6 ## organ -0.3 0.6 1.0 Next we can build out a variance product matrix, using the variances estimated in the table above. The value in each cell in this matrix is the square root of the product of the corresponding variance estimates (\\(\\sqrt {var(Y_1)var(Y_2)}\\). We do this for every combination of variances, which should yield a symmetrical matrix. B &lt;- matrix(c(3600,6000,300,6000,10000,500,300,500,25), ncol=3, byrow=F, dimnames=list(c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;), c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;) )) B ## molec biochem organ ## molec 3600 6000 300 ## biochem 6000 10000 500 ## organ 300 500 25 Heres why weve done this. The relationship between any two dependent variables, \\(Y_1\\) and \\(Y_2\\) is \\[cor(Y_1, Y_2) = \\frac{cov(Y_1, Y_2)}{\\sqrt {var(Y_1)var(Y_2)}}]\\] Therefore, the covariance matrix we want for the sigma argument in the rmvnorm function is calculated by multiplying the variance product matrix by the correlation matrix. sigmas &lt;- A*B sigmas ## molec biochem organ ## molec 3600 -4800 -90 ## biochem -4800 10000 300 ## organ -90 300 25 Since we assume homescedasticity we can use sigmas for each of the three treatment groups. Now it is just a matter of simulating the dataset with that one sigma and the means for each of the variables and groups. n &lt;- 5 negMeans &lt;- c(270, 900, 0) posMeans &lt;- c(400, 300, -20) testMeans &lt;- c(350, 450, -10) neg &lt;- rmvnorm(n,mean=negMeans, sigma=sigmas) pos &lt;- rmvnorm(n,mean=posMeans, sigma=sigmas) test &lt;- rmvnorm(n,mean=testMeans, sigma=sigmas) outMat &lt;- rbind(neg, pos, test) Parenthetically, if we are expecting heteroscedasticity, we would need to calculate a different sigma for each of the groups. Pop it into a data frame and summarise by groups to see how well the simulation meets our plans. Keep in mind this is a small random sample. Increase the sample size, \\(n\\), to get a more accurate picture. sim &lt;- data.frame(treatment=c(rep(&quot;neg_ctrl&quot;, n), rep(&quot;pos_ctrl&quot;, n), rep(&quot;test&quot;, n) ), molec = outMat[ , 1], bioch = outMat[ , 2], organ = outMat[ , 3] ) sim %&gt;% group_by(treatment) %&gt;% summarise(meanMol=mean(molec), sdMol=sd(molec), meanBio=mean(bioch), sdBio=sd(bioch), meanOrg=mean(organ), sdOrg=sd(organ)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 7 ## treatment meanMol sdMol meanBio sdBio meanOrg sdOrg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 neg_ctrl 316. 57.0 862. 181. 0.0251 9.50 ## 2 pos_ctrl 418. 62.9 260. 109. -23.8 4.86 ## 3 test 336. 75.8 439. 123. -11.3 6.75 Now we run a the sample through a manova. Were switching to base Rs manova here because it plays nicer than Manova with Monte Carlo model &lt;- lm(cbind(molec, bioch, organ) ~ treatment, data=sim) man &lt;- summary(manova(model)) man ## Df Pillai approx F num Df den Df Pr(&gt;F) ## treatment 2 1.4379 9.3793 6 22 3.719e-05 *** ## Residuals 12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Assuming this Manova test gives a positive response, we follow up with look post hoc at the key comparison, because wed want to power up an experiment to ensure this one is detected. Our scientific strategy is to assure the organ effect of the test group. Because one has nothig without the animal model, right? So out of all the posthoc comparisons we could make, well focus in on that. tidy(model) ## # A tibble: 9 x 6 ## response term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 molec (Intercept) 316. 29.4 10.8 0.000000161 ## 2 molec treatmentpos_ctrl 101. 41.6 2.44 0.0313 ## 3 molec treatmenttest 19.8 41.6 0.476 0.643 ## 4 bioch (Intercept) 862. 63.1 13.7 0.0000000112 ## 5 bioch treatmentpos_ctrl -603. 89.2 -6.76 0.0000203 ## 6 bioch treatmenttest -423. 89.2 -4.75 0.000475 ## 7 organ (Intercept) 0.0251 3.26 0.00768 0.994 ## 8 organ treatmentpos_ctrl -23.8 4.61 -5.16 0.000237 ## 9 organ treatmenttest -11.3 4.61 -2.46 0.0301 The comparison in the 9th row is between the test and the negative control for the organ variable. The estimate means its value is -11.46 units less than the negative control. The unadjusted p-value for that difference is 0.0041. Assuming wed also compare the test to negative controls for the other two variables, weres how we can collect that p-value while adjusting it for multiple comparisons simultaneously. The code below comes from finding a workable way to create test output so the parameter of interest is easy to grab. Unfortunately, output from the Manova function is more difficult to handle. p.adjust(unname(as_vector(tidy(model)[9,6])),method=&quot;bonferroni&quot;, n=3) ## [1] 0.09034928 36.7.2 An MANOVA Monte Carlo Power function The purpose of this function is to determine the sample size of suitable power for an MANOVA experiment. This function pulls all of the above together into a single script. The input of this script is a putative sample size. The output is the power of the MANOVA, and the power of the most important posthoc comparison: that between the test group and negative control for the organ variable. manpwr &lt;- function(n){ #initializers #covariance matrix A &lt;- matrix(c(1, -0.8, -0.3, -0.8, 1, 0.6, -0.3, 0.6, 1 ), ncol=3, byrow=F, dimnames=list(c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;), c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;) )) B &lt;- matrix(c(3600,6000,300, 6000,10000,500, 300,500,25), ncol=3, byrow=F, dimnames=list(c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;), c(&quot;molec&quot;, &quot;biochem&quot;, &quot;organ&quot;) )) sigmas &lt;- A*B #predicted means negMeans &lt;- c(270, 900, 0) posMeans &lt;- c(400, 300, -20) testMeans &lt;- c(350, 450, -10) #misc function running variables manpval &lt;- c() orgpval &lt;- c() i &lt;- 1 ssims &lt;- 100 alpha &lt;- 0.05 #Data simulator repeat { neg &lt;- rmvnorm(n,mean=negMeans, sigma=sigmas) pos &lt;- rmvnorm(n,mean=posMeans, sigma=sigmas) test &lt;- rmvnorm(n,mean=testMeans, sigma=sigmas) outMat &lt;- rbind(neg, pos, test) sim &lt;- data.frame(treatment=c(rep(&quot;neg_ctrl&quot;, n), rep(&quot;pos_ctrl&quot;, n), rep(&quot;test&quot;, n) ), molec = outMat[ , 1], bioch = outMat[ , 2], organ = outMat[ , 3] ) model &lt;- lm(cbind(molec, bioch, organ) ~ treatment, data=sim) manp &lt;- summary(manova(model))$stat[11] manpval[i] &lt;- manp #grab the p-value for the most important posthoc if (manp &lt; alpha) { otp &lt;- p.adjust(unname( as_vector( tidy(model)[9,6])), method=&quot;bonferroni&quot;, n=3) orgpval[i] &lt;- otp } if (i==ssims) break i=i+1 #prepare output manpwer &lt;- length(which(manpval&lt;alpha))/ssims orgpwr &lt;- length(which(orgpval&lt;alpha))/ssims } c(paste(&quot;manova power=&quot;, manpwer, sep=&quot;&quot;), paste(&quot;posthoc power=&quot;, orgpwr, sep=&quot;&quot;)) } Heres how to run manpwr. Its a check to see the power for a sample size of 5 replicates per group. The output is interesting. Although the MANOVA test is overpowered, the critical finding of interest is underpowered at this sample size. Repeat the function at higher values of n until the posthoc power is sufficient. It is crucial to understand that powering up for the posthoc results can be so important. manpwr(5) ## [1] &quot;manova power=0.99&quot; &quot;posthoc power=0.71&quot; 36.8 Summary MANOVA is an option for statistical testing of multivariate experiments. The dependent variables are random normal The test is more senstive than other parametrics to violations of normality and homogeneity of variance. MANOVA tests whether independent variables affect an abstract combination of dependent variables. For most, use MANOVA as an omnibus test followed by post hoc comparisons of interest to control FWER. Care should be taken in selecting the dependent variables of interest. #sessionInfo() "],["correl.html", "Chapter 37 Correlation 37.1 Correlation != Causation 37.2 Correlation in Multivariate Outcomes and Paired Designs 37.3 Correlation coefficients", " Chapter 37 Correlation Something for your pipe: Having high confidence in a non-zero correlation is not the same as concluding a strong correlation exists! library(tidyverse) Use correlation analysis to assess if two measured variables co-vary. If two variables co-vary they are said to be associated. Youll recall discussing the concept of association back when we dealt with discrete data. For example, an association between smoking and cancer could be tested by counting the frequency of cancers within a group of smokers, such as in a case-control design. The main way correlation differs from these previous tests of association is that correlation analysis is conducted using measured rather than discrete (ie, counted) variables. For example, in a cancer study we might attempt to correlate lifetime levels of carcinogen exposure with age of cancer diagnosis. Here, both carcinogen levels and age are measured variables, and correlation analysis would be more appropriate than association analysis. Correlation coefficients are effect sizes that are used as estimators of the correlation parameter, \\(\\rho\\), of the sampled population. Correlation coefficients estimate the strength and the direction of the linear relationship by which two variables co-vary. Covariance estimates the strength of an association between two variables. If two variables have a correlation coefficient of zero they do not co-vary; they are not associated; they are independent. An analog of the one sample t-test can be used to test the null hypothesis that a correlation coefficient is equal to zero. 37.1 Correlation != Causation It is important to recognize that correlation gives no information about causality. The reason is very simple. In a correlation analysis, when neither of the two variables is a true experimentally manipulated predictor variable causality cannot be determined. The two variables in a correlation analysis are each outcome variables (aka dependent variables). Thus, its not possible to know from correlation analysis if brain size causes larger body size, or if larger body size causes larger brain size! Please also pay particular attention to what a t-test on a correlation coefficient tells you. Its not a test of an experimental hypothesis. It is not an inferential or causal hypothesis test. It simply tests whether a non-zero correlation exists between two variables. Correlation is also not regression. Thus, the most proper way to illustrate correlation is to simply show the scatter plots of the data, without superimposing best-fit regression lines. Those lines imply a regression analysis was performed. Regression (and regression lines) should be reserved for when one of the variables is a true predictor variable and the intent is to model the values of the response variable at each level of predictor. 37.2 Correlation in Multivariate Outcomes and Paired Designs Correlation is very important in many practical ways in biomedical research wherein predictor variables are deployed. Its perhaps most important in multivariate experiments. These experiments generate two or more outcome variables from a single sample. These variables tend to be correlated. For example, imagine a study that measures the expression of several different proteins, or several different mRNAs, simultaneously at each level of some stimulus. All of these mRNAs and proteins have an inherent correlation since they arise from identical stimulus conditions within given replicates. Their covariances are accounted for when using multivariate statistical methods (eg, MANOVA) Correlation also creeps into experiments in more subtle but really important ways. For example, given their highly homogeneous nature, seemingly disparate variables measured in cell culture experiments and from laboratory mice often have striking underlying correlation. This correlation can and should be accounted for when planning experiments and analyzing their data. Weve seen a hints of this already in our discussions on paired t-tests and related measures ANOVA analysis. One reason those experimental designs are so efficient is because their analysis takes advantage of the underlying correlation structures within the subjects! 37.3 Correlation coefficients The statistical parameters used to describe the correlation between two variables is the correlation coefficient. The three different correlation coefficients well discuss are Pearsons r, Spearmans r_s and Kendalls tau. Each arrives at roughly the same answer, but in different ways. If you say to me, The correlation between brain and body sizes is 0.489, then I will ask, Is that Pearsons or Kendalls or Spearmans? 37.3.1 Pearsons correlation coefficient The key assumptions are: The sample is random. Each replicate is independent of all other replicates. The measurements are derived from a population that has a bivariate normal distribution. A bivariate normal distribution arises when two variables are each normally distributed. With small sample sizes we cant really know if this latter assumption is met. However, we can safely assume this assumption if both variables have been measured in their linear range. The Pearson correlation coefficient estimates the strength of a linear association between two measured variables. It can be calculated from the paired variables X and Y which take on the values x_i and y_i for i=1 to n pairs: \\[r=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{{\\sum\\limits_{i=1}^n}(x_i-\\bar x)^2}\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar y)^2}}\\] The standard error of \\(r\\) is: \\[SE_r=\\sqrt{\\frac{1-r^2}{n-2}}\\] Covariance is the strength of association between two measured variables and is calculated as follows: \\[Covariance(X,Y)=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{n-1}\\] The magnitude of covariance is difficult to interpret because it depends upon the values of the variables. In contrast, the correlation coefficient can be thought of as a normalized covariance, and its magnitude therefore useful as an index of effect size. The t-test for a correlation coefficient is: \\[t=\\frac{r}{SE_r}\\] Two-sided null hypothesis: \\(\\rho=0\\) One-sided null hypothesis: \\(\\rho\\le0\\) or \\(\\rho\\ge0\\) 37.3.1.0.1 Alternative formulas for Pearsons correlation coefficient \\[r=\\frac{Covariance(X,Y)}{s_x s_y}\\], where \\(s_x\\) and \\(s_y\\) are the sample standard deviations for the variables \\(X\\) and \\(Y\\), respectively. \\[cor(X,Y)=\\frac{cov(X,Y)}{\\sqrt{var(X)var(Y)}}\\] 37.3.2 Spearmans rank correlation The sample is random Each replicate is independent of all other replicates The word rank should alert you that this is a non-parametric method. Spearmans approach to calculating correlation is used when bivariate normality cannot or should not be assumed, or for when one (or both) of the outcome variables is/are ordered data. To calculate Spearmans correlation coefficient, \\(r_s\\), first each of the variables are converted to ranks in the usual way. For example, a measured variable vector comprised of the values \\(2, 4, 5, 5, 12,...\\) is converted to the rank values \\(1, 2, 3.5, 3.5, 5,...\\). Note how ties are handled by conversion to mid-ranks. As you can see below, Spearmans correlation coefficient \\(r_s\\) is calculated in the same way as for Pearsons, except that rank values are used instead. Let the values \\(v\\) and \\(w\\) represent the rank values of the variables \\(X\\) and \\(Y\\), respectively: \\[r_s=\\frac{\\sum\\limits_{i=1}^n(v_i-\\bar v)(w_i-\\bar w)}{\\sqrt{{\\sum\\limits_{i=1}^n}(v_i-\\bar v)^2}\\sqrt{\\sum\\limits_{i=1}^n(w_i-\\bar w)^2}}\\] Given a large sample size, a t-test can be used to test the null hypothesis that \\(r_s\\) differs from zero. \\[t=\\frac{r_s}{SE_{r_s}}\\], where \\(SE_{r_s}=\\sqrt{\\frac{1-r^2_s}{n-2}}\\) Sources differ on what comprises a large sample size. In R, by default the cor.test function for the Spearman method uses a t-test when n&gt;1289. Otherwise, the function generates an S statistic via permutation analysis, from which p-values are calculated. See ?cor.test for more details. 37.3.3 Kendalls tau Kendalls is an alternative to Spearmans and would be used if the bivariate normal assumption cannot be met. In this procedure, the pairs are ordered on the basis of the rank values of the \\(X\\) variable. If \\(X\\) and \\(Y\\) were perfectly correlated, the rank values of the \\(Y\\) variable would be perfectly concordant with the ranks of the \\(X\\) variable. The number of concordant pairs, \\(n_c\\) and discordant pairs, \\(n_d\\) are then counted to derive the correlation coefficient, tau. \\[tau=\\frac{n_c-n_d}{\\frac{1}{2}n(n-1)}\\] As for Spearmans, a posthoc test of null hypothesis that tau differs from zero can be calculated. P-values are calculated using exact tests for small sample sizes, or by standard normal approximation for larger sizes. 37.3.4 Which correlation method to use? Obviously, given a single data set, one is now equipped to derive 3 separate correlation coefficient values using either Pearson, Spearman or Kendall methods. Which of the 3 estimates is right? Use Pearsons if the bivariate normal assumption can be met. Otherwise, choose Spearmans or Kendalls approach. 37.3.5 R correlation analysis functions It is very easy to do correlation analysis in R, which requires two simple functions: cor and cor.test. The former is used to only generate correlation coefficients, whereas the latter will generate these coefficients and also run posthoc significance tests. Below, the random variables x and y are generated to illustrate uncorrelated data. Correlated data are illustrated by calculating the random variable z, from x and y, so that z has a pre-specified (in the example r = 0.9) correlation with y. set.seed(12345) r &lt;- 0.9 #correlation coefficient between y and z x &lt;- rnorm(300, 30, 2) y &lt;- rnorm(300, 30, 2) z &lt;- r*y+sqrt(1-r^2)*x spur &lt;- data.frame(x, y, z) 37.3.6 Plot the correlations One can readily see the shotgun pattern of an uncorrelated sample (blue) as opposed to the elliptical galaxy pattern in the correlated sample with greater centroid density. spur %&gt;% ggplot(aes(x, y)) + geom_point(color=&quot;#002878&quot;) + theme_bw() + geom_point(aes(z,y), color=&quot;#d28e00&quot;)+ labs(x=&quot;x or z&quot;) 37.3.7 Calculate a correlation coefficient and posthoc test The script below tidys up the simulated data from above. It then runs some calculations to compare outputs. Note how the output of the cor function can be replicated exactly by.hand. The functions cor, cov and var are each components of the basic stats package and have utilities in their own rights. Notice how \\(r\\) and \\(r_s\\) give similar but different values. Not unexpected, they are different calculations! The cor.test function will provide a confidence interval only for the pearson method. X &lt;- spur$x Y &lt;- spur$y Z &lt;- spur$z cor(X, Y, method=&quot;pearson&quot;) ## [1] 0.02426448 by.hand &lt;- cov(X,Y)/sqrt(var(X)*var(Y)); by.hand ## [1] 0.02426448 cor(Z,Y, method=&quot;pearson&quot;) ## [1] 0.8956038 cor(Z, Y, method=&quot;spearman&quot;) ## [1] 0.8648838 cor.test(X, Y, method=&quot;pearson&quot;, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## Pearson&#39;s product-moment correlation ## ## data: X and Y ## t = 0.41899, df = 298, p-value = 0.6755 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.08922152 0.13712853 ## sample estimates: ## cor ## 0.02426448 cor.test(Z, Y, method=&quot;pearson&quot;, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## Pearson&#39;s product-moment correlation ## ## data: Z and Y ## t = 34.754, df = 298, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8706648 0.9159499 ## sample estimates: ## cor ## 0.8956038 37.3.8 Interpretation of correlation output Two variables can either be positively correlated, or negatively correlated, or not correlated at all. Values for a correlation coefficient can range from -1 to +1, with the strongest correlations at either extreme, and weakest correlations at around zero. Thus, the correlation coefficient is a measure of effect size. If the correlation coefficient differs from zero, the two variables are correlated, or are said to be associated. However, whether a particular value for a correlation coefficient should be considered strong or weak is usually a matter of scientific judgment. The p-value should not be used as an expression of effect size. It is possible to have a very low p-value for weak correlation. In particular, large sample sizes can generate low p-values for rejecting the null of no correlation. Remember, having high confidence in a non-zero correlation is not the same as concluding a strong correlation exists! 37.3.8.1 Write Up The variables X and Y are uncorrelated (Pearsons r=0.024, 95%CI= -0.089 to 0.137, p=0.6755). The variables Z and Y are correlated (Pearsons r=0.895, 95%CI=0.87 to 0.916, p&lt;2.2e-16). "],["regress.html", "Chapter 38 Linear Regression 38.1 The linear regression model 38.2 Least squares fitting 38.3 The practical importance of linear model parameters 38.4 Linear model standard errors 38.5 Linear regression in R 38.6 Intepretation", " Chapter 38 Linear Regression Fitting models to data, fitting models to data, fitting models to data. Regression analysis is used for a few different purposes: Fitting a model to a data set; in particular, to derive model parameters. Calculating unknowns based upon a standard curve. Calculating the fraction of the response Y that is determined by a predictor X. Regression is a major technique for conducting causal inference because it has both an outcome variable (typically \\(Y\\)) and a predictor variable (typically \\(X\\)). Data suitable for regression analysis are experiments that generate \\(X,Y\\) pairs. In one scenario, the predictor variable \\(X\\) is under complete experimental control by the researcher. In a second scenario, a variable measured by the researcher out in the field is taken as a predictor variable. Although it is not necessarily under true experimental control, in some situations it can be taken as a predictor. The latter technique is used a lot in ecological research, public health and economics research, for example. In regression experiments that are the focus of this chapter, each of the outcome and the predictor variables have measured scalar properties, for example, continuous intervals. However, regression methods exist for working with outcome variables representing discrete and/or ordinal data, which well discuss when we get to general linear regression. 38.1 The linear regression model Linear regression is performed on data sampled through some process involving random selection of \\(i=1 \\ to \\ n\\) independent pairs of outcome \\(Y\\) and predictor \\(X\\) variables, whose values are \\(y_i\\) and \\(x_i\\), respectively. The two variables have a linear relationship such that a line drawn through them would have a y-intercept of \\(\\alpha\\) and slope of \\(\\beta\\): \\[Y=\\alpha+\\beta X\\] Of course, models are perfect but data are not. Most, if not all of the data points would deviate from the model. Any given value for \\(Y\\) incorporates an error term \\(\\epsilon_i\\) along with the model parameters at some value of \\(X\\): \\[y_i=\\alpha+\\beta x_i+\\epsilon_i\\] \\(\\epsilon_i\\) is known as the residual, or deviate, which is calculated as the difference in the value between \\(y_i\\) and \\(\\hat y_i\\), where the latter is a corresponding data point on the best-fit regression line. Thus, residuals are the difference between the real data and the values predicted by the model. This isnt the first time youve been exposed to the concept of the residual, which was first introduced as using the term deviate when discussing how variation is calculated in t-tests. Weve also referred to it as the variate. They all mean the same thing. Variate = deviate = residual. Sorry, thats statistical jargon for you. In regression, residuals are often useful to assess how well the model fits the data. This brings us to point about the roots of regression modeling. Models are fit to data. Data are not fit to models. Think of data as fixed and real, like a foot. Just as we try on shoes of various sizes until one fits the foot best, we fit different models to data until one fits the data best. What is meant by best fit models is they explain the data better than other models we could have chosen. In computer-assisted regression, we start with a model comprised of initial parameter estimates. The computer calculates the residuals to assess the fit, before changing the parameters and checking if the fit is better. After a few iterations of this process a best fit model is solved. 38.2 Least squares fitting The question you might ask yourself is the following: If models are perfect and data are not, then how is a perfect model fit to imperfect data? The answer is by using least squares fitting method. Lets imagine we have an imperfect data set. Let \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) represent the best fit solutions for the slope and intercept of the datas linear model. Thus, \\[y_i=\\hat\\alpha+\\hat\\beta x_i+\\epsilon_i\\] rearrange, \\[\\epsilon_i=y_i-\\hat\\alpha+\\hat\\beta x_i\\] Just as for deviates or variates relative to a group mean discussed previously, residuals summed alone cancel out, but dont if they are squared. \\[\\epsilon_i^2=(y_i-\\hat\\alpha+\\hat\\beta x_i)^2\\] The model is best fit when the sum of the squared residuals reaches its minimum form: \\[\\sum_{i=1}^n\\epsilon_i^2=\\sum_{i=1}^n(y_i-\\hat\\alpha+\\hat\\beta x_i)^2\\] In other words, the sum of the squared residuals are known as the least squares. The values for the model parameters are ideal when the sum of the squared residuals is the lowest possible value. A few important relationships arise for linear least squares models that render calculating the model parameters fairly trivial. The best fit residuals are \\(\\hat\\epsilon_i=y_i-\\hat y_i\\). The best fit slope is \\[\\hat\\beta=\\frac{\\sum_{i=1}^n(yi-\\bar y)(x_i-\\bar x)}{\\sum_{i=1}^n(x_i-\\bar x)^2}\\] And the best fit intercept is \\(\\hat\\alpha=y_i+\\hat\\beta x_i\\). Thus, the linear regression solution can be computed as \\(\\hat y_i=\\hat \\alpha+\\hat\\beta x_i\\) These same ideas are applied to nonlinear regression, too. In practice for nonlinear regression, which have more model parameters, computer-assisted fitting routines iteratively adjust the model parameters, each time checking how each change impacts the value of \\(\\sum_{i=1}^n\\epsilon_i^2\\). The least squares solution for the model is achieved when \\(\\sum_{i=1}^n\\epsilon_i^2\\) is minimized. 38.3 The practical importance of linear model parameters The intercepts and slopes of linear models typically have biological meaning and so are important in their own right. For example, lets imagine we are measuring the loss of a protein, in its concentration units, over a time period, and its loss is linear (which is rare, but whatever). The y-intercept would represent an estimate of the amount of protein in the initial state. We wouldnt have to measure it directly, which can be hard to do, but we could extrapolate from the linear model to estimate a value! The slope estimates the decay rate of the protein, as its units are concentration/time. We might wish to perform experiments testing various hypotheses about how specific conditions, or protein mutations, affect the decay rates (slope) or initial concentrations (intercept). The specific conditions and protein mutations are predictor variables, that can be at many levels. Wed set up a assay protocol to generate slope and/or intercept parameters over several independent experiments. Then wed put those parameter values in a table and compare the effects of the predictor variables, using either by t-test or ANOVA, for example! 38.4 Linear model standard errors Regression fitting is associated with uncertainty which is important to quantify. The standard error of a linear regression is derived from the residuals. \\[s_{yx}=\\sqrt{\\frac{\\sum(y_i-\\hat y_i)^2}{n-2}}\\] The standard error of the slope (b) and of the intercept (a) are calculated separately, each using \\(s_{yx}\\): \\[s_b=\\frac{s_{yx}}{\\sqrt{\\sum(x_i-\\bar x)^2}}\\] \\[s_a=s_{yx}\\sqrt{\\frac{\\sum x_i^2}{n\\sum(x_i-\\bar x)^2}}\\] 38.5 Linear regression in R First we create vectors for the values of our Y and our X variables. Although a data frame is not necessary to run the linear regression function, it will be useful for plotting latter. X &lt;- c(2, 6, 8, 12, 14, 18) Y &lt;- c(12, 14, 6, 10, 2, 4) data &lt;- data.frame(X, Y) Think of a regression model not so much as describing a relationship between X and Y, but the degree by which the values of X predicts the values of Y. Thus, the command lm(Y~X) in English would read, linear model for Y predicted by X, or Y by X. Lets name an object model, run it and print the results. In regression, its almost always useful to create objects like model since there are so many additional functions that can be run on the model: model &lt;- lm(Y~X); model ## ## Call: ## lm(formula = Y ~ X) ## ## Coefficients: ## (Intercept) X ## 14.190 -0.619 The sparse output yields two model parameters, or cryptically named coefficients: the y-intercept (intercept) and the slope (X). Theres actually quite a bit more information not being printed, including all the residual values, which you can explore by typing str(lim(Y~X)) in the console. Because 95% confidence intervals are always useful, use the confit function to calculate these ranges for both model parameters! confint(model, level=0.95) ## 2.5 % 97.5 % ## (Intercept) 5.828676 22.5522765 ## X -1.358133 0.1200381 Thus, there is a 95% chance the true y-intercept is between 5.8 and 22.5, while the sample estimates the true slope is between -1.358 and 0.12. The fact that the latter range of values includes the value of zero foreshadows that the slope will not differ from zero upon statistical testing! Rs summary function is the standard way to print additional detail about the model and its analysis. summary(model) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## 1 2 3 4 5 6 ## -0.9524 3.5238 -3.2381 3.2381 -3.5238 0.9524 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.1905 3.0117 4.712 0.00923 ** ## X -0.6190 0.2662 -2.326 0.08065 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.45 on 4 degrees of freedom ## Multiple R-squared: 0.5748, Adjusted R-squared: 0.4685 ## F-statistic: 5.408 on 1 and 4 DF, p-value: 0.08065 38.6 Intepretation Lets interpret the summary output. 38.6.1 Residuals For each value of X, these are the difference between the value of Y for the data point and the corresponding value of Y on the best fit model line. The sum of the residuals should approach zero. 38.6.2 Coefficients Not only does this provide the values for the slope and intercept, but also their standard errors, which can be used to construct confidence intervals. There is also a t-test for each. Both t tests are analogous to the one-sample t test. Both compare the estimate of the parameter from regression to the population value of zero. Thus, \\(t_b=\\frac{b-\\beta}{s_b}\\) and \\(t_a=\\frac{a-\\alpha}{s_a}\\), where \\(\\beta\\) and \\(alpha\\) are population slope and intercept, respectively, and their standard errors in the denominators are calculated as above. These t-tests evaluate the null hypothesis that the parameter values are zero. The p-value is the probability of a making a type1 error by rejecting the null. Thus, if we reject the null we agree there is slope in the relationship, and if there is slope then the predictor variable is responsible for some level of the outcome response! What fraction that is will be derived from the \\(R^2\\) statistic (see below). For the slope, the comparison is usually pretty important. If the slope is not different from zero, that implies the regression fails to show that levels of X predict levels of Y. That seems to be the case here (something seems to be going on, but the experiment it represents is probably under-powered). For the intercept, whether the t-test is worth interpreting depends on the situation. A positive t-test result simply tells us that, when X=0, there is more (or less) than zero of whatever Y represents. Sometimes a non-zero intercept is good, and sometimes it is not-so good. Sometimes that value is meaningful scientifically, such as when we use a y-intercept to represent a physical parameter (eg, Lineweaver Burke plots for \\(v_{max}\\) in Michaelis-Menten enzyme kinetics or the ratio of bound to free ligand in Scatchard Plots in linear transformation of binding assay data). 38.6.3 Degrees of freedom We lose a degree of freedom for each model parameter estimated. We estimate two model parameters from 6 pairs of data points, leaving us with 4 degrees of freedom. 38.6.4 R-squared \\(R^2\\) is the coefficient of determination that takes on the values from 0 to 1. \\(R^2\\) is generally used as an effect size to express how well a model fits the data. The larger the value, the better the fit. Multiple \\(R^2\\) is just R jargon to represent the full model with its 2 parameters. It is calculated as the variation associated with the model divided by the total variation: \\[multiple\\ R^2=\\frac{\\sum(\\hat y_i-\\bar y_i)}{\\sum(y_i-\\bar y)}\\] If were able to reject the null, for example, we would conclude that ~57% of the response to Y is associated with the levels of X. But because we cannot reject the null, we cannot make that claim! The effect size might as well be zero! Adjusted \\(R^2\\) is transformed by the sample size \\(n\\) and number of model parameters \\(p\\). \\[R_{adj}^2=1-(1-R^2(\\frac{p-1}{n-p-1})\\] Adjusted \\(R^2\\) provides a way of accounting for one of the problems with \\(R^2\\) that may not seem intuitively obvious to you, but is true: For a given model and data, \\(R^2\\) will increase with the number of predictor variables used. For example, \\(R^2\\) will increase simply by adding more time points in a time series or doses in a dose series. The adjusted \\(R^2\\) is thus a more conservative way of calculating the degree by which the predictor variable accounts for a response. It is also used when comparing models with differing numbers of parameters to a single data set. 38.6.5 F-statistic The F-statistic in linear regression is the same F-statistic that we saw in ANOVA. In fact, you can run an ANOVA on the model and get some of the same results as you see in the summary(model) output, just in different form. For example, the F statistic, dfs and the p-values are all identical! anova(model) ## Analysis of Variance Table ## ## Response: Y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## X 1 64.381 64.381 5.408 0.08065 . ## Residuals 4 47.619 11.905 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Linear regression and ANOVA are identical in this regard! They perform the same calculations. So far, Ive spoken about regression in situations where the \\(X\\) variable, the predictor, is continuous. In fact, the same regression technique can be used when the predictor variables are discrete. The results they generate are identical to ANOVA. Well discuss the general linear models later in the course. There are many researchers who are more comfortable with regression analysis, and do their work that way. Others are more comfortable with ANOVA analysis, and run their calculations with those methods and use that syntax. But its all the same mathematically, for the most part, and one arrives at the same inferential crossroads, irrespective. As you can see, the F-statistic from the regression summary is the ratio of the mean square for the regression model (\\(df_n=p-1\\), \\(p\\) = number of model parameters) to the mean square of the residuals (\\(df_d=n-p\\)). \\[F_{dfn,dfd}=\\frac{\\frac{\\sum(\\hat y_i-\\bar y)^2}{df_n}}{\\frac{\\sum( y_i-\\hat y)^2}{df_d}}\\] The F-statistic tests the null hypothesis that the variance associated with the regression model is no different than that of the residual. Put another way, it tests the null that the slope of the regression is equal to zero. 38.6.5.1 Overall If the value of the F-statistic is extreme, the null would be rejected. Not so in this case. As such, we cant draw any conclusions from the \\(R^2\\) value, either. As stated above. This is a negative result, and its not difficult to understand why: there are too few degrees of freedom to work with. In regression, its important to have enough degrees of freedom. The more parameters in the model, the more degrees of freedom will be necessary to derive the values of those parameters and to test whether a relationship exists. The need for more degrees of freedom in regression is the statistical equivalent to the need for more cowbell in a rock n roll song. 38.6.6 Plotting regression results This is very simple to do using ggplot. The basic plotting function is shown below. The stat_smooth function is quite versatile. By default, it produces an error band for the regression to illustrate the uncertainty. Note how the error varies across the range of predictor values and is greatest for extreme values of the predictor. It is typical for such confidence bands of regressions to have this bow tie look. ggplot(data, aes(X, Y)) + geom_point( size=5 )+ stat_smooth( method=&quot;lm&quot;, color=&quot;#002878&quot; )+ theme_classic( ) ## `geom_smooth()` using formula &#39;y ~ x&#39; 38.6.7 Visualizing residuals Residuals are used to evaluate, visually, how well the model fits the data. For example, a linear model may not fit well some part of the data set. To create residual plots, we can grab the residual values (for completeness) and also the predicted values (to plot) for the model object by passing our model through the residuals and predict functions, respectively. Recall we created the object model above and promised that it could be put to a lot of use. We can call produce the data while simultaneously adding them to data, our original data frame: data$residuals &lt;- residuals(model) data$predicted &lt;- predict(model) To plot residuals, we just add things by layer: ggplot(data, aes(X, Y))+ geom_segment( aes(xend=X, yend=predicted) )+ geom_smooth( method=&quot;lm&quot;, size=1, se=F, color = &quot;gray&quot; )+ geom_point( color=&quot;#002878&quot;, size=2 )+ geom_point( aes(y=predicted), color=&quot;#d28e00&quot;, size=3 )+ theme_classic( ) ## `geom_smooth()` using formula &#39;y ~ x&#39; The effect isnt particular dramatic with this trivial data set, But these residual plots become much more interesting when working with more complex models or data sets with far more values and complexity. For example, if the residuals fit better on one end of the predictor scale but not the other, we would probably seek out a better model and see if that fits the data better. "],["nonlinearintro.html", "Chapter 39 Non-linear regression introduction 39.1 Uses for nonlinear regression 39.2 Example of the process 39.3 Nonlinear models in general 39.4 nlfitr 39.5 Nonlinear regression functions in R 39.6 Nonlinear models and parameters", " Chapter 39 Non-linear regression introduction Nonlinear regression is a statistical method to fit nonlinear models to the kinds of data sets that have nonlinear relationships between independent and dependent variables. These regression fits produce estimates for the parameters of a nonlinear model. These model parameters are useful because they provide a way to quantify some biological process (eg, rate and equilibrium constants, minimal and maximal responses, Km and Kd values, Hill slopes, etc). Nonlinear regression has added importance since biological systems tend to operate in nonlinear ways. In general, nonlinear models are just mathematical equations that describe how \\(Y\\) is produced by \\(X\\). In nonlinear regression well fit a model formula to pairs of \\(X,Y\\) data from an experiment. The best fitting model parameters responsible for giving nonlinear shape to the relationship between \\(X\\) and \\(Y\\) are then determined by the regression method. The statistical analysis involving nonlinear models is nearly identical to that for linear models. The regression technique itself customizes the fit of a chosen nonlinear model to some data produced in the lab. These best fits are determined by a minimization of the sum of the squared residuals between the model values for the response and the experimentally-derived values. The only difference is that nonlinear regression solves the problem iteratively, rather than mathematically. A common misconception is that a data set is fed into some mythical omnibus function, which then identifies the right model and fits it to the data. Nobody really has that oneyet. There are too many ways nonlinear data can happen, yet in biology, only some of these ways are important. Therefore, it takes the expertise of a researcher to select the right model for the data and test whether the model fits. By important I mean that the models have some theoretical basis. Most of the time were working on experiments where the right models to choose are obvious. For example, if we are doing enzyme kinetics we are probably using the Michaelis-Menten model. If were measuring bacterial growth, we use model with an exponential term. If we are doing binding assays we are using a model based upon the mass action law. And so on. If there is no pre-existing theoretical framework, as a general rule, the nonlinear regression process is flexible enough to accept completely new models. 39.1 Uses for nonlinear regression There are three big reasons to use nonlinear regression: Artistic Interpolation (and simulation) Conducting statistical inference Sometimes we have nonlinear data and all we want to do is draw a pretty curve through the points. The objective is purely visual. Sometimes well run a model-based regression to get the pretty curve. Other times well use a smoothing regression that is, truly, just a pretty curve. Youll see how to do both of those below. Other times we have measured values of \\(Y\\) for which we want to impute values of \\(X\\). Thats achieved by nonlinear regression of an \\(X,Y\\) standard curve (eg, ELISA), which is used to interpolate values for unknown samples. Conceptually related to this is simulating nonlinear data for a range of \\(X\\) values using a given model and fixed parameter values. There are several use cases for simulation. Perhaps the most important is in trying to write a custom model that looks like data youve generated. Or to figure out what a model really means, by playing with it as in a sandbox. Theres no better way to learn it quickly than by simulating data with it. Extensions of this include creating explanatory or comparative figures, and performing Monte Carlo-based preplanning. The most common statistical use is to estimate the parameter values of the nonlinear model that best fits some sample data, and then conduct statistical inference on those values. These parameter values from our data represent constants that represent an important biological feature. 39.2 Example of the process Some common laboratory phenomena in their pre-depletion states can be described using exponential growth models. For example, bacterial growth in culture, or the polymerase chain reaction. 39.2.1 It starts with nonlinear data We wish to estimate the growth rate and initial inoculate of a bacterial culture by taking aliquots at the indicated times through measuring optical density (A600 nm) on a spectrophotometer. We have a data set (edata) of \\(x, y\\) pairs. The variable names are actually hours and A600, respectively, and plotted in panel A of the figure below. The response \\(A600\\) is clearly nonlinear on values of \\(hours\\). Figure 39.1: A. Primary nonlinear data looks exponential. B. Same data as A log-transformed looks linear. 39.2.2 Which nonlinear model describes that data? We remember enough algebra to realize an exponential function could have produced something like the data in panel A. Consistent with this, natural log transformation of the values of \\(Y\\) linearizes the response (see panel B in the figure). We can write a general exponential growth model as follows: \\[y=y_{min}\\times e^{k*x}\\] In this form the parameter \\(k\\) is a constant with units of reciprocal \\(x\\). The parameter \\(y_{min}\\) is the lower asymptote, serving to estimate the value of \\(Y\\) in the absence of \\(X\\). Therefore, \\(k\\) modifies the exponential rate by which \\(Y\\) changes with \\(X\\). The nonlinear regression method in general exists to obtain estimates for \\(k\\) and \\(y_{min}\\) from experimental data such as that shown in figure A. Thats useful when these model parameters have biological meaning. Sometimes we are interested in only \\(k\\) or in \\(y_{min}\\). Other times, both parameters are important in drawing some scientific picture. It depends on the project were working on. 39.2.3 Fitting a model to data To get those estimates we run a regression to fit an exponential model to the data. Here, were using a fitting function from the nlfitr package. Why fitegro and not some other function? The function fitegro has the right model for this purpose. myfit &lt;- fitegro(x=hours, y=A600, data=edata, k=1, ylo=0, weigh=T) myfit ## Nonlinear regression model ## model: A600 ~ (ylo) * exp(k * hours) ## data: data ## k ylo ## 0.5092 77.0827 ## weighted residual sum-of-squares: 2.074 ## ## Number of iterations to convergence: 13 ## Achieved convergence tolerance: 1.49e-08 39.2.4 Interpreting regression model output Lets go over that output. First of all, understand that fitegro is just a wrapper function for nlsLM, which is also a wrapper function for nls. It produces class nls R object. There is a lot more information produced through the regression besides the default nls output seen here. This additional information can be accessed by other generic functions (eg, confint, summary, predict, residuals, etc). See ?nlsLM for more details. Second, it states the model, not in general terms, but in terms of the input variables. In spoken English reading from right to left, the model says, fit this model to the A600 values. This is worthy of a look just to make sure we have the right model. The output estimates for the parameters k and ylo are the most important features of this regression output. They are what we most want. \\(k=0.5092 per hour\\) while \\(ylo=77.08 in A600 units\\) Well discuss later how we work with these parameter estimates statistically when doing experiments. The fact that the regression converges to a solution is hopeful. It suggests a good fit but thats not always the case. It is important to look at that fit because sometimes regression can converge to an absurd solution. 39.2.5 Check the fit graphically As a first step, the bloody obvious test is the best way to assess a fit of the model to the data. That comes from looking at the regression fit graphically. Does that curve look good and right? In the code chunk below shows two regression fits. The blue line is similar to the fitted output, but not identical because it is unweighted. The regression for the blue line is actually performed within ggplot2s geom_smooth function. Its the way to go when doing unweighted regression. Some quirks. Note how the formula must be argued in a generic form. The ggplot aes previously defined x=hours and y=A600. Notice the start argument values, put there to make it easier to get the right solution. Without them the regression may or may not self start. Id say that blue curve is a pretty good fit. But YMMV. Drawing the model that is the precise fit of the fitegro function is executed using the stat_function. Remember homework 4 when we did this with the dnorm function? We simply take the regression parameter values and pop them into a custom function. ggplot(edata, aes(hours, A600))+ geom_point()+ geom_smooth(method=minpack.lm::nlsLM, formula = &quot;y ~ ylo*exp(k*x)&quot;, #note how this is a general formula method.args = list( start=c(ylo=77,k=0.5)), se=F)+ stat_function(fun = function(.x) 77.0827*exp(.x*0.5092), color=&quot;red&quot;) Figure 39.2: Look at the models curves to check on the fit. Notice how the red line is not equivalent to the blue line. That is the effect of weighted regression. 39.2.6 Convergence? Now a word about the word convergence, which has pragmatic importance. Nonlinear regression in R (and commercial software) resolves parameter values by an iterative algorithm that converges onto the solution. Iterative minimization of the residual error via ordinary least squares lies at the core of this method. The model these algorithms are actually working with has an added term for residual error, \\(\\epsilon\\) \\[y=y_{min}\\times e^{k*x}+\\epsilon\\] The nonlinear regression algorithm calculates values for \\(y_{min}\\) and \\(k\\) that also give the lowest possible value of \\(\\epsilon\\). Best fit happens when the sum of the residual differences from the data points to the models curve are minimized with any additional changes in \\(y_{min}\\) and \\(k\\). The software method takes the data along with starting parameter estimates fed by the researcher, calculates residuals, modifies those estimates, re-calculates residuals, modifies the parameter values again, and so on. Usually it takes a half-dozen or more of these cycles to converge on a stable minimization of residual error. It took 13 cycles in the example above. 39.2.7 Convergence is not idiot proof Getting regression to converge is the key. But did it converge correctly? It is important to understand that several factors influence how well regression goes and whether it converges. The starting values the researcher suggests can affect how well convergence to a solution works. Bad estimates can cause convergence to not occur, or to veer off into a nonsense fit. The fitting algorithm itself impacts whether there is convergence to a solution. Of the 4 options (see below), I think the Levenberg-Marquardt algorithm plays best with typical biological models and over a range of data from crappy to tight. YMMV. Noisy data is hard to look at and even harder for the fitting algorithms to solve. When the data is crappy, dont expect a fit to work. This isnt reading tea leaves. Use as many data points as possible. Nonlinear regression feasts on data points. The more data points in a data set, the better things work. There are a few ways to do this. When the set has technical replicates, leave them all there. Dont average them. When doing time courses or dose-responses, add more time points or doses. The number of parameters influences convergence. Every model parameter comes at the cost of a degree of freedom. A model with too many parameters and too few data points is probably not going to work. If there are only 6 dose levels, no replicates, and 4 parameters in the model, it is probably not going to converge. YMMV. Finally, when there are results, the question is did this function produce the the best fit? Technically, yes, when convergence is achieved, by the standard of minimizing \\(\\epsilon\\), these parameter values are the best fit of that model. That is not the same as asking whether this model is the best for that data. There may be other models that fit better. Nested model nonlinear regression is a way to test if one of two related models fit. The bottom line is that compared to the other statistical methods weve covered to this point in the course, nonlinear regression is bit more hands on. We have to know our models. We have to know our data. Which means we have to know what the model parameters mean, and we have to know what values we can reasonably expect for them. 39.3 Nonlinear models in general There are many different nonlinear models to choose from that are routinely deployed for analysis of biological systems. Every model has parameters that carry specific biological meaning. Its not necessary to learn every nonlinear model. Chances are that if we need nonlinear regression in our work were working with a single model or maybe a pair. Chances are well need one that is a well-understood, off the shelf model, rather than writing a new custom model. The GraphPad Prism Statistics Guide offers an excellent comprehensive resource for nonlinear regression, including a couple of dozen different functions. You may recognize the shape of your own data in some of these. All of the basic concepts that are part of solving nonlinear regression problems using the Prism software apply to using R (or any other software). 39.4 nlfitr We (myself, Jessica Hoffman, Austin Nuckols and Lauren Jeffers) are working on a package called nlfitr. You can install the most current MVP (minimal viable product) onto your R version from Github. Our vision is for nlfitr to be a comprehensive resource for nonlinear regression in R. The ultimate goal of the package is to offer researchers off the shelf functionality in terms of specific models, along with the better behaved Levenberg-Marquardt algorithm. This can go a long ways towards making nonlinear regression in R easier. At its current stage, the functions in nlfitr fall into two classes: simulation or fitting functions. The simulation functions are intended to serve as sandboxes for learning how a given nonlinear model works. Thats through playing with the model parameters and variables and see how that affects the curves. These sim functions are also intended for use in experimental planning, such as establishing feasibility, Monte Carlo testing, and so on. The sim functions with heterscedastic simulation capability offer a way to simulate a very common feature of biological datahigher variance at higher input levels. The fitting functions are for fitting a given nonlinear model to some experimental data. In effect, they are wrapper functions for nlsLM. For the most part, they do the formula specification work for the user, which makes using nlsLM much easier. 39.5 Nonlinear regression functions in R From here on the assumption is the researcher wishes to use nls or nlsLM directly, and not nlfitr. This section also provides more information about nonlinear regression in R that is worth understanding. The main nonlinear regression functions in R are nls in the base stats package and also nlsLM from the minpack.lm package. nlsLM is a wrapper function for nls, designed to run virtually the same as nls. 39.5.1 The fitting algorithms The fitting algorithms used in nls are the real engine in nonlinear regression. The three fitting algorithm options in nls include the Gauss-Newton, Golub_Pereyra and Port. nlsLM offers a fourththe Levenberg-Marquardt. The details of these are beyond the scope. When they work, all four will give very close to the same results, but not exactly the same. Pragmatically, their main differences is that they dont always work on a given dataset. The researcher can select among these to find one that solves their own nonlinear regression. Of the four, my general impression is that the Levenberg-Marquardt plays nicest, which means it is less likely to fail to reach convergence than the others. That explains why we use it in the nlfitr package. YMMV. Lets just keep things simple for a moment and talk as if nls and nlsLM are the same thing and discuss the key features: 39.5.2 formula Nonlinear regression functions require the researcher to define a nonlinear formula. A formula is a translation of a nonlinear model equation into a format that R can work with. For example, here again is the exponential model: \\[y=y_{min}\\times e^{k*x}\\] Here is the corresponding general formula for nls: \\[y \\sim~ ylo*exp(k*x) \\] When using the nls-based functions, the formula has to be modified further to correspond to the variables in the working dataset. For our bacterial growth example above we have to tell the nlsLM function exactly which variables to regress. Note how y and x in the formula are replaced by A600 and hours: nlsLM(A600~ylo*exp(k*hours), data=edata, start=list(ylo=100, k=1)) ## Nonlinear regression model ## model: A600 ~ ylo * exp(k * hours) ## data: edata ## ylo k ## 44.8304 0.5831 ## residual sum-of-squares: 1.09e+08 ## ## Number of iterations to convergence: 13 ## Achieved convergence tolerance: 1.49e-08 A point of confusion: ggplot has a different standard (see the graph above with the fitted curve). In geom_smooth, the formula is kept in general terms. 39.5.3 data We have experimental data as \\(x, y\\) pairs. Both are continuous variables. Create a data frame that has a column for each. When the experiment has multiple replicates, whether independent or technical, list all replicates under either of the two columns. Add a third column with a replicate ID. R differs from other software. You dont want a separate column for every replicate. pivot_longer is your friend. The column names can, and should, be natural. Name the variables as you would otherwise. There is no need to label them \\(X\\) and \\(Y\\). 39.5.4 start Although nls can self Start without any hints for the values of the model parameters, its usually best to feed a list of starting values. These get the iterations off on the right track. They only need to be rough guesses of what the final values might be. 39.5.5 weighting In OLS the data points that are furthest from the models curve contribute more to the overall sums of squares. As a result, the curve tends to be pulled in the direction of these outliers. It is very common for nonlinear biological data to have greater variation at higher values of responses than at lower values. Therefore, when we dont adjust for this heteroscedasticity the regression solutions are biased towards the data points in the sample that are further away from the curve. Weighted nonlinear regression is just a method to discount this excessive variation and give all data points a more equal contribution to the overall regression solution. Weighted nonlinear regression minimizes the weighted sums of squares rather than minimizing the unweighted sums of squares. There are a variety of weighting schemes. The one recommended for continuous data is the reciprocal y^2 method. Reciprocation is a simple way of reducing variance without changing the relationship of a variables values. Thus, the sums of squares are minimized using \\(1/y^2\\) values rather than \\(y\\). 39.6 Nonlinear models and parameters A nonlinear model is just an equation or function that describes the relationship between a predictor variable \\(X\\) and an outcome variable \\(Y\\). The shape of the relationship is dictated by the equations parameters. Thus, values for \\(Y\\) are determined by a nonlinear combination of values for \\(X\\) and the equation parameter(s), which we can label generically as \\(\\beta\\). 39.6.1 Hyperbolic stimulus response functions The general hyperbolic function is, \\[Y=\\frac{y_{max}\\times X^h}{K^h + X^h}\\] This function, and its many derivatives, are used to model a diverse array of stimulus-response systems in biology, making it perhaps the most important mathematical relationship in the field. The basis of these phenomena is the mass action principle, \\[X+Y \\rightleftharpoons XY\\], where \\(XY\\) represents any response dependent upon both \\(X\\) and \\(Y\\). These include Michaelis-Menten enzyme kinetics, stimulus/dose-response phenomena, bi-molecular binding, and much more. The equation has three parameters: The maximal value of \\(Y\\) is estimated asymptotically as \\(y_{max}\\). The parameter \\(K\\) represents the value of \\(X\\) that yields the half-maximal response, or \\(\\frac{y_{max}}{2}\\). The Hill slope, \\(h\\), operates in the function as an exponential coefficient. Therefore, it determines both the steepness and the sign of the relationship between \\(Y\\) and \\(X\\). Each of these parameters can have different physical meanings, depending upon the biological process that the hyperbolic function is applied to. For example, with bi molecular binding data, \\(y_{max}\\) represents the number of binding sites or complexes, \\(XY\\); \\(K\\) represents the equilibrium dissociation constant, or affinity, between the two molecules; and the value of \\(h\\) provides insights into whether the binding deviates from simple mass action. For example, the Hill slope value may suggest positive (\\(h&gt;1\\)) or negative (\\(0&lt;h&lt;1\\)) cooperativity. Negative values of \\(h\\) generate downward-sloping curves; for example, inhibition of \\(Y\\) by \\(X\\) occur when \\(h&lt;0\\), and these can be very steep or shallow, too. Hyperbolic functions are responsible for graded responses that can occur over a wide range of values of the predictor variable. When plotted on a linear X-scale, the predictor-response relationship has a hyperbolic-like shape, thus their name. set.seed(1234) p1 &lt;- simhetdr(x=c(1,3,10,30,100,300,1000,3000), k=100, ylo=100, yhi=300, h=1, cv=0.15, reps=3, log=F) set.seed(1234) p2 &lt;- simhetdr(x=c(1,3,10,30,100,300,1000,3000), k=100, ylo=100, yhi=300, h=1, cv=0.15, reps=3, log=T) plot_grid(p1, p2, labels=&quot;AUTO&quot;) Figure 39.3: A. Linear scale plot of hyperbolic data. B. Log10 scale plot of hyperbolic data. The same data appear more S shaped when plotted on a log x-scale. However, it is important to point out that extreme values of \\(h\\) can dramatically change these more typical relationships. As values of \\(h\\) get larger, the \\(Y\\) response to \\(X\\) is more switch-like than graded. set.seed(1233) p1 &lt;- simhetdr(x=c(1,3,10,30,100,300,1000,3000), k=100, ylo=100, yhi=300, h=2, cv=0.15, reps=3, log=F) set.seed(1234) p2 &lt;- simhetdr(x=c(1,3,10,30,100,300,1000,3000), k=100, ylo=100, yhi=300, h=2, cv=0.15, reps=3, log=T) plot_grid(p1, p2, labels=&quot;AUTO&quot;) Figure 39.4: Influence of higher Hill slope. A. Linear scale plot of hyperbolic data. B. Log10 scale plot of hyperbolic data. S-shaped curves have lower and upper plateaus. These plateau values correspond to the ylo and yhi parameter values that youll see in the formulas for the nonlinear models below. The span between the plateaus represents the dynamic range of the response variable, \\(y_{max}=yhi-ylo\\). Incorporating ylo and yhi into a working formula for the hyperbolic model allows for a more regression friendly way of expressing it. This allows the operator some flexibility when initializing the regression using start estimates. For example, when working with a difficult data set the regression has a better chance for solving the other parameters, ylo or yhi, or both, when they are held at fixed values. 39.6.2 Visualizing nonlinear data and log scaling Nonlinear data generally occur over orders of magnitudes. Either the responses are log normal or, more often, the predictor variable is applied over a range spanning a few orders of magnitude. For example, doses of a drug at 10, 100 and 1000 units span three orders of magnitude. Look very carefully at the code below, which simulates nonlinear data. The code passes linear values of \\(X\\) into a hyperbolic formula thats been written for regression. Some initial, arbitrary parameter values are entered to give the data shape and location. I strongly urge you to use this code chunk as a sandbox. Leave the formula alone but play around with with the values of the initial variables, including the predictor, \\(x\\). Thats the best way to see how they all work together. The two graphs plot identical values for both the \\(Y\\) and \\(X\\) variables. The only difference is the plotting background, one is log scale and the other is not. Note how that is accomplished using ggplot arguments. You can think of the plot on the left as being drawn on linear graph paper, and the one on the right as being drawn on semi-log graph paper. The latter rescales the x-variable without changing its values. x &lt;- c(1e-8, 3e-8, 1e-7, 3e-7, 1e-6, 3e-6, 1e-5) #initial values, arbitrary units h &lt;- 1 k &lt;- 3e-7 ylo &lt;- 30 yhi &lt;- 330 #hyperbolic formula for linear scale y &lt;- ylo+ ((yhi-ylo)*x^h)/(k^h+x^h) #linear x scale p3 &lt;- ggplot( data.frame(x, y), aes(x, y))+ geom_point(size=4, color=&quot;#f2a900&quot;)+ scale_x_continuous(limits=c(1e-8, 1e-5))+ labs(title=&quot;Linear scale&quot;) #log10 x scale p4 &lt;- ggplot( data.frame(x, y), aes(x, y))+ geom_point(size=4, color=&quot;#f2a900&quot;)+ #scale_x_log10()+ scale_x_continuous(trans=&quot;log10&quot;, limits= c(1e-8, 1e-5))+ labs(title=&quot;Log10 scaled aesthetic&quot;, x=&quot;log10[X]&quot;) plot_grid(p3, p4, labels=&quot;AUTO&quot;) Figure 39.5: A closer look at linear log10 scaling. 39.6.2.1 LogX scaling the hyperbolic function As usual for R, there are a few ways to solve the log scaling problem. An alternative is transform the predictor variable to a log scale. For example, the dose range mentioned above of 10, 100 and 1000 units transformed to \\(log_{10}\\) units is 1, 2 and 3. Thus, the vector for the predictor would be x&lt;-c(1, 2, 3). When using a vector on a log scale, the regression is performed using a semi-log transformation of the hyperbolic function: \\[Y=\\frac{Y_{max}}{1+10^{(log10K-X)\\times h}}\\] Note in the code below that the \\(X\\) is transformed from a linear to log10 scale. Since we are regressing on \\(log_{10}\\) values of \\(X\\), we solve the \\(K\\) parameter in in log units, as well. The values of \\(Y\\) remain on a linear scale. Note also that the log10 of 3e-8 is -7.523, which is approximately a half log unit between 1e-8 and 1e-7. A value of 5e-8, which is half-way between 1e-8 and 1e-7 on a linear scale, is about a third of a log unit (-7.301) between -8 and -7. x &lt;- log10(c(1e-8, 3e-8, 1e-7, 3e-7, 1e-6, 3e-6, 1e-5)) #or you could just write these out: #x &lt;- c(-8, -7.523, -7, -6.523, -6, -5.523, -5) h &lt;- 1 logk &lt;- log10(3e-7) ylo &lt;- 30 yhi &lt;- 330 #hyperbolic semi-log model rewritten as a formula y=ylo+((yhi-ylo)/(1+10^((logk-x)*h))) ggplot( data.frame(x, y), aes(x, y))+ geom_point(size=4, color=&quot;#f2a900&quot;)+ labs(title=&quot;Linearized X scale by log10(X) transform&quot;) 39.6.2.2 Creating a best fit curve in graph Its very simple to generate an on-the-fly best fit curve using ggplot. This doesnt yield regression output and parameter values, but it does draw a pretty picture. This is on simulated data, but all youd need to do is pass a data frame of your own \\(x,y\\) data into ggplot and add the stat_smooth function to achieve the same effect. It really is that simple! The only trick is knowing the formula that youd like to model to the data, and coming up with a list of some starting parameter values. Note below how its been fed some start estimates that are a bit off the mark, given the data points, but it still arrives at a solution that fits well. You should experiment with changing those estimates to get a sense of how far off is too far off before the nls function fails to provide a curve. x &lt;- log10(c(1e-8, 3e-8, 1e-7, 3e-7, 1e-6, 3e-6, 1e-5)) #x &lt;- c(-8, -7.523, -7, -6.523, -6, -5.523, -5) h &lt;- 1 logk &lt;- log10(3e-7) ylo &lt;- 30 yhi &lt;- 330 #hyperbolic function y=ylo+((yhi-ylo)/(1+10^((logk-x)*h))) + rnorm(length(x), 0, 45) ggplot( data.frame(x, y), aes(x, y))+ geom_point(size=4, color=&quot;#f2a900&quot;)+ geom_smooth( method=nls, formula = &quot;y~ylo+((yhi-ylo)/(1+10^((logk-x)*h)))&quot;, method.args = list( start=c(yhi=150, ylo=50, logk=-7) ), se=F, #need this line for nls graphs and it is not obvious!! color=&quot;red&quot; )+ labs(title=&quot;Best fit for random Y values; Linearized X by Log10(X) transform&quot;) 39.6.2.3 Smoothing Smoothing is an artistic method to draw a nonlinear regression-ish line through the data. Smooths are more akin to polynomial fits. Every little twist and turn in a smooth would have a corresponding parameter if it were a polynomial regression! The underlying calculation is a regression run over a sliding window, called span in R. If that span is small, the smooth is closer to point-to-point. And if the span is larger, the smooth is smoother. You simply adjust the value of span to dial in a curve that suites your eye. set.seed(12345) x &lt;- log10(c(1e-8, 3e-8, 1e-7, 3e-7, 1e-6, 3e-6, 1e-5)) #x &lt;- c(-8, -7.523, -7, -6.523, -6, -5.523, -5) h &lt;- 1 logk &lt;- log10(3e-7) ylo &lt;- 30 yhi &lt;- 330 #hyperbolic function y=ylo+((yhi-ylo)/(1+10^((logk-x)*h))) + rnorm(length(x), 0, 45) ggplot(data.frame(x,y), (aes(x, y)))+ geom_point(size=4, color=&quot;#f2a900&quot;) + stat_smooth(method = &quot;auto&quot;, se=F, color=&quot;red&quot; , span=0.8 )+ labs(y=&quot;mRNA levels&quot;, x=&quot;time, min&quot;) Figure 39.6: Smoothing tends to overfit data 39.6.3 Selecting the right model Figure 39.7: Choosing the right model for your data is a scientific decision. We lean on scientific judgement to select the most appropriate nonlinear model for our data. Simply reach up to our metaphorical equation shelf to select one based upon our expertise (or willingness to become an expert) with the biological system. At some point, nlfitr will be such a book shelf. Fork it on Github and keep an eye on our progress. Or write your own function and send a pull request to have it incorporated into the package. Whatever. As I mentioned before, when unclear on what model to use inspiration can be found at GraphPads nonlinear regression guide. If the model you need is not there, youve probably got something pretty unusual and will need to create one. Thats doable, but usually not straightforward. 39.6.4 From models to formulas This section provides a potpourri view of nonlinear models. It focuses on translating nonlinear model equations into a formula needed to operate within a nonlinear regression function in R. 39.6.4.1 Michaelis-Menten Take for example the generic hyperbolic model, which has many descendants and cousins. For example, one descendant is the Michaelis-Menten model for reaction kinetics: \\[v=\\frac{V_{max}\\times S}{K_M + S}\\] The initial velocity of a reaction \\(v\\) at a given substrate concentration \\(S\\) is bounded by the enzymes \\(K_m\\) and maximal velocity \\(V_{max}\\). An experiment typically involves measuring product formation under initial velocity conditions over a broad range of substrate concentrations. The goal of the experiment to derive estimates for \\(V_{max}\\) and \\(K_m\\). Nonlinear regression is needed to get those estimates. Heres a generalized formula readable by Rs nonlinear regression functions that can be used to regress Michaelis-Menten data: \\[y=ylo+((yhi-ylo)*x)/(K+x)\\] The value of \\(Y\\) at its lowest and highest levels are ylo and yhi, respectively. The difference between those two values is the amplitude or dynamic range of the data, y_{max}, which corresponds to \\(V_{max}\\). The regression solves values for ylo and yhi. Later, you can calculatey_{max}. Breaking out the amplitude in this way provides some flexibility in model fitting. 39.6.4.2 Hyperbolic function inhibition Hyperbolic models can either be stimulation or inhibitory. There are a few different ways to model the latter. The simplest is to use a negative Hill slope in the standard hyperbolic formula. y &lt;- ylo+ ((yhi-ylo)*x^h)/(k^h+x^h) x &lt;- c(1:60) ylo &lt;- 0 yhi &lt;- 1 h &lt;- -1 k &lt;- 30 y &lt;- ylo+((yhi-ylo)*x^h)/(k^h+x^h) #linear plot ggplot(data.frame(x,y), aes(x,y))+ geom_point(color=&quot;blue&quot;) #log plot ggplot(data.frame(x,y), aes(x,y))+ geom_point(color=&quot;blue&quot;)+ scale_x_log10()+ labs(x=&quot;Log10(x)&quot;) Figure 39.8: Inhibition: Linear and log plots Alternately, the hyperbolic function can be inverted algebraically, deriving this model: \\[Y=\\frac{Y_{max}}{1+\\frac{X^h}{K^h}}\\] Whose formula is y=ylo+((yhi-ylo)/(1+x^h/k^h)) x &lt;- c(1:60) ylo &lt;- 0 yhi &lt;- 1 h &lt;- 1 k &lt;- 30 y &lt;- ylo+((yhi-ylo)/(1+x^h/k^h)) #linear ggplot(data.frame(x,y), aes(x,y))+ geom_point(color=&quot;blue&quot;) #log ggplot(data.frame(x,y), aes(x,y))+ geom_point(color=&quot;blue&quot;)+ scale_x_log10()+ labs(x=&quot;Log10(x)&quot;) Figure 39.9: More inhibition: Linear and log plots Finally, a model for inhibition on a log10 scale would be: \\[Y=\\frac{Y_{max}}{1+10^{(X-log10(K))*h}}\\] Its formula is y=ylo+(yhi-ylo)/(1+10^(x-log10(K))*h) x &lt;- log10(seq(1,600, 10)) ylo &lt;- 0 yhi &lt;- 1 h &lt;- 1 K &lt;- 45 y &lt;- ylo+(yhi-ylo)/(1+10^(x-log10(K))*h) ggplot(data.frame(x,y), aes(x,y))+ geom_point(color=&quot;blue&quot;)+ labs(x=&quot;Log10(x)&quot;) 39.6.4.3 Time series When performing time-series experiments, many outcome responses can be modeled using exponential association or decay equations. For example, heres a first order decay model: \\[Y=Y_0e^{-kt}\\] Here, the independent variable is time \\(t\\), in whatever units. The rate constant for a first order process has units of reciprocal time and is related to half-life: \\(k=0.693/t_{1/2}\\) Heres a generalized formula of it: y=ylo+(yhi-ylo)*exp(-k*x) A diagnostic of a first order decay process is that log10 transformation of \\(Y\\) yields a linear response with time. x &lt;- c(1:60) ylo &lt;- 0 yhi &lt;- 1 k &lt;- log(2)/30 y &lt;- ylo+(yhi-ylo)*exp(-k*x) #linear ggplot(data.frame(x, y), aes(x,y))+ geom_point(color=&quot;blue&quot;) #log ggplot(data.frame(x, y), aes(x,y))+ geom_point(color=&quot;blue&quot;)+ scale_y_log10()+ labs(y=&quot;Log10(y)&quot;) 39.6.4.4 Rhythmic functions Rhythmic phenomena can be modeled using sine-wave function for given amplitudes \\(\\alpha\\), wavelengths \\(\\lambda\\) and/or frequencies \\(\\frac{1}{\\lambda}\\) and phase shifts \\(\\varphi\\): \\[Y=\\alpha sin\\frac{2\\pi}{\\lambda}X+\\varphi\\] Heres a formula: y=ylo+(yhi-ylo)*sin(2*(pi/lambda)*x)+phi x &lt;- c(1:60) ylo &lt;- 0 yhi &lt;- 1 lambda &lt;-20 phi &lt;- 0.5 y &lt;- ylo+(yhi-ylo)*sin(2*(pi/lambda)*x)+phi ggplot(data.frame(x,y), aes(x,y))+ geom_point(color=&quot;blue&quot;) 39.6.4.5 Polynomial functions Virtually any nonlinear process can be modeled with outstanding fit using a high-order polynomial function. For example: \\[Y=\\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4X^4 + \\beta_5X^5 \\] Often, however, it is not clear exactly what are the physical parameters that correspond to those coefficients. Polynomials can give you incredibly good uninterpretable fits to data. Nevertheless, here is the formula: y=beta0+beta1*x+beta2*x^2+beta3*x^3+beta4*x^4+beta5*x^5 x &lt;- c(1:60) beta0 &lt;- 0 beta1 &lt;- 100 beta2 &lt;- 0.01 beta3 &lt;- 0.001 beta4 &lt;- 0.0001 beta5 &lt;- -0.00001 y &lt;- beta0+beta1*x+beta2*x^2+beta3*x^3+beta4*x^4+beta5*x^5 ggplot(data.frame(x, y), aes(x,y))+ geom_point(color=&quot;blue&quot;) "],["nestedregress.html", "Chapter 40 Nested-model nonlinear regression 40.1 Read and plot the data 40.2 Which model is a better fit? 40.3 Summary", " Chapter 40 Nested-model nonlinear regression Its like the difference between a glove that has five fingers instead of three. When you have five fingers, youd expect a five finger glove to fit better. If you were missing two fingers, a three finger glove would be a better fit. library(tidyverse) library(broom) library(minpack.lm) library(viridis) library(knitr) library(nlfitr) The most common approach to estimating nonlinear model parameter values is to perform a regression on each independent replicate of an experiment. After several of these we have a list of replicate values for the parameter of interest. A regression parameter is the dependent variable of the experiment. In this chapter Ill go over a nonlinear regression analysis of data from a drug metabolism assay in rat microsomes. The purpose of the assay is to estimate the rate constant for degradation of a drug. The rate constant is the dependent variable for experiments that test hypotheses on factors that might influence metabolism of the drug. In the next chapter, Ill illustrate how to work with parameter values from independent replicates in hypotheses tests on whether some condition changes the parameter. In that case, well ask if exposure to tobacco smoke changes theophylline metabolism (it should) In this chapter I want to illustrate deciding on which of two related nonlinear models serves as a better fit of the data. The theopOneRep.csv data set is an example of a single replicate of an experiment performed with duplicate measurements of the outcome variable. Well use that one replicate to illustrate how to do a nonlinear regression. The data are from an in vitro drug metabolism assay. A rat liver microsome prep was spiked with the drug theophylline (an ingredient of tea and coffee). At 2 minute intervals over a 2.5 hr period aliquots were withdrawn to measure remaining theophylline levels. In this instance, the microsomes were derived from a rat that had been treated in a microenvironment laden with cigarette smoke. Hydrocarbons in smoke are known to induce a cytochrome p450 enzyme, CYP1A2, that metabolizes theophylline more rapidly than occurs normally. The main objective of the nonlinear regression is to determine the rate constants for theophylline decay. The prediction is that exposure of the rats to cigarette smoke will cause two distinct phases of theophylline decay in the microsomes, one with a short half-life, and a second with a longer half-life. These would represent, respectively, the more rapid and the slower metabolism phases. 40.1 Read and plot the data Get familiar with the structure of the data set. The data have four variables: * min is time, a continuous predictor variable. * smoke is a discrete predictor variable with only one value in this set where Y = exposed to cigarette smoke. * id is a value assigned to identify the duplicate values, r1 or r2 * theo is theophylline concentration levels, in ng/ml The data represent one independent replicate with duplicate measurements per time point. theopOne &lt;- read.csv(&quot;datasets/theopOneRep.csv&quot;) str(theopOne) ## &#39;data.frame&#39;: 152 obs. of 5 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ min : int 0 2 4 6 8 10 12 14 16 18 ... ## $ smoke: chr &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; ... ## $ id : chr &quot;r1&quot; &quot;r1&quot; &quot;r1&quot; &quot;r1&quot; ... ## $ theo : num 94.2 80.8 88.5 70.9 63.8 57.1 55.9 65.3 56.9 53.2 ... Heres a plot of all the data. Notice how the duplicate values from the assay are shown. The curves are on-the-fly nonlinear regression fits that are run within the ggplot function. Below, well run these again to generate results. ggplot(theopOne, aes(x=min, y=theo))+ geom_point()+ #one-phase decay model stat_smooth(method=&quot;nlsLM&quot;, method.args=list( start=c(yhi=100, ylo=10, k=0.03)), formula=&quot;y~ylo+((yhi-ylo)*exp(-k*x))&quot;, se=F #you need this argument to generate nls smooths!! )+ #two-phase decay model stat_smooth(method=&quot;nlsLM&quot;, method.args=list( start=c(yhi=100, ylo=10, kf=0.05, ks=0.005, pf=0.5)), formula=&quot;y~ylo + (pf*(yhi-ylo)*(exp(-kf*x))) + ((1-pf)*(yhi-ylo)*(exp(-ks*x)))&quot;, se=F, color=&quot;red&quot; ) The formulas for the nonlinear models are for both a one-phase first order decay process (blue) and a two-phase process (red). By the bloody obvious test the two seem equivalent fits. But well be able to test, statistically, whether a two-phase first order decay model provides a better fit. Plotting and running the regression itself are hand-in-glove work tasks. It is hard to overstate the importance of creating these preliminary graphs with regression curves before conducting the regression analysis. Seeing how well your model formula fits the data helps ensure youve written the formula correctly. This process will help you interpret the regression output. A forewarning: Getting these models to regress onto data properly for these plots takes some patience. Some tips: Sometimes its possible to draw a curve without providing a start list for parameter estimates in the method.args argument. Usually it is not. If having trouble also the selfStart argument in nlsLM. Note the formula in ggplot is of the general form y ~ x rather than theo ~ min. Use the general form notation in these stat_smooth functions! Make sure the formula segments are properly demarcated using parenthesis. For example yhi-ylo*exp(-k*x) is not the same as (yhi-ylo)*exp(-k*x). Curves wont draw in ggplot without the se=F argument! Be sure to add it. Does it fail to converge on a solution? Try different algorithms (plinear or port) rather than the Gauss-Newton default. You might also try using the nlsLM function for its L-M algorithm, which is not available in nls. Fix the values of nonessential parameters. For example, set yhi=100, particularly if the data have already been transformed to %max. The data may just be crappy. You may need more or better data points for a curve to run. The more parameters you need to fit, the less likely the formula converges to a solution. If stat_smooth() fails to draw a curve, use stat_function(), which involves writing the function and imputing some values for the parameters. 40.2 Which model is a better fit? The main goal here is to derive drug half-life values. Also, because smoking induces CYP1A2 which metabolizes theophylline more rapidly, we have reason to believe these data might be better fit by a two-phase first order decay model, rather than a one-phase decay model. Well run regressions for both models to derive estimates for the parameter values for each. Then well do an F test to see if the two-site model serves as a better fit. On the basis of this latter test, well make the decision on which parameter values to report for this replicate. 40.2.1 Nonlinear models Heres a one-phase model fit. The nls object contains a lot of information. See the broom package to understand the some reporting functions. Theyre useful. For example, we could take the augment output directly into ggplot to create a residual plot for the fitted model. fit1 &lt;- nlsLM(formula=theo~(((yhi-ylo)*exp(-k*min))+ylo), data=theopOne, start=list(yhi=100, ylo=15, k=0.693/20) ) fit1 ## Nonlinear regression model ## model: theo ~ (((yhi - ylo) * exp(-k * min)) + ylo) ## data: theopOne ## yhi ylo k ## 92.55738 12.32320 0.04196 ## residual sum-of-squares: 4596 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 Heres a two-phase model fit. fit2 &lt;- nlsLM( theo~((pf*((yhi-ylo)*exp(-kf*min)))+ ((1-pf)*((yhi-ylo)*exp(-ks*min)))+ ylo), data=theopOne, start=list( yhi=115, ylo=15, pf=0.4, kf=0.06, ks=0.02) ) fit2 ## Nonlinear regression model ## model: theo ~ ((pf * ((yhi - ylo) * exp(-kf * min))) + ((1 - pf) * ((yhi - ylo) * exp(-ks * min))) + ylo) ## data: theopOne ## yhi ylo pf kf ks ## 94.86701 10.27262 0.62339 0.06155 0.02267 ## residual sum-of-squares: 4505 ## ## Number of iterations to convergence: 4 ## Achieved convergence tolerance: 1.49e-08 40.2.2 The nlfitr alternative The main advantage of using nlfitr is it saves from painstaking formula entry. Otherwise the output is the same. Because it is the same function as above. nlfit1 &lt;- fitdecay1(min, theo, theopOne, k=0.693/20, ylo=15, yhi=100, weigh=F) nlfit1 ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.04196 12.32320 92.55738 ## residual sum-of-squares: 4596 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 print(&quot;###############break##################&quot;) ## [1] &quot;###############break##################&quot; nlfit2 &lt;- fitdecay2(min, theo, theopOne, k1=0.06, k2=0.0001, range1=10, range2=65, ylo=15, weigh=F) nlfit2 ## Nonlinear regression model ## model: theo ~ range1 * exp(-1 * k1 * min) + range2 * exp(-1 * k2 * min) + ylo ## data: data ## k1 k2 range1 range2 ylo ## 0.06155 0.02267 52.73411 31.86024 10.27268 ## residual sum-of-squares: 4505 ## ## Number of iterations to convergence: 31 ## Achieved convergence tolerance: 1.49e-08 40.2.3 Troubleshooting the regression These nonlinear regressions worked. Sometimes they dont. Some of the same tips for plotting a curve apply apply to running the function. In addition, toggling weighting can help when the regression fails to converge. Note here we use the variable names in the data set, rather than \\(y\\) and \\(x\\) in the plots. See for more tips to get the regression to converge to a solution. 40.2.4 Interpreting the parameters Each model has a table of parameter estimates. The one-phase model has three parameters, \\(yhi\\), \\(ylo\\) and \\(k\\), the rate constant. The difference between \\(yhi\\) and \\(ylo\\) is the dynamic signal range, or span, of the results. The two-phase model estimates values for these span limits along with three other parameters. \\(pf\\) estimates the fraction of decay that is in a rapid component. \\(kf\\) and \\(ks\\) represent the fast and slow decay rate constants. They correspond to \\(k1\\) and \\(k2\\) in nlfitr function. 40.2.4.1 Driven by science Its important to put on the scientists hat. Everyone gets one when the start grad school. Our scientific objectives determine which of these parameters are important to you. Sometimes we are interested in the dynamic range. Other times you are interested in rate constants. Sometimes we are interested in both. Whichever to focus on depends on why we ran the experiment in the first place. Whether the t-tests in the parameter table have any utility depends upon the experimental design. Recall, they only test whether the value of the parameter differs from zero. If every data point were independent of every other data point they would have some inferential utility. In this case, where all data points are intrinsically-linked, the t-tests only offer technical utility. When the value is not different from zero, then that parameter has no predictive effect on values of the response variable. Plug a value of zero into the equation for that parameter to see what it does. This result is our first hint that the two-phase fit is not good. summary(nlfit2) ## ## Formula: theo ~ range1 * exp(-1 * k1 * min) + range2 * exp(-1 * k2 * min) + ## ylo ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## k1 0.06155 0.04129 1.491 0.1382 ## k2 0.02267 0.03066 0.739 0.4608 ## range1 52.73411 65.61905 0.804 0.4229 ## range2 31.86024 63.22695 0.504 0.6151 ## ylo 10.27268 3.94703 2.603 0.0102 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.536 on 147 degrees of freedom ## ## Number of iterations to convergence: 31 ## Achieved convergence tolerance: 1.49e-08 In contrast, the parameter values for the one-phase fit are different than zero. summary(nlfit1) ## ## Formula: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## k 0.041957 0.002152 19.50 &lt;2e-16 *** ## ylo 12.323204 0.723317 17.04 &lt;2e-16 *** ## yhi 92.557383 2.177071 42.52 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.554 on 149 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 In this case, these t-tests, which are negative for the rate constants and the fraction parameter, signal that the two-phase fit is not a good one for these data. These t-tests, which are just signal to noise ratios, signal that the parameter estimates for the two-phase model are unreliable. That is not the case for the one-phase model. However, well do an F test (below) to conduct formal inference on which model fits better. Youll see that result is consistent with the technical interpretation of the t-tests. 40.2.5 Residual plots to compare fits Residual plots are useful to compare fits. Here, the horizontal line at zero represents either model. The residuals for either model are also plotted. These represent the differences between the values of the data and the predicted values for the model at the same levels of \\(X\\). The red points are the residuals for the one-phase model. The blue points are the residuals for the two phase model. We dont see any remarkable residual differences from their models. Visually, as for the curves, its difficult to conclude one model fits better than the other. #for plotting, create one dataframe with residuals for both models fits &lt;- bind_cols(augment(nlfit1), augment(nlfit2)[,3:4]) %&gt;% rename(fit1=.fitted...3, resid1=.resid...4, fit2=.fitted...5, resid2=.resid...6) ## New names: ## * .fitted -&gt; .fitted...3 ## * .resid -&gt; .resid...4 ## * .fitted -&gt; .fitted...5 ## * .resid -&gt; .resid...6 fits ## # A tibble: 152 x 6 ## theo min fit1 resid1 fit2 resid2 ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 94.2 0 92.6 1.64 94.9 -0.667 ## 2 80.8 2 86.1 -5.30 87.3 -6.55 ## 3 88.5 4 80.2 8.34 80.6 7.90 ## 4 70.9 6 74.7 -3.80 74.5 -3.63 ## 5 63.8 8 69.7 -5.88 69.1 -5.28 ## 6 57.1 10 65.1 -7.96 64.2 -7.06 ## 7 55.9 12 60.8 -4.92 59.7 -3.84 ## 8 65.3 14 56.9 8.38 55.7 9.56 ## 9 56.9 16 53.3 3.57 52.1 4.76 ## 10 53.2 18 50.0 3.17 48.9 4.33 ## # ... with 142 more rows ggplot(fits, aes(min,resid1))+ geom_point(color=&quot;red&quot;)+ geom_point(aes(min, resid2), color=&quot;blue&quot;)+ geom_hline(yintercept=0) ### F test for model comparison There are a few ways to choose whether one regression model fits the data better than another. Only one of these methods should be used, and typically, that choice is made in advance of performing an analysis in order to limit bias. One way to decide which model provides a better fit to the data is using the extra sum of squares F-test. This F test is constructed as follows: \\[F_{df_1, df_2}=\\frac{\\frac{SSR1-SSR2}{df_1-df_2}}{\\frac{SSR2}{df_2}}\\] \\(SSR1\\) is the sum of the squared residuals for the simpler model, \\(SSR2\\) is the sum of the squared residuals for the more complex model, and \\(df_1\\) and \\(df_2\\) are the degrees of freedom for each, respectively. Hopefully, youve reached a point in your statistical knowledge that youre getting a good grasp on the concept of residuals, what they mean, and how they are used to calculate variance. They are simply the difference between the data point and the value of the model at a given level of x. Perhaps it now seems intuitive to you that the best fit model would the the one that produces the lowest residual variance. Its like the difference between a glove that has five fingers instead of three. When you have five fingers, youd expect a five finger glove to fit better. If you were missing two fingers, a three finger glove would be a better fit. Models that fit better have lower residual variance. This F ratio tests whether the variance of the difference between two models (the numerator) is greater than the variance of the model having more parameters (the denominator). Or you can think about it like this: The more the fits of the two models differ, the greater the ratio of the variance of the differences to the variance of the model having more parameters. This test is run by passing the model fits into the anova function as follows: anova(nlfit1, nlfit2) ## Analysis of Variance Table ## ## Model 1: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## Model 2: theo ~ range1 * exp(-1 * k1 * min) + range2 * exp(-1 * k2 * min) + ylo ## Res.Df Res.Sum Sq Df Sum Sq F value Pr(&gt;F) ## 1 149 4595.5 ## 2 147 4504.6 2 90.902 1.4832 0.2303 The F tests the null hypothesis that the variance for the difference between the two fits is the same as the variance of the two-phase fit. That F-test value is not extreme for a null F distribution with 2 and 147 degrees of freedom. In terms of decision-making, this test generates a p-value that is greater than the typical type1 error threshold value of 0.05. Therefore, we elect not to reject the null hypothesis that the ratio of variances of the two fits are the same. Which all means the the two-phase decay model is unsupported by the data and that we should accept the one-phase model as the better alternative. This means that the parameter values from the two-phase fit are not interpretable. Their values should be ignored. 40.2.6 Compare AIC An alternative approach to decide which of two model fits the data better is to compare the Akaike information criterion (AIC) between two models. The BIC is a highly related computation. The AIC statistic attempts to quantify the quality of the fit for a model by simultaneously accounting for its maximum likelihood and the number of model parameters. AIC is not standardized, so the value of any one AIC calculation alone is not readily interpreted. However, a comparison to another AIC for a related model can be interpreted. Furthermore, there is no p-value associated with AIC comparison. Instead a sliding scale is used for inference rather than a single threshold. The AIC is calculated using \\(n_{par}\\), the degrees of freedom for the log-likelihood estimate which can be derived from the output of running a logLik test on the model fit; \\(k\\), a per parameter penalty, which is usually set at 2; and the log-likelihood value of a fit, \\(logLik\\): \\[AIC=kn_{par}-2logLik\\] kable( fits.2 &lt;- bind_rows( glance(nlfit1), glance(nlfit2)) %&gt;% add_column( fit=c(&quot;fit1&quot;, &quot;fit2&quot;), .before=T) ) fit sigma isConv finTol logLik AIC BIC deviance df.residual nobs fit1 5.553593 TRUE 0 -474.7593 957.5187 969.6142 4595.518 149 152 fit2 5.535670 TRUE 0 -473.2409 958.4819 976.6252 4504.616 147 152 The difference between two AIC values compares two fits, always subtracting the fit with the minimal AIC value from the other. deltaAIC &lt;- fits.2[2,6]-fits.2[1,6] deltaAIC ## AIC ## 1 0.9632172 A loose scale to interpret the difference between two AIC values has been developed based upon an information theory method for comparing distributions called the KL divergence. To summarize that: When \\(\\Delta AIC &lt; 2\\) there is substantial support for the model with the larger AIC. \\(2&lt;\\Delta AIC &lt; 4\\) there is strong support for the model with the larger AIC. \\(4&lt;\\Delta AIC &lt; 7\\) there is less support for the model with the larger AIC. \\(\\Delta AIC &gt; 10\\) there is almost no support for the model with the larger AIC. In practice, youll find that application of the AIC comparison process forces you to accept the most parsimonious model unless there is overwhelming evidence that one with more parameters fits better. Youll find it works in a way that is very similar to the bloody obvious test. In the case of our present example, the \\(\\DeltaAIC\\) of 0.96 indicates that their can be little doubt that the one-phase model is the better fit. 40.2.7 Other ways to compare models For further information on comparing regression models, heres an excellent resource 40.2.8 Interpretation of this one replicate These types of experiments generate a lot of data and statistical output. Its easy to be overwhelmed or to think there is more going on (or needs to be done) than there really is. Just keep the focus on the scientific objective for the experiment. In this case, all we want is to derive an estimate for a drug half life for this single replicate. We know from the test of the one- vs- two-phase fits that the parameters from the one-phase model is most appropriate for our objective. This experiment will be repeated independently multiple times, including analysis of theophylline decay in microsomes prepared from control subjects, unexposed to cigarette smoke. The half-life values will serve as the response variables for this comparison. Thats all done in the next chapter. That half-life is related to the decay rate constants in the output above by the relationship \\(k=\\frac {log(2)}{t_{1/2}}\\). To keep things less confusing, lets stick to solving this by discussing the \\(k\\) values for now. Well calculate \\(t_{1/2}\\) once that decision is made. 40.2.8.1 Which rate constant parameter is our estimate? We have to make a decision about which is the right \\(k\\) value out of all the \\(k\\) values in the output above. Because cigarette smoke is known to induce an enzyme that more rapidly metabolizes theophylline, there was a scientific basis to believe that the data might be better fit by a two-phase decay model. That model produces two \\(k\\) values, \\(kf\\) and \\(ks\\). The one-phase model produces only a single \\(k\\) value. Which of these models to accept? Im a fan of the F-test approach, mostly because Ive used it a lot in the past. Im agnostic about whether this or the AIC comparison is a better way to go about selecting the best fit. Id need to run some simulations to convince my self one is better than the other for a given problem. The results of the anova F test comparing the two fits guide the decision. That results indicates that the difference in residual variation between the one and two phase models is not greater than would be expected if the two models fit the data differently. Therefore, we accept the most parsimonious model as the appropriate fit for the data, which is the one-phase model. Lets call the regression coefficients up from that one-phase model fit to have another look: kable(tidy(nlfit1)) term estimate std.error statistic p.value k 0.0419568 0.0021520 19.49628 0 ylo 12.3232038 0.7233171 17.03707 0 yhi 92.5573834 2.1770713 42.51463 0 For this replicate, \\(k=0.04195679\\). Thats it. All that data and all of this analysis boils down to extracting just that one simple number. In fact, we really dont need anything else in the table. The span between yhi and ylo in this particular problem has no great biological importance. It just says the signal to noise in the system is about an 8X range. The t-test statistics in the table are created by dividing the estimate value by the standard error. They tests the null hypothesis that the parameter estimate value is equal to zero. However, this serves no real inferential purpose from this one replicate because this is just an n=1 experiment. All of the data points within this sample are intrinsically-linked. This t-test would only be useful if each data point were independent of all other data points. But thats not this experimental design. 40.2.9 Alternate analysis In the example above we did the regression on the average of the technical replicates. Thats not a bad idea sometimes, particularly when the degrees of freedom are fairly low (which is not the case here). First, average the technical reps for each time point: #first, average the duplicates theopOne.avg &lt;- theopOne %&gt;% group_by(min) %&gt;% summarise(theo=mean(theo)) %&gt;% add_column(id=&quot;R&quot;, .before=T) ## `summarise()` ungrouping output (override with `.groups` argument) Next, run a one-phase first order decay nonlinear regression on the technical replicates. What youll see is that the parameter estimates are identical to their values when the duplicates were regressed. : fit1a &lt;- nlsLM( theo~(((yhi-ylo)*exp(-k*min))+ylo), data=theopOne.avg, start=list( yhi=100, ylo=15, k=0.693/20) ) kable(tidy(fit1a), caption=&quot;One-phase Fit on Average of Duplicates&quot;) Table 40.1: One-phase Fit on Average of Duplicates term estimate std.error statistic p.value yhi 92.5573833 1.6364812 56.55878 0 ylo 12.3232038 0.5437097 22.66504 0 k 0.0419568 0.0016177 25.93663 0 kable(glance(fit1a), caption=&quot;One-phase Fit on Average of Duplicates&quot;) Table 40.1: One-phase Fit on Average of Duplicates sigma isConv finTol logLik AIC BIC deviance df.residual nobs 2.951872 TRUE 0 -188.5743 385.1486 394.4716 636.0889 73 76 fit2a &lt;- nls( theo~((pf*((yhi-ylo)*exp(-kf*min)))+ ((1-pf)*((yhi-ylo)*exp(-ks*min)))+ ylo), data=theopOne.avg, start=list( yhi=115, ylo=15, pf=0.4, kf=0.06, ks=0.02) ) kable(tidy(fit2a), caption=&quot;Two-phase Fit on Average of Duplicates&quot;) Table 40.1: Two-phase Fit on Average of Duplicates term estimate std.error statistic p.value yhi 94.8670140 2.1294364 44.550292 0.0000000 ylo 10.2726207 2.9085073 3.531922 0.0007295 pf 0.6233894 0.5583784 1.116428 0.2680029 kf 0.0615519 0.0304242 2.023119 0.0468272 ks 0.0226730 0.0225941 1.003492 0.3190314 kable(glance(fit2a), caption=&quot;Two-phase Fit on Average of Duplicates&quot;) Table 40.1: Two-phase Fit on Average of Duplicates sigma isConv finTol logLik AIC BIC deviance df.residual nobs 2.884241 TRUE 7e-06 -185.7572 383.5144 397.4988 590.6379 71 76 The regression arrives at the exact same value for the rate constant, \\(k\\) as it does for when the regression was on the duplicates. The deviance (SS) and other measures of variability are lower, as are the degrees of freedom. 40.3 Summary In this chapter we went through the fairly common problem of running an analysis on technical duplicate measurements within one sample replicate. The first step is to visualize the data and to try to plot a regression line on it. That points you in the right direction in terms of regression model selection. In this case, we went through a process of running regressions on two nested models before deciding which fit better. Thats something that should be planned in advance because it involves some decisions that could generate biased solutions. Sometimes the scientific issue of choosing between models is not there, and youd just fit a model and grab the parameters of interest. "],["nonlinearreplicates.html", "Chapter 41 Nonlinear regression of independent replicates 41.1 The problem 41.2 The dataset 41.3 Munging for regression analysis 41.4 T-test on half-lives 41.5 Conclusion 41.6 Summary figure", " Chapter 41 Nonlinear regression of independent replicates library(tidyverse) library(broom) library(viridis) library(knitr) library(nlfitr) library(minpack.lm) This chapter illustrates how to analyze independent replicates of nonlinear data while testing an hypothesis that a predictor variable changes an outcome response. To some extent, this is a chapter about munging a lot of data into a form for nonlinear regression. Then it provides one common approach to conducting statistical inference on the results. 41.1 The problem The scientific prediction is that exposing rats to cigarette smoke changes microsomal theophylline metabolism. Specifically, exposure of cigarette smoke will induce CYP2A1, leading to more rapid theophylline degradation. A hypothesis to test that prediction is that theophylline half-lives differ in microsomes from smoke exposed animals compared to unexposed. The half-life of a drug, the time it takes for a given drug level to be reduced by half, is a critical parameter in drug metabolism studies. The metabolism of many (not all) drugs occurs by first order kinetics: \\[C=C_0\\times e^{-kt}\\] Here, \\(C\\) is the concentration of drug at time \\(t\\), \\(C_0\\) is the concentration at time zero. And \\(k\\) represents a first order rate constant, from which half-live values can be derived: \\[k=\\frac{log(2)}{t_{1/2}}\\] The data set contains the results of an experiment comparing in vitro theophylline metabolism using rat liver microsomal preps. One group of rats were housed in an environmental chamber laden with cigarette smoke. A second group serving as control was not exposed to smoke. Microsomes were prepared from each individual subjected to these treatments. A non-saturating level of theophylline was added to microsomes and then measured (in ng/ml units) as a function of time in duplicate. If CYP1A2 is induced by cigarette smoke in this model, theophylline should decay faster. One phase half-lives will be calculated by an exponential decay function whose formula for regression in R is \\[y=y_{lo}+(y_{hi}-y_{lo})e^{-kx}+y_{lo}\\] where k is the rate constant, x is time, in minutes, and yhi and ylo are the maximal and minimal levels of theophylline within each run. Someone might wonder how I knew to select this model for this problem? That comes from experience and scientific judgement. When were interested in some phenomenon and notice someone is conducting nonlinear modeling, pay close attention to the nonlinear model they used! 41.2 The dataset The experimental data are in two separate csv files. One file contains the replicates for the smoke-exposed rats (subjects Q, R and S). The other file contains the unexposed rats (subjects T, U and V). Each replicate is comprised of duplicate measurements. Its not unusual in this kind of work for the absolute values for the response variable to vary between replicates but be tight within replicates. #stringsAsFactors prevents some warnings in later steps smokers &lt;- read.csv(&quot;datasets/smoketheo.csv&quot;, stringsAsFactors = F) str(smokers) ## &#39;data.frame&#39;: 61 obs. of 8 variables: ## $ min : int 0 2 4 6 8 10 12 14 16 18 ... ## $ smoke: chr &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; ... ## $ q1 : num 194 184 172 148 132 ... ## $ q2 : num 188 177 163 152 136 ... ## $ r1 : num 161 141 118 122 108 ... ## $ r2 : num 159 139 135 128 102 ... ## $ s1 : num 236 208 198 178 169 ... ## $ s2 : num 228 216 204 185 172 ... nonsmokers &lt;- read.csv(&quot;datasets/nosmoketheo.csv&quot;, stringsAsFactors = F) str(nonsmokers) ## &#39;data.frame&#39;: 61 obs. of 8 variables: ## $ min : int 0 2 4 6 8 10 12 14 16 18 ... ## $ smoke: chr &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; ... ## $ t1 : num 280 266 251 242 232 ... ## $ t2 : num 271 266 264 242 232 ... ## $ u1 : num 273 260 255 242 228 ... ## $ u2 : num 262 259 259 250 232 ... ## $ v1 : num 182 176 169 161 151 ... ## $ v2 : num 184 185 171 161 148 ... 41.3 Munging for regression analysis 41.3.1 Average the technical replicates We have a few data processing steps to clean things up. First, lets average the technical duplicates for each time point. smokers1 &lt;- group_by(smokers, smoke, min) %&gt;% summarise( q=mean(c(q1,q2)), r=mean(c(r1,r2)), s=mean(c(s1,s2)) ) ## `summarise()` regrouping output by &#39;smoke&#39; (override with `.groups` argument) nonsmokers1 &lt;- group_by(nonsmokers, smoke, min) %&gt;% summarise( t=mean(c(t1,t2)), u=mean(c(u1,u2)), v=mean(c(v1,v2)) ) ## `summarise()` regrouping output by &#39;smoke&#39; (override with `.groups` argument) #check to see if it worked! str(smokers1); str(nonsmokers1) ## tibble [61 x 5] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ smoke: chr [1:61] &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; &quot;Y&quot; ... ## $ min : int [1:61] 0 2 4 6 8 10 12 14 16 18 ... ## $ q : num [1:61] 191 181 168 150 134 ... ## $ r : num [1:61] 160 140 126 125 105 ... ## $ s : num [1:61] 232 212 200 182 171 ... ## - attr(*, &quot;groups&quot;)= tibble [1 x 2] (S3: tbl_df/tbl/data.frame) ## ..$ smoke: chr &quot;Y&quot; ## ..$ .rows: list&lt;int&gt; [1:1] ## .. ..$ : int [1:61] 1 2 3 4 5 6 7 8 9 10 ... ## .. ..@ ptype: int(0) ## ..- attr(*, &quot;.drop&quot;)= logi TRUE ## tibble [61 x 5] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ smoke: chr [1:61] &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; ... ## $ min : int [1:61] 0 2 4 6 8 10 12 14 16 18 ... ## $ t : num [1:61] 276 266 257 242 232 ... ## $ u : num [1:61] 267 260 257 246 230 ... ## $ v : num [1:61] 183 180 170 161 149 ... ## - attr(*, &quot;groups&quot;)= tibble [1 x 2] (S3: tbl_df/tbl/data.frame) ## ..$ smoke: chr &quot;N&quot; ## ..$ .rows: list&lt;int&gt; [1:1] ## .. ..$ : int [1:61] 1 2 3 4 5 6 7 8 9 10 ... ## .. ..@ ptype: int(0) ## ..- attr(*, &quot;.drop&quot;)= logi TRUE 41.3.2 Create one table There are a couple of ways to combine the data into one table. One way is to convert each table into long format, before binding them together. smokers1.1 &lt;- gather(smokers1, id, theo, -smoke, -min) nonsmokers1.1 &lt;- gather(nonsmokers1, id, theo, -smoke, -min) theodata &lt;- bind_rows(smokers1.1, nonsmokers1.1) theodata ## # A tibble: 366 x 4 ## # Groups: smoke [2] ## smoke min id theo ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Y 0 q 191. ## 2 Y 2 q 181. ## 3 Y 4 q 168. ## 4 Y 6 q 150. ## 5 Y 8 q 134. ## 6 Y 10 q 130. ## 7 Y 12 q 130 ## 8 Y 14 q 106. ## 9 Y 16 q 104. ## 10 Y 18 q 93.8 ## # ... with 356 more rows 41.3.3 Plot the data Its a good idea to visualize each replicate prior any transformations. Its not clear from this view that cigarette smoke has any effect on half-life, is it? ggplot(theodata, aes(min, theo, group=id))+ geom_line()+ geom_point(aes(color=smoke))+ scale_color_viridis(discrete=T) Figure 41.1: Replicate rat microsomal theophylline metabolism data. 41.3.4 Run the regressions We dont want to write six regressions by hand! We take advantage of the fact that nls can operate on a subset of a data set. sapply feeds values of reps into the function one-by-one, each time generating an nls object for that replicate output is a list of six nls class objects; lists are a pain output &lt;- sapply( reps &lt;- c(&quot;q&quot;, &quot;r&quot;, &quot;s&quot;, &quot;t&quot;, &quot;u&quot;, &quot;v&quot;), function(reps) nlsLM(theo~(yhi-ylo)*exp(-k*min)+ylo, start=list( yhi=200, ylo=50, k=0.03), data=subset(theodata, id==reps) ), simplify = F ) Heres how to do the same thing using nlfitr. The output includes parameter estimates for each of the independent replicates nloutput &lt;- sapply( reps &lt;- c(&quot;q&quot;, &quot;r&quot;, &quot;s&quot;, &quot;t&quot;, &quot;u&quot;, &quot;v&quot;), function(reps) fitdecay1(min, theo, data=subset(theodata, id==reps), k=0.03, ylo=50, yhi=200, weigh=F), simplify = F ) nloutput ## $q ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.04583 30.12352 189.92881 ## residual sum-of-squares: 1023 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 ## ## $r ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.04408 15.86992 151.47476 ## residual sum-of-squares: 1020 ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 1.49e-08 ## ## $s ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.04078 35.35931 225.59047 ## residual sum-of-squares: 1025 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 ## ## $t ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.02302 25.17857 276.74741 ## residual sum-of-squares: 540.9 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 ## ## $u ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.02359 23.21790 273.80751 ## residual sum-of-squares: 793.5 ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 1.49e-08 ## ## $v ## Nonlinear regression model ## model: theo ~ (yhi - ylo) * exp(-1 * k * min) + ylo ## data: data ## k ylo yhi ## 0.02403 16.73564 185.23713 ## residual sum-of-squares: 867.3 ## ## Number of iterations to convergence: 4 ## Achieved convergence tolerance: 1.49e-08 41.3.5 Clean up regression results The scientific objective is to derive half-life values and then compare them between the two smoking conditions. Regression functions produce a lot of information. We only care for the rate constant. Theres really no scientific reason to compare the yhi or ylo parameters in this particular case. So the focus of the regression is to extract the rate constants k for each of the independent replicates from which half-life values will be calculated. The code below accomplishes that and a bit more. The output for one nonlinear regression has a lot of extraneous information. The broom package has tools like tidy that help clean it up. onephaseFits is just a tidy way of listing all of the parameter estimates from the output list We use kable to print out only the rate constants, k onephaseFits &lt;- bind_rows( lapply(nloutput, tidy)) %&gt;% add_column( reps=rep(reps, each=3), .before=T) kable(onephaseFits[seq(1,18,3),]) reps term estimate std.error statistic p.value q k 0.0458275 0.0014069 32.57250 0 r k 0.0440833 0.0016009 27.53712 0 s k 0.0407774 0.0010734 37.98833 0 t k 0.0230214 0.0004460 51.62305 0 u k 0.0235947 0.0005451 43.28805 0 v k 0.0240264 0.0008510 28.23277 0 41.4 T-test on half-lives We now just need to create a results table for the half-lives, which well use to pass into a t-test function. results &lt;- select( onephaseFits[seq(1,18,3),], reps, estimate ) %&gt;% mutate( halflife=log(2)/estimate ) %&gt;% bind_cols(smoke=rep(c(&quot;Y&quot;, &quot;N&quot;), each=3) ) kable(results) reps estimate halflife smoke q 0.0458275 15.12513 Y r 0.0440833 15.72359 Y s 0.0407774 16.99832 Y t 0.0230214 30.10882 N u 0.0235947 29.37728 N v 0.0240264 28.84942 N group_by(results, smoke) %&gt;% summarise( mean=mean(halflife), sd=sd(halflife) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## smoke mean sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 N 29.4 0.632 ## 2 Y 15.9 0.957 And now, finally, the t-test. This evaluates the null hypothesis that theophylline half-lives in smoking and nonsmoking microsomes are the same. t.test(halflife~smoke, data=results, var.equal=T) ## ## Two Sample t-test ## ## data: halflife by smoke ## t = 20.382, df = 4, p-value = 3.421e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 11.65775 15.33457 ## sample estimates: ## mean in group N mean in group Y ## 29.44518 15.94901 41.5 Conclusion Reject the null hypothesis. The data indicate that. 41.5.0.1 Write up Exposure to cigarette smoke about doubles the rate of theophylline metabolism in rat microsomes. The half-life of the drug is reduced from 29 +/- 0.6 min to 16 +/- 0.9 min (mean +/- SD, 95% confidence interval of the difference is 11.6 to 15.3 min, unpaired t-test, p=3.4e-5). 41.6 Summary figure The figure above is not quite ready for publication or presentation. The absolute values of theophylline differ from replicate to replicate. The figure also fails to convey, from a glance, that the smoke exposure has any effect. It is very simple to rescale the data and replot. There are a few conceivable ways to do this, but one is percent of maximum within each replicate. theodatapm &lt;- group_by(theodata, id) %&gt;% mutate(pmax=100*theo/max(theo)) theodatapm ## # A tibble: 366 x 5 ## # Groups: id [6] ## smoke min id theo pmax ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Y 0 q 191. 100 ## 2 Y 2 q 181. 94.7 ## 3 Y 4 q 168. 87.9 ## 4 Y 6 q 150. 78.6 ## 5 Y 8 q 134. 70.3 ## 6 Y 10 q 130. 68.0 ## 7 Y 12 q 130 68.2 ## 8 Y 14 q 106. 55.7 ## 9 Y 16 q 104. 54.4 ## 10 Y 18 q 93.8 49.1 ## # ... with 356 more rows Finally, a plot of the results. A couple of important things happen to the data on-the-fly within the ggplot function. Means and standard deviations are calculated to derive data points with error bars. And the nonlinear regression best fit line is calculated and drawn. This is a convincing result showing a marked effect of cigarette exposure on caffeine metabolism. ggplot(theodatapm, aes(min, pmax, color=smoke, group=smoke))+ stat_summary(fun.data= &quot;mean_sdl&quot;, fun.args = list(mult=1) )+ stat_smooth(method=&quot;nls&quot;, method.args=list( start=c(yhi=100, ylo=10, k=0.03)), formula=&quot;y~ylo+((yhi-ylo)*exp(-k*x))&quot;, se=F )+ labs(y=&quot;Theophylline, % of maximum&quot;, y=&quot;minutes&quot; )+ scale_color_viridis(discrete=T) "],["multregress.html", "Chapter 42 Multiple regression 42.1 The linear regression model 42.2 Multiple regression models 42.3 Experimental multiple regression 42.4 Multiple linear mixed models 42.5 Multiple regression of observational data", " Chapter 42 Multiple regression The previous chapters on linear and nonlinear regression deal with fairly specific use cases for regression. Specifically, youll note they involved only one predictor variable, which was continuous. The goal of those procedures was to apply linear or nonlinear models to extract the regression model parameters. These parameters imply certain biological properties (e.g., \\(K_D\\), \\(V_{max}\\), kinetic rate constants, etc) that are of primary interest when using the method. Linear regression is also useful to analyze continuous responses driven by discrete predictor variables. Furthermore, regression can be performed on responses associated with multiple predictor variables, some of which might be continuous and some of which might be discrete. Multiple regression or multiple linear regression is the most common jargon used to describe this extension of the linear regression to a comprehensive array of experimental designs. Thats the focus of this chapter. Weve actually been in this neighborhood previously, but called it other names. ANOVA and t-tests are regressions. Experimental biologists tend to use t-tests and ANOVA to analyze experiments that involve the use of discrete predictor variables. Why? Mostly out of habit. Those procedures are geared to alerting us to comparing effects of treatment and making decisions about whether an experiment worked or not. But that same information lies within multiple regression analysis, which if used otherwise would generate identical answers and inference as do t-tests and ANOVA. Multiple regression is used more commonly in the social and public health sciences. One reason is cultural, as for experimental biologists, thats just the way things are done in those fields. In part its also because much of their data is observational. As such, they tend to deal with data sets that have many more predictor variables than what an experimental biologist can comfortably manipulate when testing hypotheses. Furthermore, those fields emphasize weighing how much a predictor variable or a group of variables of interest contributes to a response while controlling for the effects of other variables. Arguably, theres more of a focus on estimating variable effect sizes and less on comparing group effects to see if they differ. Multiple regression analysis lends itself better to this kind of inference than t-tests and ANOVA. Throughout the semester weve been working through a heuristic that begins with deciding whether your response variable is continuous or discrete, and working from there to decide what to do. This is that: For nominal response data use exact tests. For ordinal data use nonparametrics. For continuous response variables with two or fewer levels of a predictor variable, choose t-tests. For three or more levels, choose ANOVA if the predictor is factorial or regression if the predictor is equal interval. When measurements are intrinsically-linked analyze using paired/related/repeated measure versions of the tests. Heres an optional heuristic: Analyze your data using a multiple regression model Having said that, multiple linear regression takes some configuration to ensure it is cohesive with the overall experimental design. The functions, or at least their configurations, to use for non-guassian response variables (basically, those for generalized linear models) differs from those used for guassian variables. The same goes for completely independent replicates versus replicates having paired/related/repeated measures. In regression jargon, data comprising the latter are called multiple linear mixed models. Conducting inference with the latter is much more difficult than for straight multiple linear models. Im going to focus below, first, on illustrating how an experimental biologist would use multiple regression when doing factorial experiments. Later, well deal with more general uses for multiple regression modeling. 42.1 The linear regression model Recall the linear model we discuses for a continuous response variable, \\(Y\\) and its predictor variable, \\(X\\): \\[y_i=\\alpha+\\beta x_i +\\epsilon_i \\] The \\(\\beta\\) coefficient is a multiplier such that for every 1 unit change in the level of \\(x_i\\), \\(y_i\\) will change by \\(\\beta\\). If \\(\\beta = 0\\) there is no effect for any level of \\(X\\). In that case, the intercept \\(\\alpha\\) estimates the value of the response variable \\(Y\\). Thus \\(\\alpha\\) is equivalent to the population mean \\(\\mu\\) for \\(Y\\) in the absence of \\(X\\). The residual error \\(\\epsilon_i\\) is the residual difference between the values of \\(y_i\\) predicted by the model and the values for the data. These residuals are normally distributed and have a variance of \\(\\sigma^2\\). 42.2 Multiple regression models Multiple regression models have more than one predictor variables. For data containing \\(i=1, 2..n\\) independent replicates and \\(j=1, 2..p\\) predictor variables \\[y_i=\\alpha+\\beta_1 X_{i1}+\\beta_2 X_{i2}+...\\beta_p X_{ip} +\\epsilon_{i}\\] These can be either continuous or discrete, or any combination thereof. \\[X=\\{nominal; eg, genotype(+/+, -/+, -/-) \\\\~\\\\ ordinal; eg, education level (HS, College, PostGrad\\\\~\\\\continuous; eg, time( 0, 1, 2, 4, ..30\\ min)\\}\\] 42.3 Experimental multiple regression Heres a simulation of a simple factorial experiment, comparing responses of a stimulus and its negative control. Every measurement is independent of all other measurements. Thus, the sample has 6 independent replicates. The predictor variable, FactorA, comes at two levels: control and stimulus. The response is a continuous variable. In your own mind substitute things for these. For me, the response is a transcription factor-driven luciferease activity, a negative control, and an agonist of a receptor is the stimulus. We obtain the following random sample: set.seed(1234) factorA &lt;- rep(c(&quot;control&quot;, &quot;stimulus&quot;), each=3) response &lt;- c(rnorm(3, 50, 1), rnorm(3,75,1)) data &lt;- data.frame(factorA, response) data ## factorA response ## 1 control 48.79293 ## 2 control 50.27743 ## 3 control 51.08444 ## 4 stimulus 72.65430 ## 5 stimulus 75.42912 ## 6 stimulus 75.50606 The linear model for this experiment can be expressed as follows: \\[response=control+\\beta \\times stimulus + residual\\] The model formula can be configured in several optional ways. Adding the term +0 to the model formula in the lm function suppresses the intercept, allowing for a glance at the sample group means. These make sense with what was coded above. model &lt;- lm(response~factorA+0, data) model ## ## Call: ## lm(formula = response ~ factorA + 0, data = data) ## ## Coefficients: ## factorAcontrol factorAstimulus ## 50.05 74.53 Knowing those group mean values helps to understand the meaning of the regression coefficients derived from the more conventional approach, which is not to suppress the intercept. model &lt;- lm(response~factorA, data) model ## ## Call: ## lm(formula = response ~ factorA, data = data) ## ## Coefficients: ## (Intercept) factorAstimulus ## 50.05 24.48 The two coefficients above are estimates of the linear models parameters. The intercept and the factorAstimulus serve as estimates of the true population values for \\(\\control\\) and \\(\\beta\\) in our linear model, respectively. We can rewrite the linear model with those estimates as \\[y=50.05+24.48x\\] Since \\(x\\) represents a factorial variable, we just assign it a value of one, when a given factor level is present, or zero when it is not. Thus, the response value in the presence of the stimulus is \\(y=50.05+24.48=74.53\\), which is the mean of the stimulus group. When the stimulus is absent, than \\(x=0\\), and the linear model predicts a response of \\(y=50.05\\), which you can see is both the value of the intercept and the mean of the control group. The coefficient for the stimulus term in this model formula is not the mean of the response to the stimulus, but is instead the difference between between the mean response to the stimulus and the mean response of the control. Perhaps that reminds you of the numerator for the unpaired t-test, between two levels of a factor, control and stimulus? \\[t=\\frac{mean_{stimulus}-mean_{control}}{SE}\\] Lets see if a t-test between stimulus and control gives us the same result as the linear regression. First, lets complete the linear model analysis. summary(model) ## ## Call: ## lm(formula = response ~ factorA, data = data) ## ## Residuals: ## 1 2 3 4 5 6 ## -1.2587 0.2258 1.0328 -1.8755 0.8993 0.9762 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.0516 0.8155 61.37 4.22e-07 *** ## factorAstimulus 24.4782 1.1534 21.22 2.91e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.413 on 4 degrees of freedom ## Multiple R-squared: 0.9912, Adjusted R-squared: 0.989 ## F-statistic: 450.4 on 1 and 4 DF, p-value: 2.914e-05 Focus on the t-value for the factorAsstimulus coefficient. That t-test determines whether the coefficient value differs from zero. The coefficient value is, again, the difference between the mean responses of the control and the stimulus. We put that in the numerator and we put the SE of that difference in the denominator: \\[t=\\frac{24.4782}{1.1534}=21.22\\] Woohoo! To be clear, when the predictor variable was continuous, we previously discussed these \\(\\beta\\) coefficients as the slope of the linear regression line. Thats still the case here. R coerces a distance of 1 on the abscissa between the factorA values of control and stimulus. Thus, the difference between their means \\(\\Delta y\\) divided by 1 \\(\\Delta x\\) is the slope of an imaginary line between them. Now lets look at an unpaired t-test between the two factors. t.test(response~factorA, data, var.equal=T) ## ## Two Sample t-test ## ## data: response by factorA ## t = -21.224, df = 4, p-value = 2.914e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -27.68045 -21.27600 ## sample estimates: ## mean in group control mean in group stimulus ## 50.05160 74.52983 Woohoo! Except for the sign, the t-statistic -21.22 is identical to what was derived in the linear regression! This t-test is calculated as follows: \\[t=\\frac{50.05160-74.52983}{1.153353}=-21.22352\\] That denominator is the standard error of the difference between means, which is not part of the t-test output, but can be calculated by hand by dividing the t-statistic value by the value for the difference between the means. You can see that it equals the coefficient value within the lm output. Thus, by comparing lm and t.test output when run on the same data set we see that the same basic calculations are performed. Theyre just presented differently. Put another way, a t-test is actually a linear regression using a factorial variable at two levels. The t.test output is more explicit about group means, and it provides a confidence interval for the difference between group means, while hiding its calculation for the standard error for that difference. The lm is explicit about the difference between the group means, and gives you that standard error, but only shows one group mean, that for the intercept. But theyre each making the same signal-to-noise calculation (t-test)! The real differences are more related to interpretation. We use the t.test to ask a simple threshold question: Do the two groups differ? Yes or no? The linear regression question is obviously related but just different enough to matter: How much does the stimulus affect the response relative to control? That distinction is important. In the context of the linear model, the result says, \"the stimulus accounts for 24.48 \\(Y\\) response units when controlled for the negative control group. Thus, you can use multiple linear regression to run the equivalent of an unpaired t-test. But you can also use it to declare the effect of a factorial variable relative to a reference response. Most practitioners of multiple regression use it for the latter purposes. 42.3.0.1 Why is that sign negative in the t-test?? Thats just a quirk of R, but its an important quirk to understand. When you give R these variables (at least in this format) it doesnt know what factor level you want to list first. So by default it arranges groups alphanumerically. The c in control comes before the s in stimulus, so R works with control first in both the linear regression and the t-test. In the t-test, the calculation for the difference between groups is 2nd from 1st, or \\(c-s\\). Thus the negative value for the t-statistic. The lm function is engineered to use the intercept as a reference. The linear regression assigns the lowest alphanumeric as the intercept, which it then subtracts from the mean response of the next variable, \\(s-c\\). 42.3.0.2 Specifying the intercept Often youll name variables the way you like. But youll want to specify which group should be the intercept. Usually its your control condition. In regression jargon, it is the condition by which the other variables will be controlled for. Using the relevel function provides you a bit more control to define the intercept value in linear regression compared to the default alphanumeric way. Heres some simulated data. The predictor variable, hart, has 4 levels: art, bart, cart, and dart. Its on an alphanumeric scale. Smelly is the response variable. As you can see, each level of hart yields a different smelly response. Even though I list the hart variable levels from d to a in the dataframe, R coerces them from a to d in the regression model, thereby forcing art to serve as the intercept art &lt;- rnorm(3, 50, 1) bart &lt;- rnorm(3, 100, 1) cart &lt;- rnorm(3, 0, 1) dart &lt;- rnorm(3, 200, 1) hart &lt;- rep(c(&quot;dart&quot;, &quot;cart&quot;, &quot;bart&quot;, &quot;aart&quot;), each=3) data &lt;- data.frame(hart, smelly=c(dart, cart, bart, art)) data ## hart smelly ## 1 dart 199.88971451 ## 2 dart 199.48899049 ## 3 dart 199.08880458 ## 4 cart -0.77625389 ## 5 cart 0.06445882 ## 6 cart 0.95949406 ## 7 bart 99.10996217 ## 8 bart 99.52280730 ## 9 bart 99.00161356 ## 10 aart 49.42526004 ## 11 aart 49.45336814 ## 12 aart 49.43554800 lm(smelly~hart, data) ## ## Call: ## lm(formula = smelly ~ hart, data = data) ## ## Coefficients: ## (Intercept) hartbart hartcart hartdart ## 49.44 49.77 -49.36 150.05 What do those coefficient values mean? Well, they are all in units of smelly, referenced to the level of smelly in the presence of art. Because it begins with the letter a, art is the lowest alphanumeric value of the four predictors. Therefore, it is assigned as the intercept factor. In smelly units, the intercept is the response to art; bart is about 50 more than art; cart is about 50 less than art; and dart is about 150 more than art. But lets say wed instead like to reference everything to the value of cart, which has a smelly value of 0. Wed use the relevel function for that. First we have to define the variable hart as a factorial variable explicitly. Then, we relevel the values of hart so that cart is referenced as the intercept. hart &lt;- factor(hart) #don&#39;t skip this step, without it the next gives an error!!! data$hart &lt;- relevel(hart, ref=&quot;cart&quot;) lm(smelly~hart, data) ## ## Call: ## lm(formula = smelly ~ hart, data = data) ## ## Coefficients: ## (Intercept) hartaart hartbart hartdart ## 0.08257 49.35549 99.12889 199.40660 The linear model for this data is \\[smelly = -0.658+50.5\\times art+100.1\\times bart+200.6 \\times dart\\] When \\(art=bart=0\\), \\(smelly=200\\) The relevel is particularly useful manipulation for experimentalists, who often have explicit negative controls in their data sets. Social and public health scientists use relevel when they want to control the response for one of their variables. For example, when they want to control for the age of subjects, they relevel the age variable to the intercept. In that way, all other variable coefficients are relative to the magnitude of the effect of age. 42.3.1 A one factor experiment with 3 groups T-tests are for comparing 2 groups. ANOVAs are for comparing 3 or more groups. It turns out, you can use multiple linear regression to mimic an ANOVA test, too. Lets add one more level for the predictor variable (and its responses) to the experiment. Since it now has 3 groups, and 9 independent replicates, we shouldnt do t-tests. We could do ANOVA, but we could also do a multiple regression model, too. That model is \\[response=control+\\beta_1 stimulus1+\\beta_2 stimulus2\\] set.seed(1234) factorA &lt;- rep(c(&quot;control&quot;, &quot;stimulus1&quot;, &quot;stimulus2&quot;), each=3) response &lt;- c(rnorm(3, 50, 1), rnorm(3,75,1), rnorm(3, 175, 1)) data &lt;- data.frame(factorA, response) model &lt;- lm(response~factorA, data) summary(model) ## ## Call: ## lm(formula = response ~ factorA, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.87553 -0.01280 0.01531 0.89930 1.03284 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.0516 0.6659 75.16 3.73e-10 *** ## factorAstimulus1 24.4782 0.9417 25.99 2.14e-07 *** ## factorAstimulus2 124.3865 0.9417 132.08 1.27e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.153 on 6 degrees of freedom ## Multiple R-squared: 0.9997, Adjusted R-squared: 0.9996 ## F-statistic: 9792 on 2 and 6 DF, p-value: 2.873e-11 Notice how the coefficient value for stimulus2 is the difference between the mean response value for stimulus2 (~175 units) and the mean response value for control (~50 units). Each coefficient t-test asks whether the value of the coefficient differs from zero. In linear regression with only a single predictor variable, that coefficient was the slope of a straight line through all the \\(xy\\) data pairs. In multiple linear regression you have to imagine separate straight lines connecting the mean intercept response value and the response values for each of the predictors. We can compare the regression model output to that for an ANOVA test on the data. Here is the equivalent of a one-way completely randomized ANOVA. Anova(model) ## Anova Table (Type II tests) ## ## Response: response ## Sum Sq Df F value Pr(&gt;F) ## factorA 26053 2 9792.1 2.873e-11 *** ## Residuals 8 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F-test calculated in this ANOVA is the same as that calculated in the regression model. The F is a ratio of model variance to the residual variance. In the null F distribution, model and residual variance would be about equal. An extreme value of F means that the model explains a lot more of the variance than random residual differences. In this case, the model explains 9792.1 times more variance than does residual. If we were making ANOVA inferences, we would conclude that factorA has an effect. There are some differences between the means of the three groups. If we were making a regression inference, wed predict values for \\(y\\) on the basis of this model \\[response = 50.05+24.48\\times stimulus1+124.39\\times stimulus2\\] and wed say that stimulus1 and stimulus 2 cause 24.48- and 124.39-more units of response compared to control. The t-test results also allow us to conclude the two groups differ from the control. 42.3.2 A two factor experiment with 6 groups Now well add a second factorial variable, factorB, to the design above. FactorB has two levels. The simulated design is akin to a two-way completely randomized ANOVA. Well also simulate in an interaction effect between the two variables. set.seed(1234) factorA &lt;- rep(rep(c(&quot;control&quot;, &quot;stimulus1&quot;, &quot;stimulus2&quot;), each=3), 2) factorB &lt;- rep(c(&quot;cofactor1&quot;, &quot;cofactor2&quot;), each=9) response &lt;- c(rnorm(3, 50, 1), rnorm(3,75,1), rnorm(3, 175, 1), rnorm(3, 100, 1), rnorm(3,300,1), rnorm(3, 700, 1) ) data &lt;- data.frame(factorA, factorB, response) There are a handful of linear models we could run on this. They should be scientifically driven. Since we wouldnt design an experiment like this unless we thought there would be an interaction between the two factors, well focus on that. The coefficients for the interacting model would serve as effect sizes to describe the level of interaction that occurs. Coefficients can be confusing. Heres a way to clarify whats going on. It turns out, a models intercept is calculated differently, depending upon how the model is defined. First, a factor-less model. This output represents the grand mean of all the values in the data. lm(response~1, data) ## ## Call: ## lm(formula = response ~ 1, data = data) ## ## Coefficients: ## (Intercept) ## 233 Next is a linear model just for the 3 levels of factorA. The intercept is the mean for all the data in the control groups under both cofactor1 and cofactor2. lm(response~factorA, data) ## ## Call: ## lm(formula = response ~ factorA, data = data) ## ## Coefficients: ## (Intercept) factorAstimulus1 factorAstimulus2 ## 74.63 112.67 362.33 Heres a linear model for the second variable alone. The intercept represents the mean of all the groups under cofactor1 lm(response~factorB, data) ## ## Call: ## lm(formula = response ~ factorB, data = data) ## ## Coefficients: ## (Intercept) factorBcofactor2 ## 99.67 266.59 Now we have the linear model for the two factors combined. This intercept is the sum of the intercepts for factorA and factorB, less the grand mean for all the data. The negative value is a hint of an interaction effect between the two factors. lm(response~factorA+factorB, data) ## ## Call: ## lm(formula = response ~ factorA + factorB, data = data) ## ## Coefficients: ## (Intercept) factorAstimulus1 factorAstimulus2 factorBcofactor2 ## -58.66 112.67 362.33 266.59 Finally we have the full linear model for the two factors combined, including an interaction term. Here the intercept is the value of control. All coefficients values are the difference between their absolute effect sizes and the value for control (intercept). lm(response~factorA*factorB, data) ## ## Call: ## lm(formula = response ~ factorA * factorB, data = data) ## ## Coefficients: ## (Intercept) factorAstimulus1 ## 50.05 24.48 ## factorAstimulus2 factorBcofactor2 ## 124.39 49.16 ## factorAstimulus1:factorBcofactor2 factorAstimulus2:factorBcofactor2 ## 176.39 475.89 Since the values each discrete predictor variable level can take on is either 1 (if present) or 0 (if not), all of these coefficient values serve as an effect size for the indicated group, relative to control (intercept). The linear model is \\[Y=50.05+24.48\\times stimulus1+124.39\\times stimulus2+49.16\\times cofactor2+176.39\\times stimulus1/cofactor2 +475.89\\times stimulus2/cofactor2\\] summary(lm(response~factorA*factorB, data)) ## ## Call: ## lm(formula = response ~ factorA * factorB, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.87553 -0.18276 -0.00135 0.37825 1.03284 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.0516 0.5260 95.15 &lt; 2e-16 *** ## factorAstimulus1 24.4782 0.7439 32.91 3.93e-13 *** ## factorAstimulus2 124.3865 0.7439 167.21 &lt; 2e-16 *** ## factorBcofactor2 49.1599 0.7439 66.08 &lt; 2e-16 *** ## factorAstimulus1:factorBcofactor2 176.3929 1.0520 167.67 &lt; 2e-16 *** ## factorAstimulus2:factorBcofactor2 475.8913 1.0520 452.36 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9111 on 12 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 2.183e+05 on 5 and 12 DF, p-value: &lt; 2.2e-16 The model summary recapitulates the coefficient values. Using this configuration, all are relative to the intercept. Each t-tests determines whether that coefficient differs from zero. In this case, all coefficients differ from zero, meaning the factor level corresponding to each has some effect on the response over and beyond that of the control level. Note that these p-values are NOT adjusted for multiple comparisons. These coefficient values can be used to make declarative statements about effect sizes in response to specific levels of the variables: In the presence of cofactor1, Stimulus1 and 2 cause a 24 and 124 unit response, respectively, over control. But in the presence of cofactor2, they cause 176- and 475-unit responses, indicating a synergy interaction. Or something like that. The F test is a ratio of the variance associated with the model to the residual variance. Its not particularly useful other than to announce that the model explains the data better than does residual variation. Passing the model into an the Anova function from the car package provides F tests for the main effects of each factor and for the interaction, providing the familiar ANOVA way of interpreting the data if desired. Anova(lm(response~factorA*factorB, data)) ## Anova Table (Type II tests) ## ## Response: response ## Sum Sq Df F value Pr(&gt;F) ## factorA 412618 2 248548 &lt; 2.2e-16 *** ## factorB 319811 1 385289 &lt; 2.2e-16 *** ## factorA:factorB 173643 2 104597 &lt; 2.2e-16 *** ## Residuals 10 12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All in all, multiple regression is an alternative way to conduct analyses on data for which every replicate is independent from all others, such as in unpaired/completely randomized designs that are otherwise performed using t-tests or ANOVA. 42.4 Multiple linear mixed models Multiple regression can also be useful for analyzing data within which measurements are intrinsically-linked, such as in experiments that have paired/related/repeated measures. There are three key distinctions from the multiple regression above. First, well need to use the lmer function in the lme4 package (or the lme function in the nlme package), rather than lm in Rs base stats package. These use maximum likelihood estimation to calculate coefficient values, rather than least squares. Second, is cutting through the jargon. Third, it is dramatically more difficult to extract inference from mixed multiple regression modeling than from multiple regression modeling. The experimental researcher is probably better off sticking to ANOVA. There are exceptions to this. The coefficients solved for through linear regression are called the fixed effects of a linear model. They correspond to the true values in the sampled population the alpha and the beta. Recall that models are perfect data are not. So they are the perfect part of the model, making them fixed. The error term \\(\\epsilon\\) accounts for the extent by which actual data diverge from that predicted by these fixed effects. \\(\\eplison\\) exists to account for all the other unknown factors that conspire to influence a response. But we can control for some of this when we take multiple measures on each replicate. In regression jargon, the replicate-to-replicate variation is called the random effect. So mixed multiple linear regression models have a mixture of terms for the fixed and for the random effects within an experiment. Mixed models are therefore the linear regression analogs of paired/related measure designs. The sleepstudy data set in the lme4 package is one such mixed effect study. The study measured reaction times (in ms) under conditions of worsening sleep deprivation. Repeated reaction time measurements were taken from each of 18 subjects over a 9 day period of sleep deprivation. These 18 subjects represent a random sample of the type of person who might become sleep deprived. Heres a chart showing how it went for each subject. For some, reaction times are noticeably longer. For others, reaction lengthening is less bloody obvious. For subject 335 sleep deprivation might have slightly quickened reaction times! ggplot(sleepstudy, aes(Days, Reaction))+ geom_point()+ geom_smooth(method=&quot;lm&quot;, se=F)+ facet_wrap(~Subject) ## `geom_smooth()` using formula &#39;y ~ x&#39; Heres the question everybody wants answered from this study: What effect does sleep deprivation have on reaction time? Note, the question is NOT, is the effect of sleep deprivation on reaction timesignificant?\"\" Thats much less compelling than actually quantifying the effect size. If we completely ignore the presumed correlation within the repeated measurements within each subject we would use the lm function in Rs base stats package. summary(lm(Reaction ~ Days, sleepstudy)) ## ## Call: ## lm(formula = Reaction ~ Days, data = sleepstudy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -110.848 -27.483 1.546 26.142 139.953 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 251.405 6.610 38.033 &lt; 2e-16 *** ## Days 10.467 1.238 8.454 9.89e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 47.71 on 178 degrees of freedom ## Multiple R-squared: 0.2865, Adjusted R-squared: 0.2825 ## F-statistic: 71.46 on 1 and 178 DF, p-value: 9.894e-15 This shows the average reaction time on day zero is 251.40 ms, and it lengthens by 10.47 ms each day of the sleep deprivation protocol. Both of these estimates differ from zero. The F test is for the main effect of Days and has an extreme value. Thus, the length of sleep deprivation changes reaction time. This variable accounts for about 28% of the overall variation in the data. Taken together, we can conclude that a period of sleep deprivation extends reaction times by about 10 ms daily. \\[Reaction=251.4+10.4\\times Days\\] But we shouldnt ignore the repeated measures of the design. And it would also be nice to have some estimate for the standard deviation of the effect. For those reasons we run a mixed model regression to account for the random effect of the 18 subjects. mm1 &lt;- lmer(Reaction ~ Days + (Days|Subject), REML=F, sleepstudy) summary(mm1) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (Days | Subject) ## Data: sleepstudy ## ## AIC BIC logLik deviance df.resid ## 1763.9 1783.1 -876.0 1751.9 174 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.9416 -0.4656 0.0289 0.4636 5.1793 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 565.48 23.780 ## Days 32.68 5.717 0.08 ## Residual 654.95 25.592 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.405 6.632 37.907 ## Days 10.467 1.502 6.968 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.138 The mixed model regression generates the identical estimates for the fixed effects as previously. What we have now that we didnt have before are standard deviation estimates for both the slope and the intercept coefficients. Those are found in the random effect output in the summary. So we would conclude that the reaction time is 251.4+/-24.7 ms, which increases with sleep deprivation by 10.47+/-5.92 ms daily. Unlike linear modeling, linear mixed modeling doesnt generate F tests. Thats mostly due to the fact that underlying maximum likelihood estimation doesnt lend itself to that. As far as inference goes, we have t values but not t-tests (the df is missing!!) 42.5 Multiple regression of observational data The Western Collaborative Group Study data below is epidemiological. The data are derived from cases taken in 1960-61. The focus was to study the relationship between behavior and cardiovascular disease risk in middle aged men. The data set below is comprised of over three thousand cases with 14 variables. Since I know a little bit about blood pressure, but mostly because blood pressure is a continuous variable, I thought it would be interesting to explore how the various cofactors might influence systolic blood pressure. So Ill use systolic blood pressure as a response variable in the regressions below. Some of the other variables will be used as predictor variables. I should make an important point. In practice you dont just crack open and explore a data set willy nilly. Im not the PI of the study. Im just writing another chapter on statistical methods using data to illustrate statistical concepts. The authors of the study were testing the hypothesis that the so-called type-A personality was at higher risk of coronary heart disease. They are bound by restricting their analysis to testing preset hypotheses. These systolic blood pressure readings are just a variable that were vacuumed up in the study design, but were not hypothesized as the intended response variable. data(wcgs) head(wcgs) ## id age0 height0 weight0 sbp0 dbp0 chol0 behpat0 ncigs0 dibpat0 chd69 ## 1 2001 49 73 150 110 76 225 2 25 1 0 ## 2 2002 42 70 160 154 84 177 2 20 1 0 ## 3 2003 42 69 160 110 78 181 3 0 0 0 ## 4 2004 41 68 152 124 78 132 4 20 0 0 ## 5 2005 59 70 150 144 86 255 3 20 0 1 ## 6 2006 44 72 204 150 90 182 4 0 0 0 ## typechd time169 arcus0 ## 1 0 1664 0 ## 2 0 3071 1 ## 3 0 3071 0 ## 4 0 3064 0 ## 5 1 1885 1 ## 6 0 3102 0 fit &lt;- lm(sbp0~age0+height0+weight0+ chol0+ncigs0+ dibpat0+chd69, wcgs) summary(fit) ## ## Call: ## lm(formula = sbp0 ~ age0 + height0 + weight0 + chol0 + ncigs0 + ## dibpat0 + chd69, data = wcgs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.215 -9.675 -1.969 7.325 101.331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 122.06552 7.99171 15.274 &lt; 2e-16 *** ## age0 0.40531 0.04625 8.764 &lt; 2e-16 *** ## height0 -0.85157 0.11915 -7.147 1.10e-12 *** ## weight0 0.23450 0.01423 16.477 &lt; 2e-16 *** ## chol0 0.02621 0.00593 4.420 1.02e-05 *** ## ncigs0 0.04247 0.01766 2.404 0.0163 * ## dibpat0 1.16580 0.50899 2.290 0.0221 * ## chd69 4.24199 0.94327 4.497 7.14e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.06 on 3134 degrees of freedom ## (12 observations deleted due to missingness) ## Multiple R-squared: 0.1295, Adjusted R-squared: 0.1275 ## F-statistic: 66.59 on 7 and 3134 DF, p-value: &lt; 2.2e-16 The model for systolic blood pressure level, in mmHg predicts \\[sbp=122+0.4/yr-0.85/inch+0.23/pound+0.026/mg/dl+0.04/cig/day+1.16\\times typeA+4.24\\times coronary event\\] 42.5.0.1 Sandbox Heres a simulated sample. The causes of the outcome are known. Only the multiple model predicts the outcome accurately. Knowing what covariates to include in the model is not straightforward. set.seed(123) covariate &lt;- sample(0:1, 100, replace=TRUE) exposure &lt;- runif(100,0,1)+(0.3*covariate) outcome &lt;- 2.0+(0.5*exposure)+(0.25*covariate) lm(outcome~exposure) ## ## Call: ## lm(formula = outcome ~ exposure) ## ## Coefficients: ## (Intercept) exposure ## 1.9888 0.6965 lm(outcome~exposure+covariate) ## ## Call: ## lm(formula = outcome ~ exposure + covariate) ## ## Coefficients: ## (Intercept) exposure covariate ## 2.00 0.50 0.25 "],["logregress.html", "Chapter 43 Logistic regression 43.1 Uses of logistic regression 43.2 Derivation of the logistic regression model 43.3 Stress and survival", " Chapter 43 Logistic regression Sometimes our experiments generate discrete outcome responses as either of two conditions: yes or no, up or down, in or out, absent or present, sick or healthy, pregnant or not, alive or dead. You get the idea. An event can either not happen, or it can happen. Since the beginning of this course weve been calling these sorted outcome variables. Other commonly used jargon refers to these as nominal, dichotomous, binary or binomial. To recall, proportions are the number of successes or events in a trial of size \\(n\\). Proportions take on the values from 0 to 1. A given proportion can serve as a point estimate of the sampled population. In a long run the proportion has some probability between the extremes of 0 and 1, which can be modeled using the binomial distribution. We discuss the statistical treatment of sorted data in Chapter 17 for relatively simple experimental designs involving one or two levels of a predictor variables. For example, the proportion of cells that survive after exposure to a toxin compared to a control. Or the proportion of cells on a dish in which the nuclei are stained positive for an antigen after a stimulus, compared to a control. Logistic regression extends proportion analysis. It provides a way to analyze data sets in which more than two proportions must be evaluated simultaneously or for when the response is binary over a range of predictor values. Were now at the point where were able to close a loop of sorts. If you recall from way back at the beginning of the course, we dealt with tests that compare two proportions. In other words, we were only equipped to perform experiments comparing two groups. Now that we are more knowledgeable about regression techniques, we can apply regression to deal with more than two proportions. For measured outcome data, we have t-tests to compare two or fewer groups, but then acquired facility using ANOVA and linear regression (or multiple linear regression) for experiments with more than two groups to compare. For ordered outcome data, we have sign rank and rank sum tests for two samples or less, and have Kruskal Wallis or Friedmans test for more groups to compare. With logistic regression we can now analyze experiments that produce sorted outcome responses and that involve more than two groups of predictors! 43.1 Uses of logistic regression Use logistic regression when an outcome is a nominal categorical variable and for isolating the influence of individual variables on a response. predicting the odds and probability of a response. measuring the interplay of multiple variables in a response. making inferential decisions about whether a predictor causes a response. 43.1.1 Sidebar: Doing logistic regression is machine learning If you get logistic regression, you are well down the road towards becoming a machine learning savant. ML works a bit like this: A logistic regression model is trained by fitting to a data set. The data set is comprised of one or more explanatory variables and a simple, nominal outcome variable. The best-fit regression coefficients, one for every explanatory variable, are plugged into the ML algorithm, which has learned to choose the correct outcomes on a case by case basis. 43.2 Derivation of the logistic regression model A simple linear model for an event probability in response to a continuous explanatory variable \\(X\\) has the form \\(Y=\\beta_0 +\\beta_1X + \\epsilon\\), where \\(Y\\) is a probability that can only take on values \\(0\\le P\\le 1\\). For now, well dismiss random error to simplify. It can be shown that in order for \\(Y\\) to meet the latter condition the following proportion is true (equation 1) \\[P(Y=1|X)=\\frac{exp(\\beta_0 +\\beta_1X)}{exp(\\beta_0+\\beta_1X)+1}\\] That relationship can be transformed algebraically to express the odds of an event, \\(\\frac{p}{1-p}\\), as an exponential function (equation 2) \\[\\frac{p}{1-p}=exp(\\beta_0+\\beta_1X)\\], which when transformed using the natural logarithm becomes a linear function on X. (equation 3) \\[log(\\frac{p}{1-p})=\\beta_0+\\beta_1X\\]. The left-sided term \\(log(\\frac{p}{1-p})\\) is referred to as the logit or as log odds. It represents, of course, the natural logarithm of the odds. Transforming eq3 using the exponential generates eq2. 43.2.1 Relationship of logit to odds to the model coefficients and probability Probabilities can be calculated from odds following exponential transformation of logit. Thus, for a linear logistic model: (equation 4) \\[odds=exp(logit)=exp(\\beta_0+\\beta_1X)\\] (equation 5) \\[p=\\frac{odds}{odds+1}\\] (equation 6) \\[for\\ one\\ coefficient, e.g: odds=exp(\\beta)\\] (equation 6) \\[percent \\ change=(odds-1)*100\\] 43.2.1.1 Graphical relationships between proportions, odds and logit Sometimes its helpful to visualize the relationships between these parameters graphically. The first graph, which is on a linear scale, illustrates how odds are an exponential function of probability. The second graph is on a semi-log scale. Log transformation of odds yields logit, which has a log-linear relationship with probability. a &lt;- seq(99, 1, -1) d &lt;- seq(1, 99, 1) p &lt;- a/(a+d) odds &lt;- p/(1-p) logit &lt;- log(p/(1-p)) ggplot()+ geom_point(aes(p, odds)) Figure 43.1: Odds are an exponential function of probability. ggplot()+ geom_point(aes(p, logit)) Figure 43.2: Odds are an exponential function of probability. The graph below provides, I think, the most intuitive way to think about what logit values mean. Their relationship to probability is almost switch-like. For example, a log odds value of 3 indicates that an outcome is highly probable. logit &lt;- seq(-6, 6, 1) p &lt;- exp(logit)/(1+exp(logit)) ggplot()+ geom_point(aes(logit, p))+ scale_x_continuous(breaks = -6:6) Figure 43.3: Changing the axis illustrates the nonlinear switch-like relationship between logit and probability. 43.2.2 Additional types of logistic regression models Logistic regression can be adapted to virtually any other experimental design described by a general linear model, a few examples are listed below. 43.2.2.1 one factorial explanatory variable \\(ln(\\frac{p}{1-p})=\\beta_0+\\beta_1 A\\) 43.2.2.2 two factorial explanatory variables \\(ln(\\frac{p}{1-p})=\\beta_0+\\beta_1 A+\\beta_2 B\\) 43.2.2.3 multiple explanatory variables of any type \\(ln(\\frac{p}{1-p})=\\beta_0+\\beta_1X_1+\\beta2X_2..+\\beta_nX_n\\) 43.2.2.4 Running logistic regression in R So as you can see, at a higher level, logistic regression is not really different than linear regression modeling. The model options determined by the predictor variables in the study. The right hand of the equation is some general linear model that accounts for the predictor variable(s) of your experiment. The left hand of the left hand of the equation is logit. We use the glm() function in R rather than the lm() function. The reason for that is logistic regression is a type of generalized linear model. These models generalize nonlinear functions (such as logit) as linear models. In linear and nonlinear regression (and also t-tests and ANOVA) model variation is accounted for using ordinary least squares (OLS) estimation. OLS serves as a common statistical thread that ties together the various parametric techniques, from t-tests to more complex nonlinear regression models and multiple regression. Instead of OLS, generalized linear models such as logistic regression use maximum likelihood estimation (MLE) to derive parameter values. MLE generates parameter value estimates through iterative calculations that maximize the likelihood the regression model produced the data that were actually observed. Why MLE and not OLS? The short answer is that generalized linear models are frequently used on data that violate the assumptions for valid use of OLS-based statistical methods. Logistic regression is a perfect example: OLS has no utility in the analysis of binary outcomes! 43.3 Stress and survival Blas(2007) measured plasma corticosterone(ng/ml) in nestling storks after inducing an experimental stress. The birds were then followed over 5 years, and recorded as either survived (0) or dead (1) by the end of that period. By convention, a value of 1 is assigned to events and 0 assigned to nonevents for binary outcome data. In this instance, an event was death. It is VERY important to recognize that every stork is statistically independent of every other stork. The experiment below has 34 independent replicates. Note how the outcome variable, death, is comprised of zeroes and ones. Nonevents and events. The variable cort is measured corticosterone levels in nestlings after they had received a stress. But it is used here as a predictor variable. It serves as an index of the severity induced in each nestling treatment. A common stress treatment delivered to each nestling is refactored to a continuous predictor variable that signals the impact of that treatment. cort &lt;- c(26,28.2,29.8,34.9,34.9,35.9,37.4,37.6,38.3,39.9,41.6,42.3,52,26.6,27,27.9,31.1,31.2,34.9,35.9,41.8,43,45.1,46.8,46.8,47.4,47.4,47.7,47.8,50.7,51.6,56.4,57.6,61.1) death &lt;- c(1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) stork &lt;- data.frame(cort, death) ggplot(stork, aes(cort, death))+ geom_point(shape=1, size=8, color=&quot;blue&quot;)+ labs(y=&quot;P(dead)&quot;, x=&quot;corticosterone, ng/nl&quot;) This plot can be a bit disorientating. Remember, deaths are events and so are assigned a value of 1. If you assume higher cortisol is proportional to stress intensity experienced by the birds, a quick glance suggests higher early stress events offer a survival advantage. Given the nature of the binary response variable, we can model it logistically as such: \\[logit(survive)=\\beta_0+\\beta_1(cort)\\] Lets run a quick linear regression. We use glm because we cant use lm on binomial response data, and we need to tell it what family of data for the dependent variable (response) to model. stork.model &lt;- glm(death~cort, family=binomial(), stork) summary(stork.model) ## ## Call: ## glm(formula = death ~ cort, family = binomial(), data = stork) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4316 -0.8724 -0.6703 1.2125 1.8211 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.70304 1.74725 1.547 0.1219 ## cort -0.07980 0.04368 -1.827 0.0677 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45.234 on 33 degrees of freedom ## Residual deviance: 41.396 on 32 degrees of freedom ## AIC: 45.396 ## ## Number of Fisher Scoring iterations: 4 43.3.1 Interpretation of output First, lets try to make sense of this graphically. Using augment from the broom package is a nifty way to see the predicted values of the fitted model. Well plot those out three different ways below to visualize the survival response to stress levels. In each graph, the fitted values of the model are used. Theyre just transformed in the latter two. augment(stork.model) ## # A tibble: 34 x 8 ## death cort .fitted .resid .std.resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 26 0.628 0.925 0.978 0.106 1.14 0.0353 ## 2 1 28.2 0.453 0.992 1.04 0.0868 1.14 0.0331 ## 3 1 29.8 0.325 1.04 1.08 0.0740 1.14 0.0312 ## 4 1 34.9 -0.0821 1.21 1.24 0.0432 1.13 0.0256 ## 5 1 34.9 -0.0821 1.21 1.24 0.0432 1.13 0.0256 ## 6 1 35.9 -0.162 1.25 1.27 0.0395 1.13 0.0251 ## 7 1 37.4 -0.282 1.30 1.32 0.0355 1.13 0.0253 ## 8 1 37.6 -0.298 1.31 1.33 0.0352 1.13 0.0254 ## 9 1 38.3 -0.353 1.33 1.35 0.0341 1.13 0.0260 ## 10 1 39.9 -0.481 1.39 1.41 0.0333 1.13 0.0288 ## # ... with 24 more rows A plot of the fitted linear model values in units of logit onto the corticosterone scale. The plot is, of course, linear. The ordinate is scale logit or log odds. Logit is not particularly intuitive unless you use it a lot. ggplot(augment(stork.model), aes(cort, .fitted))+ geom_point()+ ylab(&quot;.fitted aka logit&quot;)+ ggtitle(&quot;logit=2.703-0.0798*cort&quot;) Next, well rescale the fitted values as odds through taking the exponent on both sides. The graphs ordinate is now in units of odds. Recall, odds are the ratio of the probability of an event to its complement. The result is an exponential relationship, negative in this case. The odds of stork death drop with the magnitude of nestling corticosterone (as a stress marker). ggplot(augment(stork.model), aes(cort, y=exp(.fitted)))+ geom_point()+ ylab(&quot;exp(.fitted) aka odds&quot;)+ ggtitle(&quot;odds=exp(2.703-0.0798*cort)&quot;) Lastly, well transform the fitted values to probabilities. This also happens to be the scale of the original data, so well add those experimental values to the fitted values. The fitted values are nonlinear on this scale, an S-shaped curve is ever-so-faint. The probability of death is reduced as stress levels increase. ggplot(augment(stork.model), aes(cort, y=exp(.fitted)/(exp(.fitted)+1)))+ geom_point()+ ylab(&quot;odds/odds+1 aka P(death)&quot;)+ ggtitle(&quot;P=exp(2.703-0.0798*cort)/exp(2.703-0.0798*cort)+1&quot;)+ geom_point(data=stork, aes(y=death), shape=1, size=8, color=&quot;blue&quot;) 43.3.1.1 Coefficient values kable(tidy(stork.model)) term estimate std.error statistic p.value (Intercept) 2.7030391 1.7472461 1.547028 0.1218564 cort -0.0798047 0.0436801 -1.827025 0.0676959 The coefficient estimates and standard error values are derived from maximum likelihood estimation. They are linear model parameteres that, generally, predict the linear relationship between X and Y, where the latter is in logit units. In this case, the fixed coefficients between nestling stress (corticosterone) and the log odds of death. Bear in mind that Y is logit, or log-odds units, allows for linearization of a nonlinear phenomenon and may not be intuitively useful. Nevertheless, plugging the coefficient values into the formula the linear model for the data is \\(logit(Y)=2.7030-0.0798*X\\) It can be more interpretable to convert from log-odds to odds or to proportions. To convert log-odds to ordinary proportions we use equation 1 above. For example, what proportions of storks with a corticosterone level of 30 or 60 ng/ml are predicted to die? #let `l` be our linear model c &lt;- c(30, 60) l &lt;- 2.7030-0.0798*c p &lt;- exp(l)/(exp(l)+1) p ## [1] 0.5766412 0.1105633 Thus, a 57.7% death rate is predicted for nestling stress corresponding to cort levels of 30 ng/ml. Doubling the stress reduced the death rate to 11%. The coefficients allow for predicting the fractional contribution of a unit change in level of the explanatory variable: \\(percent \\ change=(logit-1)*100\\) or \\(percent \\ change=(odds-1)*100\\) For example, by what percent does a 1 ng/ml increase in cort change the odds of survival? To calculate in terms of odds, note how we do an exponential transform of the value of the coefficient from log-odds to odds: (exp(-0.0798)-1)*100 ## [1] -7.669901 Thus, a 1 ng/ml increase in cort would be associated with a 7.67% decrease in death. If wed like to have confidence intervals, in logit units, those are easy to compute: confint(stork.model) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -0.5583249 6.434585e+00 ## cort -0.1746984 2.847104e-05 43.3.1.2 Deviance The regression model includes the calculation of two deviance values: kable(glance(stork.model)) null.deviance df.null logLik AIC BIC deviance df.residual nobs 45.23389 33 -20.69785 45.3957 48.44843 41.3957 32 34 The term deviance signals that maximum likelihood estimation is used in these calculations. As you might imagine, deviance residuals are analogous to residuals calculated using ordinary least squares. Deviance represents the deviation from values predicted by the model to the values in the data set. They are calculated differently than if by OLS, but share general properties. For example, they are roughly symmetrical across a well-fit model. The overall null deviance is calculated for an intercept-only model, whereas the residual deviance is calculated by including a coefficient(s) for the explanatory variable(s). In general, addition of a coefficient to a model improves the fit, the greater the calculated difference between the null and residual deviances. You can see that the residual deviance is lower than the null deviance, indicating that adding the coefficients to the model reduces the deviance, at least somewhat. It will be up to inference to decide whether the magnitude of that difference adds meaningful clarity to the overall picture of how storks die. The actual values of these deviances are in \\(\\chi^2\\) units. For null \\(\\chi^2\\)=45.234 for 33 df, and the residual \\(\\chi^2\\)=41.396 with 32 df. 43.3.1.3 Inference There are two basic but interrelated levels of inference that can be drawn from generalized linear modeling. The first involves inferring how well the model fits the data. Sometimes our data include variables that dont improve the explanatory value of a less comprehensive model. Model fitting has a lot of value for choosing whether a more complex model offers a better fit. The second area for inference focuses on the model coefficient values as effect sizes for a variable. This goes a step beyond whether a variable has earned its seat in the model by ascribing the level of impact the variable can have. 43.3.1.3.1 Analysis of deviance This model produces two deviances, which can be used to test the null hypothesis that nestling stress level does not predict death. The null logistic regression model is \\(logit(Y)=\\beta_0\\). The alternate model is: \\(logit(Y)=\\beta_0+\\beta X\\). Since we can think of the intercept as the death rate uninfluenced by stress, another way to think about this is that these two deviances can be used to test the intercept only model fits better than a model that includes the corticosterone variable. In other words, the null hypothesis is that the null (intercept only) deviance is less than or equal to the model deviance. There are a few ways to test this null hypothesis. Fortunately, both yield the same result. By hand, simply derive a p-value from the \\(\\chi^2\\) distribution for the difference between the two model deviances: 1-pchisq(45.234-41.396, 33-32) ## [1] 0.05010325 #which is the same as... pchisq(45.234-41.396, 33-32, lower.tail=F) ## [1] 0.05010325 Or by anova, specifying the chi-square test as an argument. This way gives the same result as above. When youre used to ANOVA tables youll find this can be less confusing the others. anova(stork.model, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: death ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 33 45.234 ## cort 1 3.8382 32 41.396 0.0501 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 43.3.1.3.2 Wald tests for coefficients Alternately, we can focus on the z value in the coefficient table. The z value is the ratio of a coefficient estimate divided by its standard deviation. This z-value is otherwise known as the Wald statistic. In the model configuration in this example, the z values are calcuated with reference to the coefficient value of the intercept. \\[z=\\frac{\\beta - \\beta_0}{SE_\\beta}\\sim N(0,1\\] The p-value is derived from a standard normal distribution as \\(2*(1-pnorm(z))\\). Thus, the test is akin to a two sided, one-sample t-test that the null coefficient is equal to zero. If a p-value for a coefficient is below our threshold, say 0.05, and we reject the null and include the coefficient in the model. In this instance, neither the intercept nor the corticosterone coefficients have p-values below our threshold. We cannot reject the hypothesis that their values are zero. Of the two, the cort coefficient is the more important, because it represents the slope. And the slope is not different than zero. What this means is that, despite the lovely graphs above, theres no strong statistical support for a linear relationship between nestling cort and survival. If we had previously set a 5% type 1 error threshold, since the p-value for the analysis of deviance and/or these coefficient values is greater than 5%, althought just barely, we dont reject the null. We would conclude that we have insufficient evidence to conclude the goodness of fit for the null model is improved by including the corticosterone variable. Thus, whether stress of stork nestlings improves survival remains unclear. This is consistent with the conclusion from above in the logistic model summary table that the slope for the coefficient does not differ from zero. 43.3.1.3.3 Putting it all together The data simply dont provide a strong estimate for the value of the cort coefficient. Either test we tried (the \\(\\chi^2\\) or z-test) can be used to draw inference and should, as a general rule, agree. I should mention here that in real life, we would use only one test and not the other. That choice would have been made in advance of actually conducting the experiment. The difference between the two statistical tests is that the z-tests the null that the slope is zero, and the \\(\\chi^\\) tests whether the null model deviance \\(\\le\\) the difference between the deviances of a model that includes that predictor and the null. Which test is better? Neither, really. In some ways, the choice of one or the other depends upon your focus. For example, if your focus is mostly on estimating the magnitude of the coefficient values and their individual contributions to the outcome, you might want to stick with the z-test. If your focus is on the overall model and big picture question (e.g., Does stress improve survival?), whether the model can be a good predictive tool, youd focus on the deviance ratio tests for inference. Simply make a judgement during experimental planning which of these options will be chosen for making inference. Then stick with that choice. 43.3.1.4 AIC The Akaike Information Criterion is somewhat analogous to \\(R^2\\), providing information on goodness of fit. Unlike \\(R^2\\), the value of AIC is only useful when comparing nested models. In that case, a simpler model is a better fit relative to a more complex model if the AIC value of the former is lower than that for the latter. "],["mixedlogistic.html", "Chapter 44 Mixed model logistic regression 44.1 Mixed models, fixed and random effects 44.2 Data 44.3 Alternative analysis", " Chapter 44 Mixed model logistic regression library(tidyverse) library(lme4) library(viridis) library(multcomp) library(broom) A mixed model logistic regression is an appropriate test for experimental designs where paired/repeated/related measures are taken and the outcome variable is a proportion. In general, mixed model regression should be used when there is a heirarchical structure in the design yielding measurements that are non-independent. Take for example an cell culture-based experiment. Independent replicates are performed. On a given day, working from a common batch of cells plated at the same time, none of the measurements from within a batch are independent of each other. However, we can assume that measurements collected between batches are independent. The mixed in the title is jargon referring to the fact that the model has both fixed and random components. Which is more jargon to deconvolute, I suppose. 44.1 Mixed models, fixed and random effects Brief synopsis, in regression jargon the fixed effects of an experiment are those represented by the intercept and by the predictor Recall that statistical models are perfect, while data are not. The fixed effects in a simple linear regression model are the background level of the response in the system, estimated by the intercept coefficient \\(\\beta_0\\), and those effects due to the predictor variable, which is estimated by the slope coefficient, \\(\\beta_1\\). Heres a simple linear model: \\[Y=\\beta_0+\\beta_1X+\\epsilon\\] Performing a regression on a data set allows us to generate estimates for these fixed effects based upon the behavior of a sample. The error term, \\(\\epsilon\\) accounts for the residual differences between the points predicted by the model and the values for the actual data. For example, lets imagine were measuring whether a protein is in the nucleus or in the cytosol, as a binary dependent variable \\(Y\\). The experiment involes stimulating cultured cells with different types of mitogens \\(X\\). This condition is followed counting some number of cells and deciding how many show nuclear protein and how many cells show the protein in the cytosol. Furthermore, the experiment is replicated independently several times. The word fixed effects comes from assuming that the slopes and intercepts of the regression model have a fixed value in the wider populaton of cells that are sampled. A regression routine run on experimental data generates estimates for those fixed values. We use these model coefficients to draw perfect lines or curves through some messy data. In the statistical model world the data are imperfect! That explains why we add a residual term to the model, so we can capture in it the stochastic deviation from the these fixed regression coefficients that the actual data has. We learned way back in ANOVA about experimental designs where multiple measurements are derived from a given experimental unit. In such designs it becomes possible to attribute some of the residual error as the random variation among independent replicates. The jargon random in regression is the same as the repeated/related measures jargon used in ANOVA. In other words, in a cell culture experiment, we would expect random variation between cell culture passages might affect nuclear location of the protein. This random variation could arise from anything, including the state of the cells and the preparation of fresh reagents. But within a replicate we would expect those random variations to affect each cell culture well similarly. We account for this repeated measure in our regression models as the random effect. The term comes from thinking that the replicate to replicate variation in a population is what is random. Usually, controlling for random effects in a model has the effect of lowering the stochastic residuals. By definition, these are the stochastic variation in the data unaccounted for by the random and fixed effects of a model. Thus, mixed models have both fixed and random effects. Heres an example for a common experimental design for a logistic linear mixed model. The outcome response involves the researcher making a decision about whether the outcome variable belongs in one one category or another. And the responses to all of the treatment levels are measured within each independent replicate. 44.1.1 NFAT localization within smooth muscle cells The NFAT transcription factors mediate gene expression responses under the control of mitogen signaling. In perfectly quiescent cells they are cytosolic. When cells are activated by stimuli that increase cytosolic calcium, the phosphatase calcineurin strips NFATs of phosphate groups, exposing a nuclear localization signal. The NFATs then translocate to the nucleus where they contribute to gene expression responses after binding to enhancer elements in genes. Heres a simple question: Can NFAT distinguish between different types of triggers that regulate calcium signaling? If so, this might be evident by measuring different degrees of nuclear translocation. To assess NFAT activation directly, the ability of 3 different mitogens (AngII, UTP and PDGF) to cause nuclear translocation was measured and compared to a vehicle control. Cells in culture were treated with an agent for 30 min before they were fixed and stained with an NFAT antibody. A blinded observer randomly selected a region and counted a total of one hundred cells on each plate. To determine the proportion showing nuclear NFAT the cells were scored as having mostly nuclear NFAT or mostly cytosolic NFAT. The experiment and counting procedure was replicated independently 5 times. A few features of this design are notable: * The predictor variable is treatment at 4 levels (Veh, AngII, UTP and PDGF) * The outcome variable is discrete and has a binomial distribution (nuclear or cytoplasmic NFAT) * This is cell culture based. A replicate is defined as a day in which all stimuli were administered side-by-side on different wells from a common cell culture source passage. Therefore, within a replicate all counts are intrinsically-linked. The regression model will have a random term to account for this. 44.2 Data The data are in a csv file. The treatment code is as follows: a=vehicle, b=AngII, c=UTP and d=PDGF. data &lt;- read.csv(file=&quot;datasets/smnfat.csv&quot;) Heres a poor mans heat plot of the data. The replicates are color-coded, so you can see that all of the cells counted from all of the replicates are represented. ggplot(data, aes(x=treatment, y=as.factor(counts), color=reps))+ geom_jitter(width=0.3, size=3, alpha=0.8)+ scale_color_viridis(discrete=T) + scale_y_discrete(&quot;counts&quot;, labels=c(&quot;0&quot;=&quot;Cyto&quot;, &quot;1&quot;=&quot;Nuc&quot;))+ scale_x_discrete(&quot;treatment&quot;, labels=c(&quot;a&quot;=&quot;Veh&quot;, &quot;b&quot;=&quot;AngII&quot;, &quot;c&quot;=&quot;UTP&quot;, &quot;d&quot;=&quot;PDGF&quot;))+ theme_classic() ## Linear model Since these are discrete nominal counts the data are expect to fit a binomial distribution. Thus, the binomial family is chosen for the generalized linear model. Within each replicate every cell scored under the different exposure conditions are clearly not independent from every other cell. The cell culture conditions are highly homogenous within each plate and between the different plates on any given day. They were measured on cultured cells of the same passage date and involve the use of a common source of reagents. All intracellular localization counts within each replicate are intrinsically-related. This is a scientific, not a statistical, judgement. In the smnfat data set, reps is a grouping variable indicating the replicate id. Adding (1|reps) to the model accounts for this, assuring that a random intercept will be calculated for each replicate. See Table 2 in the lme4 vignette for more information. Finally, because we want to compare all levels of the predictor groups to each other, were using the offset key 0+. Here, the model will report fixed coefficient values for each level of the variable, rather than values that are in reference to a common intercept. This provides a logit score for each of the 4 stimuli. The regression output has a lot of information, but were only going to compare the coefficient values for the different levels of treatment: model2 &lt;- glmer(counts~0+treatment + (1 | reps), family = binomial, data) summary(model2) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: counts ~ 0 + treatment + (1 | reps) ## Data: data ## ## AIC BIC logLik deviance df.resid ## 2523.0 2551.0 -1256.5 2513.0 1995 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.3497 -0.9820 -0.4524 0.8350 2.2103 ## ## Random effects: ## Groups Name Variance Std.Dev. ## reps (Intercept) 0.01025 0.1012 ## Number of obs: 2000, groups: reps, 5 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## treatmenta -1.47943 0.12367 -11.963 &lt; 2e-16 *** ## treatmentb -0.03208 0.10036 -0.320 0.749195 ## treatmentc 0.36488 0.10168 3.589 0.000332 *** ## treatmentd 0.49077 0.10277 4.776 1.79e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## tretmnt trtmntb trtmntc ## treatmentb 0.165 ## treatmentc 0.163 0.201 ## treatmentd 0.161 0.199 0.196 44.2.1 Inference The tests above in the model summary are for the null that the coefficient estimates equal zero. Sometimes thats a useful test. Here it is not useful, for scientific reasons. A logit of zero corresponds to a 50:50 proportion of NFAT for the nucleus to cytosol. In fact, given the much lower nuclear to cytosol ratio in unstimulated cells (see the figure), a 50:50 proportion would represent some level of intrinsic activation! Instead, what were interested are group comparisons. The scientific question driving this experiment is whether the effects of the three mitogens differ from each other, and from the negative control. The fixed effect coefficient values in the summary output, which are in units of logit, are estimates of the effect sizes for each level of treatment. The question is, how to compare these effect size values? We can do so using the same Wald test, but in a way that compares one group to the next. The difference between two coefficient values divided by a standard error gives us the z statistic, which is standard normal distributed. \\[z=\\frac{\\beta-\\beta_0}{SE_\\beta}\\sim N(0,1)\\] This is a multiple comparison problem, basically identical to the multiple comparison problem we see after an ANOVA when we wish to compare many group means. Here, we want to compare logit values. Fortunately, packages have been devised for this purpose when dealing with linear model objects. The multcomp package offers the glht function with which to run these tests. In regression jargon, we specify the contrasts of interests (meaning, we tell it what coefficient comparisons we want made). Recall the Tukey HSD? You pull it out when you wish to make all possible comparisons. Thats done below. model3 &lt;- summary(glht(model2, linfct=mcp(treatment=&quot;Tukey&quot;))) summary(model3) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: glmer(formula = counts ~ 0 + treatment + (1 | reps), data = data, ## family = binomial) ## ## Linear Hypotheses: ## Estimate Std. Error z value Pr(&gt;|z|) ## b - a == 0 1.4473 0.1458 9.925 &lt;0.001 *** ## c - a == 0 1.8443 0.1468 12.566 &lt;0.001 *** ## d - a == 0 1.9702 0.1475 13.354 &lt;0.001 *** ## c - b == 0 0.3970 0.1277 3.108 0.0103 * ## d - b == 0 0.5229 0.1286 4.066 &lt;0.001 *** ## d - c == 0 0.1259 0.1296 0.971 0.7649 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) Recall that a=Veh, b=AngII, c=UTP and d=PDGF. Thus, only UTP and PDGF responses are equivalent. All other comparisons show a differential response. Each mitogen induces nuclear translocation relative to veh. And both UTP and PDGF differ from AngII in the level of response they evoke. Youll note that the p-values are adjusted for multiple comparisons. 44.3 Alternative analysis What researchers commonly do with data like these is to calculate proportions and run t-tests on same. This script munges the proportions out of the original count data, ultimately yielding the mean and SD for the replicates within each treatment level. group_by(data, reps, treatment) %&gt;% summarise(prop=sum(counts)/100) %&gt;% group_by(treatment) %&gt;% summarise(mean=mean(prop), se=sd(prop)/sqrt(5) ) ## `summarise()` regrouping output by &#39;reps&#39; (override with `.groups` argument) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 3 ## treatment mean se ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 0.186 0.0225 ## 2 b 0.492 0.00860 ## 3 c 0.59 0.0352 ## 4 d 0.62 0.0207 The script calculates the estimates for the proportion sizes from the logit coefficient estimates produced through linear modeling. You can see they are in almost perfect agreement. Thats because MLE is #calculate mean in prob units from logit units coef &lt;- summary(model2)$coefficients[,1] round(exp(coef)/(exp(coef)+1), 3) ## treatmenta treatmentb treatmentc treatmentd ## 0.186 0.492 0.590 0.620 However, you can see that the SE estimates for each of the coefficients derived by logistic regression are quite different from the SEs that would become part of the t-test calculation. #calculate SE in prob units from logit units coef2 &lt;- summary(model2)$coefficients[,2] exp(coef2)/(exp(coef2)+1) ## treatmenta treatmentb treatmentc treatmentd ## 0.5308775 0.5250684 0.5253974 0.5256687 Heres the main problem I have t-testing proportions (as opposed to Wald testing): Proportions dont have anything that remotely resembles a Gaussian distribution. Proportions have limits at the lower (0) and upper (1) values. Gaussian distributions are limitless. Binomial distributions are skewed. Gaussian distributions are symmetrical. The t-test should be reserved for variables that have a Gaussian distribution. "],["poissonreg.html", "Chapter 45 Poisson regression 45.1 Why not ANOVA? 45.2 The Poisson generalized linear model 45.3 Length of hospital stay 45.4 Output interpretation", " Chapter 45 Poisson regression library(tidyverse) library(COUNT) Poisson regression is analogous to logistic regression. Both are for outcome variables that are discrete. Whereas logistic regression is conducted when the dependent variables are proportions, Poisson regression operates on discrete counts representing frequency data. These have been discussed previously in some detail. They are variables representing events counted in some time or space. The method well use to conduct Poisson regression analysis is the generalized linear model. This will involve the functions glm for regular regression or glmerfor mixed model regression. Mixed model is just regression jargon for repeated/related measures. The Poisson regression models that well cover are also known as log linear models. In this chapter well deal with the statistical analysis of experiments that generate count data and involve three or more predictor groups. Think of this as the ANOVA equivalent but for count data. 45.1 Why not ANOVA? A very common mistake is for researchers to conduct parametric tests (t-tests, ANOVA, linear and general regression) on count data, either directly or after transformation to percents or some other pseudo-continuous scale. In other words, they take the averages of counts, calculate their standard deviations and standard error of means, variances, and so forth. This can be a mistake for a few reasons. First, counts are discrete whereas parametric tests should be reserved for continuous variables. Second, count data is frequently skewed. Parametric tests assume normally-distributed variables. Third, count data are lower-bounded at zero. It is not possible to have negative counts or counts of events that do not happen. This becomes a problem in two ways. Some variables represent low frequency events in which zero values or values near zero are common. With such data parametric regression will sometimes predict coefficients with negative values, which are absurd. This touches on a more generally important phenomenon. When count data is transformed to continuous scales and compared to Poisson regression analysis of the same data, although type1 error rates are probably no different for well-powered studies, the estimates for effect sizes can be far off of the mark compared to the coefficients produced via Poisson regression. This calls for using Poisson regression over transformation and parametric testing. 45.1.1 Counting markers For example, we are interested in whether cells bear a certain marker. We have some method to label that marker and then count cells in a sample that show it. We are interested in knowing whether various treatments influence the expression of that marker, from stimuli, to suppression or activation of genes, to strain of animal and more. All experiments with the technique generate frequency measurements. If the technique involves a fluorescent probe, we dont confuse the intensity of fluorescence, which is a continuous variable, for whether the signal satisfies a threshold so that it deserves to be counted. 45.1.2 Counting depolarizations Every biological scientist learned in middle school that the depolarization of an excitable cell is an all or none phenomenon. We poke a neuron or some other excitable cell with an electrode. We stimulate with a current or some other stimulus and count the number of times the cell depolarizes in response. We repeat this paradigm under a variety of treatment conditions. We might be interested in how a drug or a gene or co-factor or anything else influences the number of depolarizations. These are just plain old counts. Discrete. Non-continuous. All or none event. We dont have any information on the number of times the cell fails to depolarize. From one condition or replicate to another the counts have only integer values. There are no decimal places in any row of the data set. 45.1.3 Lever presses In behavioral research subjects can be trained to request more of a reward by pressing a lever. The technique is common in addiction research, for example. The protocol involves recording the number of times the test subjects presses a lever to request a reward from the researcher. We are interested in how different variables, such as pyschostimulants, influence this reward-seeking behavior. We dont have a count for all of the cells or places that dont bear the marker of interest. We cant count the number of times the cell fails to depolarize. We cant count the number of times a subject does not press the lever. But we do have a record of the frequency of events in response to treatments. 45.2 The Poisson generalized linear model The Poisson distribution is frequently used to model count data. When \\(Y\\) is the number of discrete counts the Poisson probability distribution is \\[f(y)=\\frac{\\mu^ye^{-\\mu}}{y!}\\] where \\(\\mu\\) is the average number of occurrences and \\(E(Y)=\\mu=var(Y)\\). In Poisson regression the effects of predictors on \\(Y\\) are therefore modeled through the parameter \\(\\mu\\). Poisson model assumes sets of counts to be compared come from an equivalent exposure. For example, the few cells that take up a specific marker dye are counted on a culture plate. These counts come from a fixed area of the plate containing unstained cells. The total number of cells in this area bounds the exposure. Although the count data are not ratio transformed for analysis, such as counts per \\(mm^2\\), they might latter referred to by their exposure as depolarizations over three minutes. That there is an equivalent exposure is an underlying assumption of the Poisson generalized linear model: \\[E(Y_i)=\\mu_i=n_i\\theta_i\\] Here \\(Y_i\\) is the number of stained cells in response a set of conditions. This depends on a product between the total number of cells \\(n_i\\) within the counting area and other conditions \\(\\theta_i\\) that influence the counts. Indeed, \\(\\theta_i\\) depends upon the predictor variables as \\[\\theta_i=e^{\\beta_1 X_1+\\beta_2 X_2+..\\beta_p X_p}\\] The generalized linear model is \\[E(Y)=\\mu_i=n_ie^{\\beta_1 X_1+\\beta_2 X_2+..\\beta_p X_p}\\\\Y_i\\sim Po(\\mu_i)\\] and the link function is the natural logarithmic function: \\[log(\\mu_i)=log(n_i)+\\beta_1 X_1+\\beta_2 X_2+..\\beta_p X_p\\] 45.3 Length of hospital stay The azpro data set in the COUNT package counts the length of hospital stay, in days, of patients treated for coronary disease. Days are counted as discrete integer values. One of the predictor variables is procedure, which is either a percutaneous transluminal coronary angioplasty (PTCA) or a coronary artery bypass graft (CABG). This histogram shows the length of hospital stay variable (los), with patients colored on the basis of procedure (0=PTCA, 1=CABG). The two distributions are low bounded at zero, overlap extensively but not exactly, and show the typically Poisson-like left-leaning skewed shape. data(azpro) ggplot(data.frame(azpro))+ geom_histogram(aes(x=los, fill=as.factor(procedure)), binwidth=1)+ scale_fill_manual(values=c(&quot;#002878&quot;, &quot;#d28e00&quot;))+ labs(x=&quot;Length of stay, days&quot;) To keep things simple, well start by regressing length of stay only on the procedure and admit variables. Together, they provide k=4 groups of predictor variables. This asks whether the length of stay differs due to both the procedure type and admit condition. modpa &lt;- glm(los~procedure+admit, family=poisson, data=azpro) summary(modpa) ## ## Call: ## glm(formula = los ~ procedure + admit, family = poisson, data = azpro) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3427 -1.1413 -0.4825 0.5084 12.2296 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.40475 0.01334 105.31 &lt;2e-16 *** ## procedure 0.94910 0.01215 78.08 &lt;2e-16 *** ## admit 0.34265 0.01207 28.38 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 16265.0 on 3588 degrees of freedom ## Residual deviance: 9089.2 on 3586 degrees of freedom ## AIC: 22601 ## ## Number of Fisher Scoring iterations: 5 45.4 Output interpretation Ideally, the deviance residuals would be symmetrically distributed, as log normal. Thats not the case here. This could be a sign that the Poisson model is not a good one for this data. Perhaps due to over-dispersion. The values for the coefficient estimates are log counts. The log count for the effect of procedure is 0.949, when exponentiated it is equivalent to about 2.5 days. Each coefficient has an Wald test to determine if the coefficient value differs from zero. They do. The interpretation is that a one unit change in procedure adds log count 0.949 days to the length of stay. Since PCTA is keyed as 0 and CABG as 1, this means that the CABG procedure extends the length of stay about 2.5 days compared to PCTA. The admit status also matters. This variable was keyed at 0 = elective and 1 = urgent. The log count for admit is 0.342, or about 1.4 days. Thus, and urgent admit extends the length of stay about 1.4 days relative to an elective admit. Whether the intercept differs from zero is usually not interesting. This coefficient represents the log count of days that are not explained by the model. The intercept inflates as the predictor variables are removed, as shown below. modp &lt;- glm(los~procedure, family=poisson, data=azpro) summary(modp) ## ## Call: ## glm(formula = los ~ procedure, family = poisson, data = azpro) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3517 -1.4993 -0.5318 0.5354 12.9430 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.64093 0.01007 163.03 &lt;2e-16 *** ## procedure 0.92563 0.01213 76.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 16265.0 on 3588 degrees of freedom ## Residual deviance: 9925.1 on 3587 degrees of freedom ## AIC: 23435 ## ## Number of Fisher Scoring iterations: 5 And the intercept is reduced further as more variables are factored into the regression. summary(glm(los~ procedure + admit + sex + age75 + hospital, family=poisson, data=azpro)) ## ## Call: ## glm(formula = los ~ procedure + admit + sex + age75 + hospital, ## family = poisson, data = azpro) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.1981 -1.2081 -0.4208 0.4802 12.6572 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.4566687 0.0212952 68.404 &lt;2e-16 *** ## procedure 0.9602916 0.0122175 78.600 &lt;2e-16 *** ## admit 0.3266013 0.0121244 26.937 &lt;2e-16 *** ## sex -0.1239367 0.0118125 -10.492 &lt;2e-16 *** ## age75 0.1222160 0.0124491 9.817 &lt;2e-16 *** ## hospital -0.0001489 0.0030984 -0.048 0.962 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 16265.0 on 3588 degrees of freedom ## Residual deviance: 8874.1 on 3583 degrees of freedom ## AIC: 22392 ## ## Number of Fisher Scoring iterations: 5 Of all the variables, only the hospital is inconsequential. Furthermore, hospital length of stay is reduced 0.124 log counts for males relative to females. All other factors increase the length of stay. Finally we can test whether one nested model is a better fit than another. The result below indicates that the addition of the admit factor improves the model fit compared to its absence. anova(modp, modpa, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: los ~ procedure ## Model 2: los ~ procedure + admit ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 3587 9925.1 ## 2 3586 9089.2 1 835.86 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 However, the result below illustrates the addition of a factor to the model that does not improve the fit. modfull &lt;- glm(los~ procedure + admit + sex + age75 + hospital, family=poisson, data=azpro) modless1&lt;- glm(los~ procedure + admit + sex + age75, family=poisson, data=azpro) anova(modless1, modfull, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: los ~ procedure + admit + sex + age75 ## Model 2: los ~ procedure + admit + sex + age75 + hospital ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 3584 8874.1 ## 2 3583 8874.1 1 0.0023087 0.9617 "],["surv.html", "Chapter 46 Survival Analysis 46.1 Data records 46.2 Kaplan-Meier estimator 46.3 Glioma data 46.4 Kaplan-Meier plots 46.5 Log rank tests 46.6 Cox proportional hazards analysis 46.7 Summary", " Chapter 46 Survival Analysis library(tidyverse) library(lubridate) library(survival) library(survminer) library(knitr) library(kableExtra) library(coin) Survival analysis derives its name from experiments designed to study factors that influence the time until discrete death events occur, such as deaths due to cancer or heart disease. When survival is plotted as a function of time, the resulting lines drawn between the data points are called survival curves. The slopes of these curves are called the hazard. The hazard is the rate of death during some period within that curve. For example, if 80% of mice implanted with a tumor die within 1 month the tumors hazard is 80% per month. These ghoulish terms persist even though survival analysis is very useful more generally for studies that involve tracking time to any kind of event, deadly or not. The two common statistical techniques associated with survival analysis that we will cover are Kaplan-Meier (KM) estimation (including the log rank test to compare two curves) and Cox proportional hazards regression. KM estimation mostly offers a descriptive way to quantify time to event and survival. KM plots have the characteristic stair step survival curves, where the abscissa is time and the ordinate is survival probability. A KM estimate is the value on these y-axes at a given time. A KM estimate represents the probability of surviving beyond a specific unit of time. For example, when the analysis declares a median survival time for a cancer of 23 months, that implies half of the subjects can be expected to survive beyond 23 months. KM analysis also allows for testing hypotheses about whether two treatments cause any difference in survival times. The log rank tests employed for this, in effect, test whether the distributions of two survival curves differ from each other. Although these tests report out median survival parameters, they are calculated on the basis of all of the data within a survival curve and are non-parametric. Cox proportional hazards regression offers a related inferential procedure. The name has three origins. It was invented by Sir David Cox. It is a method to calculate and compare hazard coefficients. It is based upon the assumption that the hazard rates of the two curves being compared are proportional and constant through their full extent. When this latter condition is true, two survival curves can be compared in terms of relative risk. Cox proportional hazards regression is the basis for making assertions such as, A person with that cancer has a 20% risk of dying per year. That statement comes from a survival curve with a hazard rate of 0.2 per year. There may be some study that reports a new treatment, which reduces the hazard rate for that cancer by 50%. The treated hazard rate is now 0.10 per year. Therefore, the treatment reduces the risk of death by half. 46.1 Data records For a moment I want to distinguish a data frame from a data record. The might be something the researcher keeps while the survival experiment is on-going. A data frame for a typical survival analysis study is comprised of many cases. Each case represents an independent replicate. The minimal number of variables recorded for each case are as follows: The event status The duration in time until the event occurred A grouping variable (when testing hypotheses) That dataframe will be minimally necessary to run survival analysis and statistical functions. In practice, cases come and go throughout an enrollment and follow up period. Our pre-processed records have an ID for the case, an enrollment date or time, a date or time when the event status changed, the event status and the grouping variable, and perhaps some notes. id &lt;- c(LETTERS[1:4]) start_date &lt;- c(&quot;2020-01-14&quot;, &quot;2020-01-12&quot;, &quot;2020-01-30&quot;, &quot;2020-01-29&quot;) end_date &lt;- c(&quot;2020-03-29&quot;, &quot;2020-01-19&quot;, &quot;2020-04-04&quot;, &quot;&quot;) status &lt;- c(1,1,0, &quot;&quot;) treatment &lt;- c(&quot;placebo&quot;,&quot;placebo&quot;,&quot;placebo&quot;,&quot;placebo&quot; ) notes &lt;- c(&quot;&quot;, &quot;&quot;, &quot;left area, unenrolled&quot;, &quot;&quot; ) study &lt;- data.frame(id, start_date, end_date, status, treatment, notes) kable(study) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) id start_date end_date status treatment notes A 2020-01-14 2020-03-29 1 placebo B 2020-01-12 2020-01-19 1 placebo C 2020-01-30 2020-04-04 0 placebo left area, unenrolled D 2020-01-29 placebo The event status and end date are intimately linked. An event status is typically entered as a discrete variable code for what occurred on the end date. For example, a status of \\(1\\) would indicate the event (eg, death) occurred for that case. An event status of \\(0\\) would indicate the case was censored after that amount of time. The concept of censor is discussed below. A grouping variable would be something like a treatment variable. For example, a variable named treatment might have two levels, placebo and drug. There may be additional grouping variables. Given a record of start and end dates/times, well either have to first calculate their difference before passing such data into survival functions, or argue a function so that it will do this given this information. 46.1.1 Deriving times from recorded dates The duration to an event is something that is calculated during data processing. It is important to pass into the Surv function (below) values in units of time. They should not be simple numeric values. Survival analysis can be conducted over any time scale. Scales of days, weeks, months and even years are common. In such designs calendar date variables are commonly used. For example, in a cancer trial a date is recorded for the date a mouse is implanted with a tumor. Subsequently, the date an event (eg, death or censure) occurs is also recorded. The time between these days is then calculated during data processing. It turns out that dates can be a tricky variable in any computer language due to their imprecision. Shifts such as time zones, the number of days in a month, daylight savings, leap years and even leap seconds occur. Strictly, unlike for seconds (which always have a fixed duration) time variables in units of minutes, hours, days, weeks, months and years (and more) might be approximate unless we account for these shifts. See ?POSIXct for more information. The lubridate package has several utilities to deal with date/time data. Our main concern is converting date or date/time entries into time values on whatever scale useful for our survival analysis algorithms. Lets image the simple case where we record the start dates for enrollment for each case in a study, and then record the date of the event or censure entry. Note how each vector is that of character strings start_date &lt;- c(&quot;2020-01-14&quot;, &quot;2020-01-12&quot;, &quot;2020-01-30&quot;, &quot;2020-01-29&quot;) end_date &lt;- c(&quot;2020-07-29&quot;, &quot;2021-01-19&quot;, &quot;2020-08-04&quot;, &quot;2020-02-29&quot;) The simplest way to calculate the time interval is to first convert the character strings into date objects using the ymd function. Then subtract. Note how in this case, by default, time units of days are the output. Note this produces a difftime object. days &lt;- ymd(end_date) - ymd(start_date) days ## Time differences in days ## [1] 197 373 187 31 class(days) ## [1] &quot;difftime&quot; When the need is for numeric values as output rather than conversion to difftime, or when calcuating intervals greater than week (difftime does not have units of months or years), the method below takes character strings and produces numeric values in the sought for units. # illustrating how to calculate four different time scales # usually there is only one scale of interest # The divisor can be set to any length: days(7) == week(1) dateSet &lt;- tibble(start_date, end_date) %&gt;% mutate(days = interval(start_date, end_date)/days(1), weeks = interval(start_date, end_date)/weeks(1), months = interval(start_date, end_date)/months(1), years = interval(start_date, end_date)/years(1) ) dateSet ## # A tibble: 4 x 6 ## start_date end_date days weeks months years ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-01-14 2020-07-29 197 28.1 6.48 0.538 ## 2 2020-01-12 2021-01-19 373 53.3 12.2 1.02 ## 3 2020-01-30 2020-08-04 187 26.7 6.16 0.511 ## 4 2020-01-29 2020-02-29 31 4.43 1 0.0847 46.1.2 Censor The concept of censor is important in survival studies. Censoring occurs in either of two ways: The study period ends without an event having occurred for that case. The case is de-enrolled prematurely from an active study for reasons other than meeting the event criterion. For example, imagine a survival study in mice implanted with experimental tumors where death is the study event. One day, a live mouse jumps out of a cage and escapes from our care, never to be seen again. That mouse should be censored on the date it was lost. A score of \\(0\\) is entered for the event variable and the date the case was lost is also recorded. Its gone. Since we dont know if it died or when it died we have no choice but to record its event as censored. Censored cases should not be counted as events nor should they be ignored completely once enrolled in a trial. In either circumstance they would skew results if not censored. Censoring reduces the number of subjects at risk that remain in the study, which influences the survival probability calculation (see below). If a study is designed to last for a limited period, or a decision is made to end the study, then all remaining survivors are censored on that end date. 46.2 Kaplan-Meier estimator Everyone will recognize the familiar step-down Kaplan-Meier survival curve, even though we might not know its name or function. These plot an updated survival probability each time the number at risk (\\(N_t\\)) in the study changes. The number at risk can change either due to an event (\\(D_t\\)) or due to censoring (\\(C_t\\)). \\(S_{t+1}\\) is the surival probability we are computing due to an event or censoring, while \\(S_t\\) is the survival probility just before this. \\[S_{t+1}=S_t \\frac{N_{t+1}-D_{t+1}}{N_{t+1}}\\] Perhaps this is easier to understand through illustration. We begin a study with 10 subjects. We have only one group (no comparisons are involved) and everyone is enrolled simultaneously, to keep it simple. Thus, 10 subjects are at risk of not surviving the protocol. At the intial time, 100% are survivors. months &lt;- c(0,5,10, 15, 20, 25) Nt &lt;- c(10,10,8, 7, 6, 5) Dt &lt;- c(&quot;&quot;, 1, 1, 1, &quot;&quot;, 2) Ct &lt;- c(&quot;&quot;, 1, &quot;&quot;, &quot;&quot;, 1, &quot;&quot;) Stplus1&lt;- c(1, 0.9, 0.788, .675, .675, .404) kable(data.frame(months, Nt, Dt, Ct, Stplus1)) months Nt Dt Ct Stplus1 0 10 1.000 5 10 1 1 0.900 10 8 1 0.788 15 7 1 0.675 20 6 1 0.675 25 5 2 0.404 Exactly five months later one subject dies while a second subject is censored. At the time of these two events there were still 10 subjects at risk. Only the previous survival probability (1), the number of deaths at 5 months (1) and the numbers at risk (10) before this event happened factor into how the survival probability is changed. \\(S_{t+1}=(1)\\frac{10-1}{10}=0.9\\). The study protocol continues with 8 subjects at risk. Another five months later a second death occurs, meaning we have to update the survival probability. The number at risk is now 8, since there were previously 1 death and 1 censor. At 10 months, the updated survival probability is \\(S_{t+1}=(0.9)\\frac{8-1}{8}=0.788\\). The study protocol continues with 7 subjects at risk, awaiting the next event. Fifteen months after starting the third death occurs. The number at risk prior to this death is 7, due to the loss at the 8 month time point. \\(S_{t+1}=(0.788)\\frac{7-1}{7}=0.675\\). There are now 6 subjects at risk moving forward. Twenty months after starting a subject is censored. Nobody dies. The number at risk at this time point was 6 due to the previous death. Since there has been no death, the survival probability remains unchanged. \\(S_{t+1}=(0.675)\\frac{6-0}{6}=0.675\\) But moving forward there are now 5 subjects at risk in the protocol. Twenty-five months after starting two subjects die. To update the survival probability the number at risk is down to 5 due to the 1 censor at the prior time. \\(S_{t+1}=(0.675)\\frac{5-2}{5}=0.404\\) Moving forward 3 subjects are at risk. Their survival probability is 0.404. The R survival functions will accurately calculate these survival probabilities. Hopefully it is apparent from this example that each updated survival probability is forward looking and applies to the remaining subjects at risk. Furthermore, hopefully it is evident the effect of censoring is to reduce the numbers at risk without affecting survival probability. More generally, \\(S_{t}\\) serves as an estimator of the survival function, which predicts the probability of surviving longer than time t. Median survival times are a very common use of the KM estimator as a descriptive statistic for assessing the influence of some condition on time-to-event. In this paradigm median survival is when \\(S_{t}\\) is 0.5. Median survival times can be devined from the intercept of the survival curve and a horizontal line extending from \\(S_{t}\\) = 0.5 on ordinate. The time point corresponding to that intercept is the median survival time. 46.3 Glioma data Lets look at some data for exploring statistical inference. The glioma data set below comes from the coin package. Note it contains 37 cases listing the survival times for people with either Grade3 or GBM gliomas. Notice also that it contains a few grouping factors, including histology, sex, and group. Well just focus on survival associated with histology, which youll recognize as two different types of glioma. The first step is to create a survival model of the data. The Surv function from the survival package in conjunction with the survfit function summarizes the events while calculating the survival probability for each event and its associated time points (which are in months), along with some useful summary statistics. Hopefully the formula Surv(time, event) ~ 1 below strikes as reminiscent of that which is used in regression. Here, the survfit function calculates the numbers at risk, the numbers of events and the survival, along with standard error and confidence interval for each time point. This is how to regress the data WITHOUT accounting for the effects of any of the predictors. This is being done to illustrate a couple of points. modelAll &lt;- survfit(Surv(time, event) ~ 1, data = glioma) summary(modelAll) ## Call: survfit(formula = Surv(time, event) ~ 1, data = glioma) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 5 37 1 0.973 0.0267 0.922 1.000 ## 6 36 1 0.946 0.0372 0.876 1.000 ## 8 35 4 0.838 0.0606 0.727 0.965 ## 9 31 1 0.811 0.0644 0.694 0.947 ## 11 30 1 0.784 0.0677 0.662 0.928 ## 12 29 1 0.757 0.0705 0.630 0.908 ## 13 28 1 0.730 0.0730 0.600 0.888 ## 14 27 3 0.649 0.0785 0.512 0.822 ## 15 24 1 0.622 0.0797 0.483 0.799 ## 19 23 1 0.595 0.0807 0.456 0.776 ## 20 22 1 0.568 0.0814 0.428 0.752 ## 25 21 2 0.514 0.0822 0.375 0.703 ## 31 18 1 0.485 0.0824 0.348 0.677 ## 32 17 1 0.456 0.0824 0.321 0.650 ## 34 16 1 0.428 0.0820 0.294 0.623 ## 36 15 1 0.399 0.0813 0.268 0.595 ## 53 8 1 0.349 0.0851 0.217 0.563 modelAll ## Call: survfit(formula = Surv(time, event) ~ 1, data = glioma) ## ## n events median 0.95LCL 0.95UCL ## 37 23 31 15 NA The overall median survival calculated for these 37 cases is 31 months (95% CI is 15 to NA). modelAll ## Call: survfit(formula = Surv(time, event) ~ 1, data = glioma) ## ## n events median 0.95LCL 0.95UCL ## 37 23 31 15 NA Now lets focus on comparing the two groups under the histology variable to ask whether the type of tumor influences survival. We are NOT going to factor in the effects of age, sex or the group variable, in order to keep things simple. First we change the formula from above: modelHis &lt;- survfit(Surv(time, event) ~ histology, data = glioma) summary(modelHis) ## Call: survfit(formula = Surv(time, event) ~ histology, data = glioma) ## ## histology=GBM ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 5 20 1 0.95 0.0487 0.8591 1.000 ## 6 19 1 0.90 0.0671 0.7777 1.000 ## 8 18 4 0.70 0.1025 0.5254 0.933 ## 11 14 1 0.65 0.1067 0.4712 0.897 ## 12 13 1 0.60 0.1095 0.4195 0.858 ## 13 12 1 0.55 0.1112 0.3700 0.818 ## 14 11 3 0.40 0.1095 0.2339 0.684 ## 15 8 1 0.35 0.1067 0.1926 0.636 ## 20 7 1 0.30 0.1025 0.1536 0.586 ## 25 6 1 0.25 0.0968 0.1170 0.534 ## 31 5 1 0.20 0.0894 0.0832 0.481 ## 36 4 1 0.15 0.0798 0.0528 0.426 ## ## histology=Grade3 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 9 17 1 0.941 0.0571 0.836 1.000 ## 19 16 1 0.882 0.0781 0.742 1.000 ## 25 15 1 0.824 0.0925 0.661 1.000 ## 32 13 1 0.760 0.1048 0.580 0.996 ## 34 12 1 0.697 0.1136 0.506 0.959 ## 53 7 1 0.597 0.1341 0.385 0.927 The two groups differ in median survival. For the cases with GBM it is 14 months (95%CI is 11 to 31 months). The median survival for Grade3 cases is undefined. Thats because the survival rate is greater than 0.5they have not reached the median point! modelHis ## Call: survfit(formula = Surv(time, event) ~ histology, data = glioma) ## ## n events median 0.95LCL 0.95UCL ## histology=GBM 20 17 14 11 31 ## histology=Grade3 17 6 NA 53 NA 46.4 Kaplan-Meier plots The survival package has base plotting functions, but we use functions in the survminer package to generate ggplots of survival curves. First a bare bones plot of the unsegmented data. Notice how the median surival line is drawn. Also notice the discrete nature of the 95% confidence intervals (shaded). ggsurvplot(modelAll, surv.median.line = &quot;hv&quot;, # Specify median survival palette = c(&quot;#002878&quot;)) Figure 46.1: Overall survival in the glioma data, irrespective of tumor type. Next we plot the survival curves based upon the histology variable. Note how Grade3 survival never reaches 0.5, thus a median survival time is not possible to calculate. ggsurvplot(modelHis, conf.int = TRUE, surv.median.line = &quot;hv&quot;, # Specify median survival ggtheme = theme_bw(), # Change ggplot2 theme palette = c(&quot;#002878&quot;, &quot;#d28e00&quot;)) Figure 46.2: Glioma survival by type of tumor. 46.5 Log rank tests The key question we address with log rank testing is whether survival differs between GBM and Grade3. It is a very simple question. If true, it implies the two conditions differ. The experimental researcher is typically only interested in survival curve differences and uninterested in hazard ratios or relative risk. For example, in an implanted tumor model, the researcher wishes to manipulate the immune system in some way to test if it alters survival. Period. Log rank tests (there are several) are nonparametric tests for comparing survival distributions and thus answering this question. The differences between the various log rank tests is beyond the intended scope. See coin::?logrank_test for an entry into the broader universe of the various tests, their features and arguments and use cases. In the case of the glioma data, we test the null hypothesis that the survival distributions for GBM and Grade3 are equivalent. \\(H_0\\): The GBM and Grade3 survival distributions are the same. \\(H_1\\): The GBM and Grade3 survival distributions differ. The parameterless wishy-washiness of the null hypothesis should remind us of nonparametric hypothesis tests. There is no parameter here. The log rank test is a nonparametric test, which only evaluate the relative locations of the two distributions without consideration of their central tendency or any other parameter. It is very common to summarize the differences between survival curves through describing their relative median survival times. Therefore, median survival times serve as a useful pseudo-parameter. However, the log rank test is definitely not comparing medians. One can imagine instances where the values of median are roughly the same (if not identical) yet the distributions differ markedly, because survivals diverge at times beyond a common median survival time point. The log rank tests are likely to pick up those differences in survival, because they factor into their calculations the full breadth of the survival curves. The default mode for the survdiff function in the survival package runs a Mantel-Haenszel log rank test producing a \\(\\chi^2\\) test statistic with 1 degree of freedom. \\[\\chi^2=\\frac{(\\sum O_{1t}-\\sum E_{1t})^2}{\\sum Var(E_{1t})}\\] Here \\(\\sum O_{it}\\) and \\(\\sum E_{it}\\) are the sum of the observed and expected number of events in group 1 over all event times. The denominator is the sum of the variances for all events, \\[Var(E_{1t})=\\frac{N_{1t}\\times N_{2t}\\times (N_t-O_t)}{N_t^2\\times(N_t-1)}\\] Here \\(N\\) represents the total numbers at risk at a given time point, while \\(N_1\\) and \\(N_2\\) are the at risk within each group. Here is how to run the test. survdiff(Surv(time, event) ~ histology, data = glioma) ## Call: ## survdiff(formula = Surv(time, event) ~ histology, data = glioma) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## histology=GBM 20 17 8.82 7.57 13.4 ## histology=Grade3 17 6 14.18 4.71 13.4 ## ## Chisq= 13.4 on 1 degrees of freedom, p= 2e-04 Note that the column labeled (O-E)^2/E reports the \\(\\chi^2\\) values for each of the two groups. When summed, they become the classic Mantel-Cox log rank test statistic, which can be used as an alternative for inference rather than the default. The column sum is \\(\\chi^2=12.28\\) with 1 degree of freedom. Use the chisq distribution to compute its p-value: pchisq(12.28, df=1, lower.tail=F) ## [1] 0.0004578384 Although the test statistic and p-values for these two log rank tests are similar, they are not identical. The values should agree in well-powered experiments. To avoid p-hacking temptations, make the decision in advance to use one or the other. Nevertheless, either test generates an extreme \\(\\chi^2\\) test statistic and corresponding low p-value, below the typical type1 error threshold of 0.05. On this basis reject the null hypothesis that the survival associated with both cancers is the same. 46.5.1 Write up of log rank test Survival with Grade3 (median survival = undetermined months, 95%CI = 53 to undetermined) differs from survival with GBM (median survival = 14 months, 95%CI = 11 to 31 months; Mantel-Haenszel log rank test, chisq= 13.4, p=2e-4). 46.5.2 Comparing more than two survival curves The log rank test can be performed on multiple curves simultaneously. There are two inferential options. Conclude from the overall chi-square test that at least one of the treatment levels differs from the others, using the bloody obvious test to draw further conclusions. Use the overall chi-square test as an omnibus test, much like an ANOVA F-test, granting permission to perform multiple post-hoc comparisons. For the latter we run the pairwise_survdiff function from the survminer package. This is very simple to execute. Selecting none as a p-value adjustment method allows for pulling out a vector of unadjusted p-values, to focus on only the comparisons of interest. 46.6 Cox proportional hazards analysis Log rank testing only leaves us with a test statistic. Cox proportional hazards analysis is invoked when the researcher seeks to describe the treatment effect by deriving a quantitative parameter from the survival data. Specifically, for when one wants to describe quantitatively how survival rates are influenced by treatment conditions. The Cox model is otherwise known as the hazard function. The jargon used to define the survival rate of a treatment condition is the hazard, \\(\\lambda(t)\\). The baseline hazard, \\(\\lambda_0(t)\\), is the survival rate in the control setting, in the absence of any treatment condition. The Cox model defines the hazard as a function of a linear combination of predictor variables, \\(X\\): \\[\\lambda(t) = \\lambda_0(t)\\times e^{\\beta_1 X_1+\\beta_2 X_2..+beta_p X_p}\\] In the simple case where the predictor variable \\(X\\) is discrete at two levels, such as a control or a treatment, for the control \\(X=0\\) then \\[\\lambda(t) = \\lambda_0(t)\\], or is the baseline hazard. When treatment is present, \\(X=1\\) then \\[\\lambda(t) = \\lambda_0(t)\\times e^{\\beta X}\\] or \\[\\frac{\\lambda(t)}{\\lambda_0(t)}=e^{\\beta}\\] Finally, natural log transformation creates a linear form of the equation \\[log(\\frac{\\lambda(t)}{\\lambda_0(t)})=\\beta\\] \\(e^{\\beta}\\) equals the hazard ratio or the relative risk \\(\\beta\\) equals the log hazard ratio or log relative risk This (and a bit more mathematical proofing of the Cox model) implies that the ratio of the two hazards are a constant and independent of time, from whence the term proportional is derived. In other words, the procedure is based upon the assumption that the hazard ratio is constant across all times of the study period. 46.6.1 Cox proportional hazards regression of glioma In the code below Cox regression is executed on the glioma data, testing only the histology variable for the difference in survival between the GBM and Grade3 gliomas. Again, we ignore the other predictors. modelHis.cox &lt;- coxph(Surv(time, event) ~ histology, data = glioma) modelHis.cox ## Call: ## coxph(formula = Surv(time, event) ~ histology, data = glioma) ## ## coef exp(coef) se(coef) z p ## histologyGrade3 -1.6401 0.1940 0.4881 -3.36 0.000779 ## ## Likelihood ratio test=13.24 on 1 df, p=0.0002738 ## n= 37, number of events= 23 print(&quot;/////////////////////////////////////&quot;) ## [1] &quot;/////////////////////////////////////&quot; print(&quot;/////////////////////////////////////&quot;) ## [1] &quot;/////////////////////////////////////&quot; summary(modelHis.cox) ## Call: ## coxph(formula = Surv(time, event) ~ histology, data = glioma) ## ## n= 37, number of events= 23 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## histologyGrade3 -1.6401 0.1940 0.4881 -3.36 0.000779 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## histologyGrade3 0.194 5.156 0.07452 0.5049 ## ## Concordance= 0.709 (se = 0.041 ) ## Likelihood ratio test= 13.24 on 1 df, p=3e-04 ## Wald test = 11.29 on 1 df, p=8e-04 ## Score (logrank) test = 13.6 on 1 df, p=2e-04 46.6.2 Interpretation of Cox regression output From the output above, printing the regression model alone is comprehensive except for confidence intervals. Passing the model into the summary function pulls out this and some additional detail 46.6.2.1 Log hazard ratio/relative risk The coef value corresponds to \\(\\beta\\) from the hazard equation above, which in general terms equals \\(log(\\frac{\\lambda(t)}{\\lambda_0(t)})\\). Thus we can say the log hazard ratio is -1.6401. But what does that actually mean, and who is what in this case? First, recall Rs quirk in regression. R does not know if we want GBM or Grade3 to be the intercept. When we dont tell it, that means that it will go choose the first alphanumerical as the intercept. In this case, that is GBM. Remember that \\(\\lambda\\) symbolizes survival rates. And there are two survival rates of interest in this case: one for GBM and another for Grade3. Furthermore, we can bloody obvious tell from the plot above that the survival rate for Grade3 is larger than that for GBM. Think about that carefully for a moment before reading on. Log fractions can confuse. Here we know that \\(log(\\lambda(t))-log(\\lambda_0(t))=-1.6401\\). The negative value of this difference means that \\(\\lambda_0(t)\\) must be greater than than \\(\\lambda(t)\\). Therefore we can deduce that this R function has coerced Grade3 as the denominator, or \\[coef=log \\ hazard \\ ratio=log(\\frac{\\lambda_{GBM}(t)}{\\lambda _{Grade3}(t)})=-1.6401\\] The z statistic and p-value for coef correspond to a Wald test (a ratio of the coefficient value to its SE). This tests the null hypothesis that coef= 0 (if two survival rates are the same, their ratio is 1, null coef=log(1)=0). If the null is true it would indicate there is no evidence that factor associated with that coefficient affects the log hazard ratio. 46.6.2.2 Hazard ratio/relative risk Now for the exp(coef). This is the hazard ratio, otherwise known as the relative risk. Either of these two terms are used commonly and interchangeably. They also have greater descriptive efficacy than the log hazard ratio simply because logs tend to confuse people. Hazard == survival rate. It is worth repeating that the hazard ratio nothing more complicated than a ratio of survival rates.and no logs are involved. \\[\\frac{\\lambda(t)}{\\lambda_0(t)}=e^{\\beta}=\\frac{\\lambda_{GBM}(t)}{\\lambda _{Grade3}(t)}=0.1940\\] Since the value of the hazard ratio is below 1, and \\(\\lambda\\) symbolizes survival rates, this result shows that the survival rate with GBM is lower than the survival with Grade3 tumors. That is bloody obvious from inspection of the plot above. In fact, the GBM survival rate is 19.4% the survival rate for Grade3. Or we can say the relative risk of death due to Grade3 tumors is 19.4% of GBM tumors. or we can say hazard associated with Grade3 tumors is 19.4% of that for GBM. Or we would just report the hazard ratio as a descriptive: Grade3 to GBM HR = 0.194. Now please note the exp(-coef) column. This is merely the reciprocal of the hazard ratio. A personal preference might be to use this. Is so, one would say that the hazard rate of GBM is 5.156 times greater than that for Grade3. Alternately the relative risk of GBM to Grade3 glioma is 5.156. 46.6.2.3 Confidence interval We can say that there is a 95% chance the hazard ratio (or relative risk) of Grade3 to GBM tumor has the values between 0.07452 and 0.5049. It takes professional judgement to decide whether that range is scientifically meaningful. Is is reasonable to think Grade3 tumors are anywhere from a tenth to half as deadly as GBM. An expert in the field could assess that conclusion. Importantly the confidence interval does not include the value of 1, which for a ratio would indicate the two tumors have the same hazard. 46.6.2.4 Statistical tests Finally on to the statistical tests. This function by default generates 3 inferential tests and the researcher chooses one of these via preference. In well-powered experiments all three should converge to a similar conclusion but wont have the exact same values. On close calls, when the researcher decides in advance of testing which one to use the temptation to p-hack is averted. The log rank test is handled in the section above. The Wald test wont be discussed here in detail. It is an extension of the Wald test used above for the single coefficient value. The main difference is that the Wald test for the overall regression handles many coefficients simultaneously. Thats not an issue in this simple case, but it would be if other predictor variables were added to the regression model. Likelihood ratio tests are most commonly used. They always compare the fits of two different models, usually nested, to a common data set. The likelihood function first calculates parameter values for each model that are most likely to fit the data the best. The test statistic is then the ratio of these two likelihoods, generally: \\[LR=-2log\\frac{L(resricted \\ model)}{L(full \\ model)}=2(loglike(full \\ model)-loglike(restricted \\ model)\\] LR is distributed approximately \\(\\chi^2\\) with degrees of freedom equal to the number of restricted parameters. In this case the full model is that which includes the histology variable and corresponding \\(\\beta\\) coefficient as a parameter. The restricted model lacks any predictor variable altogether. The restrictive model is the overall survival of glioma, irrespective of tumore type. Therefore, it helps answer the question: Does the histology variable influence survival at all, relative to the overall survival. Clearly, the answer is yes. 46.6.3 Write up We can conclude that survival with GBM is approximately one-fifth that for Grade3 glioma (hazard ratio = 0.194, hazard ratio 95%CI 0.07 to 0.509, Cox proportional hazards regression, LR = 13.24, df=1, p=0.00027, n =37 with 23 events). Or we could conclude that survival with GBM is approximately one-fifth that for Grade3 glioma (relative risk = 0.194, relative risk 95%CI 0.07 to 0.509, Cox proportional hazards regression, LR = 13.24, df=1, p=0.00027, n =37 with 23 events). 46.7 Summary Thinking of survival analysis as time-to-event can broaden the experiments we might apply this design to. Log rank tests are a straight forward nonparametric method of asking whether two survival curves differ. Cox proportional hazards regression is used to obtain quantified parameters associated with a treatment model. We covered a simple binary event scenario, but events can be more complex than this (eg, two or more outcome events other than censor). This chapter is intended to serve as an introduction to survival analysis for the researcher. The Cox proportional hazards model is flexible and can accommodate considerably more intricate statistical designs then listed here, including multiple variables and interaction tests between them. None of this is covered here. This is probably one of those areas of statistics where a reliable textbook should be consulted when diving into more sophisticated experiments than shown here. Heres a good discussion on the different books available and which might best suite you. "],["classification.html", "Chapter 47 Classification 47.1 Distances 47.2 Clustering methods 47.3 Principal component analysis 47.4 Synthesize some data 47.5 Visualize the data set 47.6 Rescaling data 47.7 Are there clusters? 47.8 How many clusters? 47.9 Clustering 47.10 PAM and CLARA 47.11 Hierarchical clustering 47.12 Heatmaps 47.13 Summary", " Chapter 47 Classification library(tidyverse) library(factoextra) library(viridis) library(cluster) library(clustertend) Data classification is an important first step in exploratory data analysis. The various statistical classification methods share in common an ability to build explanatory groups data sets that have many measurements. The motivations to do this are many-fold: segmentation analysis, identifying underlying structure, reducing complexity to fewer explanatory dimensions. These classification techniques replace impulses to draw groupings arbitraily. As such they are referred to by the jargon term unsupervised. The researcher supervises the algorithm and setting up the data, but thereafter the algorithm operates unsupervised, drawing the groupings where the groupings best fit. Recently, it has become common to refer to cluster analysis as unsupervised machine learning. 47.1 Distances Cluster analysis is based upon minimizing distances between coordinates of centroids (imaginary points representing the centers of a cluster) and the coordinate pairs for every point within a cluster. These distances are no different than the variates or deviates or residuals that weve encountered already in the course. There are several options for calculating these distances. Like many things in statistics, these just represent different ways to solve the same problem. The code below illustrates how distance calculations are made using Euclidian, Manhattan and Pearson methods. The Pearson method, of course, is also known as correlation, and has been covered previously. If optional for a given function, iterating through these or other distance calculations can sometimes help drive a solution for balky datasets. set.seed(1234) x &lt;- sample(11:20, replace=F) y &lt;- sample(1:10, replace=F) paste(&quot;Euclidian:&quot;, sqrt(sum((x-y)^2))) ## [1] &quot;Euclidian: 34.2636833980236&quot; paste(&quot;Manhattan:&quot;, sum(abs(x-y))) ## [1] &quot;Manhattan: 100&quot; paste(&quot;Pearson:&quot;, 1-(sum((x-mean(x))*(y-mean(y))))/(sqrt(sum((x-mean(x))^2*sum((y-mean(y))^2))))) ## [1] &quot;Pearson: 1.05454545454545&quot; 47.2 Clustering methods Cluster analysis is the most common classification technique. Clusterings goal is to separate data groups in a way that minimizes intragroup variation while maximizing intergroup variation. Cluster analysis techniques themselves can be clustered! Partition clustering includes k-means, partition around medioids (PAM) and clustering large applications (CLARA). Heirarchical clustering can be either agglomerative or divisive. In clustering, although the researcher is not involved in deciding which data points belong in which clusters or dendroids, it is sometimes necessary to make a decision on how many clusters to include in the model. 47.3 Principal component analysis Principal component analysis (PCA) is a method used in identifying the number of clusters to model. The method works to reduce the dimensional complexity of a matrix of measurements. A simple experiment with 4 explanatory groups each generating many measurements (for example, measuring the expression of many genes simultaneously), \\(p\\), has \\(p(p-1)/2 = 6\\) dimensions. These could be viewed, for example, by drawing all 6 possible correlations between the 4 groups. If youve ever looked at a large correlation matrix you probably got lost pretty quickly. PCA offers the ability to reduce the complexity to a fewer number of principal explanatory dimensions. We can think of these as latent grouping variables. The more groups in the experiment, the more necessary it becomes to be less concerned about dimensions of the data that dont allow for much explanatory insight. NCSS has a very thorough mathematical and practical treatment of principal components if you are interested in learning more. 47.4 Synthesize some data Simulated data are used here to illustrate how to run cluster analysis using R. Well see how well the technique works to identify the four groups that are built into the dataset. The data below are 1000 simulated random normal measurements (rows) for each of 4 groups (columns). Each group is built to have a unique combination of measurements from a continuous scale. The four groups correspond to four variables called foo, fu, foei, and fuz. You can imagine what df might be. For example, every row might represent a unique mRNA id. Every column might represent a different cell type or tissue. Every value is an expression level in some kind of units. Obviously, from the code, we can see they each are sampled from N(mu, sigma). Note, df is a matrix object, not a dataframe. #create a matrix set.seed(1234) df &lt;- rbind( cbind(rnorm(250, 1, 10), rnorm(250, 3, 10), rnorm(250, 10, 10), rnorm(250, 30, 10) ), cbind(rnorm(250, 30, 10), rnorm(250, 10, 10), rnorm(250, 3, 10), rnorm(250, 1, 10) ), cbind(rnorm(250, 1, 10), rnorm(250, 30, 10), rnorm(250, 3, 10), rnorm(250, 100, 10) ), cbind(rnorm(250, 3, 10), rnorm(250, 10, 10), rnorm(250, 30, 10), rnorm(250, 100, 10) ) ) Some housekeeping: Add column names. Randomize the row to order in the data set just to mix things up. Add row names after randomizing the cases. Note that the final product is a matrix, not a data frame. Because we need to feed a matrix into the cluster analysis functions. #add column names colnames(df) &lt;- c(&quot;foo&quot;, &quot;fu&quot;, &quot;foei&quot;, &quot;fuz&quot;) #randomize cases by row df &lt;- df[sample(1:nrow(df)), ] #add case id&#39;s to each row. note how this keeps the overall dataset matrix. row.names(df) &lt;- paste(&quot;S&quot;, 1:nrow(df), sep=&quot;&quot;) 47.5 Visualize the data set First some housekeeping: Create a data frame because ggplot needs one, which includes creating an id variable for the x-axis. Make it long. id &lt;- row.names(df) df1 &lt;- data.frame(id,df) %&gt;% pivot_longer(-id, names_to=&quot;variable&quot;, values_to=&quot;value&quot;) This is just a plot of some randomly selected rows. To some extent, they just look like random crappy data. The point is, looking at this you wouldnt probably imagine these datapoints come from a dataset with an underlying clustered structure. ggplot(df1, aes(id, value, color=variable))+ geom_point(shape=1,size=4)+ scale_color_viridis_d()+ scale_x_discrete(limits=c(&quot;S3&quot;, &quot;S232&quot;, &quot;S236&quot;, &quot;S582&quot;, &quot;S704&quot;, &quot;S757&quot; ))+ labs(y=&quot;value of variable&quot;, x=&quot;case id&quot;, title=&quot;4 variables (by color) measured in each of 6 cases (for examples)&quot;)+ theme_classic() Figure 47.1: Representative row samples drawn from data set. Now here is a plot of all the data. It is a bit more evident of some structure. Many, but not all, of the fuz variable is much higher than the others. But with the others it is hard to see any structure. ggplot(df1, aes(id, value, color=variable))+ geom_point(shape=1, size=2)+ scale_color_viridis_d()+ scale_x_discrete(breaks=NULL)+ labs(y=&quot;value of variable&quot;, x=&quot;case id&quot;, title=&quot;4 dependent variables (by color) measured in each of 1000 cases&quot;)+ theme_classic() Figure 47.2: All of the data values plotted. 47.6 Rescaling data Each dependent variable needs to be on the same scale as all others. Thats accomplished by rescaling the data. One way is z-standardization. This z-transforms each column of df. Note how this is a matrix. Well be passing this matrix into the clustering functions. df2 &lt;- scale(df) But first, lets look at the standardized data. we have to first make a dataframe out of the df2 matrix. Still a hint of some structure due to fuz, otherwise, not so much. df3 &lt;- data.frame(id, df2) %&gt;% pivot_longer(-id, names_to=&quot;variable&quot;, values_to=&quot;value&quot;) ggplot(df3, aes(id, value, color=variable))+ geom_point(shape=1, size=2)+ scale_color_viridis_d()+ scale_x_discrete(breaks=NULL)+ labs(y=&quot;z-score variable&quot;, x=&quot;1000 cases&quot;, title=&quot;4 STANDARDIZED variables (by color) in each of 1000 cases&quot;)+ theme_classic() Figure 47.3: Log transformed data set. 47.7 Are there clusters? Principal component analysis identifies the principal components, which will be used to decide how many clusters to model. pr &lt;- prcomp(df2, scale. = F, center = T) summary(pr) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.3589 1.0926 0.8308 0.51918 ## Proportion of Variance 0.4616 0.2984 0.1725 0.06739 ## Cumulative Proportion 0.4616 0.7601 0.9326 1.00000 From the summary output we see that the first principal component explains 46.16% of the variance, PC2 explains 29.84% of the variance, while PC3 and PC4 explain successively less variance. The default output of the prcomp is the rotation table. The values are the eigenvalues of the covariance matrix. They are best thought of as the coefficients (linear predictors) for each of the eigenvectors. pr ## Standard deviations (1, .., p=4): ## [1] 1.3588598 1.0926165 0.8307504 0.5191753 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## foo 0.5534303 -0.11290119 0.7485530 -0.3473279 ## fu -0.3168913 -0.75306737 0.3375968 0.4674375 ## foei -0.3761199 0.64094934 0.5509381 0.3797170 ## fuz -0.6721820 -0.09657536 0.1488758 -0.7188050 The \\(x\\) table is perhaps the most useful. It provides the \\(x,y\\) coordinates for plotting clusters for all of the replicates. head(pr$x) ## PC1 PC2 PC3 PC4 ## S1 1.081396 0.49837980 -1.4214491 0.05566199 ## S2 2.166028 -0.08855379 0.1352516 -0.33131648 ## S3 1.328890 0.80113921 -0.8603810 -0.58532180 ## S4 -1.893312 0.45258854 0.4644622 0.24081669 ## S5 1.340275 0.45462150 0.7926811 0.22741040 ## S6 -1.043440 1.00173799 2.1027785 0.29594449 Heres a way to visualize all of the data on the basis of its first two principal components: Dim1 and Dim2. Well always see the greatest separation by running a correlation plot between the first two dimensions. fviz_pca_ind(prcomp(df2), geom=&quot;point&quot;) Heres correlation plot between the first two principal components for a uniform distribution of objects, for comparison. It would take an active imagination to draw out clusters from this. Its pretty uniform. set.seed(12345) sn &lt;- matrix(c(rnorm(1000, 0,1), rnorm(1000, 0, 1)), ncol=2, nrow=500) prcomp(sn, scale. = F, center = F) ## Standard deviations (1, .., p=2): ## [1] 1.0073077 0.9932779 ## ## Rotation (n x k) = (2 x 2): ## PC1 PC2 ## [1,] -0.1470789 0.9891248 ## [2,] 0.9891248 0.1470789 fviz_pca_ind(prcomp(sn), geom=&quot;point&quot;) 47.8 How many clusters? fviz_nbclust(df2, kmeans, method=&quot;wss&quot;) This is known as a scree plot. Its useful as a tool to make decisions about how many clusters to model. It shows the amount of variation each dimension can explain. The critical question in clustering is how many dimensions should be modeled? If we model them all we can explain all of the variation by a large number of clusters. But thats over-fitting. Statistical models with too many parameters are always harder to interpret and to explain. Lets move through the points from the top left on down the curve. The segment between points 1 and 2 explain a LOT of variation; more than a quarter and less than one-half of the total variation. We can think of this segment as the first principal component or the first dimension of the data set. The segment between points 2 and 3 explains a lot more variation, but less variation than explained by the first. Perhaps not quite 1/4th of the overall variation is explained by this second principal component, or 2nd dimension. So does the segment between points 3 and 4 seems to explain about 1/8th of the overal variation. It is the 3rd principal component, or 3rd dimension. The segment between points 4 and 5 accounts for a lot less variation, as do the segments through each of the succeeding points. At the elbow the additional segments are not explaining that much more variation. In other words, to model more clusters becomes something of an over-fit. Its arguable to say the segment between 4 and 5 is really explaining a good amount of variation. The take away from this scree plot is that the data are explained probably by 3 clusters, maybe by 4 clusters. The bloody obvious test will be used to decide. The silhouette method is another way to decide on the number of clusters. The idea is to the peak average silhouette width == the number of clusters to model. YMMV. If not, the number just below the peak. fviz_nbclust(df2, kmeans, method=&quot;silhouette&quot;) 47.9 Clustering The function kmeans does the clustering given a number of clusters to model. Here it is instructed fit a 3 cluster model to the data. It assigns every row to one of these 3 model clusters. Explore the object kmdf2, because it is packed with information. Note how it assigns every row to a cluster. km.df2 &lt;- kmeans(df2, centers=3, nstart=100) Aggregate is a good way to subset the clusters by the grouping variables. Aggregate says, \"Given the three clusters, here are their values for each of the grouping variables. If you prefer, you could add the cluster ID to a dataframe along with df2, and then use group_by and summarize to do the same thing. aggregate(df, by=list(cluster=km.df2$cluster), mean) ## cluster foo fu foei fuz ## 1 1 0.8173983 30.669165 2.263343 97.62785 ## 2 2 19.1839140 6.814412 3.746465 12.90336 ## 3 3 1.5462796 9.145715 27.293735 86.44426 For example, the average values of the foo variable in each of the 3 clusters are 1.5, 0.8 and 19.2. And we can visualize the clusters. Note, were passing in the kmeans model of 3 clusters into this. Toggle the ellipse argument fviz_cluster(km.df2, data=df, palette=c(&quot;#440154FF&quot;, &quot;#39568CFF&quot;, &quot;#1F968BFF&quot;, &quot;#73D055FF&quot;), geom=&quot;point&quot;, ellipse=F, shape=1, pointsize=4, ggtheme=theme_bw()) Now lets model this with 4 clusters. We recreate the km.df2 object, check the aggregate. Toggle the ellipse on and off. What do you think? Is this a better fit than 3 clusters? km.df2 &lt;- kmeans(df2, 4, nstart=100) aggregate(df, by=list(cluster=km.df2$cluster), mean) ## cluster foo fu foei fuz ## 1 1 30.0729920 10.759380 2.105854 4.179751 ## 2 2 -0.2360265 2.161194 8.770106 29.889766 ## 3 3 3.5135916 10.512234 29.754152 98.821411 ## 4 4 1.1075641 31.329134 2.434274 99.879498 fviz_cluster(km.df2, data=df, palette=c(&quot;#440154FF&quot;, &quot;#39568CFF&quot;, &quot;#1F968BFF&quot;, &quot;#73D055FF&quot;), geom=&quot;point&quot;, ellipse=F, shape=1, pointsize=4, ggtheme=theme_bw()) 47.10 PAM and CLARA PAM and CLARA are alternative algorithms to kmeans. All three are likely to correlate pretty well but note how PAM and kmeans differ quite markedly in the 3 cluster model, yet arrive at about the same clustering for the 4 cluster model. pam.df2 &lt;- pam(df2, 4) fviz_cluster(pam.df2, palette=c(&quot;#440154FF&quot;, &quot;#39568CFF&quot;, &quot;#1F968BFF&quot;, &quot;#73D055FF&quot;), geom=&quot;point&quot;, ellipse=F, shape=1, pointsize=4, ggtheme=theme_bw()) pam.df2 &lt;- pam(df2, 3) fviz_cluster(pam.df2, palette=c(&quot;#440154FF&quot;, &quot;#39568CFF&quot;, &quot;#1F968BFF&quot;, &quot;#73D055FF&quot;), geom=&quot;point&quot;, ellipse=F, shape=1, pointsize=4, ggtheme=theme_bw()) 47.11 Hierarchical clustering Hierarchical clustering is a step more informative than kmeans clustering because it provides information about relationships between clusters. We start with the data set matrix, dffrom above. Agglomerative hierarchical clustering is performed using the agnes function. Agglomerative operates by using each measurement as a seed, to which the most similar other measurements are merged. This turns out to be fairly intensive, computationally, compared to the previous techniques. Exect delays. As in kmeans, we define the number of clusters the algorithm should model. rag &lt;- agnes(x=df, stand=T, metric=&quot;euclidian&quot;, method=&quot;ward&quot;) fviz_dend(rag, cex=0.01, k=4) The interpretation of these output is fairly straight forward. Any two closely linked elements have less distance between them than to elements to which they are less closely linked. For example, in the figure above the orange and green clusters are more related to each other than they are to the blue, and all three are more related than any one is to the purple. Divisive hierarchical clustering is performed using the diana function. This operates opposite of agglomerative clustering. All measurements begin in a single cluster and then are successively divided into heterogenous clusters. rdi &lt;- diana(x=df, stand=T, metric=&quot;euclidian&quot;) fviz_dend(rdi, cex=0.01, k=4, method=&quot;ward&quot;) The output differences between the two hierarchical techniques are subtle but clearly evident. They are different algorithms and therefore they provide different results. Neither is more wrong than the other. There are several additional hierarchical vizualizations that are possible, phylogenic trees, circular dendrograms, and more. 47.12 Heatmaps Heatmaps are just a visual descriptive technique to pair the magnitude of measurements with hierarchical clusters. This particular function is also performing some clustering. This viz pretty much makes the case for four clusters. But the functions arguments could be modified to see how well that holds. heatmap(df2, scale=&quot;column&quot;) 47.13 Summary Classification techniques using R are probably the main gateway to exploratory data analysis. Once cluster grouping has been determined, the sky is the limit for how that information can be explored further. We havent talked about inferential methods. It is possible to make inferential decisions about the value of some outcome, particularly in consideration of alternatives. "],["rnaseq.html", "Chapter 48 RNA-seq with R 48.1 Install Bioconductor 48.2 Import raw count data 48.3 Munge to simpler table 48.4 Filtering 48.5 DGEList 48.6 Visualization 48.7 Classification 48.8 Hierarchical clustering 48.9 Summary", " Chapter 48 RNA-seq with R library(tidyverse) library(edgeR) library(limma) library(gplots) library(viridis) This example is adopted extensively from RNA-seq analysis in R by Belinda Phips and colleagues. The data are available here and here. These are part of a broader workship presented by the group with very easy to follow protocols for RNA-seq analysis with R. There is a publication, also The data below are from a study of mammary gland development showing the pro-survival gene Mcl1 as a master regulator gland development. They represent duplicate RNA-seq measurements from each of 6 groups: basal and luminal cells from virgin, pregnant and lactating mice. The purpose of having this in JABSTB at this stage is to offer a gentle introduction to RNA-seq data and analysis. The focus here is on the classification methods within this example. 48.1 Install Bioconductor This chapter uses functions in Bioconductor. Bioconductor is a universe of packages and functions geared for bio/omics data analysis. It is managed separately from the broader CRAN universe. As for the CRAN universe, we dont automagically have the full universe of Bioconductor packages by default. We just get a working base suite. If we want more, we have to find it and install it. The most important difference is to use the Bioconductor package manager to install packages. When we want to install additional Bioconductor packages we would do so this way, BiocManager::install(\"newthing\"), not this way, install.packages(\"newthing\") Go to that link and read more about Bioconductor. Then install Bioconductor by running the script below in your console. There is much to install. This will take a while. Do NOT compile anything If you see the prompt below, always choose no: Do you want to install from sources the package which needs compilation (yes/no/cancel)? # if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) # install.packages(&quot;BiocManager&quot;) # BiocManager::install(version = &quot;3.10&quot;) 48.2 Import raw count data By this point, there has already been considerable processing of the sequence data. Sequences have been matched to their genes and to each other. The values in this table are counts of the number of transcript reads per gene, in each of the 12 samples. Read them into an object. seqdata &lt;- read.delim(&quot;datasets/GSE60450_Lactation-GenewiseCounts.txt&quot;) head(seqdata) ## EntrezGeneID Length MCL1.DG_BC2CTUACXX_ACTTGA_L002_R1 ## 1 497097 3634 438 ## 2 100503874 3259 1 ## 3 100038431 1634 0 ## 4 19888 9747 1 ## 5 20671 3130 106 ## 6 27395 4203 309 ## MCL1.DH_BC2CTUACXX_CAGATC_L002_R1 MCL1.DI_BC2CTUACXX_ACAGTG_L002_R1 ## 1 300 65 ## 2 0 1 ## 3 0 0 ## 4 1 0 ## 5 182 82 ## 6 234 337 ## MCL1.DJ_BC2CTUACXX_CGATGT_L002_R1 MCL1.DK_BC2CTUACXX_TTAGGC_L002_R1 ## 1 237 354 ## 2 1 0 ## 3 0 0 ## 4 0 0 ## 5 105 43 ## 6 300 290 ## MCL1.DL_BC2CTUACXX_ATCACG_L002_R1 MCL1.LA_BC2CTUACXX_GATCAG_L001_R1 ## 1 287 0 ## 2 4 0 ## 3 0 0 ## 4 0 10 ## 5 82 16 ## 6 270 560 ## MCL1.LB_BC2CTUACXX_TGACCA_L001_R1 MCL1.LC_BC2CTUACXX_GCCAAT_L001_R1 ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 3 10 ## 5 25 18 ## 6 464 489 ## MCL1.LD_BC2CTUACXX_GGCTAC_L001_R1 MCL1.LE_BC2CTUACXX_TAGCTT_L001_R1 ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 2 0 ## 5 8 3 ## 6 328 307 ## MCL1.LF_BC2CTUACXX_CTTGTA_L001_R1 ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 10 ## 6 342 To see the data sets dimensions. dim(seqdata) ## [1] 27179 14 There are 14 columns and over 27000 rows. The information about the sample will be used later. Read it into the environment now. sampleinfo &lt;- read.delim(&quot;datasets/SampleInfo_Corrected.txt&quot;) sampleinfo ## FileName SampleName CellType Status ## 1 MCL1.DG_BC2CTUACXX_ACTTGA_L002_R1 MCL1.DG basal virgin ## 2 MCL1.DH_BC2CTUACXX_CAGATC_L002_R1 MCL1.DH basal virgin ## 3 MCL1.DI_BC2CTUACXX_ACAGTG_L002_R1 MCL1.DI basal pregnant ## 4 MCL1.DJ_BC2CTUACXX_CGATGT_L002_R1 MCL1.DJ basal pregnant ## 5 MCL1.DK_BC2CTUACXX_TTAGGC_L002_R1 MCL1.DK basal lactate ## 6 MCL1.DL_BC2CTUACXX_ATCACG_L002_R1 MCL1.DL basal lactate ## 7 MCL1.LA_BC2CTUACXX_GATCAG_L001_R1 MCL1.LA luminal virgin ## 8 MCL1.LB_BC2CTUACXX_TGACCA_L001_R1 MCL1.LB luminal virgin ## 9 MCL1.LC_BC2CTUACXX_GCCAAT_L001_R1 MCL1.LC luminal pregnant ## 10 MCL1.LD_BC2CTUACXX_GGCTAC_L001_R1 MCL1.LD luminal pregnant ## 11 MCL1.LE_BC2CTUACXX_TAGCTT_L001_R1 MCL1.LE luminal lactate ## 12 MCL1.LF_BC2CTUACXX_CTTGTA_L001_R1 MCL1.LF luminal lactate 48.3 Munge to simpler table Create a table that has only count data so that we can do several manipulations of the gene expression data. countdata &lt;- seqdata[, 3:14] Note below how the EntrezGeneID is placed back in as row names. The row names are not a variable. rownames(countdata) &lt;- seqdata[,1] This is a data frame now with row and column names. Each column corresponds to a biological replicate, each row a gene id. Every cell in a row is the raw transcript counts for that gene under that replicate condition. Shorten the column names. They start with 7 non-identical characters which are good identifiers. colnames(countdata) &lt;- substring(colnames(countdata), 1,7) head(countdata) ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL MCL1.LA MCL1.LB ## 497097 438 300 65 237 354 287 0 0 ## 100503874 1 0 1 1 0 4 0 0 ## 100038431 0 0 0 0 0 0 0 0 ## 19888 1 1 0 0 0 0 10 3 ## 20671 106 182 82 105 43 82 16 25 ## 27395 309 234 337 300 290 270 560 464 ## MCL1.LC MCL1.LD MCL1.LE MCL1.LF ## 497097 0 0 0 0 ## 100503874 0 0 0 0 ## 100038431 0 0 0 0 ## 19888 10 2 0 0 ## 20671 18 8 3 10 ## 27395 489 328 307 342 Now were in pretty good shape in terms of having a simple view of the raw count data. 48.4 Filtering Next we need to filter out the genes for which there are no reads, or there are inconsistent reads across replicate samples, or there are low reads. This is a multistep process. The first step is to choose a normalization technique. RPKM (reads per kilo base per million) and CPM (counts per million) are common options. Well use the latter. Our filtering rule is to keep transcripts that have CPM &gt; 0.5 in at least two samples. A CPM of 0.5 corresponds to roughly 10-15 counts per gene in this sized library. This threshold decision is a scientific judgement based upon ones experience regarding the sensitivity of the count measurement. Thresholds of 1 or 2 CPM are not uncommon. First, convert raw counts to CPM. myCPM &lt;- edgeR::cpm(countdata) head(myCPM) ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL ## 497097 18.85684388 13.77543859 2.69700983 10.45648006 16.442685 14.3389690 ## 100503874 0.04305215 0.00000000 0.04149246 0.04412017 0.000000 0.1998463 ## 100038431 0.00000000 0.00000000 0.00000000 0.00000000 0.000000 0.0000000 ## 19888 0.04305215 0.04591813 0.00000000 0.00000000 0.000000 0.0000000 ## 20671 4.56352843 8.35709941 3.40238163 4.63261775 1.997275 4.0968483 ## 27395 13.30311589 10.74484210 13.98295863 13.23605071 13.469996 13.4896224 ## MCL1.LA MCL1.LB MCL1.LC MCL1.LD MCL1.LE MCL1.LF ## 497097 0.0000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000 ## 100503874 0.0000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000 ## 100038431 0.0000000 0.0000000 0.0000000 0.00000000 0.0000000 0.0000000 ## 19888 0.4903857 0.1381969 0.4496078 0.09095771 0.0000000 0.0000000 ## 20671 0.7846171 1.1516411 0.8092940 0.36383085 0.1213404 0.4055595 ## 27395 27.4615975 21.3744588 21.9858214 14.91706476 12.4171715 13.8701357 Next, impose the threshold. First, this script is a simple logical that identifies genes and groups that satisfy the first part of the filtering rule. thresh &lt;- myCPM &gt; 0.5 head(thresh) ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL MCL1.LA MCL1.LB ## 497097 TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE ## 100503874 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 100038431 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 19888 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 20671 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## 27395 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## MCL1.LC MCL1.LD MCL1.LE MCL1.LF ## 497097 FALSE FALSE FALSE FALSE ## 100503874 FALSE FALSE FALSE FALSE ## 100038431 FALSE FALSE FALSE FALSE ## 19888 FALSE FALSE FALSE FALSE ## 20671 TRUE FALSE FALSE FALSE ## 27395 TRUE TRUE TRUE TRUE Heres a summary of that result. table(rowSums(thresh)) ## ## 0 1 2 3 4 5 6 7 8 9 10 11 12 ## 10857 518 544 307 346 307 652 323 547 343 579 423 11433 There are 10857 genes that are \\(\\le 0.5\\) CPM in all twelve samples. There are 518 genes that have greater than 0.5 CPM in only 1 sample. 544 genes have greater than 0.5 CPM in only two samples, 307 genes with greater than 0.5 CPM in 3 samples, and so on. There are 11433 genes which have greater than 0.5 CPM in all twelve samples. Second, identify the genes for which the second condition above is satisfied. This is another logical. And it just creates a long logical vector, with a True or False corresponding to each row name. keep &lt;- rowSums(thresh) &gt;= 2 summary(keep) ## Mode FALSE TRUE ## logical 11375 15804 Thus, there are 15804 genes which have greater than 0.5 CPM in at least two samples. Here is an updated counts dataset containing only those genes that are filtered. This is the final filtered dataset which will be used for the statistical analysis. # Subset the rows of countdata to keep the more highly expressed genes counts.keep &lt;- countdata[keep,] head(counts.keep) ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL MCL1.LA MCL1.LB MCL1.LC ## 497097 438 300 65 237 354 287 0 0 0 ## 20671 106 182 82 105 43 82 16 25 18 ## 27395 309 234 337 300 290 270 560 464 489 ## 18777 652 515 948 935 928 791 826 862 668 ## 21399 1604 1495 1721 1317 1159 1066 1334 1258 1068 ## 58175 4 2 14 4 2 2 170 165 138 ## MCL1.LD MCL1.LE MCL1.LF ## 497097 0 0 0 ## 20671 8 3 10 ## 27395 328 307 342 ## 18777 646 544 581 ## 21399 926 508 500 ## 58175 60 27 15 woot! 48.5 DGEList The counts.keep dataframe is converted into a list object for ease of analysis moving forward. It will be passed into other functions within the Bioconductor universe. y &lt;- edgeR::DGEList(counts.keep) y ## An object of class &quot;DGEList&quot; ## $counts ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL MCL1.LA MCL1.LB MCL1.LC ## 497097 438 300 65 237 354 287 0 0 0 ## 20671 106 182 82 105 43 82 16 25 18 ## 27395 309 234 337 300 290 270 560 464 489 ## 18777 652 515 948 935 928 791 826 862 668 ## 21399 1604 1495 1721 1317 1159 1066 1334 1258 1068 ## MCL1.LD MCL1.LE MCL1.LF ## 497097 0 0 0 ## 20671 8 3 10 ## 27395 328 307 342 ## 18777 646 544 581 ## 21399 926 508 500 ## 15799 more rows ... ## ## $samples ## group lib.size norm.factors ## MCL1.DG 1 23218026 1 ## MCL1.DH 1 21768136 1 ## MCL1.DI 1 24091588 1 ## MCL1.DJ 1 22656713 1 ## MCL1.DK 1 21522033 1 ## 7 more rows ... 48.6 Visualization A lot of the -omic visualizations use the plotting functions of R base. It is fairly easy to get something in the box with these plotting functions, but a bit more difficult to get them gussied up all nice and pretty. Comparing library sizes checks for any anomalies. There are none. Each sample library size is about the same. barplot(y$samples$lib.size,names=colnames(y),las=2) # Add a title to the plot title(&quot;Comparison of library sizes&quot;) Figure 48.1: Sizes of RNAseq libraries for each sample. Heres a plot of the raw counts, in order to illustrate they are not normally-distributed, which is typical of discrete count data. boxplot(y$counts, las=2) Figure 48.2: Distribution of counts across the 12 samples. Note they are not normally-distributed. Heres a plot of the CPM, which is also not normally distributed. This illustrates that CPM is just a simple linear transform of count data. boxplot(cpm(y$counts), las=2) Figure 48.3: Distribution of CPM-transformed counts, still not normally-distributed. Heres a plot of a log transformation of CPM data. This transformation yields an approximately Gaussian distribution of the count values within each sample, though there are clearly outliers. The log transformed CPM data will be used in a lot of the statistical analysis because of this Gaussian property. So well go ahead and make an object for that. # Get log2 counts per million logcounts &lt;- cpm(y,log=TRUE) Now look at the logcount date by plotting box plots. # Check distributions of samples using boxplots boxplot(logcounts, xlab=&quot;&quot;, ylab=&quot;Log2 counts per million&quot;,las=2) # Let&#39;s add a blue horizontal line that corresponds to the median logCPM abline(h=median(logcounts),col=&quot;blue&quot;) title(&quot;Boxplots of logCPMs (unnormalised)&quot;) Figure 48.4: Natural log transformed CPM counts, normally-distributed but some outliers. 48.7 Classification Multidimensional scaling is a cousin of principal component analysis. It provides a simple way to see if sample groups separate along their first and second dimensions. This is based upon a leading fold-change metric. This examines the subset of genes that exhibit the largest fold-differences between samples. The graph below shows separation and clustering of the 12 samples, but it is a bit hard to see what is what unless youve remembered what each sample represents. limma::plotMDS(y) Figure 48.5: Multi-dimensional scaling, a form of PCA. Well munge out some better plots below, focusing on the the classification of the predictor variables. Recall there are 12 samples, duplicates of gene expression in basal and luminal cells, for each of the following three conditions: the cells were derived from mice that were virgin, pregnant, or lactating. First well color by the feature that involves basal and luminal. # Let&#39;s set up colour schemes for CellType # How many cell types and in what order are they stored? levels(as.factor(sampleinfo$CellType)) ## [1] &quot;basal&quot; &quot;luminal&quot; col.cell &lt;- c(&quot;#012169&quot;,&quot;#b58500&quot;)[as.factor(sampleinfo$CellType)] data.frame(sampleinfo$CellType,col.cell) ## sampleinfo.CellType col.cell ## 1 basal #012169 ## 2 basal #012169 ## 3 basal #012169 ## 4 basal #012169 ## 5 basal #012169 ## 6 basal #012169 ## 7 luminal #b58500 ## 8 luminal #b58500 ## 9 luminal #b58500 ## 10 luminal #b58500 ## 11 luminal #b58500 ## 12 luminal #b58500 The plot below clearly illustrates that the basal/luminal feature represents the first dimension of separation between the samples. In other words, the differences between basal and luminal cell gene expression accounts for most of the variation in the set. # Redo the MDS with cell type colouring plotMDS(y,col=col.cell) # Let&#39;s add a legend to the plot so we know which colours correspond to which cell type legend(&quot;topleft&quot;, fill=c(&quot;#012169&quot;,&quot;#b58500&quot;),legend=levels(as.factor(sampleinfo$CellType))) # Add a title title(&quot;Cell type&quot;) Figure 48.6: MDS shows variation due to cell type explains the first dimension of the data. Well do the same coloring trick for status. col.status &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;)[as.factor(sampleinfo$Status)] col.status ## [1] &quot;green&quot; &quot;green&quot; &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;green&quot; &quot;green&quot; &quot;blue&quot; ## [10] &quot;blue&quot; &quot;red&quot; &quot;red&quot; The plot below colors each sample on the basis of whether it is from a virgin, pregnant, or lactating mouse. There is some separation of them along the 2nd dimension. Note how the duplicates of the same condition are very similar (the MCL1.DL lactating sample is missing for some reason?) plotMDS(y,col=col.status) legend(&quot;topleft&quot;,fill=c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;), legend=levels(as.factor(sampleinfo$Status)),cex=0.8) title(&quot;Status&quot;) Figure 48.7: Variation due to status represents the 2nd dimension of the data set. Thus, although these dimensions are latent, it is often possible to explain what variables are most responsible for the observed variation, and thus explain the first two principal components. The Bioconductor package PCAtoolsis a veritable smorgasbord of PCA functions and visualizations. Learning this package is strongly recommended. For more on it, see this vignette 48.8 Hierarchical clustering Hierarchical clustering is a way to cluster while visualizing relationships to other clusters. Before illustrating this technique some processing is in order. Were interested in seeing the genes driving the dimensional pattern seen above. What genes differ between the two cell types? What genes differ between the 3 status conditions? Is there an interaction between cell type and status? We can assume the most useful approach to answer this would be to focus on the genes that have the highest expression variance across the 12 samples. Rather than cluster all 15000+ genes, well cluster the 500 that are most variable. This is an arbitrary cutoff. This script creates a vector of variances with associated with each GeneID. Just like in ANOVA, when row variances are high, differences in grouping factors will be greatest. var_genes &lt;- apply(logcounts, 1, var) head(var_genes) ## 497097 20671 27395 18777 21399 58175 ## 13.6624115 2.7493077 0.1581944 0.1306781 0.3929526 4.8232522 Now produce a vector with the GeneID names for those that have the greatest to lower variances, up to the 500th. # Get the gene names for the top 500 most variable genes select_var &lt;- names(sort(var_genes, decreasing=TRUE))[1:500] head(select_var) ## [1] &quot;22373&quot; &quot;12797&quot; &quot;11475&quot; &quot;11468&quot; &quot;14663&quot; &quot;24117&quot; For your information, here is the expression pattern for the GeneID 22373 with the greatest variance. It encodes Wap, a known regulator of mammary epithelium. There is much lower expression in virgin basal cells compared to the others. logcounts[&quot;22373&quot;,] ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL MCL1.LA ## 0.4660671 0.9113837 7.4350797 7.8078998 9.2883180 9.3184826 6.6277452 ## MCL1.LB MCL1.LC MCL1.LD MCL1.LE MCL1.LF ## 6.6884102 12.1273320 13.1502579 15.6481000 15.5696858 From logcounts we select the rows corresponding to these 500 most variable genes. # Subset logcounts matrix highly_variable_lcpm &lt;- logcounts[select_var,] dim(highly_variable_lcpm) ## [1] 500 12 head(highly_variable_lcpm) ## MCL1.DG MCL1.DH MCL1.DI MCL1.DJ MCL1.DK MCL1.DL MCL1.LA ## 22373 0.4660671 0.9113837 7.435080 7.807900 9.288318 9.318483 6.6277452 ## 12797 10.0429713 9.6977966 11.046567 11.358857 11.605894 11.491773 0.8529774 ## 11475 12.3849005 12.3247093 13.989573 14.180048 14.285489 14.032486 3.4823396 ## 11468 7.1537287 6.8917703 9.325436 9.661942 9.491765 9.424803 -1.8086076 ## 14663 1.9717614 1.9471846 9.091895 8.756261 9.539747 9.504098 6.1710357 ## 24117 7.8378853 7.8995788 8.634622 8.582447 6.704706 6.777335 -1.3824015 ## MCL1.LB MCL1.LC MCL1.LD MCL1.LE MCL1.LF ## 22373 6.6884102 12.1273320 13.1502579 15.6481000 15.569686 ## 12797 1.6034598 2.3323371 2.4601069 0.8055694 1.288003 ## 11475 4.3708241 5.2116574 5.0788442 3.6997655 3.965775 ## 11468 -0.6387584 0.5244769 0.6694047 -0.4412496 -1.014878 ## 14663 6.2328260 13.7571928 14.2506761 16.0020840 15.885390 ## 24117 -0.5387838 -0.2280797 0.1243601 -2.9468278 -2.945610 Now we simply pass this select group of the 500 most variable genes into the heatmap.2 function. The values represented here are logCPM values. # Set up colour vector for celltype variable col.cell &lt;- c(&quot;#012169&quot;,&quot;#b58500&quot;)[sampleinfo$CellType] # Plot the heatmap heatmap.2(highly_variable_lcpm, col=viridis, trace=&quot;none&quot;, main=&quot;Top 500 most variable genes across samples&quot;, ColSideColors=col.cell, scale=&quot;row&quot;) Expression varies from low (dark) to high (light). Inspection of the horizontal clustering illustrates how it picks up the the experimental design very well. There are two main groups (corresponding to the luminal (Emory gold) and basal (Emory blue) cell types). There are also 3 groups within each of these, corresponding to the status. The duplicates line up very well together. This is tight data. The vertical clustering is very interesting. Over 3 quarters of the genes define the cell type differentiation, while the rest differentiate the status (virgin, pregnant, lactating). There is a clear interaction between cell type and status, as well. 48.9 Summary This chapter is derived from an excellent workshop on using R to work with RNA-seq data. The workshop material is an excellent starting point for learning how to work with this data. Ive only covered a small portion of that material and made just a few very modest changes. My goal is making a gentle introduction to working with the data, keeping a focus on classification (MDS and hierarchical clustering). Install Bioconductor to use this material. Before doing so, it is important to recognize how Bioconductor relates to R and to CRAN. I strongly recommend installing additional Bioconductor packages using the BiocManager. This workflow becomes more important when the time comes to updating, whether R or Bioconductor. The fundamental currency of RNA-seq data are transcript counts. To work with them requires transformation via normalization (such as CPM or RPKM). Counts are not normally distributed. For many statistical treatments the CPM need conversion to a Gaussian distribution. Natural log transformation usually gets this done. Scientific judgements are necessary to limit the scope of the datasets. Working with RNA-seq data demands R skills related to creating and working with a variety of on-the-fly data objects, all while keeping ones rows and columns copacetic. "]]
