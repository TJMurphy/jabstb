[
["index.html", "JABSTB: Statistical Design and Analysis of Experiments with R Preface", " JABSTB: Statistical Design and Analysis of Experiments with R TJ Murphy PhD, Department of Pharmacology, School of Medicine, Department of Biostatistics and Bioinformatics, Rollins School of Public Health, Emory University, Atlanta, GA biostats538@gmail.com 2018-11-19 Preface This “book” is actually a bunch of RMarkdown handouts stitched together. This is intended as a resource for a semester-long “biostats” course that I teach at Emory University entitled, Statistical Design and Analysis of Experiments. Not included is additional material (eg, take home and group exercises, slide decks, data sets, cartoons, etc). The audience is (mostly) students in Emory University’s interdepartmental biomedical and biological sciences PhD programs. There is the ocassional Emory undergrad, students from Emory’s public health school, and usually a few Georgia Tech students. Each chapter corresponds to a RMarkdown document. The first few chapters deal with statistical fundamentals that are applicable to all types of statistical analysis. From then on, beginning with a chapter on the statistics of categorical variables, the material covers the statistical analysis of the three main types of data: sorted, ordered and measured. The basic idea is to illustrate how to analyze a certain kind of data phenotype, within a certain kind of experimental structure. Most of the chapters have R script embedded in code chunks. These scripts generally serve two purposes. The first is to illustrate a statistical principle. The second is so studentds can read and learn how to write their own R code for that specific purpose. For that reason, the html of the code chunks in the book are admittedly bit long at times. To have the chunks in hand, the reader can download this book from Github. To more conveniently access those RMarkdown documents and all that script. Just bear in mind, this book is a living document and will be subjected to a lot of on the fly revision. Speaking of which, if you find any errors or have any suggestions please send me a pull request via Github ((???)). You are free to use this material as you wish, available via the Creative Commons license. Copyright 2018 © TJ Murphy "],
["about.html", "Chapter 1 About 1.1 About the author 1.2 About this book 1.3 Software 1.4 Getting started with R", " Chapter 1 About 1.1 About the author I’m a biomedical scientist. But I’ve always had a high level of interest in statistical methods. I learned “statistical design and analysis of experiments” as a graduate student at Mizzou in a couple of deeply impactful courses offered by the late Gary Krause, then a professor and statistician in Mizzou’s agricultural college. It was in Gary’s course entitled Experimenal Design that the light turned on for me. That’s when I finally was able to see through the fog of mathematical statistics to “get” the pragmatic value of statistics for the researcher, which at its core is nothing more or less than a framework for conducting unbiased reasearch. I was a biomedical student working on my PhD in pharmacology. But the typical student in Gary’s courses was working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. The problem my classmates shared, which was not one that really affected me, is having one growing or mating season by which to run a fully replicated experiment. One shot. hamilton image Under that constraint it was all about the planning. They needed to map out their experimental design in advance. Once the experiment began, any new wrinkles or oversights would have to wait a growing season. They didn’t have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups. Of course, these experiments were planned in advance using statistical design principles. Statistics weren’t just post-hoc tests, but also an intrinsic aspect of the planning process so that the experimental design is capable of addressing specific questions. The overall idea, which is not very complicated, was to be sure to plan an experiment that can be analyzed after it ran! And to run experiments the way they are planned. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the planned approach. That’s how unbiased research is performed. That’s how experimental statistics should be used. Have an idea. Put some effort into planning a test for it. And then follow that plan as closely as possible. It has become common for biomedical researchers to take more of a “fly by the seat of their pants” approach to running experiments and their replicates. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation. Statistics is practices as an afterthought, rather than a planning tool. Unfortunately, that’s an approach that is riddled with biases and can lead to a large number of negative outcomes…mostly in terms of a researchers career. In fact, experimental statistics was never intended to be exploited in that way. Experimental statistics was invented by the founders as a means of instilling some structure into the discovery and inference process in the planning stages. 1.2 About this book This is one of those books that’s actually a collection of RMarkdown handouts that I prepared for the course when I switched over to using R as the computational resource. The purpose is so that the student has a resource on the material not only for the course, but afterwards, too. What I try to get across in the book is the “how and why” of statistical design and analysis. The focus of this course is in teaching statistics as experimental design. The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance and appreciate why this is so important to reduce bias. That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs. Most of which she will forget immediately. And since I emphasize pre-planning, this book is full of simulations. That’s the really great advantage of using R to teach biostats, in my view. I’m not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work “under the hood”. When I do, it is mostly for stuff I think is helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don’t believe I’m the person who should be teaching it. Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That’s why operating within a statistical design framework is so important. For the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy experiments and prolonged time to degrees. 1.3 Software The course requires that you use the latest version of R. It also assumes you are using the RStudio environment. Both are free. They need to be installed sequentially, in the proper order, for a smooth experience. Installation is pretty straight forward but not idiot proof. One sign you are doing it wrong if you find yourself about to purchase something. The software you need for this course is free. Note for Windows students: If you have a 64bit processor, be sure to install and work with the 64bit R version. R is open source. There is base R and then there are packages that you’ll need to install separately. At the start of every chapter I list the packages that will be necessary to run the scripts in that chapter. From time-to-time your machine is not configured correctly to play well with some package you are using. You will have to figure out what R is trying to do and then how to get your machine configured so it can do it. A place to start is to copy and paste R error codes into your browser search bar. To get up and running so you can do R on your machine do the following in this sequence: Step 1. Install the latest version of R. Go here. Step 2. Install RStudio only after you have completed installing R. Go here. Select the free option. If you installed R prior to RStudio as instructed, launching RStudio on your machine should automagically configure R to work within the RStudio console. You should be good to go. Poke around with the RStudio menu bar and the RStudio panes. Under Tools select Global Options and experiment with an appearance configuration to your liking. 1.4 Getting started with R Once you have R and RStudio up and running I highly recommend taking the R Programming module in the interactive swirl package. Install and load the swirl library by typing the following into the R console within R studio. #{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE} install.packages(&quot;swirl&quot;) library(swirl) Swirl does a quick overview of the basics. But there are a lot of basics, most of which you’ll forget! As you go through swirl I suggest you open up a fresh R script or R markdown file to take notes (eg, copy/paste code snippets) for a custom cheatsheet. There are many, many other resources with which to get started. If you are ready to roll up your sleaves and play with R go here. As you work with R you’ll make mistakes. Of these, &gt;90% will be typos. If you didn’t mistype a command, just copy and paste error messages into your browser search bar. Most of the errors you will make have happened before. Most of what you want to do someone has done before. The best documented solutions are usually found in StackOverflow. The bottom line is the best way to learn R is to just start using it to solve the problems you need to solve. I’ll provide you the problems you need to solve in this course. They are about learning statistics, but you’ll end up learning how to use R as a side benefit. "],
["bigpic.html", "Chapter 2 The Big Picture 2.1 What are experimental statistics? 2.2 Brief History of Experimental Design", " Chapter 2 The Big Picture To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher library(tidyverse) library(Hmisc) Let’s start by listing out some characteristics that biomedical experiments share in common. Biomedical research experiments… tend to involve the use of relatively small sample sizes. are usually highly exploratory in nature. generate data that are either discrete counts or scalar measurements. are structured by a small group of fairly common experimental designs. are usually interpreted in a binary way; as having “worked”, or not. test hypotheses (though too often these are unstated). aspire for rigor, replicability and reproducibility. aspire to be unbiased. The stakes can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge and opening the door to real opportunities to create solutions for human diseases and suffering. But no less motivating are the issues related to the professional practice. These include ego, a natural impulse to seek validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. Scientific discovery is driven by ideals and by biases. The statistics covered in this course, it turns out, were invented to with these drivers in mind. The statistical design and analysis of experiments will be taught as a framework. Researchers who operate within this framework are well-positioned to make discoveries. This framework not only helps detect the signals in the noise of experimental data, but also provides a method to keep biases in check. We’ll discuss statistics in terms of experimental design. The idea here is to front load all of the statistical decision making in the planning stages. It turns out that there is a role for software The RA Fisher quote above really needs to be taken to heart. How often is an experiment revealed as hopelessly flawed in its design when the statistical analysis isn’t considered until all of the data are collected? The answer is, “way too often”. For that reason we’ll emphasize planning out the statistical analysis ahead of an experiment. The unbiased researcher only has to follow the plan. In fact, we’ll learn how simulating the data and the analysis a priori generates incredible insight into the two things any scientist would want to know before running an experiment: is it properly designed and is it feasible? 2.1 What are experimental statistics? Experimental statistics were invented about a century ago to solve several problems that researchers of the day faced in dealing with data. Then, as now, experimental statistics are used to summarize data into simpler descriptive models. as procedures to draw inferences from samples. as procedures that guide the design of experiments. to serve as framework for conducting unbiased research with a modicum of integrity. What did you think statistics or “biostats” is before you read that? Chances are you thought it was one or two of those bullets, and probably not the latter two. 2.1.1 Descriptive modeling Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics, such as the sample mean and standard deviation, or median and ranges or percentiles. The alternative is to discuss individual replicates in explicit, gory detail. What most people think of as statisical tests are also a way of conveying descriptive information about the experimental model. If you say, “I analyzed this by a two-tailed paired t-test,” say no more. I know exactly what kind of experiment you ran! Regression models are used to describe patterns of data behavior. For example, the well-known Michalis-Menten model that describes product formation as a function of substrate concentration. \\[ [P]=\\frac{[S][Emax]}{[S]+Km}\\] It is a special case of a more general hyperbolic model that a large number of nonlinear biological processes can be fit to. \\[[Y]=\\frac{[X][\\beta_0]}{[X]+\\beta_1} \\] Several other nonlinear regression models are useful to describe other common phenomena such as multimeric binding relationships or processes that are modeled as time series, such as growth or decay. In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. Take a simple example of two groups shown here. Each group has been fit to a simple mean and standard deviation model. Clearly, that model fits the control group much better compared to the treatment group fit. Why? For the latter most of the data values are greater than the mean of the group. The data are skewed. Sure, a mean can be calculated from any group of values. But does it summarize the data accurately? Perhaps some other model (or group of statistical parameters) would better convey to you how that data look? Figure 2.1: This is a caption 2.1.2 Statistical inference There are two types of inference for the experimentalist. One type is to infer whether an experiment “worked” or not. This familiar process involves calculating a test statistic from the data (t-test, F-tests, etc). These values are then located on a null distribution of the test statistic to derive p-values. In essence, we set up threshold rules based upon these p-values that allow us to decide whether the data, transformed into a test statistic, are so extremely unexpected they warrant rejection of a null hypothesis. A second type of inference is to extrapolate from a sample some estimate for the values of the variables within the population that was sampled. This is usually important when we’re measuring standardized variables. Whether it is serum glucose levels, or ligand affinity for a protein, or rate constants. By standardized variables I imply stuff that should have the same value no matter where on planet earth they are measured. In some situations, the precision and accuracy by which we estimate these values becomes fairly important. Estimation can also be important when dealing with variables on relative scales, too. For example, we might expect the effect size for some treatment, relative to control, be the same no matter where on the planet it is measured. 2.1.3 Experimental design Statistically rigorous experiments are planned out in advance. As will become clear in the history section of this chapter, poor experimental planning by researchers of his time is what inspired RA Fisher to invent much of the statistical methods and procedures that we use today. Experimental design is the researcher’s most potent weapon against bias. It involves establishing a clear, testable statistical hypothesis and establishing series of decision rules in advance of the research. These range from subject selection and arrangement, predetermination of replicate number using a priori power analysis, setting data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, on up to what analysis will be performed on the data. . Experimental design is very common in prospective clinical research. Statisticians are usually consulted during the design phase of clinical trials, where all of the sampling and analysis protocols are carefully vetted. The statistical design and analysis of clinical studies is extremely important in the drug and medical device discovery process. Unfortunately, very few basic biomedical scientists practice anything remotely like this. The vast majority of the published work today is retrospective, if the approach that research teams take to sampling and analysis is taken into account. Yet, they all tend to use statistics that are largely intended for prospective designs. 2.1.4 A role for Monte Carlo simulation I am convinced that Monte Carlo simulations of experiments have a lot of utility in experimental design and planning and for that reason they are something I emphasize in the course. We’ll use them to show how sample sizes can be predetermine. But they are also really useful to visualize expected output. By forcing a researcher to go through an analysis they can help expose flaws in the experiment before any data are actually produced and to truly assess the feasibility of a planned experiment. Monte Carlo simulations are actually pretty simple to run with software like R. All that is needed is a rough idea of the expected values for your outcome variables. One needs some idea of what would be considered a minimally viable or scientifically meaningful effect size. This information is usually available in preliminary experiments. Having that, one simply simulates a dataset and runs the expected statistical analysis on it. A cycle of data simulation and testing is repeated a thousand times to get a long run average of how well the experiment will work! This course will emphasize the importance of the following questions, which should be given a high priority by biomedical researchers when planning experiments: What statistical approaches should I use in the planning stages? What statistical methods should I use to analyze the data? Why should I analyze the data in the way that was planned? How should the analysis be interpreted? I am aware that many researchers arrive at this point having skipped the first of these question and who have no intention to ask of the third. 2.1.5 Anti-bias framework If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct. The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn’t provide us a magic woo-woo shield to protect us from our biases. Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias. The following are some of the biggies. 2.1.6 Cognitive biases From a statistical point of view biases can be classified into two major groupings. The first are Cognitive biases. These are how we think (or fail to think) about our experiments and our data. These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, “how could this not work!” you are in the throes of a pretty deep bias and probably don’t realize it. In bench research, these cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or telling glitches in the data suggesting something else we are not considering is going on. 2.1.7 Systematic biases The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed. Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line. 2.1.8 Scientific misconduct To limit bias to just these two categories is not to admit malicious biases exist, too. These include forbidden practices such as data fabrication and falsification. This is obviously a problem of integrity, but very few scientists working today are immune from the high stakes careerism drivers that seem to expose weaknesses with integrity. In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them. The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite liveable. If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you’ve given it your best shot. That is hard to conclude when the latter occurs. 2.2 Brief History of Experimental Design Researchers in the pre-statistics days lacked the statistical framework that today’s researchers take for granted! Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they were weak in designing experiments, in summarizing data, and in drawing inference from it. They just lacked a reliable framework for all of that. A statistical approach to experimental design was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. This is to say that at the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. His career took that trajectory by accident. For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwin’s theory of evolution, the hot discovery of the day. Fisher’s side hustle was mathematical problems related to genetics and inspired by evolution. Today, we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. That’s certainly where his career ambitions seem laid. One big contribution he made during this early stage in his illustrious career was no small feat. He defined variance as the square of the standard deviation. He proposed that variance is useful as a descriptive statistic for the variability within a population (and much more…it would soon become the foundation of multigroup exprimental designs). In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of it all. Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the station’s data, some of which represented serial experiments that had been running for decades. Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of most of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds familiar, it was meant to. Here’s when the paradigm shifted. Fisher began to think about the process by which experimental data should be collected. Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. His invention of the analysis of variance extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well. It was a truly remarkable period. In 1925 Fisher published a small book, Statistical Methods for Research Workers. In 1934 he published its extension, Design of Experiments. In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would cross oceans…and then disciplines. Today, experiments that we would recognize as statistically rigorous are those in which Fisher’s early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. There’s much more to this history, including many additional players. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today. "]
]
