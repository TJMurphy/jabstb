[
["index.html", "JABSTB: Statistical Design and Analysis of Experiments with R Preface", " JABSTB: Statistical Design and Analysis of Experiments with R TJ Murphy PhD, Department of Pharmacology, School of Medicine, Emory University, Atlanta, GA biostats538@gmail.com 2018-12-12 Preface This book is intended primarily as a resource for students enrolled in a biostats course that I teach by the same title. The students are primarily in one of Emory’s biomedical and biological sciences PhD programs. There are the ocassional Emory honors program undergrads, students from Emory’s public health school, and usually a few Georgia Tech graduate students, also. They’ll need this book for the coursework. If they choose to use R as their main analytic tool, perhaps they’ll find this book to be a useful resource. Not included in this book are additional materials for the course (eg, take home and group exercises, slide decks, data sets, my extensive collection of stats cartoons, etc). The basic use for the book is that it has many examples for running and interrpeting various statistical functions. Copy and paste my examples into your R script or R markdown file, and get to work. Every chapter is self-contained with respect to packages. If any are needed to run scripts in a given chapter, they are listed at the top of the chapter. Each chapter corresponds to a RMarkdown document. If you wish to grab those documents instead of using this material from HTML, simply fork, clone or download them from the Github jabstb repo. Just bear in mind, this book is a living document, which I expect to subject to a lot of on-the-fly revision. Stuff will be added and eliminated over time. As I write these words, in Dec 2018, my main disclaimer is that it is definitely an MVP. If you find errors, have any suggestions, or would otherwise like to contribute, please submit a pull request or contact me by email. Copyright 2018 © TJ Murphy MIT license. "],
["author.html", "Chapter 1 About the author", " Chapter 1 About the author A few years ago I adopted this course from Frank Gordon, a colleague who had retired. Like Frank, I’m a biomedical scientist who happens to have high level of interest in statistical methods. I learned this material as a graduate student at Mizzou. There I took several stats courses. The ones with the most impact were taught by the late Gary Krause, then a professor and statistician in Mizzou’s agricultural college. The light turned on for me while taking Gary’s Experimenal Design course. That’s when the fog of mathematical statistics cleared enough so I could finally “get” the pragmatic value of statistics for the researcher. What became most clear is that experimental design is a statistical framework for conducting unbiased research. That concept permeates my course and this book. I was working on my PhD in pharmacology within the medical school. But most of my classmates in Gary’s courses were working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. The problem my classmates shared, which was not one that really affected me, is having one growing or mating season by which to run a fully replicated experiment. One shot. That one shot changes everything. Planning was a priority for them. They needed to map out their experimental design in advance. Once the experiment began, any new wrinkles or oversights would have to wait until the next growing season. They didn’t have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups. Planning was based upon statistical design principles, often in consultation with Gary. Statistics were a priori planning and post-hoc tests. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the planned approach. In contrast, it is fair to say that most biomedical scientists fail to incorporate statistical design into their plans. That failure opens up a whole can of worms that can generally be characterized as doing statistics in ways it was never meant to be done. All too common is the biomedical researchers who takes a more “fly by the seat of their pants” approach to running experiments and collecting data. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation. Unfortunately, that approach is riddled with biases, and sometimes other negative consequences that are even more challenging. Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made. The focus of this course is in teaching statistics as experimental design. The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance and appreciate why this is so important to reduce bias. That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs. Most of which she will forget immediately. And since I emphasize pre-planning, this book is full of simulations. That’s the really great advantage of using R to teach biostats, in my view. I’m not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work “under the hood”. When I do, it is mostly for stuff I think is helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don’t believe I’m the person who should be teaching it. Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That’s why operating within a statistical design framework is so important. For the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments and prolonged time to degrees. "],
["history.html", "Chapter 2 A Brief History of Experimental Design", " Chapter 2 A Brief History of Experimental Design Researchers in the pre-statistics days lacked the statistical framework that today’s researchers take for granted. Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they struggled with designing experiments, in summarizing the data, and in drawing inference from it. The statistical approach to experimental design we use today was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. Figure 2.1: RA Fisher in 1913, from the Adelaide Digital Archive At the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. He took that trajectory by accident. For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwin’s theory of evolution, which was the hot discovery of the day, of course. Fisher’s side hustle was to work on mathematical problems related to evolutionary genetics. Today, in those early days we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. That’s certainly where his career ambitions seem laid. He never lost an interest in evolution and would go on to become, unfortunately, a prominent eugenicist. Still, one big contribution he made during this early stage was no small feat. He defined variance as the square of the standard deviation. He proposed that variance is useful as a descriptive statistic for the variability within a population. Further developed, it would soon become the foundation of the multigroup exprimental designs that called ANOVA, the analysis of variance, which are widely used today. In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar (WWI) agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of this data repository. Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the station’s data, some of which represented serial experiments that had been running for decades. As he dug in Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of much of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds familiar then I’ve achieved my objective for mentioning it. Here’s when the paradigm shifted. Fisher began to think about the process by which experimental data should be collected. Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. As I mentioned above, his invention of the analysis of variance extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well. It was a truly remarkable period. In 1925 Fisher published a small book, Statistical Methods for Research Workers. In 1934 he published its extension, Design of Experiments. In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would soon cross oceans…and then disciplines. Today, experiments that we would recognize as statistically rigorous are those in which Fisher’s early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. There’s much more to this history, including many additional players and plenty of controversy that remains unsettled to this day. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today. "],
["bigpic.html", "Chapter 3 The Big Picture 3.1 What are experimental statistics?", " Chapter 3 The Big Picture To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher library(tidyverse) library(Hmisc) Let’s start by listing out some key characteristics that most biomedical experiments share in common. They… tend to involve the use of relatively small sample sizes. are usually highly exploratory in nature. generate data that are either discrete counts or measurements of continuous scalars. are structured by a small group of fairly common experimental designs. are usually interpreted in a binary way; as having “worked”, or not. test hypotheses (though too often these are unstated). aspire for rigor, replicability and reproducibility. aspire to be unbiased. The stakes of our work can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge, and opening the door to create impactful solutions, particularly in the realm of human diseases and suffering. But no less motivating are the issues more related to the professional practice of science. These include ego, the completely natural impulse to seek out validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. The point is that the process of scientific discovery is driven both by ideals and by biases. This is nothing new. The one big concept that I hope you embrace is that the statistical design and analysis of experiments serves as a working framework within which the biomedical researcher can conduct reasonably unbiased work. The statistical approaches covered in this course, it turns out, were invented long ago with all of these drivers in mind. 3.1 What are experimental statistics? Experimental statistics are used to summarize data into simpler descriptive models. as procedures to draw inferences from samples. as procedures that guide the design of experiments. to serve as framework for conducting unbiased research. Chances are you thought biostats was just one or two of those bullets, and probably not the latter two. 3.1.1 Descriptive modeling Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics. Thus, we use parameters like the sample mean and standard deviation, or median and ranges or percentiles. The alternative is unthinkable today (but common long ago), which is to discuss each replicate individually. To emphasize that sample parameters differ from population parameters, the standard in statistical notation is to use roman characters to indicate samples and greek characters to indicate the population. For example, parameter sample population mean \\(\\bar y\\) \\(\\mu\\) standard deviation \\(s\\) \\(\\sigma\\) variance \\(s^2\\) \\(\\sigma^2\\) Thus, the sample mean, \\(\\bar y\\) is an estimate of the population mean, \\(\\mu\\). Statistical tests also have a descriptive element in that they convey information about the experimental design. If you say, “I’m working up a two-tailed paired t-test,” say no more. From that alone I know something about your hypothesis, how your replicates are handled, the number of predictor groups, and the type of data you’re measuring. Regression models also describe data. For example, here is the well-known Michaelis-Menten model that describes product formation as a function of substrate concentration. \\[[P]=\\frac{[S][Vmax]}{[S]+Km}\\] That’s a model we might fit to certain kinds of enzyme kinetic data, because we use it to estimate scientifically meaningful parameters, like \\(V_{max}\\) and \\(K_m\\). In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. Take a simple example of two groups shown here. Each group has been fit to a simple model: that for the mean and standard deviation. Clearly, that model fits the control group much better than it fits the treatment group. Figure 3.1: Is the mean for each these groups a good descriptive model? Why do I say that? The treatment group data are much more skewed. Most of the data values are greater than the mean of the group. Sure, a mean can be calculated for that group, but it serves as a fairly crappy summary. Perhaps some other model (or group of statistical parameters) would better convey how these data behave? This is to point out that learning statistics is about learning to make judgments about which models are best for describing a given data set. 3.1.2 Statistical inference There are two main types of inference researchers make. One type is to infer whether an experiment “worked” or not…the so-called “significance test”. This familiar process involves calculating a test statistic from the data (eg, t-test, F-tests, etc) and then applying a threshold rule to its value. If the test passes the rule, we conclude the experiment worked. I cover this type of inference in much more detail in the p-value chapter ??, and we’ll talk about it over and again throughout the course. A second type of inference is to extrapolate from a sample some estimate for the values of the variables within the population that was sampled. Both descriptive and statistical inference are subject to error. By random chance alone our sample could be way off the mark, even with perfectly calibrated instrumentation. The real difficulty with inference is we can never know for certain whether we are right or wrong. They are called random variables for a reason. It pays to have a very healthy respect for the role played by random chance in determining the values of our parameter estimates. If we were to completely redo a fully replicated experiment once more, we would almost certainly arrive at different numbers. In a well behaved system, they’d likely be in the same ballpark as those of the first experiment. But they would still differ. To illustrate, copy and paste the code chunk below. It replicates a random triplicate sample six times, taking six means. Unlike in real life, the population parameters are known (because I coded them in): \\(\\mu=2\\) and \\(\\sigma=0.4\\). You can run that chunk tens of thousands of times and never get a “sample” with one mean that has a value of exactly 2, even though that’s the true mean of the population that was sampled. x &lt;- replicate(6, rnorm(3, 2, 0.4)) apply(x, 2, mean) ## [1] 1.824038 2.211391 1.476627 2.096710 1.978010 2.206209 3.1.3 Experimental design Experimental planning that involves dealing with statistical issues is referred here as experimental design. This involves stating a testable statistical hypothesis and establishing a series of decision rules in advance of data collection. These rules range from subject selection and arrangement, predetermination of sample size using a priori power analysis, setting some data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, declaring a primary outcome, on up to what statistical analysis will be performed on the data. Experimental design is very common in prospective clinical research. Unfortunately, very few basic biomedical scientists practice anything remotely like this. Most biomedical researchers begin experiments with only vague ideas about the statistical analysis, which is usually settled on after the fact. Much of the published work today is therefore retrospective, rather than prospective. Yet, most researchers tend to use statistics that are largely intended for prospective designs. That’s a problem. 3.1.4 Statistics as an anti-bias framework If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct. The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn’t provide us a magic woo-woo cloak to protect us from our biases. Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias. Here are some of the more important biases. 3.1.4.1 Cognitive biases From a statistical point of view biases can be classified into two major groupings. The first are Cognitive biases. These are how we think (or fail to think) about our experiments and our data. These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, “how could this not work!” you are in the throes of a pretty deep cognitive bias. In bench research, cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or recognizing telling glitches in the data as meaningful. 3.1.4.2 Systematic biases The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed, and how data is handled or transformed. Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line. 3.1.4.3 Scientific misconduct More malicious biases exist, too. These include forbidden practices such as data fabrication and falsification. This is obviously a problem of integrity. Very few scientists working today are immune from the high stakes issues that pose threats to our sense of integrity. In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them. The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite livable. If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you’ve given it your best shot. That is hard to conclude when the latter occurs. "],
["sampling.html", "Chapter 4 Statistical Sampling 4.1 Experimental units 4.2 Independent Replicates 4.3 Random process 4.4 Statistically valid samples 4.5 Independence of replicates", " Chapter 4 Statistical Sampling An experiment is no more reliable than is its sample. -TJ Murphy A statistically valid sample is comprised of independent replicates of the experimental unit, which are generated using some random process. To unpack this let’s think about each of the following terms: What are experimental units? What do we mean by independent replicates? What is a random process? When is statistical validity even important? 4.1 Experimental units The experimental unit is the source of the measurement. An experimental unit can generate one or many measurement values. I prefer the concept of an experimental unit to the concept of subject, though they often mean the same thing. I’ve found the word subject carries more ambiguity, especially for people first learning sampling and sample size concepts. In some experimental designs (eg, unpaired or completely randomized) each experimental unit generates a single measurement value. Here there is a one-to-one correspondence exists between the number of experimental units and the number of measurement values within a data set. In other designs (eg, paired or matched or repeated/related measure), a single experimental unit can generate more than one measurement values for the same variable. Such data sets have more values than experimental units. Here are some guidelines for deciding what is the experimental unit in an experiment, with full recognition that sometimes there are gray areas. Ultimately the researcher has to use scientific judgment to recognize or define the experimental unit. 4.1.1 A simple test to define the experimental unit When defining an experimental unit I recommend using a simple test: Are these measurements intrinsically-linked? If two or more measurement values are intrinsically-linked then they would comprise paired or matched or related measures from a single experimental unit. So how could you judge whether two or more measurements are intrinsically-linked? For the most part, this happens when the source of those measurements doesn’t differ. Here are a few examples: A before and after design. A mouse is scored on how well it performs a behavioral test at baseline, before a treatment. After that same mouse receives a treatment it is run through the behavioral test once more to get a second score. Those two scores are intrinsically-linked because they were taken from the same mouse. All that differs between the scores is the absence or presence of the treatment, the effect of which the researcher is trying to measure. We would also say those two scores are matched, paired or related/repeated measures. A single mouse from which two scores are derived is an independent replicate of the experimental unit. Twinning. Take for example a study involving human identical twins. In these studies identical twin pairs are modeled as a single experimental unit due to their high level of instrinsic relatedness. There are two human subjects but they are modeled statistically as a single experimental unit. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. One of the pair receives a control condition while the other receives a treatment condition. A measurement is taken from each person. There are two measurements in total, and two people, but only a single experimental unit. Given that the twins are so identical we could reasonably conclude these two measurements are intrinsically-linked. We can model the pair as one. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. Unpaired or completely randomized In contrast, imagine a study using the same control and treatment conditions using unrelated humans (or some other outbred animal species) as subjects. Each subject is assigned either a treatment or a control, and only a single measurement is taken from them. Since the subjects are each very different from each other, we could not conclude that measurements taken from them are intrinsically-linked. Each person stands alone as an experimental unit. The data would be analyzed using an unpaired, unmatched or completely randomized test. Intrinsically-linked measurements are very common in bench work. In fact they are too often overlooked for what they are and mistakenly analyzed as unmatched. Experiments involving batches of biological material, cultured cells and/or littermates of inbred animal strains routinely involve intrinsically-linked measurements. As a general rule, these should always be designed and analyzed using matched/paired/related measures procedures. Cell cultures Cell cultures are remarkably homogeneous. The typical continuous cell line is a monoculture passaged across many doubling generations. Imagine a test conducted on a 6 well multi-well cell culture plate. Each well receives a different level of some treatment condition, such as a dosing or time-course study. All of the wells were laid down at the same time from a common batch of cells. Each well is very highly related to all of the other wells. The intrinsic differences between wells would be relatively minor and mostly due to technical variation. There’s no real inherent biological variation from well-to-well other than that attributable to the level of treatment the well receives. As a result, all of the measurements taken from a plate of wells are intrinsically-linked to each other. The experimental unit is the plate. They should be designed and analyzed using matched/paired/related measure statistical procedures. Furthermore, any other plates laid down at the same time from the same source of cells are virtually identical clones of each other. If we were to expose the wells in all of those plates to various treatments followed by taking some measurement, then it is pretty easy to argue that all of those measurements taken on that passage of cells are intrinsically-linked. None of the wells are independent of any of the other wells, irrespective of the plate. Together, all of the plates represent a single experimental unit. Inbred mice In many regards, the high level of relatedness within inbred mouse strains doesn’t differ from human identical twins, or from cultured cells, for that matter. A given strain of these animals are inbred to genetic homogeneity across several generations. For all intents and purposes all mice derived from a given strain are immortalized clones of each other. Two mice from the same litter are identical twins. Indeed, two mice from different litters from the same strain are identical twins. Due to their clonal identity all measurements taken from any of these highly related subjects are intrinsically-linked. Just as for cell culture, protocols must be contrived to break up the homogeneity. A common approach is to treat the litter as the experimental unit and take measures from littermates as intrinsically-linked. Split tissue Imagine two slices of an organ (or two drops of blood) taken from a single animal. Although the two slices (or drops of blood) are obviously different from each other, any measurements derived from each are intrinsically-linked. The experimental unit would be the animal from which that biological material is derived. Batches Finally, imagine a batch of a purified protein or other biochemical material. The batch was isolated from a single source and prepared through a single process. The material in the batch is highly homogeneous, irrespective of whether it is stored away in aliquots. Any measurement taken from that batch are highly related to any other measurement. They are intrinsically-linked. The batch would be the experimental unit. 4.1.2 Blocking We have to contrive protocols to break up experimental units that have high inherent homogeneity. The statistical jargon used for this is blocking, such that blocks are essentially grouping factors that are not scientifically interesting. Going back to culture plates. Let’s say we prepared three plates on Friday. An assay performed on one plate on Monday would represent one experimental unit of intrinsically-linked measures. An assay repeated on Tuesday on a second plate would represent a second experimental unit. Wednesday’s assay on the third plate is also its own experimental unit. Here the blocking factor is the day of the week. Assuming we created fresh batches of reagents each day, there would be some day-to-day variation that wouldn’t exist if we assayed all threee plates at once on a single day. But we’re not particularly interested in that daily variation, either. More conservatively, cell line passage number can be used as a blocking factor to delineate experimental units. Each passage number would represent an experimental unit and the overall replicated experiment would be said to be blocked on passage number. Defining the experimental unit and any blocking factors requires scientific judgement. That can be difficult to do when dealing with highly homogenous material. What should be avoided is creating a design that limits random chance too severely. To measure on Monday all three plates that were laid down on Friday will probably yield tighter results than if they were blocked over the course of the week. This has to be thought through carefully by the researcher in each and every case. Reasonable people can disagree what whether one approach is superior to some other. Therefore, what is important is to make defensible decisions. To do that, you need to think through this problem carefully. When in doubt, I suggest leaning towards giving random chance a fair shot at explaining the result you’re observing. For example, you can make the case that measurements from two cell culture plates that were laid down on the same day but are collected on different days are not intrinsically-linked. That’s a harder case to make if they are collected on the same day. You will almost certainly have to make the case that measurements taken from two mice on different days or if they are from different litters are not intrinsically-linked. Before going there, we need to chat about what we mean by independent replication. 4.2 Independent Replicates That we should strive for biological observations that are repeatable seems self evident. An experiment is comprised of independent replicates of treatment conditions on experimental units. The total number of independent replicates comprises an experiment’s sample size. A primary goal in designing an experiment is to assess independent replicates that are not biased to the biological response of a more narrowly defined group of experimental units. A replicate is therefore independent when a repeat is on an experimental unit that differs materially from a previous experimental unit. A material difference could involve a true biological replicate. Measurements taken from two unrelated human subjects have a material difference. In bench biological work with fairly homogenous systems (eg, cell lines and inbred animals) a material difference will usually need to be some separation among replicates in time and space in applying the experimental treatments. 4.2.1 A simple test for independence How willing am I to certify this is a truly repeatable phenomenon when replicated in this way? A new scientific discovery would be some kind of repeatable phenomenon. 4.2.2 Some replication examples If we are performing an experiment using pairs of human twins, each pair that is studied stands as an independent replicate. Because the pair is the experimental unit, a study involving 5 pairs will have five, rather than ten, independent replicates. If we conduct an experiment using unrelated human volunteers, or someother out bred animals, each person or animal from whom a measurement is recorded is considered an independent replicate. Their biological uniqueness defines their independence. We wander into gray areas pretty quickly when thinking about the independence of experimental units in studies involving cultured cells, batches of biological material, and inbred mice. Working with these systems it is difficult to achieve the gold standard of true biological independence. The focus instead should be on repeatability….“Working with new batches of reagents and different days do I get the same response?” Imagine a 6 well plate of cultured cells. No well differs biologically from any other. If each well received a repeat of the same treatment at the same time we shouldn’t consider any measurements from that plate independent from others. Otherwise, the sample would be biased to that plate of cells measured at that particular time with a given set of reagents under those particular conditions. It is too biased to that moment. What if we screwed up the reagents and don’t know it? Rather than being independent, it is best to consider the 6 measurements drawn from the plate as technical replicates or pseudo replicates. The data from the 6 wells should be averaged or totaled somehow to improve the estimate of what happened on that plate that day. A better approach with cultured cells is to use passage numbers to delineate independence. Thus, a 6 well plates from any one passage are independent experimental units relative to all other passages. Obviously, given the homogeneity of cells in culture, it’s unlikely there is much biological variation even by these criteria. But to achieve true biological independence would require re-establishing the cell line each time an independent replicate was needed. That’s rarely feasible. Inbred mice pose much the same problem. Scientific judgment is needed to decide when 2 mice from the same strain are independent of each other. One mark of delineation is the litter. Each litter would be independent of other litters. Outcomes of two (or more) littermates could be considered matched or related-measures and thus one experimental unit. 4.3 Random process You can probably sense intuitively how randomization can guard against a number of biases, both systematic and cognitive. Systematic artifacts become randomly distributed amongst the sample replicates, whereas you are less tempted to treated a replicate as preferred if you don’t know what is its treatment level. Mathematical statistics offers another important reason for randomization. In classical statistics the effect size of some treatment is assumed to be fixed. Our estimate of that real value is the problem. Thus, when we measure a value for some replicate, that value is comprised of a combination of these fixed effects and unexplained effects. The variation we observe in our outcome variables, the reason it is a random variable, arises from these unexplained effects. These can be particularly prominent in biological systems. Randomization procedures assures those random effects are truly random. Otherwise we might mistake them for the fixed effects that are of more interest us! This concept will be discussed more formally in the section on general linear models. Suffice to say for pragmatic purposes that random sampling is crucial for limiting intentional and unintentional researcher biases. Either the experimental units should be selected at random, or the experimental units should be assigned treatments at random, and/or the outcome data should be evaluated at random (eg, blind). Sometimes, doing a combination of these would be even better. Usually, the researcher supervises this randomization using some kind of random number generator. R’s sample() function gets that job done for most situations. Let’s design an experiment that involves two treatments and a total of 12 independent experimental units. Thus, 6 experimental units will each receive either of the two treatments. Let’s say that my experimental units each have an ID, in this case, a unique letter from the alphabet. Using sample(1:12) we randomly assign a numeric value to each ID. This numeric value will be the order by which the experimental unit, relative to the other experimental units, is subjected to the experimental treatment. ID’s that are assigned even random numbers get one of the two treatments, and odd numbered ID’s get the other treatment. What we’ve done here is randomize both the order of replication and the assignment of treatment. That’s a well-shuffled deck. You can see how this approach can be readily adapted to different numbers of treatment levels and sample sizes. set.seed(1234) ID &lt;- letters[1:12] order &lt;- sample(1:12, replace=F) plan &lt;- data.frame(ID, order) plan ## ID order ## 1 a 2 ## 2 b 7 ## 3 c 11 ## 4 d 6 ## 5 e 10 ## 6 f 5 ## 7 g 1 ## 8 h 12 ## 9 i 3 ## 10 j 8 ## 11 k 4 ## 12 l 9 4.4 Statistically valid samples For any statistical test to be valid, each replicate within a sample must satisfy the following two criteria: The replicate should be generated by some random process. The replicate must be independent of all other replicates. Why? Statistical tests are one of the last stages of a hypothesis testing process. All of these tests operate, formally, on the premise that at least these two conditions are true. When these conditions have not been met the researcher is collecting data without testing a hypothesis. To run a statsitical test is to pretend a hypothesis has been tested, when it has not. 4.4.1 Select random subjects Let’s say we want to do an experiment on graduate students and need to generate a representative sample. There are 5 million people in the US who are in graduate school at an given time. Let’s imagine they each have a unique ID number, ranging from 1 to 5,000,000. We can use R’s sample() function to randomly select three individuals with numbers corresponding to that range. Sampling with replacement involves throwing a selection back into a population, where it can potentially be selected again. In that way, the probability of any selection stays the same throughout the random sampling process. Here, the replace = FALSE argument is there to ensure I don’t select the same individual twice. sample(x=1:5000000, size=3, replace = FALSE) ## [1] 1413668 4617167 1461579 All that needs to be done is to notify the three people corresponding to those IDs and schedule a convenient time for them to visit so we can do our experiment. You can imagine several variations to randomly select graduate students for measurements. You just need a way to find graduate students, then devise a way(s) to ensure the sampling is as representative as possible. Selecting subjects from a real population is pretty straight forward, a bit like picking 8 lotto balls from a spinning container. A lot of times in experimental work the number of subjects available to the researcher is fixed and smaller. The size of the population to be sampled can be much closer to the number of replicates needed for the experiment rather than a sample from a large pool. In these cases we have to come up with other ways to randomize. 4.4.2 Randomize to sequence For example, let’s say we want to compare condition A to condition B. We have 6 subjects to work with, each of which will serve as an independent replicate. We want a balanced design so will have 3 replicates for each of the 2 conditions. Let’s imagine we can only perform an experiment on one subject, one day at a time. In that case, it makes sense to randomize treatment to sequence. We can randomly generate a sequence of 6 even and odd numbers, and assign them to the daily sequence (MTWTFM) based on which random number is first on its list. We can make a rule that subjects assigned even numbers will receive condition A, whereas condition B is meted out to subjects associated with odd numbers. sample(x=11:16, size=6, replace = FALSE) ## [1] 16 12 15 11 13 14 4.4.3 Randomize to location Let’s imagine 3 treatments (negative control, positive control, experimental), that we will code 1,1,2,2,3,3. These will be applied in duplicate to cells on 6-well cell culture plate. We’ll code the plate wells with letters, a, b, c, d, e, f from top left to bottom right (ie, a and b are wells in the top row). Now we’ll generate a random sequence of those six letters. sample(letters[1:6], replace=F) ## [1] &quot;b&quot; &quot;a&quot; &quot;e&quot; &quot;d&quot; &quot;f&quot; &quot;c&quot; Next, we’ll map the sequence 1,1,2,2,3,3 to those letters. Thus, negative control goes to the wells corresponding to the first two letters in that sequence, positive control to the 3rd and 4th letters, and so forth. 4.4.4 Randomize to block In statistical lingo, a block is a subgroup within a sample. A blocked subject shares some feature(s) in common with other members of its block compared to other subjects in the overall sample. But usually, we’re not interested in block as a variable, per se. Here are some common blocks at the bench are One purified enzyme preparation vs a second preparation of the same enzyme, nominally purified the same way. The two enzyme preps represent two different blocks. A bunch of cell culture dishes plated on Friday from passage number 15 vs ones plated on Tuesday from passage number 16. The two passages represent 2 different blocks. A litter of mouse pups born in January vs a litter born in February. The two different litters represent two different blocks. An experiment run with freshly prepared reagents on Monday vs one run on Tuesday, with a new set of freshly prepared reagents. Each experimental day represents a block. Frequently, each block is taken as an independent replicate. 4.5 Independence of replicates In biomedical research the standard is for biological independence; when we speak of “biological replicates” we mean that each independent replicate represents a distinct biological entities. That standard is difficult to meet when working with many common biological model systems, particularly cell lines and inbred animals. The definition of statistical independence is grounded in the mathematics of probability: Two events are statistically independent when they convey no information about the other, or \\[p(A \\cap B)=p(A)p(B)\\]. Here the mathematics is not particularly helpful. Imagine two test tubes on the bench, each receives an aliquot of biological material from a common prep (eg, a purified protein). One tube then receives treatment A and the other treatment B. As best we know, the two tubes aren’t capable of influencing each other. But we can reasonably assume their responses to the treatments will at least be correlated, given the common source of biological material. Should each tube be treated as if it were statistically independent? Replicate independence that meets statistical validity therefore has to take on a more pragmatic and nuanced definition. My preference is to define a replicate as the independent experimental unit receiving treatment. I like this because it allows for defining the experimental unit differently depending upon the experimental design. "]
]
