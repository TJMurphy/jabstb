[
["index.html", "JABSTB: Statistical Design and Analysis of Experiments with R Preface", " JABSTB: Statistical Design and Analysis of Experiments with R TJ Murphy PhD, Department of Pharmacology, School of Medicine, Emory University, Atlanta, GA biostats538@gmail.com 2018-12-12 Preface This book is intended primarily as a resource for students enrolled in a biostats course that I teach by the same title. The students are primarily in one of Emory’s biomedical and biological sciences PhD programs. There are the ocassional Emory honors program undergrads, students from Emory’s public health school, and usually a few Georgia Tech graduate students, also. They’ll need this book for the coursework. If they choose to use R as their main analytic tool, perhaps they’ll find this book to be a useful resource. Not included in this book are additional materials for the course (eg, take home and group exercises, slide decks, data sets, my extensive collection of stats cartoons, etc). The basic use for the book is that it has many examples for running and interrpeting various statistical functions. Copy and paste my examples into your R script or R markdown file, and get to work. Every chapter is self-contained with respect to packages. If any are needed to run scripts in a given chapter, they are listed at the top of the chapter. Each chapter corresponds to a RMarkdown document. If you wish to grab those documents instead of using this material from HTML, simply fork, clone or download them from the Github jabstb repo. Just bear in mind, this book is a living document, which I expect to subject to a lot of on-the-fly revision. Stuff will be added and eliminated over time. As I write these words, in Dec 2018, my main disclaimer is that it is definitely an MVP. If you find errors, have any suggestions, or would otherwise like to contribute, please submit a pull request or contact me by email. Copyright 2018 © TJ Murphy MIT license. "],
["author.html", "Chapter 1 About the author", " Chapter 1 About the author A few years ago I adopted this course from Frank Gordon, a colleague who had retired. Like Frank, I’m a biomedical scientist who happens to have high level of interest in statistical methods. I learned this material as a graduate student at Mizzou. There I took several stats courses. The ones with the most impact were taught by the late Gary Krause, then a professor and statistician in Mizzou’s agricultural college. The light turned on for me while taking Gary’s Experimenal Design course. That’s when the fog of mathematical statistics cleared enough so I could finally “get” the pragmatic value of statistics for the researcher. What became most clear is that experimental design is a statistical framework for conducting unbiased research. That concept permeates my course and this book. I was working on my PhD in pharmacology within the medical school. But most of my classmates in Gary’s courses were working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. The problem my classmates shared, which was not one that really affected me, is having one growing or mating season by which to run a fully replicated experiment. One shot. That one shot changes everything. Planning was a priority for them. They needed to map out their experimental design in advance. Once the experiment began, any new wrinkles or oversights would have to wait until the next growing season. They didn’t have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups. Planning was based upon statistical design principles, often in consultation with Gary. Statistics were a priori planning and post-hoc tests. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the planned approach. In contrast, it is fair to say that most biomedical scientists fail to incorporate statistical design into their plans. That failure opens up a whole can of worms that can generally be characterized as doing statistics in ways it was never meant to be done. All too common is the biomedical researchers who takes a more “fly by the seat of their pants” approach to running experiments and collecting data. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation. Unfortunately, that approach is riddled with biases, and sometimes other negative consequences that are even more challenging. Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made. The focus of this course is in teaching statistics as experimental design. The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance and appreciate why this is so important to reduce bias. That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs. Most of which she will forget immediately. And since I emphasize pre-planning, this book is full of simulations. That’s the really great advantage of using R to teach biostats, in my view. I’m not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work “under the hood”. When I do, it is mostly for stuff I think is helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don’t believe I’m the person who should be teaching it. Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That’s why operating within a statistical design framework is so important. For the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments and prolonged time to degrees. "],
["history.html", "Chapter 2 A Brief History of Experimental Design", " Chapter 2 A Brief History of Experimental Design Researchers in the pre-statistics days lacked the statistical framework that today’s researchers take for granted. Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they struggled with designing experiments, in summarizing the data, and in drawing inference from it. The statistical approach to experimental design we use today was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. Figure 2.1: RA Fisher in 1913, from the Adelaide Digital Archive At the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. He took that trajectory by accident. For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwin’s theory of evolution, which was the hot discovery of the day, of course. Fisher’s side hustle was to work on mathematical problems related to evolutionary genetics. Today, in those early days we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. That’s certainly where his career ambitions seem laid. He never lost an interest in evolution and would go on to become, unfortunately, a prominent eugenicist. Still, one big contribution he made during this early stage was no small feat. He defined variance as the square of the standard deviation. He proposed that variance is useful as a descriptive statistic for the variability within a population. Further developed, it would soon become the foundation of the multigroup exprimental designs that called ANOVA, the analysis of variance, which are widely used today. In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar (WWI) agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of this data repository. Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the station’s data, some of which represented serial experiments that had been running for decades. As he dug in Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of much of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds familiar then I’ve achieved my objective for mentioning it. Here’s when the paradigm shifted. Fisher began to think about the process by which experimental data should be collected. Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. As I mentioned above, his invention of the analysis of variance extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well. It was a truly remarkable period. In 1925 Fisher published a small book, Statistical Methods for Research Workers. In 1934 he published its extension, Design of Experiments. In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would soon cross oceans…and then disciplines. Today, experiments that we would recognize as statistically rigorous are those in which Fisher’s early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. There’s much more to this history, including many additional players and plenty of controversy that remains unsettled to this day. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today. "],
["bigpic.html", "Chapter 3 The Big Picture 3.1 What are experimental statistics?", " Chapter 3 The Big Picture To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher library(tidyverse) library(Hmisc) Let’s start by listing out some key characteristics that most biomedical experiments share in common. They… tend to involve the use of relatively small sample sizes. are usually highly exploratory in nature. generate data that are either discrete counts or measurements of continuous scalars. are structured by a small group of fairly common experimental designs. are usually interpreted in a binary way; as having “worked”, or not. test hypotheses (though too often these are unstated). aspire for rigor, replicability and reproducibility. aspire to be unbiased. The stakes of our work can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge, and opening the door to create impactful solutions, particularly in the realm of human diseases and suffering. But no less motivating are the issues more related to the professional practice of science. These include ego, the completely natural impulse to seek out validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. The point is that the process of scientific discovery is driven both by ideals and by biases. This is nothing new. The one big concept that I hope you embrace is that the statistical design and analysis of experiments serves as a working framework within which the biomedical researcher can conduct reasonably unbiased work. The statistical approaches covered in this course, it turns out, were invented long ago with all of these drivers in mind. 3.1 What are experimental statistics? Experimental statistics are used to summarize data into simpler descriptive models. as procedures to draw inferences from samples. as procedures that guide the design of experiments. to serve as framework for conducting unbiased research. Chances are you thought biostats was just one or two of those bullets, and probably not the latter two. 3.1.1 Descriptive modeling Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics. Thus, we use parameters like the sample mean and standard deviation, or median and ranges or percentiles. The alternative is unthinkable today (but common long ago), which is to discuss each replicate individually. To emphasize that sample parameters differ from population parameters, the standard in statistical notation is to use roman characters to indicate samples and greek characters to indicate the population. For example, parameter sample population mean \\(\\bar y\\) \\(\\mu\\) standard deviation \\(s\\) \\(\\sigma\\) variance \\(s^2\\) \\(\\sigma^2\\) Thus, \\(\\bar y\\) is an estimate of \\(\\mu\\). I’ll do my best to stick with this convention. Statistical tests also have a descriptive element in that they convey information about the experimental design. If you say, “I’m working up a two-tailed paired t-test,” say no more. From that alone I know something about your hypothesis, how your replicates are handled, the number of predictor groups, and the type of data you’re measuring. Regression models also describe data. For example, here is the well-known Michaelis-Menten model that describes product formation as a function of substrate concentration. \\[[P]=\\frac{[S][Vmax]}{[S]+Km}\\] That’s a model we might fit to certain kinds of enzyme kinetic data, because we use it to estimate scientifically meaningful parameters, like \\(V_{max}\\) and \\(K_m\\). In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. Take a simple example of two groups shown here. Each group has been fit to a simple model: that for the mean and standard deviation. Clearly, that model fits the control group much better than it fits the treatment group. Figure 3.1: How good of a descriptive model is the mean for each these groups? Why do I say that? The treatment group data are much more skewed. Most of the data values are greater than the mean of the group. Sure, a mean can be calculated for that group, but it serves as a fairly crappy summary. Perhaps some other model (or group of statistical parameters) would better convey how these data behave? This is to point out that learning statistics is about learning to make judgments about which models are best for describing a given data set. 3.1.2 Statistical inference There are two main types of inference researchers make. One type is to infer whether an experiment “worked” or not…the so-called “significance test”. This familiar process involves calculating a test statistic from the data (eg, t-test, F-tests, etc) and then applying a threshold rule to its value. If the test passes the rule, we conclude the experiment worked. I cover this type of inference in much more detail in the p-value chapter ??, and we’ll talk about it over and again throughout the course. A second type of inference is to extrapolate from a sample some estimate for the values of the variables within the population that was sampled. Both descriptive and statistical inference are subject to error. By random chance alone our sample could be way off the mark, even with perfectly calibrated instrumentation. The real difficulty with inference is we can never know for certain whether we are right or wrong. They are called random variables for a reason. It pays to have a very healthy respect for the role played by random chance in determining the values of our parameter estimates. If we were to completely redo a fully replicated experiment once more, we would almost certainly arrive at different numbers. In a well behaved system, they’d likely be in the same ballpark as those of the first experiment. But they would still differ. To illustrate, copy and paste the code chunk below. It replicates a random triplicate sample six times, taking six means. Unlike in real life, the population parameters are known (because I coded them in): \\(\\mu=2\\) and \\(\\sigma=0.4\\). You can run that chunk tens of thousands of times and never get a “sample” with one mean that has a value of exactly 2, even though that’s the true mean of the population that was sampled. x &lt;- replicate(6, rnorm(3, 2, 0.4)) apply(x, 2, mean) ## [1] 2.190308 2.107834 2.216556 2.099592 2.354583 1.846594 3.1.3 Experimental design Experimental planning that involves dealing with statistical issues is referred here as experimental design. This involves stating a testable statistical hypothesis and establishing a series of decision rules in advance of data collection. These rules range from subject selection and arrangement, predetermination of sample size using a priori power analysis, setting some data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, declaring a primary outcome, on up to what statistical analysis will be performed on the data. Experimental design is very common in prospective clinical research. Unfortunately, very few basic biomedical scientists practice anything remotely like this. Most biomedical researchers begin experiments with only vague ideas about the statistical analysis, which is usually settled on after the fact. Much of the published work today is therefore retrospective, rather than prospective. Yet, most researchers tend to use statistics that are largely intended for prospective designs. That’s a problem. 3.1.4 Statistics as an anti-bias framework If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct. The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn’t provide us a magic woo-woo cloak to protect us from our biases. Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias. Here are some of the more important biases. 3.1.4.1 Cognitive biases From a statistical point of view biases can be classified into two major groupings. The first are Cognitive biases. These are how we think (or fail to think) about our experiments and our data. These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, “how could this not work!” you are in the throes of a pretty deep cognitive bias. In bench research, cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or recognizing telling glitches in the data as meaningful. 3.1.4.2 Systematic biases The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed, and how data is handled or transformed. Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line. 3.1.4.3 Scientific misconduct More malicious biases exist, too. These include forbidden practices such as data fabrication and falsification. This is obviously a problem of integrity. Very few scientists working today are immune from the high stakes issues that pose threats to our sense of integrity. In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them. The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite livable. If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you’ve given it your best shot. That is hard to conclude when the latter occurs. "]
]
