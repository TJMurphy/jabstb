[
["introanova.html", "Chapter 25 Introduction to ANOVA 25.1 Jargon: Factors and levels 25.2 ANOVA models: One-, Two-, and Three-way 25.3 ANOVA inference protocol 25.4 ANOVA calculations 25.5 Completely randomized or related measures 25.6 Two-way ANOVA 25.7 Other ANOVA models 25.8 Alternatives to ANOVA", " Chapter 25 Introduction to ANOVA library(tidyverse) library(RColorBrewer) The choice of any statistical design and analysis is always driven by the type of outcome and predictor variables involved. Recall that all variables are either continuous or discrete. Furthermore, it’s helpful to think of outcome variables further classified as either measured, ordered or sorted, where measured variables are continuous, and ordered and sorted are discrete. Once the dependent variable is deemed to be continuous measured, the model for the experiment is determined by the number of levels of the independent variables. Figure 25.1: ANOVA heuristic The analysis of variance (ANOVA) is a method to design and evaluate experiments in which the predictor variable(s) are discrete factors with three or more levels and when the outcome variable is on some continuous measured scale. ANOVA is also univariate, in so far as the analysis involves only a single outcome variable. The validity of an ANOVA depends upon fulfilling the following assumptions. These are basically the same as we discussed for t-testing. Strong assumptions, do not use ANOVA when violated Every replicate is independent of all others. Some random process is used when generating measurements. Weaker assumptions, ANOVA is robust to violations of the following The distribution for the residuals from which the outcome variable is derived is continuous random normal. The variances of the groups are approximately equal. With small samples it is usually difficult to conclude that the 3rd and 4th assumptions are met. Both affirmative and negative results of tests of normality, tests for homogeneity of variance and outlier tests should be taken with a grain of salt. The smaller the sample size, the larger the grain of salt. Use your judgment. Do you have any reason to believe the variable is not normally distributed? Are you not up against a zero boundary? Are you confident the variable is measured in a linear range? Data that appears markedly skewed can be transformed using log or reciprocal functions. The resulting distributions are ~ normal. ANOVA testing can be performed on those transformed values. As we discussed with previously, the nonparametric analogs of ANOVA (Kruskal-Wallis and Friedman tests) are suitable options when normality is violated. There are alternative analytic options as will be detailed below. In particular, there is no need to use ANOVA for discrete types of outcome data. ANOVA is not designed to analyze such data, which is not continuous. Additionally, regression, either linear or nonlinear, is often a preferred method of analysis when the predictor variable is continuous rather than discrete. ANOVA represents a family of about a dozen or so statistical tests. These differ by how many predictor factors are involved and whether or not the measurements are intrinsically-related such that replicates are completely randomized or repeated/related. ANOVA experiments tend to design themselves since they are fairly intuitive way of asking questions. In fact, ANOVA is probably the most widely used experimental design in the biomedical sciences. If we randomly access any article in our favorite journal, chances are it will have a figure or table depicting an ANOVA design. They tend to look something like these. Figure 25.2: Typical graphs depicting some of the different ANOVA designs. The reasons why ANOVA is so popular are very simple. First, ANOVA is versatile. ANOVA allows for testing many groups simultaneously, for one or more factors, each at several levels. We can test for the main effect of each factor, or for interactions between factors. We can also test for differences between specific individual groups, using post hoc analysis. Repeated/related designs are readily accommodated, including mixed designs where one factor is completely randomized and another is related measure within a single multi-factor experiments. Second, ANOVA is efficient. As a general rule, fewer experimental units are needed to make the same number of pairwise group comparisons than would otherwise be necessary using a t-test-based experimental design. That efficiency can improve modestly as the number of groups increases. Each individual hypothesis test carries a risk of type1 error. In one sense, ANOVA serves as a protocol to detect differences between many groups while ensuring that the overall type1 error, the so-called experimentwise error or the family-wise error rate (FWER), remains fixed at the same tolerable threshold we’d set for a single t-test between two groups. 25.1 Jargon: Factors and levels In ANOVA jargon, predictor variables are classified as “factors”. ANOVA designs are said to be factorial. They are multifactorial if more than a single factor is involved. In other corners, ANOVA is referred to as factorial analysis (which should not be confused with factor analysis). Where some people describe an ANOVA experiment as a “one-way ANOVA” others might describe it as “one-factor ANOVA”. It’s all the same. The factors of ANOVA represent categorical, discrete variables that are each applied at two or more levels. For example, a factor at three levels is a predictor variable that has three discrete values. Imagine an experiment to explore how a particular gene influences blood glucose levels. Blood glucose levels, a continuous response variable, are measured in experimental units comprising a total of three different genotypes: wild-type, heterozygous knockouts of that gene, and homozygous knockouts of the gene. Here, genotype is a discrete predictor variable, a factor, which has three levels. To run functions related to ANOVA, R requires that your predictor variables are classified as factors in data sets. The following script creates a vector object called genotype. The object is a representation of the genotype variable. The data class for that vector is character because it is comprised of character strings. But look what happens when it is packaged into a data frame called my.factors. R coerces genotype into a factor variable with 3 levels. Which is nice. genotype &lt;- c(&quot;wild-type&quot;, &quot;heterozygote&quot;, &quot;homozygote&quot;) class(genotype) ## [1] &quot;character&quot; my.factors &lt;- data.frame(genotype) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 1 variable: ## $ genotype: Factor w/ 3 levels &quot;heterozygote&quot;,..: 3 1 2 But beware. The tidyverse tibble function is an analog of data.frame. Yet tibble does not coerce factors from character vectors. We’ll need to do that by hand if using tibble or functions that create tibbles, such as read_csv. genotype &lt;- c(&quot;wild-type&quot;, &quot;heterozygote&quot;, &quot;homozygote&quot;) class(genotype) ## [1] &quot;character&quot; my.factors &lt;- tibble(genotype) str(my.factors) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3 obs. of 1 variable: ## $ genotype: chr &quot;wild-type&quot; &quot;heterozygote&quot; &quot;homozygote&quot; as_factor(genotype) ## [1] wild-type heterozygote homozygote ## Levels: wild-type heterozygote homozygote This coercion won’t occur if a factor in an experiment represents a variable with numeric values. To illustrate what I mean by this, imagine adding a factor to the genotype experiment. We would test for the effect of an antidiabetic drug on blood glucose at 0, 10 and 30 microgram/kg. These effects would be measured at each level of the genotype factor. We would create the vector drug as an object representing the drug variable and its three levels as follows. Note however that here, R does not coerce numeric values as factors: drug &lt;- c(0, 10, 30) my.factors &lt;- data.frame(genotype, drug) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ genotype: Factor w/ 3 levels &quot;heterozygote&quot;,..: 3 1 2 ## $ drug : num 0 10 30 class(drug) ## [1] &quot;numeric&quot; That’s easily fixed using the as.factor function drug &lt;- as.factor(c(0, 10, 30)) my.factors &lt;- data.frame(genotype, drug) str(my.factors) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ genotype: Factor w/ 3 levels &quot;heterozygote&quot;,..: 3 1 2 ## $ drug : Factor w/ 3 levels &quot;0&quot;,&quot;10&quot;,&quot;30&quot;: 1 2 3 class(drug) ## [1] &quot;factor&quot; Alternately, we would enter the drug levels as character strings, which would be coerced into a factor at two levels when added to a data frame, like for the first example above: drug &lt;- c(&quot;zero&quot;, &quot;ten&quot;, &quot;thirty&quot;) I bring this up to point out that you can do ANOVA with continuous predictor variables. You just have to treat them as factors. When a continuous predictor variable is used at many levels (eg, a time series, a dose series, etc), regression per se may can be a better alternative to ANOVA. Regression allows for capturing additional information encoded within continuous variables (eg, rate constants, half-lives, etc). But that decision is more scientific than it is statistical. For example, you would use regression rather than ANOVA if the goal is to derive regression parameter estimates, such as slopes for rates or affinity constants. Only use ANOVA when the goal is to determine whether the factor has any effect at all, or if it interacts with some other factor. Just be aware that R will bark out error messages when we use continuous variables as factors without actually converting them to factor class objects. 25.2 ANOVA models: One-, Two-, and Three-way If an experiment has only one factor, it a one-way ANOVA design (alternately, “one-factor ANOVA”). If an experiment has two factors, it is a two-way ANOVA or two factor design. Three factors? Three-way ANOVA. These models can also be either completely randomized, repeated/related measures, or mixed. In a completely randomized structure, every level of the factor(s) is randomly assigned to replicates. Every replicate measurement is independent from all others. In a related measures structure, measurements within an experimental unit are intrinsically-linked. Every replicate receives all levels of a factor (eg, before-after, stepped dosing, or when subjects are highly homogeneous, such as cultured cells or inbred animals). Thus, the possible ANOVA models are quite diverse: One-way ANOVA -completely randomized One-way ANOVA -related measures Two-way ANOVA -completely randomized on both factors Two-way ANOVA -related measures on both factors Two-way ANOVA -mixed, one factor completely randomized, the other factor related measures Three-way ANOVA -can be CR, RM or mixed, I should mention that ANOVA for even more than three factors is conceptually possible. However, such large, complex designs have considerable downside where, by ANOVA analysis, it is difficult to tease out which factor and level is responsible for what. Three-way ANOVA, for example, allows for such a large number of hypotheses to be tested (three different main effects and four possible interaction effects, not to mention the large number of post hoc group comparisons) that it can be difficult to conclude what is responsible for any observed effects. These larger designs also tend to break the efficiency rule, it’s fair to say that three way ANOVA designs tend to be over-ambitious experiments…over-designed and usually under powered for the large number of hypotheses they can test. Designing an experiment as completely randomized or related measures is largely a scientific, not a statistical, decision. What that means is this: measurements are either intrinsically-linked or they are not. Making that determination is a scientific call, based upon the nature of the biological material that we have at hand. When we’ve concluded that measurements are intrinsically-linked, then the choice must be a related measures design and analysis. Similarly, choosing to run experiments as one-way or two-way ANOVA designs is also scientific. In fact, we choose these mostly to test whether two factors interact. When uninterested in whether two factors interact it is usually best not to combine them in an experiment. The questions could be answered using separate one-way ANOVAs, instead. We’ll discuss interaction hypotheses and effects in more detail later. 25.3 ANOVA inference protocol ANOVA can be used inferentially as stand alone test, or as an omnibus test. Most of the time we see it used as the latter. The test statistic for ANOVA is the F-test, which will be described below. A positive F-test result can be used to infer whether a factor, or an interaction between factors, is effective. Given the example above, I can use positive F-test to conclude that genotype at a given locus influences blood glucose. And just leave it at that, without demonstrating which conditions differ from each other. Alternately, a positive F-test result can be used as an omnibus. Here, a positive F-test implies that at least two levels of a factor differ from each other. The positive F-test grants access to find where those differences exist. This is done by making posthoc pairwise comparisons. The decision to use the F-test as a stand alone or as an omnibus is driven by our scientific objectives. Figure 25.3: ANOVA work flow if working at a 5% type1 error threshold. These “post hoc” pairwise comparisons are, essentially, any of several variations on the t-test designed to adjust p-values on the basis of the multiple comparisons. The choices of comparisons after the F-test are driven by scientific, rather than statistical, reasoning. We can compare all levels of all factors to each other, or we can compare a much more limited subset. What is important, statistically, is to make adjustments to the p-value threshold given all the comparisons made, so that the overall experimentwise type1 error does not exceed our declared threshold (usually 5%). In an experiment whose number of groups equals \\(k\\), there are a total of \\(m=\\frac{k(k-1)}{2}\\) possible comparisons to make. Let’s use the simplest case of a \\(k=3\\) groups ANOVA as an example. There are \\(m=\\frac{3(3-1)}{2}=3\\) comparisons that can be made. Using the Bonferroni correction (\\(p_{adjust}=\\frac{0.05}{m}=0.01667\\)). Thus, our adjusted threshold for declaring a statistical difference between 2 groups is now p &lt; 0.01667, rather than p &lt; 0.05. 25.4 ANOVA calculations The simplest way to think about ANOVA is that it operates like a variance budgeting tool. In the final analysis, the higher the ratio of the variance associated with the grouping factor(s) compared to the residual variance, the more likely that some group means will differ. ANOVA uses the least squares method to derive and account for sources of variation within a data set. Recall that the variance of a random variable \\(Y\\) is estimated through sampling. Our measurements have values of \\(y_i\\). Variance is calculated by dividing the sum of its squared deviates, \\(SS\\), by the sample degrees of freedom (df). \\[var(Y)=\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{n-1}=\\frac{SS}{df}=MS\\] In ANOVA jargon the variance is also commonly referred to as the mean square \\(MS\\). This jargon emphasizes that variance can be thought of as an averaged deviation for sample measurements from their mean values. Now, that formula only illustrates how variance is calculated for a single sample group. What about multiple groups? As you might imagine, we have to incorporate information from all of the groups. To begin to understand that, recognize that all ANOVAs, irrespective of the specific design have two fundamental sources of variation: Variation due to the experimental model, which is determined by the nature of responses to the predictor variables. Residual variation, which is variation that cannot be explained by predictor variables. Using sums of squared deviates the total variation within an experiment, whether due to known or known sources, can be accounted for. That total variation within an experiment can be expressed as the sum of the squared deviates. This represents the sum of the squared deviates for both the model and residual components: \\[SS_{total}=SS_{model}+SS_{residual} \\] \\(SS_{model}\\) is the variation we can explain by the effects of our factors. \\(SS_{residual}\\) is the variation our factors cannot explain. Perhaps it helps if we first think about this visually. Let’s imagine a simple one-way completely randomized ANOVA data set that looks like the graph below. There is only one factor, which has three levels. Each group has five independent replicates, for a total of 15 replicates within the entire experiment. Our model seeks to account for the effect of genotype \\[SS_{model}=SS_{genotype} \\] and thus \\[SS_{total}=SS_{genotype}+SS_{residual}\\] The graph illustrates each data point, the means for each group (black bars) and the grand mean of the sample (gold bars). We can readily imagine the distances from the data points to the group means and to the grand means. One can also appreciate and the distances from the group means to the grand mean. We probably have a harder time visualizing the squares of those distances. My mind sees it Euclidean (geometrically). Larger distances, squared, lead to bigger boxes! The bigger the boxes, the greater that replicate contributes to the variance. With that picture in mind, think of the variances within the experiment as follows: Total variance: average squared distances of the all the points to the grand mean Model variance: weighted average squared distances of group means to the grand mean Residual variance: average squared distances of the points to the group means Figure 25.4: A completely randomized one way ANOVA, the genotype factor has three levels. Gold bar = grand mean, black bar = group means 25.4.1 Sums of Squares partitioning The first step in an ANOVA involves partitioning the variation in a data set using sums of squares. In this experiment, there are \\(i=1, 2..n\\) independent replicates. There are also \\(j=1, 2..k\\) groups. The total sum of squares is the sum of the squared deviation from all data points to \\(\\hat y\\), which is the grand mean of the sample. \\[SS_{total}=\\sum_{j=1}^k\\sum_{i=1}^n(y_i-\\hat y)^2\\] The sum of squares for the genotype effect is sum of the weighted squared deviation between the group means, \\(\\bar y_j\\) and the grand mean. Here, \\(n_j\\) is the sample size within the \\(j^{th}\\) group. Thus, the group sample sizes are creating that weight. \\[SS_{genotype}=\\sum_{j=1}^kn_j(\\bar y_j-\\hat y)^2\\] Some software refers to this variation as the “treatment” sum of squares. Parenthetically, let’s pause to reflect for a moment to consider one consequence of how that equation shows the weighting of group deviation by sample size. The level of model variation can skew to one group when its sample size differs markedly from the others. This explains why you want to keep group sizes reasonably balanced, or roughly equivalent, when designing experiments. Finally, the residual sum of squares is calculated as the sum of the squared deviation between replicate values and group means, \\[SS_{residual}=\\sum_{j=1}^k\\sum_{i=1}^n(y_i-\\bar y_j)^2\\] which, because the total variation amount of is fixed, can also be calculated as follows: \\[SS_{residual}=SS_{total}-SS_{genotype}\\] In some software residual variation is referred to as “error”. The term “error” arises from ANOVA theory, which holds that the true population means represented by these sample groups are “fixed” in the population. Thus, any variation associated with our estimate of the values for means must be in error. Residuals are the measurements of that “error”. Perhaps we can intuit a few things. First, the residual variation is the variation unaccounted for by the model. Meaning that whatever its causes, they are not under experimental control. Second, if the variation around each group mean remains similar, but as the group means differ from each other more, the greater the fraction of the overall variation that will be associated with the model, and the less that will be associated with the residual. Third, when the noise around those group means increases, less of the total variation will be associated with the model of group means, and the more with the residual. In other words, noisy experiments tend to hide detectable differences between means, while clean experiments favor detecting these differences. If this seems bloody obvious, and it is simple, then we should not have any problem processing how ANOVA works. The following two graphs emphasize these observations. In the null graph, the group means are roughly equivalent and very nearly the same as the grand mean. There’s very little model variation relative to the grand mean. Most of the variation is in the residuals, relative to each group mean. In the effective treatment graph, where the means truly differ because I coded them to differ, the residual variation is about the same as the null. The distance from each point to its group mean is about the same as in the null case. But we can see there is a lot more model variation, at least compared to the null graph. The group means (black bars) really separate from the grand mean (gold line). ANOVA tests become very simple when we understand this. If the model variation is the signal and the residual variation is the noise, and the ratio between the two grows high, then the model explains the signal!! 25.4.2 Degrees of freedom Again, variance is an averaged deviation. To calculate a variance we’ll need to divide the sum of squares for a component by its degrees of freedom. Just as sum of squares are calculated differently depending on whether it is total, or model or residual, so too are degrees of freedom. The theory behind \\(df\\) is a bit more complicated than this, as a general rule, we lose a degree of freedom every time the calculation of a mean is involved in determination of a given \\(SS\\). The basic idea is this: For that mean value to be true, one of the replicates must remain fixed, while all the others are free to vary. The degrees of freedom for total variance are \\(df_{total}=N-1\\). We use all 15 replicates, \\(N\\), to calculate \\(\\hat y\\), the grand mean. We lose a degree of freedom because for that grand mean to be true, one of those replicate values must be fixed while the others are free to vary. We have \\(k\\) predictor levels. The degrees of freedom for the genotype model variance are \\(df_{genotype}=k-1\\), because the calculation is based upon the group means, two of which can be free to vary. The residual degrees of freedom are \\(df_{residual}=N-k\\). 25.4.3 The mean squares The mean squares are ANOVA jargon to represent variances. Think of variance as averaged variation. Total variance is \\[MS_{total}=\\frac{SS_{total}}{df_{total}} \\] The variance associated with the model is \\[MS_{model}=\\frac{SS_{model}}{df_{model}} \\] And the residual variance is \\[MS_{residual}=\\frac{SS_{residual}}{df_{residual}} \\] 25.4.4 The ANOVA table The typical ANOVA table lists the following: source of variation, its \\(df\\), * its \\(SS\\), * its \\(MS\\) * an F-test, where appropriate * a p-value from the F-test ANOVA functions in R vary in their output. But here’s the ANOVA table output for the data in the last previous figure: anova(lm(blood_glucose ~ genotype, data)) ## Analysis of Variance Table ## ## Response: blood_glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 2 21176.9 10588.4 22.979 7.877e-05 *** ## Residuals 12 5529.4 460.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 25.4.5 The F statistic The value of F is the ratio of two variances. For the result in the ANOVA table above \\(F=\\frac{10588.4}{460.8}=22.979\\). We can interpret that as the variance associated with the genotype model is 22.979x greater than the residual variance. But is an F-statistic value of 22.979 with 2 and 12 degrees of freedom extreme? The p-value is derived from an F probability distribution that has 2 and 12 degrees of freedom. The p-value result in the table (Pr(&gt;F)) can be mimicked using R’s pf function): pf(22.979, 2, 12, lower.tail=F) ## [1] 7.87784e-05 The example above is a simple one factor completely randomized model for which there is only one F test, because it has only one component. Experimental designs that have more components, such as two or more factors with our without related measures, will have more F tests if completely randomized. A three-way ANOVA performed a certain way can have about a dozen F tests. 25.4.6 Post-hoc group comparisons A positive F test only tells you that the model (or its component) explains the response better than null. An final stage of ANOVA, which is optional but done very commonly, is to run what are essentially t-test comparisons between groups. These posthoc tests identify the specific factor levels that actually differ from each other. For example, an experiment with \\(k\\) groups may be designed to test which of several groups differ from a negative control. The total number of comparisons that could be made are \\(\\frac{k(k-1)}{2}\\). Yet, for scientific reasons, only a specific \\(k-1\\) subset are of any interest. There are several ways to run these post-hoc group comparisons. What is most important, however, is these be done in a way that keeps the family-wise error rate (FWER) below the pre-set type1 error threshold. That’s commonly 5% in biomedical research, but its value can be whatever you decide. 25.5 Completely randomized or related measures Up until now we’ve discussed ANOVA in its simplest use case, the one-way completely randomized (CR) ANOVA. Related measures (RM) ANOVA is done when the measurements are not completely independent, but instead are intrinsically-linked. Examples of intrinsically-linked subjects include two measurements, one from each of a pair of identical human twins, before and after on a single subject, all plates and wells from a single batch or passage of a cell culture, a protein preparation from a single batch, a single cell in culture, split tissues from one animal subject, and litter mates of isogenic animal strains. There are certainly others. An RM design involves taking multiple measurements from each independent replicate. Each replicate may vary randomly. That random variation between replicates can be accounted for, too. Because we can account for the variation within each replicate, that source of variation is no longer in the residual error term, but can be taken right into the model. 25.5.1 The problem of lost data in related measures designs The CR vs RM design decision has a few important consequences. First, when within-subject correlation is high, RM are much more efficient and less costly to produce. How much more? You can run Monte Carlo simulations to establish this for virtually any set of conditions. Second, over the course of any experiment it is possible to lose specific response values here and there. For example, a data value may be lost due to a bad lane in a replicate western blot, you accidentally throw away a tube from a series, or any of a number of such primitive errors. CR ANOVA is tolerant of such losses. That leads to a missing replicate, and an unbalanced data sets, but it’s only one or a few values out of many. That is not the case with related measures designs. All of the values for every level of every factor for every replicate must be included. If any values are missing for a given replicate, all of the remaining values for that replicate either have to be censored, or the missing values should be imputed. The missing data problem becomes amplified in two way and three way related measures ANOVA! Those experiments tend to have more groups, meaning more data is at risk of being censored. Researchers often ask if it is reasonable to ‘flip’ to a completely randomized analysis when they notice too many values are missing from their data set. No, it is not reasonable. The type of experimental design is scientifically-driven. Intrinsically-linked measurements are not independent, and should not be analyzed as if they are independent. To do so violates one of the two primary assumptions of the statistical analysis. There are a few options to deal with this, and both happen in planning. First, where possible, include an extra replicate or two as a hedge over what the power analysis suggests is necessary. Second, don’t make a RM design too over-ambitious. Limit the number of levels to that which is scientifically important. Third, be aware of the risk of lost values. Is the experimental protocol difficult? Are any protocol steps at high risk of failure? Are there any intrinsic barriers to efficiently collecting the data? 25.6 Two-way ANOVA This is a method to investigate the effects of two factors simultaneously. Thus, the variation associated with each factor can be partitioned. This also allows for assessing the variation associated with an interaction between the two factors. What is an interaction? Simply, it is a response that is greater (or lesser) than the sum of the two factors combined. Let’s go back to the genotype blood_glucose problem. We’ll add a factor, and simplify the study a bit. We’re interested in a gene that, when absent, raises blood glucose. We’re also interested in a drug that, when present, lowers blood glucose. We have reason to hypothesize that a genotype:drug interaction might exist. For example, the gene might encode a protein that metabolizes our drug, thus impairing the drug’s ability to lower blood glucose. We’ll simulate a \\(2\\times 2\\) experiment that has only the presence or absence of each of these factors. Here’s the data: set.seed(12345) blood_glucose &lt;- round(c(rnorm(5, 100, 20), rnorm(5, 75, 20), rnorm(5, 200, 20), rnorm(5, 100, 20)), 1) drug &lt;- as.factor(rep(rep(c(0, 30),each=5),2)) genotype &lt;- rep(c(&quot;WT&quot;, &quot;KO&quot;), each=10) test &lt;- data.frame(genotype, drug, blood_glucose) y0 &lt;- mean(subset(test, genotype==&quot;WT&quot; &amp; drug==0)$blood_glucose) y30 &lt;- mean(subset(test, genotype==&quot;WT&quot; &amp; drug==30)$blood_glucose) yend0 &lt;- mean(subset(test, genotype==&quot;KO&quot; &amp; drug==0)$blood_glucose) yend30 &lt;- mean(subset(test, genotype==&quot;KO&quot; &amp; drug==30)$blood_glucose) ggplot(test, aes(genotype, blood_glucose, color=drug))+ geom_jitter(size=6, width =0.3)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ stat_summary(fun.y=mean, geom=&quot;point&quot;, shape = 95, size= 15)+ scale_x_discrete(limits=c(&quot;WT&quot;, &quot;KO&quot;))+ labs(y=&quot;blood glucose&quot;)+ geom_segment(aes(x=&quot;WT&quot;, y=y30, xend=&quot;KO&quot;, yend=yend30))+ geom_segment(aes(x=&quot;WT&quot;, y=y0, xend=&quot;KO&quot;, yend=yend0)) An interaction effect can be represented by the differing slopes of those two lines. If the lines are not parallel, it means that the effect of the drug is not the same at both levels of genotype. Or you could say the effect of the genotype is not the same at both levels of the drug. Whatever. The statistical term used to describe such phenomena is that the two factors interacted. Thus, a statistical interaction occurs when the effects of two factors are not the same across all of their levels. Here’s a quick ANOVA table for those data. It has an F test for each of the factors genotype and drug, and an F test for the genotype:drug interaction. All three F-tests are extreme. anova(lm(blood_glucose ~ genotype + drug +genotype*drug, test)) ## Analysis of Variance Table ## ## Response: blood_glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 1 25127.0 25127.0 94.333 4.116e-08 *** ## drug 1 26028.1 26028.1 97.716 3.225e-08 *** ## genotype:drug 1 4845.4 4845.4 18.191 0.0005923 *** ## Residuals 16 4261.8 266.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since the F-test for the genotype:drug interaction is extreme, we can reject the null that there is no interaction between them. Furthermore, the existence of the interaction effect complicates the interpretation of the effects of the genotype and drug factors. Generally, an interaction effect supersedes an effect of either factor, alone. Although genotype seems to have a strong effect on glucose levels, that’s really blunted in the presence of drug. And although the drug seems to reduce glucose, that effect becomes remarkably prominent when the genotype changes. When interactions occur, the effect of one factor cannot be interpreted without condition on the other factor! It might be useful to see the data for the discussion that follows. genotype drug blood_glucose WT 0 111.7 WT 0 114.2 WT 0 97.8 WT 0 90.9 WT 0 112.1 WT 30 38.6 WT 30 87.6 WT 30 69.5 WT 30 69.3 WT 30 56.6 KO 0 197.7 KO 0 236.3 KO 0 207.4 KO 0 210.4 KO 0 185.0 KO 30 116.3 KO 30 82.3 KO 30 93.4 KO 30 122.4 KO 30 106.0 Four sources of variation are accounted for: the main effect of genotype, the main effect of drug, the interaction between drug and genotype, and the residual error. Three F tests were performed. Each of these used \\(MS_{residual}\\) in the denominator and the \\(MS\\) for the respective source in the numerator. How has this variation been determined? The experimental model is as follows: \\[SS_{model}=SS_{genotype}+SS_{drug}+SS_{genotype\\times drug} \\] The grand mean of all the data is computed as before: \\[\\hat y=\\frac{1}{n}\\sum_{i=1}^ny_i \\] There are two factors, each at two levels. Thus, there are a total of \\(j=4\\) experimental groups. Each group has a sample size of \\(n_j=5\\) replicates and represent a combination of predictor variables: WT/0, WT/30, KO/0, KO/30. The mean of each group is \\(\\bar_j\\). Therefore, \\[SS_{model}= \\sum_{j=1}^kn_j(\\bar y_j-\\hat y)^2 \\] represents the total model variation. We can also artificially group these factors and levels further. Two groups correspond to the levels of the genotype factors and two correspond to the levels of the drug factor. Their means are \\(\\bar y_{wt}, \\bar y_{ko}\\) and \\(\\bar y_0, \\bar y_{30}\\), respectively. Each of these groups has a sample size of \\(2n_j\\). These contrived means are used to isolate for the variation of each of the two factors: \\[SS_{genotype}= \\sum n_{wt}(\\bar y_{wt}-\\hat y)^2+n_{ko}(\\bar y_{ko}-\\hat y)^2 \\] \\[SS_{drug}= \\sum n_{0}(\\bar y_{0}-\\hat y)^2+n_{30}(\\bar y_{30}-\\hat y)^2 \\] All that remains is to account for the variation associated with the interaction effect. That can be solved for algebraically: \\[SS_{genotype\\times drug}=SS_{model}-SS_{genotype}-SS_{drug} \\] Because two way ANOVA’s have two factors, one of the factors can be applied completely randomized, and the other can be applied as related measures. Or both factors can be completely randomized, or both can be related measures. 25.7 Other ANOVA models For all ANOVA experiments, irrespective of the design, the total amount of deviation in the data can be partitioned into model and residual terms: \\[SS_{total}=SS_{model}+ SS_{residual}\\] What’s interesting is that different ANOVA experimental designs have different models. We’re interested in two factors, factorA and factorB, and have the ability to study each at multiple levels. The interaction between factorA and factorB is \\(A\\times B\\) One way CR: \\(SS_{model}=SS_{factorA}\\) One way RM: \\(SS_{model}=SS_{factorA}+SS_{subj}\\) Two way CR: \\(SS_{model}=SS_{factorA}+SS_{factorB}+SS_{A\\times B}\\) Two way RM on A factor: \\(SS_{model}=SS_A+SS_B+SS_{A\\times B}+SS_{subj\\times A}\\) Two way RM on both factors: \\(SS_{model}=SS_A+SS_B+SS_{A\\times B}+SS_{subj\\times A}+SS_{subj\\times B}+SS_{subj\\times A\\times B}\\) The big difference between completely randomized and related measure designs is that in the latter, we’re now accounting for the deviation associated with each replicate in the model! Otherwise, that replicate deviation would have been blended into the residuals. This turns out to be a pretty big deal. When that deviation due to the subjects is pulled out of the residual, it lowers the value of the denominator of the F statistic. Thus making the F statistic larger! 25.7.1 R and ANOVA There are a handful of ways to conduct ANOVA analysis on R. These are not necessarily more right or wrong than the others. What is important to know, however, is that they do perform calculations differently under certain circumstances (eg, Type 1 v Type 2 v Type 3 SS calculations). Therefore, they produce distinct results. This always confuses researchers, particularly when comparing R’s results to other software we might be more comfortable with. We wonder which output is “right”. Chances are they are all “right”. We have that output because of the way we argued the analysis. This again emphasizes the need to share specific details of the analysis in our publications. In this case, specify using R, specify the package version and the R function used, and even specify the arguments. Or append an R script file as supplemental information to illustrate exactly how the analysis is performed. Given data and a group of arguments we’ll call foo, R’s ANOVA function options are as follows: anova - A function in R’s base. eg, anova(lm(foo)) aov - A function in R’s base. eg, `aov(foo) Anova - A function in R’s car package. eg, Anova(lm(foo)) ezAnova - A function in R’s ez package, ezAnova(foo) There are others. For example, since ANOVA analyses are also general linear models the same basic problem can also be solved using lm(foo)without ANOVA. Passing an lm(foo) into an ANOVA function mostly just provides output in the familiar ANOVA notation. The statistical fundamentals of lm(foo) and ezAnova(foo)are identical. For the ANOVA part of this course, we’ll use ezANOVA from the ez package. In particular, it is a bit more straightforward to use (and teach) in terms of arguing the experiment’s ANOVA model. Key Jargon to understand to do ezANOVA in R Specify a completely random design by defining the ‘between’ variable as your factor name. The between here is meant to imply comparisons between groups. Specify a related measures design by defining the within variable as your factor name. The within here is meant to imply comparisons within replicates. 25.7.1.1 Type of calculation When analyzing 2-way and 3-way ANOVA’s there are three different methods to calculate, which are referred to as type I, type II and type III. When an experiment is balanced, which is to say it has equal sample sizes per group, the type of calculation is immaterial. In that case,type I, II and III yield exactly the same output. Unbalanced experiments are those in which the sample sizes of groups are not the same. As long as the differences are not too large, the presence of unbalance is usually not a problem. But it will always impact the precise output of different ANOVA functions, depending upon whether they perform type I, II or III calculations. This is what really causes confusion when we notice different output from different functions. This is particularly confusing when comparing the output of different ANOVA functions in R (eg, Anova vs aov vs anova vs ezAnova) and/or commercial software (eg, SAS, SPSS, Prism). The researcher scratches her head, wonders which is “correct”. In one sense, they are all correct. Type I, II and II sum of squares calculations are explained here. Suffice to say this is important to not overlook. This serves to illustrate how providing good detail about the software used to analyze data is important for reproducibility. The most significant point to understand is that some commercial software uses type 3 calculations by default. As a consequence, given the same data set, the results from those packages may not coincide perfectly with those of ezANOVA unless using a type = 3 argument in the function. My recommendation is to use type = 2 when interested in only testing hypotheses about the main effects of factors, and there is no interest in an interaction if working on a two- or three-way ANOVA data set. That’s because type = 2 is purported to yield consistently higher power for main effects. Use type = 3 when, instead, the experiment is designed to test whether an interaction occurs between factors. When an interaction occurs, the main effects are not interpretable. Type I sum of squares are calculated when using the anova and aov functions of base R. This is otherwise known as “sequential” sum of squares calculation. On multifactor data with those functions, the results can differ given the order by which the factors are argued. Thus, aov(lm(outcome~factorA + factorB)) might yield slightly different results compared to aov(lm(outcome~factorB + factorA)). The idea is to calculate the effect on a factor that is most interesting to you scientifically, while “controlling” for the effect of the other factor. When using ezANOVA we can explicitly argue which calculation we wish to run, removing some ambiguity about what actually happened. 25.8 Alternatives to ANOVA When the outcome variable is measured and the design is completely randomized, the data can be analyzed using the general linear model using R’s lm function, rather than by ANOVA. This allows for analyzing interaction effects between factors. The results will be the same as ANOVA. If the design has a related measures component, then a linear mixed effects model should be run instead. In that case, use lmer in the lme4 package. Alternately a nonparametric analysis can be performed using either the Kruskal-Wallis (completely randomized) for the Friedman (related measures) test. Bear in mind that no nonparametric analog for the two-way ANOVA exists. Thus, hypotheses related to interaction effects are not testable using nonparametric statistics. Finally there is the generalized linear model (glm) for completely randomized designs or the generalized linear mixed model (glmer) for designs that incorporate related measures, respectively. Each of these allow for testing interactions between factors. The generalized linear models also allow for a flexible array of outcome variables. These should be used, rather than ANOVA, when the outcome variable is non-normal or is discrete. For example, these are the tools of choice when the outcome variable is binomial (logistic regression) or frequency data (Poisson regression) and there are 3 or more groups to compare. Additional families are possible. 25.8.1 Screw ANOVA, Just Tell Me How to t-Test Everything OK, fine. This is far from ideal because an ANOVA-free approach risks introducing bias through cherry-picking comparisons after unpacking the data (the HARK fallacy). You don’t have to do ANOVA for an experiment with 3 or more groups (or anything else for that matter–you just have to be able to defend your choices). A major purpose of ANOVA is to maintain an experiment-wise type1 error of 5%. But there are other ways to accomplish this objective. For example, you might skip the ANOVA step and simply run serial t-tests comparing all of the groups in an experiment. Or run t-tests to compare a pre-planned sublist of all possible comparisons. The emphasis here on pre-planning is important. Make decisions ahead of time about what is to be compared, then make only those comparisons. No more and no less. Otherwise, you’re snooping. Once you’re in snooping mode, you’re deeply biased towards opportunistic outcomes. For example, you may run multiple control groups within your experiment to signal that some important aspect of the protocol is working properly, but these controls are otherwise not scientifically interesting (with respect to testing new hypotheses). You may not wish to expend any of your type1 error budget doing comparisons on these controls. With those reservations noted, what follows are two ways to go about this. To begin, if we have \\(k\\) levels of predictor variables in our experiment it has a total of \\(m=k(k-1)/2\\) possible comparisons that could be made. The pairwise.t.test is the function to use for this purpose. Use it to make the group comparisons that interest you. choosing the p.adjust.method that strikes your fancy. Two of the latter are listed below. 25.8.1.0.1 Bonferroni Correction If \\(m\\) is the number of comparisons to be tested, whether or not it is equal to \\(k(k-1)/2\\), and if \\(\\alpha\\) is the type1 error threshold you’ve set for the entire experiment, then the corrected type1 error threshold for each comparison is \\(\\alpha_m=\\frac{alpha}{m}\\). Thus, you would reject the null hypothesis for any comparison for which a t-test yields a p-value that is less than \\(\\alpha_m\\). 25.8.1.0.2 Sidak Correction This is a modestly more liberal alternative to the Bonferroni correction. Here, for \\(m\\) comparisons and an experimentwise type1 error threshold \\(\\alpha\\), the corrected per comparison threshold would be \\(\\alpha_m=1-(1-\\alpha)^\\frac{1}{m}\\). Again, for a given comparison reject, the null if its p-value is less than \\(\\alpha_m\\). "]
]
