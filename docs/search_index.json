[
["index.html", "JABSTB: Statistical Design and Analysis of Experiments with R Preface", " JABSTB: Statistical Design and Analysis of Experiments with R TJ Murphy PhD, Department of Pharmacology, School of Medicine, Emory University, Atlanta, GA biostats538@gmail.com 2018-12-31 Preface This book is a resource for students enrolled in my biostats course. The students are primarily in one of Emory’s biomedical and biological sciences PhD programs. There are the ocassional Emory honors program undergrads, students from Emory’s public health school, and usually a few Georgia Tech graduate students, also. When you teach stats you have to choose software and maybe a textbook. I’d been wrestling with the decision to switch my course over to R for a few years, but never really found the right stats book that covers the subject the way I like to emphasize. When I finally took the plunge I needed to prepare a bunch of handouts with coding examples for the various types of analyses I’d been teaching. Then I decided to write some intro handouts. Before you know it, you have a “book”. This is that. JABSTB. Not included in this book are additional materials for the course (eg, take home and group exercises, slide decks, data sets, my extensive collection of stats cartoons, etc). The scope of the book is to provide a) some background on statistical fundamentals that are most relevant to the biomedical researcher who is testing ideas by generating data (sampling, error, statistical hypotheses, experimental design) and then, b) provide examples for running and interpreting various statistical functions. These are meant to be starting points. What’s really nice about R is its adaptability. Copy and paste my examples into your R script or R markdown file, understand how they work by trying to break them, and then get to work modifying them for your own purposes. Most chapters have code that require R packages beyond those that come in the base R installation. Those are listed at the head of each chapter. Each chapter has a corresponding RMarkdown document. If you wish to grab those documents instead of using this material as HTML, go grab it on Github. Simply fork, clone or download them from the Github jabstb repo. You’ll just need the Rmd files. All of the other files are there for publishing this up as bookdown. By the way, I’ve optimized these Rmd’s to make good HTML pages. I have not even checked to see if they work as PDFs. This book is a living document. It is subject to a lot of on-the-fly revision. Stuff will be added and eliminated over time. As I write these words, in Dec 2018, my main disclaimer is that it is definitely an MVP. If you find errors, have any suggestions, or would otherwise like to contribute, I welcome your help. Please submit a pull request and/or contact me by email. I welcome your additions. Copyright 2018 © TJ Murphy MIT license. "],
["author.html", "Chapter 1 About the author", " Chapter 1 About the author A few years ago I adopted this course from Frank Gordon, a colleague who had retired. Like Frank, I’m a biomedical scientist who happens to have high level of interest in statistical methods. I learned this material as a graduate student at Mizzou. There I took several stats courses as electives. The ones that impacted me the most were taught by the late Gary Krause, then a professor and statistician in Mizzou’s agricultural college. The light turned on for me during Gary’s Experimenal Design course. That’s when the fog of mathematical statistics cleared enough so I could finally “get” the pragmatic value of statistics for the researcher. What became most clear is that experimental design is a statistical framework for conducting unbiased research. That concept permeates my course and this book. I was working on my PhD in pharmacology within the medical school. But most of my classmates in Gary’s courses were working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. The problem my classmates shared, which was not one that really affected me, is having one growing or mating season by which to run a fully replicated experiment. One shot. That one shot changes everything. Planning was a priority for them. They needed to map out their experimental design in advance. Once the experiment began, any new wrinkles or oversights would have to wait until the next growing season. They didn’t have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups. Planning was based upon statistical design principles, often in consultation with Gary. Statistics were a priori planning and post-hoc tests. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the planned approach. In contrast, it is fair to say that most biomedical scientists fail to incorporate statistical design into their plans. That failure opens up a whole can of worms that can generally be characterized as doing statistics in ways it was never meant to be done. All too common is the biomedical researchers who takes a more “fly by the seat of their pants” approach to running experiments and collecting data. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation. Unfortunately, that approach is riddled with biases, and sometimes other negative consequences that are even more challenging. Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made. The focus of this course is in teaching statistics as experimental design. The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance and appreciate why this is so important to reduce bias. That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs. Most of which she will forget immediately. And since I emphasize pre-planning, this book is full of simulations. That’s the really great advantage of using R to teach biostats, in my view. I’m not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work “under the hood”. When I do, it is mostly for stuff I think is helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don’t believe I’m the person who should be teaching it. Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That’s why operating within a statistical design framework is so important. For the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments and prolonged time to degrees. "],
["history.html", "Chapter 2 A Brief History of Experimental Design", " Chapter 2 A Brief History of Experimental Design Researchers in the pre-statistics days lacked the statistical framework that today’s researchers take for granted. Our ancestor scientists were remarkably adept at the scientific method, in making observations, and in collecting data with great care. However, they struggled with designing experiments, in summarizing the data, and in drawing unbiased inference from it. The statistical approach to experimental design we use today was first enumerated about a century ago, largely by Sir RA Fisher. His story is interesting in part because it is just so classically accidental. Figure 2.1: RA Fisher in 1913, from the Adelaide Digital Archive At the outset of his career Fisher did not foresee authoring the foundational principles of experimental design and statistics practiced by most of us today. He took that trajectory by accident. For about five years after graduating from Cambridge, Fisher worked as a census bureaucrat and part time math teacher. He was smitten by Darwin’s theory of evolution, which was the hot discovery of the day, of course. Fisher’s side hustle was to work on mathematical problems related to evolutionary genetics. Today, we would probably recognize him as a hobbyist quantitative geneticist or perhaps even as one of the first bioinformaticians. That’s certainly where his career ambitions seem laid. He never lost an interest in evolution and would go on to become, unfortunately, a prominent eugenicist. The take-away from that, alone, is that statistics is not a fool-proof antibias framework. Still, one big contribution he made during this early stage was no small feat. He defined variance as the square of the standard deviation. He proposed that variance is useful as a descriptive statistic for the variability within a population. Further developed, it would soon become the foundation of the multigroup exprimental designs that called ANOVA, the analysis of variance, which are widely used today. In 1919 Fisher was hired as a temporary statistician by Sir John Russell, the new director of the Rothamsted Experimental Research center in England. After decades of underfunding Rothamsted had become a bit rundown. Russell, an agricultural chemist who today we would probably categorize as a biochemist, was hired to beef up postwar (WWI) agricultural research in the UK. Upon arrival he realized the station had a large repository of data. Fully expecting to create even more under his leadership. Russell believed bringing a mathematician on board could help him make sense of this data repository. Thus, Russell hired Fisher to take a temporary position. Today, we would recognize Fisher in his Rothamsted role as a freelance data scientist charged with conjuring meaning from reams of the station’s data, some of which represented serial experiments that had been running for decades. As he dug in Fisher saw a lot of flaws in the Rothamsted dataset. He had difficulty making sense of much of it. Mostly because the experiments were, in his view, so poorly designed the results were uninterpretable. If that sounds familiar then I’ve achieved my objective for mentioning it. Here’s when the paradigm shifted. Fisher began to think about the process by which experimental data should be collected. Almost immediately after digging into his Rothamsted work he invented concepts like confounding, randomization, replication, blocking, the latin square and other factorial designs. As I mentioned above, his invention of the analysis of variance extended his prior work on variance. The procedure of maximum likelihood estimation soon followed, as well. It was a truly remarkable period. In 1925 Fisher published a small book, Statistical Methods for Research Workers. In 1934 he published its extension, Design of Experiments. In these works lay the foundations of how researchers today approach their experiments. His statistical procedures, developed with agricultural science in mind, would soon cross oceans…and then disciplines. Today, experiments that we would recognize as statistically rigorous are those in which Fisher’s early principles operate as procedures. We know today that randomization and pre-planned levels of replication are essential for doing unbiased research. The block ANOVA designs he mapped out then are among the most common experimental designs that we see in the biological and biomedical literature today. There’s much more to this history, including many additional players and plenty of controversy that remains unsettled to this day. I emphasize Fisher mostly because his experimental design and analysis procedures remain the standard for prospective experiments today. "],
["bigpic.html", "Chapter 3 The Big Picture 3.1 What are experimental statistics?", " Chapter 3 The Big Picture To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher library(tidyverse) library(Hmisc) Let’s start by listing out some key characteristics that most biomedical experiments share in common. They… tend to involve the use of relatively small sample sizes. are usually highly exploratory in nature. generate data that are either discrete counts or measurements of continuous scalars. are structured by a small group of fairly common experimental designs. are usually interpreted in a binary way; as having “worked”, or not. test hypotheses (though too often these are unstated). aspire for rigor, replicability and reproducibility. aspire to be unbiased. The stakes of our work can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge, and opening the door to create impactful solutions, particularly in the realm of human diseases and suffering. But no less motivating are the issues more related to the professional practice of science. These include ego, the completely natural impulse to seek out validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. The point is that the process of scientific discovery is driven both by ideals and by biases. This is nothing new. The one big concept that I hope you embrace is that the statistical design and analysis of experiments serves as a working framework within which the biomedical researcher can conduct reasonably unbiased work. The statistical approaches covered in this course, it turns out, were invented long ago with all of these drivers in mind. 3.1 What are experimental statistics? Experimental statistics are used to summarize data into simpler descriptive models. as procedures to draw inferences from samples. as procedures that guide the design of experiments. to serve as framework for conducting unbiased research. Chances are you thought biostats was just one or two of those bullets, and probably not the latter two. 3.1.1 Descriptive modeling Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics. Thus, we use parameters like the sample mean and standard deviation, or median and ranges or percentiles. The alternative is unthinkable today (but common long ago), which is to discuss each replicate individually. To emphasize that sample parameters differ from population parameters, the standard in statistical notation is to use roman characters to indicate samples and greek characters to indicate the population. For example, parameter sample population mean \\(\\bar y\\) \\(\\mu\\) standard deviation \\(s\\) \\(\\sigma\\) variance \\(s^2\\) \\(\\sigma^2\\) Thus, the sample mean, \\(\\bar y\\) is an estimate of the population mean, \\(\\mu\\). Statistical tests also have a descriptive element in that they convey information about the experimental design. If you say, “I’m working up a two-tailed paired t-test,” say no more. From that alone I know something about your hypothesis, how your replicates are handled, the number of predictor groups, and the type of data you’re measuring. Regression models also describe data. For example, here is the well-known Michaelis-Menten model that describes product formation as a function of substrate concentration. \\[[P]=\\frac{[S][Vmax]}{[S]+Km}\\] That’s a model we might fit to certain kinds of enzyme kinetic data, because we use it to estimate scientifically meaningful parameters, like \\(V_{max}\\) and \\(K_m\\). In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. Take a simple example of two groups shown here. Each group has been fit to a simple model: that for the mean and standard deviation. Clearly, that model fits the control group much better than it fits the treatment group. Figure 3.1: Is the mean for each these groups a good descriptive model? Why do I say that? The treatment group data are much more skewed. Most of the data values are greater than the mean of the group. Sure, a mean can be calculated for that group, but it serves as a fairly crappy summary. Perhaps some other model (or group of statistical parameters) would better convey how these data behave? This is to point out that learning statistics is about learning to make judgments about which models are best for describing a given data set. 3.1.2 Statistical inference There are two main types of inference researchers make. One type is to infer whether an experiment “worked” or not…the so-called “significance test”. This familiar process involves calculating a test statistic from the data (eg, t-test, F-tests, etc) and then applying a threshold rule to its value. If the test passes the rule, we conclude the experiment worked. I cover this type of inference in much more detail in the p-value chapter 7, and we’ll talk about it over and again throughout the course. A second type of inference is to extrapolate from a sample some estimate for the values of the variables within the population that was sampled. Both descriptive and statistical inference are subject to error. By random chance alone our sample could be way off the mark, even with perfectly calibrated instrumentation. The real difficulty with inference is we can never know for certain whether we are right or wrong. They are called random variables for a reason. It pays to have a very healthy respect for the role played by random chance in determining the values of our parameter estimates. If we were to completely redo a fully replicated experiment once more, we would almost certainly arrive at different numbers. In a well behaved system, they’d likely be in the same ballpark as those of the first experiment. But they would still differ. To illustrate, copy and paste the code chunk below. It replicates a random triplicate sample six times, taking six means. Unlike in real life, the population parameters are known (because I coded them in): \\(\\mu=2\\) and \\(\\sigma=0.4\\). You can run that chunk tens of thousands of times and never get a “sample” with one mean that has a value of exactly 2, even though that’s the true mean of the population that was sampled. x &lt;- replicate(6, rnorm(3, 2, 0.4)) apply(x, 2, mean) ## [1] 1.655246 2.070122 2.086785 1.549263 2.324917 2.108283 3.1.3 Experimental design Experimental planning that involves dealing with statistical issues is referred here as experimental design. This involves stating a testable statistical hypothesis and establishing a series of decision rules in advance of data collection. These rules range from subject selection and arrangement, predetermination of sample size using a priori power analysis, setting some data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, declaring a primary outcome, on up to what statistical analysis will be performed on the data. Experimental design is very common in prospective clinical research. Unfortunately, very few basic biomedical scientists practice anything remotely like this. Most biomedical researchers begin experiments with only vague ideas about the statistical analysis, which is usually settled on after the fact. Much of the published work today is therefore retrospective, rather than prospective. Yet, most researchers tend to use statistics that are largely intended for prospective designs. That’s a problem. 3.1.4 Statistics as an anti-bias framework If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct. The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn’t provide us a magic woo-woo cloak to protect us from our biases. Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias. Here are some of the more important biases. ####Cognitive biases From a statistical point of view biases can be classified into two major groupings. The first are Cognitive biases. These are how we think (or fail to think) about our experiments and our data. These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, “how could this not work!” you are in the throes of a pretty deep cognitive bias. In bench research, cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or recognizing telling glitches in the data as meaningful. ####Systematic biases The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed, and how data is handled or transformed. Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line. ####Scientific misconduct More malicious biases exist, too. These include forbidden practices such as data fabrication and falsification. This is obviously a problem of integrity. Very few scientists working today are immune from the high stakes issues that pose threats to our sense of integrity. In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them. The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite livable. If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you’ve given it your best shot. That is hard to conclude when the latter occurs. "],
["sampling.html", "Chapter 4 Statistical Sampling 4.1 Experimental units 4.2 Independent Replicates 4.3 Random process 4.4 Statistically valid samples 4.5 Independence of replicates", " Chapter 4 Statistical Sampling An experiment is no more reliable than is its sample. -TJ Murphy A statistically valid sample is comprised of independent replicates of the experimental unit, which are generated using some random process. To unpack this let’s think about each of the following terms: What are experimental units? What do we mean by independent replicates? What is a random process? When is statistical validity even important? 4.1 Experimental units The experimental unit is the source of the measurement. An experimental unit can generate one or many measurement values. I prefer the concept of an experimental unit to the concept of subject, though they often mean the same thing. I’ve found the word subject carries more ambiguity, especially for people first learning sampling and sample size concepts. In some experimental designs (eg, unpaired or completely randomized) each experimental unit generates a single measurement value. Here there is a one-to-one correspondence exists between the number of experimental units and the number of measurement values within a data set. In other designs (eg, paired or matched or repeated/related measure), a single experimental unit can generate more than one measurement values for the same variable. Such data sets have more values than experimental units. Here are some guidelines for deciding what is the experimental unit in an experiment, with full recognition that sometimes there are gray areas. Ultimately the researcher has to use scientific judgment to recognize or define the experimental unit. 4.1.1 A simple test to define the experimental unit When defining an experimental unit I recommend using a simple test: Are these measurements intrinsically-linked? If two or more measurement values are intrinsically-linked then they would comprise paired or matched or related measures from a single experimental unit. So how could you judge whether two or more measurements are intrinsically-linked? For the most part, this happens when the source of those measurements doesn’t differ. Here are a few examples: A before and after design. A mouse is scored on how well it performs a behavioral test at baseline, before a treatment. After that same mouse receives a treatment it is run through the behavioral test once more to get a second score. Those two scores are intrinsically-linked because they were taken from the same mouse. All that differs between the scores is the absence or presence of the treatment, the effect of which the researcher is trying to measure. We would also say those two scores are matched, paired or related/repeated measures. A single mouse from which two scores are derived is an independent replicate of the experimental unit. Twinning. Take for example a study involving human identical twins. In these studies identical twin pairs are modeled as a single experimental unit due to their high level of instrinsic relatedness. There are two human subjects but they are modeled statistically as a single experimental unit. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. One of the pair receives a control condition while the other receives a treatment condition. A measurement is taken from each person. There are two measurements in total, and two people, but only a single experimental unit. Given that the twins are so identical we could reasonably conclude these two measurements are intrinsically-linked. We can model the pair as one. The two measurements would be analyzed using a statistical method configured for paired or matched or repeated/related measures. Unpaired or completely randomized In contrast, imagine a study using the same control and treatment conditions using unrelated humans (or some other outbred animal species) as subjects. Each subject is assigned either a treatment or a control, and only a single measurement is taken from them. Since the subjects are each very different from each other, we could not conclude that measurements taken from them are intrinsically-linked. Each person stands alone as an experimental unit. The data would be analyzed using an unpaired, unmatched or completely randomized test. Intrinsically-linked measurements are very common in bench work. In fact they are too often overlooked for what they are and mistakenly analyzed as unmatched. Experiments involving batches of biological material, cultured cells and/or littermates of inbred animal strains routinely involve intrinsically-linked measurements. As a general rule, these should always be designed and analyzed using matched/paired/related measures procedures. Cell cultures Cell cultures are remarkably homogeneous. The typical continuous cell line is a monoculture passaged across many doubling generations. Imagine a test conducted on a 6 well multi-well cell culture plate. Each well receives a different level of some treatment condition, such as a dosing or time-course study. All of the wells were laid down at the same time from a common batch of cells. Each well is very highly related to all of the other wells. The intrinsic differences between wells would be relatively minor and mostly due to technical variation. There’s no real inherent biological variation from well-to-well other than that attributable to the level of treatment the well receives. As a result, all of the measurements taken from a plate of wells are intrinsically-linked to each other. The experimental unit is the plate. They should be designed and analyzed using matched/paired/related measure statistical procedures. Furthermore, any other plates laid down at the same time from the same source of cells are virtually identical clones of each other. If we were to expose the wells in all of those plates to various treatments followed by taking some measurement, then it is pretty easy to argue that all of those measurements taken on that passage of cells are intrinsically-linked. None of the wells are independent of any of the other wells, irrespective of the plate. Together, all of the plates represent a single experimental unit. Inbred mice In many regards, the high level of relatedness within inbred mouse strains doesn’t differ from human identical twins, or from cultured cells, for that matter. A given strain of these animals are inbred to genetic homogeneity across several generations. For all intents and purposes all mice derived from a given strain are immortalized clones of each other. Two mice from the same litter are identical twins. Indeed, two mice from different litters from the same strain are identical twins. Due to their clonal identity all measurements taken from any of these highly related subjects are intrinsically-linked. Just as for cell culture, protocols must be contrived to break up the homogeneity. A common approach is to treat the litter as the experimental unit and take measures from littermates as intrinsically-linked. Split tissue Imagine two slices of an organ (or two drops of blood) taken from a single animal. Although the two slices (or drops of blood) are obviously different from each other, any measurements derived from each are intrinsically-linked. The experimental unit would be the animal from which that biological material is derived. Batches Finally, imagine a batch of a purified protein or other biochemical material. The batch was isolated from a single source and prepared through a single process. The material in the batch is highly homogeneous, irrespective of whether it is stored away in aliquots. Any measurement taken from that batch are highly related to any other measurement. They are intrinsically-linked. The batch would be the experimental unit. ###Blocking We have to contrive protocols to break up experimental units that have high inherent homogeneity. The statistical jargon used for this is blocking, such that blocks are essentially grouping factors that are not scientifically interesting. Going back to culture plates. Let’s say we prepared three plates on Friday. An assay performed on one plate on Monday would represent one experimental unit of intrinsically-linked measures. An assay repeated on Tuesday on a second plate would represent a second experimental unit. Wednesday’s assay on the third plate is also its own experimental unit. Here the blocking factor is the day of the week. Assuming we created fresh batches of reagents each day, there would be some day-to-day variation that wouldn’t exist if we assayed all threee plates at once on a single day. But we’re not particularly interested in that daily variation, either. More conservatively, cell line passage number can be used as a blocking factor to delineate experimental units. Each passage number would represent an experimental unit and the overall replicated experiment would be said to be blocked on passage number. Defining the experimental unit and any blocking factors requires scientific judgement. That can be difficult to do when dealing with highly homogenous material. What should be avoided is creating a design that limits random chance too severely. To measure on Monday all three plates that were laid down on Friday will probably yield tighter results than if they were blocked over the course of the week. This has to be thought through carefully by the researcher in each and every case. Reasonable people can disagree what whether one approach is superior to some other. Therefore, what is important is to make defensible decisions. To do that, you need to think through this problem carefully. When in doubt, I suggest leaning towards giving random chance a fair shot at explaining the result you’re observing. For example, you can make the case that measurements from two cell culture plates that were laid down on the same day but are collected on different days are not intrinsically-linked. That’s a harder case to make if they are collected on the same day. You will almost certainly have to make the case that measurements taken from two mice on different days or if they are from different litters are not intrinsically-linked. Before going there, we need to chat about what we mean by independent replication. 4.2 Independent Replicates That we should strive for biological observations that are repeatable seems self evident. An experiment is comprised of independent replicates of treatment conditions on experimental units. The total number of independent replicates comprises an experiment’s sample size. A primary goal in designing an experiment is to assess independent replicates that are not biased to the biological response of a more narrowly defined group of experimental units. A replicate is therefore independent when a repeat is on an experimental unit that differs materially from a previous experimental unit. A material difference could involve a true biological replicate. Measurements taken from two unrelated human subjects have a material difference. In bench biological work with fairly homogenous systems (eg, cell lines and inbred animals) a material difference will usually need to be some separation among replicates in time and space in applying the experimental treatments. 4.2.1 A simple test for independence How willing am I to certify this is a truly repeatable phenomenon when replicated in this way? A new scientific discovery would be some kind of repeatable phenomenon. 4.2.2 Some replication examples If we are performing an experiment using pairs of human twins, each pair that is studied stands as an independent replicate. Because the pair is the experimental unit, a study involving 5 pairs will have five, rather than ten, independent replicates. If we conduct an experiment using unrelated human volunteers, or someother out bred animals, each person or animal from whom a measurement is recorded is considered an independent replicate. Their biological uniqueness defines their independence. We wander into gray areas pretty quickly when thinking about the independence of experimental units in studies involving cultured cells, batches of biological material, and inbred mice. Working with these systems it is difficult to achieve the gold standard of true biological independence. The focus instead should be on repeatability….“Working with new batches of reagents and different days do I get the same response?” Imagine a 6 well plate of cultured cells. No well differs biologically from any other. If each well received a repeat of the same treatment at the same time we shouldn’t consider any measurements from that plate independent from others. Otherwise, the sample would be biased to that plate of cells measured at that particular time with a given set of reagents under those particular conditions. It is too biased to that moment. What if we screwed up the reagents and don’t know it? Rather than being independent, it is best to consider the 6 measurements drawn from the plate as technical replicates or pseudo replicates. The data from the 6 wells should be averaged or totaled somehow to improve the estimate of what happened on that plate that day. A better approach with cultured cells is to use passage numbers to delineate independence. Thus, a 6 well plates from any one passage are independent experimental units relative to all other passages. Obviously, given the homogeneity of cells in culture, it’s unlikely there is much biological variation even by these criteria. But to achieve true biological independence would require re-establishing the cell line each time an independent replicate was needed. That’s rarely feasible. Inbred mice pose much the same problem. Scientific judgment is needed to decide when 2 mice from the same strain are independent of each other. One mark of delineation is the litter. Each litter would be independent of other litters. Outcomes of two (or more) littermates could be considered matched or related-measures and thus one experimental unit. 4.3 Random process You can probably sense intuitively how randomization can guard against a number of biases, both systematic and cognitive. Systematic artifacts become randomly distributed amongst the sample replicates, whereas you are less tempted to treated a replicate as preferred if you don’t know what is its treatment level. Mathematical statistics offers another important reason for randomization. In classical statistics the effect size of some treatment is assumed to be fixed. Our estimate of that real value is the problem. Thus, when we measure a value for some replicate, that value is comprised of a combination of these fixed effects and unexplained effects. The variation we observe in our outcome variables, the reason it is a random variable, arises from these unexplained effects. These can be particularly prominent in biological systems. Randomization procedures assures those random effects are truly random. Otherwise we might mistake them for the fixed effects that are of more interest us! This concept will be discussed more formally in the section on general linear models. Suffice to say for pragmatic purposes that random sampling is crucial for limiting intentional and unintentional researcher biases. Either the experimental units should be selected at random, or the experimental units should be assigned treatments at random, and/or the outcome data should be evaluated at random (eg, blind). Sometimes, doing a combination of these would be even better. Usually, the researcher supervises this randomization using some kind of random number generator. R’s sample() function gets that job done for most situations. Let’s design an experiment that involves two treatments and a total of 12 independent experimental units. Thus, 6 experimental units will each receive either of the two treatments. Let’s say that my experimental units each have an ID, in this case, a unique letter from the alphabet. Using sample(1:12) we randomly assign a numeric value to each ID. This numeric value will be the order by which the experimental unit, relative to the other experimental units, is subjected to the experimental treatment. ID’s that are assigned even random numbers get one of the two treatments, and odd numbered ID’s get the other treatment. What we’ve done here is randomize both the order of replication and the assignment of treatment. That’s a well-shuffled deck. You can see how this approach can be readily adapted to different numbers of treatment levels and sample sizes. set.seed(1234) ID &lt;- letters[1:12] order &lt;- sample(1:12, replace=F) plan &lt;- data.frame(ID, order) plan ## ID order ## 1 a 2 ## 2 b 7 ## 3 c 11 ## 4 d 6 ## 5 e 10 ## 6 f 5 ## 7 g 1 ## 8 h 12 ## 9 i 3 ## 10 j 8 ## 11 k 4 ## 12 l 9 4.4 Statistically valid samples For any statistical test to be valid, each replicate within a sample must satisfy the following two criteria: The replicate should be generated by some random process. The replicate must be independent of all other replicates. Why? Statistical tests are one of the last stages of a hypothesis testing process. All of these tests operate, formally, on the premise that at least these two conditions are true. When these conditions have not been met the researcher is collecting data without testing a hypothesis. To run a statsitical test is to pretend a hypothesis has been tested, when it has not. 4.4.1 Select random subjects Let’s say we want to do an experiment on graduate students and need to generate a representative sample. There are 5 million people in the US who are in graduate school at an given time. Let’s imagine they each have a unique ID number, ranging from 1 to 5,000,000. We can use R’s sample() function to randomly select three individuals with numbers corresponding to that range. Sampling with replacement involves throwing a selection back into a population, where it can potentially be selected again. In that way, the probability of any selection stays the same throughout the random sampling process. Here, the replace = FALSE argument is there to ensure I don’t select the same individual twice. sample(x=1:5000000, size=3, replace = FALSE) ## [1] 1413668 4617167 1461579 All that needs to be done is to notify the three people corresponding to those IDs and schedule a convenient time for them to visit so we can do our experiment. You can imagine several variations to randomly select graduate students for measurements. You just need a way to find graduate students, then devise a way(s) to ensure the sampling is as representative as possible. Selecting subjects from a real population is pretty straight forward, a bit like picking 8 lotto balls from a spinning container. A lot of times in experimental work the number of subjects available to the researcher is fixed and smaller. The size of the population to be sampled can be much closer to the number of replicates needed for the experiment rather than a sample from a large pool. In these cases we have to come up with other ways to randomize. 4.4.2 Randomize to sequence For example, let’s say we want to compare condition A to condition B. We have 6 subjects to work with, each of which will serve as an independent replicate. We want a balanced design so will have 3 replicates for each of the 2 conditions. Let’s imagine we can only perform an experiment on one subject, one day at a time. In that case, it makes sense to randomize treatment to sequence. We can randomly generate a sequence of 6 even and odd numbers, and assign them to the daily sequence (MTWTFM) based on which random number is first on its list. We can make a rule that subjects assigned even numbers will receive condition A, whereas condition B is meted out to subjects associated with odd numbers. sample(x=11:16, size=6, replace = FALSE) ## [1] 16 12 15 11 13 14 4.4.3 Randomize to location Let’s imagine 3 treatments (negative control, positive control, experimental), that we will code 1,1,2,2,3,3. These will be applied in duplicate to cells on 6-well cell culture plate. We’ll code the plate wells with letters, a, b, c, d, e, f from top left to bottom right (ie, a and b are wells in the top row). Now we’ll generate a random sequence of those six letters. sample(letters[1:6], replace=F) ## [1] &quot;b&quot; &quot;a&quot; &quot;e&quot; &quot;d&quot; &quot;f&quot; &quot;c&quot; Next, we’ll map the sequence 1,1,2,2,3,3 to those letters. Thus, negative control goes to the wells corresponding to the first two letters in that sequence, positive control to the 3rd and 4th letters, and so forth. 4.4.4 Randomize to block In statistical lingo, a block is a subgroup within a sample. A blocked subject shares some feature(s) in common with other members of its block compared to other subjects in the overall sample. But usually, we’re not interested in block as a variable, per se. Here are some common blocks at the bench are One purified enzyme preparation vs a second preparation of the same enzyme, nominally purified the same way. The two enzyme preps represent two different blocks. A bunch of cell culture dishes plated on Friday from passage number 15 vs ones plated on Tuesday from passage number 16. The two passages represent 2 different blocks. A litter of mouse pups born in January vs a litter born in February. The two different litters represent two different blocks. An experiment run with freshly prepared reagents on Monday vs one run on Tuesday, with a new set of freshly prepared reagents. Each experimental day represents a block. Frequently, each block is taken as an independent replicate. 4.5 Independence of replicates In biomedical research the standard is for biological independence; when we speak of “biological replicates” we mean that each independent replicate represents a distinct biological entities. That standard is difficult to meet when working with many common biological model systems, particularly cell lines and inbred animals. The definition of statistical independence is grounded in the mathematics of probability: Two events are statistically independent when they convey no information about the other, or \\[p(A \\cap B)=p(A)p(B)\\]. Here the mathematics is not particularly helpful. Imagine two test tubes on the bench, each receives an aliquot of biological material from a common prep (eg, a purified protein). One tube then receives treatment A and the other treatment B. As best we know, the two tubes aren’t capable of influencing each other. But we can reasonably assume their responses to the treatments will at least be correlated, given the common source of biological material. Should each tube be treated as if it were statistically independent? Replicate independence that meets statistical validity therefore has to take on a more pragmatic and nuanced definition. My preference is to define a replicate as the independent experimental unit receiving treatment. I like this because it allows for defining the experimental unit differently depending upon the experimental design. "],
["hypotheses.html", "Chapter 5 Framing statistical hypotheses 5.1 The decision process 5.2 Popper and falsification 5.3 Statistical hypothesis rubric", " Chapter 5 Framing statistical hypotheses “There is no more to science than its method, and there is no more to its method than Popper has said.”-Hermann Bondi Hypothesis-driven research tests predictions about the nature of the world. Testing hypotheses statistically provides a pragmatic framework for making decisions about the validity of those predictions. When planning an experiment the primary goal should be to bring hyper-focused clarity to the hypothesis. This is the time to distill your thinking down to the exact question you want answered. What are you studying? What is not known? What is your prediction? How will you measure it? What are your variables? Are they discrete or continuous? How will you test it? How will you decide whether what you predicted happened or not? Will this actually answer the question you’re asking? The statistics taught in this course are for assessing the validity of experimental outcomes in a somewhat odd way: Formally, we test the null hypothesis. The expectation is to generate observations of such extreme magnitude that we can reject the null, the hypothesis that nothing happened. At first blush that might come off as absurd. Like a Seinfeld episode, where nothing is what is most important. Hopefully this won’t seem so odd after I describe what this accomplishes and explain why it is done this way. 5.1 The decision process Everybody knows something about the p-value. When it’s low enough, the experiment “worked”. Before diving into the nitty gritty of p-values, let’s jump into a wider angle format to flesh out how they are used. The framework can be broken down into 5 key steps: We begin with a null hypothesis–yes, the boring one about nothing. Experiments generate data. The data are transformed into test statistics. P-values are calculated from the experiment’s test statistic value. Based upon a priori thresholds, a decision is made to reject a null hypothesis, or not, depending upon the extremeness of the result. knitr::include_graphics(&quot;images/hypothesis.jpg&quot;) Figure 5.1: Statistical hypotheses test the null in a multistep process Low p-values are associated with extreme values of a test statistic. Extreme values of test statistics happen when the effect sizes of the results are high. Rejecting a null on the basis of a p-value means our test statistic value is too extreme to belong in the distribution of null test statistic values. Thus, a low p-value means the effect size is improbably high if it were, in fact, not truly effective. If you learn nothing more in this course, learn that the statistics discussed here are tests of the null hypothesis. Learn that every p-value you see in R output is coupled to a test statistic value. These p-values represent the probability your evidence belongs in the null test statistic distribution. 5.2 Popper and falsification Using data to falsify an hypothesis, even if that hypothesis is the null, is a decision framework that plays well with philosopher Karl Popper’s assertion that scientific theories are probative and that unscientific theories are not. To Popper, the grandest scientific theories are those that can be falsified. In Popperian logic, the truth of nature is unknowable and unproveable….even though it is testable. Thus, the scientific method advocated by Popper doesn’t allow for proving an hypothesis, but at the same time it doesn’t forbid us from rejecting hypotheses that are inconsistent with observations. Thus enters the null hypothesis, which predicts, of course, that nothing happens. The null is an incredibly handy device because if we make observations that are extremely inconsistent with the null, meaning we have observed that something happens, we are obligated to reject the null. Thus, the null is falsifiable when we have positive results! Imagine an experiment to test whether a drug lowers blood glucose in people who have diabetes. When the glucose-lowering effect size for the drug in the sample is large enough, we can reject the hypothesis that the drug didn’t have any effect. In other words, we will accept an observation as evidence for a positive result by formally concluding that same evidence is inconsistent with a negative result. Some argue that this logic forces the researcher to test the “wrong” hypothesis and to also accept an alternate hypothesis that itself may not be true. For example, although blood glucose may be lower in the drug treatment arm of the sample, that may have occured by random chance. An unknown confounder variable could be responsible for the observation that drug-treatment is associated with lower blood glucose. In that case we would make an error by rejecting the null when it is actually true. Of course, rejecting the null is provisional. All gained knowledge is provisional. Drawing a conclusion from one experiment doesn’t preclude testing the experiment some other way. If the problem is important enough (and “real”), it will be tested from multiple angles. It will need to survive the preponderance of the evidence. I’m convinced the alternative approach, which is to seek out evidence that affirms an hypothesis, is not better. This lies at the heart of what Popper stood against. There is an inherent confirmation bias in seeking out affirmation of ideas. In the proper light, any evidence can be made to look attractive. Furthermore, what if, in seeking affirmation, nothing happens? Negative results are very difficult to interpret because the absence of evidence cannot be interpreted as the evidence of absence. So I’d hope the researcher who gives this some thought will find null falsification more pragmatic, if not ingenious. It allows us to move forward on the basis of positive evidence (granted, which may be wrong and we don’t know it), while at the same time practicing a more sound, more unbiased scientific methodology (hypothesis falsification rather than affirmation). Meanwhile, this statistical framework does allow for the possibility of designing experimental conditions to minimize false positive (type1) and false negative (type2) errors. We can operationalize our tolerance for those kinds of mistakes in meaningful ways such that we are less likely to become victims of bad luck. Finally, the decision to reject a null hypothesis can stand alone. It need not be the same as a decision to accept the alternate. Rejecting the null only asserts that the experimental evidence is inconsistent with the null. In no way does that “prove”\" the alternative hypothesis. For most, some of these concerns should become even less of a problem when the null and alternate hypotheses are explicitly framed in terms of population parameters and their mutually exclusive and collectively exhaustive outcomes. This approach doesn’t leave much room for ambiguity about what is being declared at the decision step. For example, the null hypothesis for the diabetes case is very explicit: \\(null, H_0: \\mu_{placebo} = \\mu_{drug}\\), Here \\(\\mu\\), since it is greek notation, represents the mean blood glucose in concentration units in the populations corresponding to the two sampled groups. Now that we have a bona fide null hypothesis, we can state the alternate hypothesis as everything the null can’t be: \\(alternate, H_1: \\mu_{placebo}\\ \\ne \\mu_{drug}\\) In other words, the inference operates on the basis of straightforward mathematical principles. Two parameters that are compared either meet our prescribed expectations, or they do not. In this case, if we reject the hypothesis that the means of the two groups are equal, then they can only be not equal. Are they truly not equal? We can never know for sure, but we are operating within a framework of known error tolerances. 5.3 Statistical hypothesis rubric Researchers have to grapple with two types of hypotheses. One type is the grand, paradigm-driving assertion of some key insight, which is designed to express the big picture in forward thinking terms. It is also designed to wow study sections and seminar audiences. The other type is the null hypothesis, which is designed to be tested statistically. The null predicts nothing will happen. The null is as boring as it gets. You’d never propose the null in a specific aims page, but you should get in the habit of thinking in terms of testing the null with your statistics. Only the null hypothesis has any statistical utility, whereas the grand hypothesis has no statistical utility. This is a conceptual hurdle that most students struggle with. The grand hypothesis is for marketing, the null hypothesis is for mattering. For that reason I’ve created a rubric for forming a statistically testable hypothesis. The rubric begins with a conceptual overview of a problem, and it ends with how the results will be interpreted. At some point during the semester you’ll have a major assignment that asks you to go through this rubric for a problem of your own choosing. That assignment is a major test for whether you “get” statistical design of experiments. Step 1: Lay out the big picture of the problem in a way that leads to a “What is not yet known” assertion. Type 2 diabetes is associated with high blood glucose levels and obesity, which each have long term effects associated with high morbidity. Exenatide is GLP-1 receptor agonist that can control blood glucose levels. When delivered as an osmotic minipump exenatide lowers blood glucose. A standard of care for type2 diabetics is to put them on a weight loss program while giving them drugs that manage blood glucose. It is not known if continuous administration via osmotic minipump can lead to greater weight loss while on this standard of care. Step 2: Transform the “What is not known” statement into a bold and simple scientific prediction, as if “what is not known” were answered: Long-term administration of exenatide via osmotic minipump to type-2 diabetics will cause weight loss. Step 3: Now frame the experimental plan in terms of the independent and dependent variables, written as an if/then statement. In narrative format, if you manipulate what predictor variables, then what outcome do you expect to observe? If an exenatide osmotic minipump is implanted into type-2 diabetics, then their weight loss will differ compared to placebo. Step 4: Define the dependent and the independent variables of the experiment. What type of variables are these? What are the experimental units? Are the measurements intrinsically-linked, or not? The dependent variable will be weight loss, calculated as the weight difference between pre-study to post-study for each human subject. Each subject is the experimental unit. The independent variable is treatment. Treatment is a discrete, factoral variable that will be at two levels, placebo and exenatide. Although pre- and post-study weights will be measured for each subject and are themselves intrinsically-linked, they are used to derive the dependent variable (weight loss), which are not instrinsically-linked. Step 5: Write the null and alternate hypothesis on the basis of the statistical parameters to be tested. Note here that greek notation is used to symbolize that the hypothesis is about the sampled population parameters, rather than the sample. Where \\(\\mu\\) represents the mean weight loss of the populations corresponding to the sampled groups, the null and alternate hypotheses are \\[H_0:\\mu_{exenatide}=\\mu_{placebo}\\] and \\[H_1: \\mu_{exenatide}\\ne\\mu_{placebo}\\] Step 6: What statistical test will be used to test the null hypothesis? What are the decision rules? A two-sided, unpaired t-test for comparing group means. The sample size will be based upon a power of 90%, which means that the tolerance level for type2 error will be 10%. The decision threshold for type1 error will be 5%. Thus, the null hypothesis will be rejected at a p-value of less than 0.05. 5.3.0.1 Two-sided vs one-sided hypothesis The above is an example for a two-sided hypothesis. In a two-sided hypothesis \\(\\ne\\) is mutually exclusive and collectively exhaustive of \\(=\\). By rejecting the null that two things are equal, we implicitly (and provisionally) accept the alternative hypothesis that they are not equal. Notice how this hypothesis doesn’t predict the direction of an effect. It only predicts there will be a difference between the two groups. If you’re willing to predict the direction of an effect, you would choose to make a one-sided hypothesis. One-sided hypotheses can happen in either of two ways. In one case we can predict one mean will be greater (\\(&gt;\\))than another mean. In the other case, we can predict one mean will be less than (\\(&lt;\\)) another mean. The mutually exclusive and collectively exhaustive alternatives to these one sided hypotheses are therefore \\(\\ge\\) and \\(\\le\\), respectively. In other words, if one mean is not greater than another mean, then the only alternative possibilities are that it is less than or equal to it. The decision to test a one- or two-sided hypothesis should be based upon scientific reasoning. In the example above, I’m unwilling to test a one-sided hypothesis that exenatide will cause a greater weight loss than placebo, even though that is the expectation (and hope!). Were I willing to test the direction of the effect, the one-sided hypothesis test would be written like this: \\[H_0:\\mu_{exenatide}&lt;\\mu_{placebo}\\] and \\[H_1: \\mu_{exenatide}\\ge\\mu_{placebo}\\] If the data show that mean weight loss is greater in the exenatide group, as expected, that null hypothesis can be rejected. But what if, unexpectedly, weight loss is greater in the placebo group? It would generate a high p-value. According to the pre-planned hypothesis, the null could not be rejected. Worse, given they are already enrolled in a standard of care weight loss program, to know the drug actually impairs weight loss would be an important finding. But in choosing the incorrect one-sided hypothesis, there is nothing to do with the result. It is a negative result. I can’t flip the tail to the other direction to get a significant result that I wasn’t planning upon. That would be extremely biased! In practice, some researchers caught in this conundrum create a whole new can of worms by simply changing the pre-planned hypothesis after the fact. It’s done flippantly but is actually a fairly serious violation of scientific integrity. Changing the hypothesis so that it is consistent with the results is not what anybody would consider sound scientific method. 5.3.0.2 Stick to two-sided hypotheses Unlike the case above, when being wrong about the direction of an effect is not a big deal, then one-sided tests are not a bad option. The example above serves to illustrate how a two-sided hypothesis would have been a better choice than a one-sided hypothesis. There are a few other reasons why it is probably better to get in the habit of always testing two-sided nulls: the two-sided test is more conservative because the p-value threshold is a bit lower. Furthermore, multiple tests and confidence intervals easier perform and to interpret, respectively. "],
["error.html", "Chapter 6 Error 6.1 Setting type 1 and type 2 error thresholds 6.2 Striking the right balance 6.3 False discovery rate", " Chapter 6 Error library(tidyverse) library(treemapify) library(pwr) In a jury trial under the American system of justice the defendant stands accused of a crime by a prosecutor. Both sides present evidence before a jury. The jury’s duty is to weigh the evidence then vote in favor of or against a conviction. The jury doesn’t know the truth. A jury is at risk of making two types of mistakes: An innocent person might be convicted, or a guilty person might be acquitted. They can also make two correct calls: Convict a guilty person or acquit someone who is innocent. Without ever knowing for sure what is actually true, they are instructed by the judge to record their decision on the basis of a threshold rule. In a trial the rule is vote to convict only when you believe “it is beyond a reasonable doubt” the accused is guilty. In science the researcher is like a jury. The experiment is like a trial. At the end, the researcher has the same problem that jurors face. There is a need to conclude whether the experiment worked or not. And there’s no way to know with absolute certainty. Mistaken judgments are possible. Whereas the jury works within the “beyond a reasonable doubt” framework, researchers operate within a framework that establishes tolerance limits for error. Every hypothesis tested risks two types of error. A type 1 error is committed when the researcher rejects the null when in fact there is no effect. This is also known as a false positive. A type 2 error is not rejecting the null when it should be rejected, which is known as a false negative. Or the researcher might not make an error at all. The sensitivity of an experiment is conclude correctly there is no effect, and power (also known as specificity) is concluding correctly there is an effect. Sensitivity and power are the complements of type 1 and type 2 error, respectively 6.1 Setting type 1 and type 2 error thresholds In the planning stages of an experiment the researcher establishes tolerance for these errors. A balance has to be struck between aversion for each error type, the ability to make the right call, and the costs involved for being either wrong or right. 6.1.1 Setting alpha-the type 1 error In the biological sciences the standard for type 1 error is 5%, meaning in any given experiment (no matter the number of comparisons to be made), the chance of generating a false positive should be limited to 5%. The acceptable type 1 error limit is labeled alpha, or \\(\\alpha\\). In several R statistical functions, it is controlled by adjusting its complement, the confidence level. Why is \\(\\alpha\\) 5% and not some other value? Credit for that is owed largely to R.A. Fisher who offered that a 1 in 20 chance of making such a mistake seemed reasonable. That number seems to have stuck, at least in the biological sciences. The researcher is always free to establish, and defend, some other level of \\(\\alpha\\). In the field of psychology, for example, \\(\\alpha\\) is historically 10%. There is nothing to stop a researcher from selecting a threshold below or above 5%. She just needs to be prepared to defend the choice. 6.1.1.1 The decision rule The \\(\\alpha\\) is stated before an experiment begins, but operationalized during the final statistical analysis on the basis of p-values generated from statistical tests. The null hypothesis is rejected when a p-value is less than this preset \\(\\alpha\\). 6.1.1.2 Experimentwise error An experiment that just compares two groups (eg, placebo vs drug) generates only one hypothesis. An experiment comparing \\(k\\) groups (eg, placebo vs drug1, vs drug2…drugk-1) generates \\(m=\\frac{k(k-1)}{2}\\) hypotheses. For experiments that generate multiple hypotheses it is important to maintain the overall \\(\\alpha\\) for the experiment at 5%. If not checked, the experiment-wise error would inflate with each hypothesis tested. Several methods have been devised to maintain experiment-wise \\(\\alpha\\) for multiple comparisons. The most conservative of these is the Bonferroni correction \\(\\alpha_m=\\frac{\\alpha}{m}\\). Thus, if \\(m = 10\\) hypotheses are tested, the adjusted threshold for each, \\(\\alpha_m\\), is 0.5%, or a p-value of 0.005. If 1000 hypotheses are tested, such as in a mini-gene screen, the p-value threshold for each would be 0.00005. 6.1.2 Power: Setting beta-the type 2 error In the biological sciences the tolerance for type 2 error, otherwise symbolized as \\(\\beta\\), is generally in the neighborhood of 20%. It’s a bit easier to discuss \\(\\beta\\) through its complement, \\(1-\\beta\\) or power. Thus, experiments run at 80% power, which are generally regarded as well-designed, run at 20% risk of type 2 error. Operationally, an experiment is designed to hit a specific level of power via planning of the sample size. “Power calculations” return sample size by integrating intended power, \\(\\alpha\\), and an estimated effect size. Students tend to fret over effect size estimates. They are nothing more than a best guess of what to expect. A crude estimate. The researcher should use values representing a minimum for a scientific meaningful effect size. The effect size is estimated on the basis of scientific judgment and preliminary data or published information. If the effect size estimate turns out to be accurate, an experiment run at that sample size should be close to the intended power. In a perfect world, we might consider powering up every experiment to 99%, completely minimizing the risk of \\(\\beta\\). As you’ll see in the simulation below, the incremental gain in power beyond ~80% diminishes with sample size. In other words, perfect power and very low \\(\\beta\\) comes at a high cost. The choice of what power to run an experiment should strike the right balance between the risk of missing out on a real effect against the cost burden of additional resources and time. R’s pwr package has a handful of functions to run power calculations for given statistical tests. These, unfortunately, do not cover all of the statistical tests, particularly for the most common experimental designs (eg, ANOVA). In this course, we’ll emphasize performing power calculations using custom Monte Carlo functions, which can be custom adapted for any type of experiment involving a statistical test. Here’s a custom Monte Carlo-based power function for a t-test. To illustrate the diminishing returns argument, the function calculates power comparing samples drawn from \\(N(0,1)\\) to samples drawn from \\(N(1,1)\\). The graph is generated by passing a range of sample sizes into the function. Note how the gain in power plateaus. t.pwr &lt;- function(n){ #Intitializers. Means and SD&#39;s of populations compared. m1=1; sd1=1; m2= 0; sd2=1 # the monte carlo ssims=1000 p.values &lt;- c() i &lt;- 1 repeat{ x=rnorm(n, m1, sd1); y=rnorm(n, m2, sd2); p &lt;- t.test(x, y, paired=F, alternative=&quot;two.sided&quot;, var.equal=F, conf.level=0.95)$p.value p.values[i] &lt;- p if (i==ssims) break i = i+1 pwr &lt;- length(which(p.values&lt;0.05))/ssims } return(pwr) } #Run t.pwr over a range of sample sizes and plot results frame &lt;- data.frame(n=2:50) data &lt;- bind_cols(frame, power=apply(frame, 1, t.pwr)) #plot ggplot(data, aes(n, power))+ geom_point() + scale_y_continuous(breaks=c(seq(0, 1, 0.1)))+ scale_x_continuous(breaks=c(seq(0,50,2)))+ labs(x=&quot;n per group&quot;) ## Validation by comparisonC to pwr package results pwr.t.test(d=1, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;) ## ## Two-sample t test power calculation ## ## n = 16.71472 ## d = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 6.2 Striking the right balance The script below provides a way to visualize how the relationship between correct (green) and incorrect (red) decisions varies with error thresholds. The idea is to run experiments under conditions by which green is the dominant color. Unfortunately, most published biomedical research appears to be severely underpowered findings. alpha &lt;- 0.05 beta &lt;- 0.20 panel &lt;- data.frame(alpha, sensitivity=1-alpha, power=1-beta, beta) panel &lt;- gather(panel, key=&quot;threshold&quot;, value=&quot;percent&quot;) panel &lt;- bind_cols(panel, truth=c(&quot;no effect&quot;, &quot;no effect&quot;, &quot;effective&quot;, &quot;effective&quot;), decision=c(&quot;effective&quot;, &quot;no effect&quot;, &quot;effective&quot;, &quot;no effect&quot;), choice=c(&quot;error&quot;, &quot;correct&quot;, &quot;correct&quot;, &quot;error&quot;)) panel ## threshold percent truth decision choice ## 1 alpha 0.05 no effect effective error ## 2 sensitivity 0.95 no effect no effect correct ## 3 power 0.80 effective effective correct ## 4 beta 0.20 effective no effect error ggplot(panel, aes(area=percent, fill=choice, label=threshold))+ geom_treemap(color=&quot;white&quot;)+ geom_treemap_text( fontface = &quot;italic&quot;, colour = &quot;white&quot;, place = &quot;centre&quot;, grow = F )+ scale_fill_manual(values = alpha(c(&quot;green3&quot;, &quot;red&quot;), .3)) 6.3 False discovery rate The false discover rate, or FDR is another way to estimate experimental error. \\[FDR=\\frac{false\\ positives}{false\\ positives + false\\ negatives}\\] FDR varies given \\(\\alpha\\), \\(\\beta\\) and the probability of the effect. The probability of the effect bears some comment. Think of it as a prior probability, or the likelihood that an effect being studied is “real”. It takes some scientific judgment to estimate these probability values. The graph below illustrates how FDR inflates, particularly when running experiments for low probability effects when tested at low power, even at a standard \\(\\alpha\\). These relationships clearly show that the lower the likelihood of some effect that you would like to test in an experiment, the higher the stringency by which it should be tested. px &lt;- seq(0.1, 1.0, 0.1) #a range of prior probabilities tests &lt;- 10000 fdr_gen &lt;- function(beta, alpha){ real_effect &lt;- px*tests true_pos &lt;- real_effect*(1-beta) false_neg &lt;- real_effect*beta no_effect &lt;- tests*(1-px) true_neg &lt;- tests*(1-alpha) false_pos &lt;- no_effect*alpha FDR &lt;- false_pos/(true_pos + false_pos) return(FDR) } upss &lt;- fdr_gen(0.6, 0.05)#under-powered, standard specificity wpss &lt;- fdr_gen(0.2, 0.05)#well-powered, standard specificity uphs &lt;- fdr_gen(0.6, 0.01)#under-powered, high specificity wphs &lt;- fdr_gen(0.2, 0.01)#well-powered, high specificity fdrates &lt;- data.frame(px,upss, wpss, uphs, wphs) colnames(fdrates) &lt;- c(&quot;Probability&quot;, &quot;5% alpha, 60% beta&quot;, &quot;5% alpha, 20% beta&quot;, &quot;1% alpha, 60% beta&quot;, &quot;1% alpha, 20% beta&quot;) #convert to long format fdrates &lt;- gather(fdrates, tests, FDR, -Probability) ggplot(fdrates, aes(Probability,FDR, group=tests))+ geom_point(aes(color=factor(tests)))+ geom_line(aes(color=factor(tests))) "],
["pvalues.html", "Chapter 7 P Values 7.1 How p-values are calculated 7.2 How p-values should be interpreted 7.3 Interpretation 7.4 Criticisms of p-values", " Chapter 7 P Values library(tidyverse) You’ll see soon enough that when you run a statistical test function in R, it generates list objects that are chock full of useful information. Invariably, the researcher’s eyes will go right to the p-value. This is understandable, since most researchers have been trained to associate their own success with a p-value falling below some pre-set \\(\\alpha\\). Who could really blame them for peeking at the p-values first? The p-value is an instrument by which an important decision will be made. As such, it is worth understanding how that instrument works. Hypothesis-driven experiments are designed to test the null hypothesis. That null will be rejected if the effect size is large enough. Extreme effect sizes correspond to extreme values of test statistics. A p-value is the probability that a given test statistic value could be as large as it is, or even more extreme, if the null hypothesis were actually true. In other words, the p-value is an error probability. It is also a random variable. Which means that it is always possible for an experiment to generate an extreme test statistic by simple random chance. The p-value asserts the probability that this is the case. One of the reasons I like using R for experimental statistics is that R makes it easy to simulate p-values. Because of that you build an intuitive sense for how they operate. R makes it easy to understand p-values. 7.1 How p-values are calculated You can think of test statistics as a transformation of sample data. There are many test statistics. The one to use for a given data set depends on the experimental design. Each test statistic has a probability distribution. P-values are derived from the probability distributions of these test statistics and serve as a way to standardize the decision making process irrespective of the experimental design and test statistic. Probably the simplest test statistic to understand is the z-score. The z-score is a transformation of data from whatever scale it is on, to a standard normal scale. It’s usually appropriate for continuous data. \\[z_i=\\frac{y_i-\\mu}{\\sigma}\\] Let’s say we have single blood glucose value of 122 mg/dl. What is its p-value? Is the z-score corresponding to that glucose value too extreme to belong in the null distribution of z-scores? First, the blood glucose values is transformed into a z-score. We’ll say the mean and standard deviation of blood glucose in the sampled population is 100 and 10 mg/dl, respectively. The z-score for a value of 122 is therefore: z &lt;- (122-100)/10; z ## [1] 2.2 z-score units are in standard deviations. Thus, a z-score value of 2.2 indicates it is 2.2 standard deviation units greater than the standard normal mean (which is zero, of course). Next, we’ll pass that z-score value of 2.2 into the standard normal density function, pnorm. We cause the function to produce a p-value for that z-score by using a lower.tail=FALSE argument: pnorm(2.2, mean=0, sd=1, lower.tail=FALSE) ## [1] 0.01390345 In the z probability distribution below, the blue shaded region illustrates what this p-value looks like. The p-value covers the probabilities for z-score values of 2.2 and higher. The p-value is thus the area under the curve for the z probability distribution for that value of z and for more extreme values. ggplot(data.frame(zscore = c(-5, 5)), aes(zscore)) + stat_function(fun = dnorm) + stat_function(fun = dnorm, xlim= c(2.2, 5), geom = &quot;area&quot;, fill=&quot;blue&quot;)+ ylab(&quot;p(z)&quot;)+ scale_x_continuous(breaks=seq(-5,5,1)) 7.2 How p-values should be interpreted The question that’s ringing in your ears right now is, “Is a z-score value of 2.2 so extreme we can reject that it belongs to the null distribution of z-scores?” The answer to that question depends upon what threshold you deem is too extreme. Remember, a threshold is our tolerance for error; in this case, for type 1 error. If the threshold for an acceptable risk of type 1 error is 5% (\\(p &lt; 0.05\\)), then let’s see how those look on the z-distribution. First, let’s calculate z-scores corresponding the area outside 95% of the z-scores. Since extreme z-scores can lay on both the right and the left sides of the z-distribution, which is symmetrical. Therefore we split the 5% in half and use the quantile function qnorm to calculate z-scores for each: qnorm(0.025, lower.tail = F) ## [1] 1.959964 qnorm(0.025, lower.tail = T) ## [1] -1.959964 Thus, the 95% confidence limits for the z-scores are ~ +/- 1.96, almost 2 standard deviations from the mean. We plug those values as limits into our plot: ggplot(data.frame(zscore = c(-5, 5)), aes(zscore)) + stat_function(fun = dnorm) + stat_function(fun = dnorm, xlim= c(1.96, 5), geom = &quot;area&quot;, fill=&quot;red&quot;)+ stat_function(fun = dnorm, xlim= c(-1.96, -5), geom = &quot;area&quot;, fill=&quot;red&quot;)+ ylab(&quot;p(z)&quot;)+ scale_x_continuous(breaks=seq(-5,5,1)) Any z-score values corresponding to the red-shaded areas would be deemed too extreme to belong to the null. The limit on the right side is 1.96. Therefore, yes, a z-score of 2.2 (\\(p=0.0139\\)) is too extreme to belong to the standard null distribution. 7.3 Interpretation Every time we do an experiment we operate on the assumption that our data represent the null. This is analogous to considering a defendant innocent until proven guilty. So we think of test statistic values we calculate from our data, unless proven otherwise, as belonging to the null distribution of test statistic values. The interpretation of \\(p=0.0139\\) is the probability that z-score (and its corresponding glucose value of 122 mg/dl) are that large by chance is 0.0139. There’s about a 1.4% chance we are making an error by rejecting the null that it belongs to the \\(N(100, 10)\\). 7.4 Criticisms of p-values There are several criticisms of p-values, many of which are legitimate. I’ll address a few key ones here. They are too confusing, nobody understands them. I get that. I confess that p-values are a struggle to teach in a way that’s simple and memorable. Especially for students who only consider statistics episodically, perhaps a few times a year. This year I’m teaching this with a bit more emphasis upon Popper and the merits of null hypothesis falsification as the cornerstone of the scientific method and how p-values fit into that tradition. Here it is: All statistical tests (the ones I teach in this course) are tests of the null hypothesis. When the test result is extreme, we reject the null. The p-value is the probability we’re rejecting the null in error. Despite the merit of this particular criticism, p-values are not going away. They are an important inferential tool used by most biological scientists, even if poorly understood and implemented. Like any tool in the lab, it is incumbant upon the researcher to learn how it works. I think a great way to get a better intuitive understanding for p-values is to play around with the various test statistic probability and quantile distributions in R (pnorm, qnorm, pt, qt, pf, pf, pchisq, qchisq, psignrank, qsignrank etc). Use them to run various scenarios, plot them out…get a sense for how the tools work by using them. p-Values poorly protect from false discovery This is undoubtedly true. Since David Colquhoun goes over this in blistering detail I won’t repeat his thorough analysis here. The researcher MUST operate with skepticism about p-values. Since Colquhoun’s argument is entirely based in simulation it also inspires an approach for dealing with this problem. Through simulation a priori, a researcher can design and run experiments in silico that strikes the right balance between the threshold levels she can control (eg, \\(\\alpha\\) and \\(\\beta\\)) and feasibility in a way that best minimizes the risk of false discovery. Before ever lifting a finger in the lab. This criticism explains why I am such a strong advocate of Monte Carlo simulation in experimental design. With software like R, there really is no excuse anymore for the researcher being a victim of this problem. p-Values aren’t the probability I’m interested in Researchers who raise this criticism generally are interested in something the p-value was never designed to deliver: the probability that their experiment worked. A p-value doesn’t provide that information because it is an error probability. Specifically, it is the probability of making a type 1 error. For these researchers, embracing Bayesian statistics is probably a better option. I don’t teach Bayesian statistics in this course for a couple of reasons, but mostly because I don’t understand it well enough to teach it, and I don’t see how it offers a superior approach for experimental research. People use p-values as evidence for the magnitude of an effect. Sure, but they are wrong. This is more a criticism of the people who use p-values, and not the p-value. But the criticism raises the point that it is a mistake to rely solely on a p-value to interpret the outcome of an experiment. A p-value&lt;0.05 only means that there is less than 1 out of 20 chance of having detected an extreme effect when the null is true. A low p-value doesn’t provide evidence that the treatment effect is real. As a result, a p-value can’t provide any information about the magnitude of the treatment effect. Neither is a low p-value synonymous with scientific significance. A simple example of this comes from 2 way ANOVA F test analysis. When the test suggests a postive result for an interaction effect, that finding supercedes the main effects. Thus, should any main effects also have low p-values they are not scientifically meaningful. Researchers should therefore analyze p-values in conjunction with other parameters, such as effect sizes and the confidence intervals. "],
["data.html", "Chapter 8 Data Classification 8.1 Dependent and independent variables 8.2 Discrete or continuous variables", " Chapter 8 Data Classification library(datapasta) library(tidyverse) The starting point in any statistical design is to understand the types of data that are involved. Ask yourself whether the variables are discrete or continuous. Then ask if they measured, ordered or sorted? If you don’t understand those two questions, just read on. Because the answers will point you in the proper analytical direction. This is one of the most important things to learn in this course. If you don’t get the concept that not all data types are equivalent, you won’t get statistics. In this section data classification will be discussed. In all likelihood this material will sound simplistic or even obvious to you, but I cannot emphasize enough the importance of data classification in mastering a statistical framework. If for no other reason, understanding how data are classified is crucial in selecting the most appropriate statistical analysis. If you were to approach me to ask, “here’s my stuff, what statistical test should I do?” I would ask, “tell me more about your data.” And we would probably spend a lot of time with you answering my questions until I was sure I understood your data classification. Therefore, a major learning objective for you is, given a data set, to know which variables are dependent and which are independent, and whether the variables involved are continuous (measured) or discrete (ordered or sorted). 8.1 Dependent and independent variables For the experimental researcher there are two basic types of variables. An independent variable is the predictor or explanatory variable imposed by the researcher upon a system. Independent variables have values, the levels of which are determined by the researcher. For example, in a blood glucose drug study, the independent variable “Treatment” would come in two levels, “Placebo” and “Drug”. In R, we’d call treatment a factor variable with two levels. Conventionally, the independent variable is plotted on the abcissa, or x-axis, scale of some graph. A dependent variable is the response or outcome variable collected in an experiment. The values that dependent variables take on are determined by, or dependent upon, the level of the independent variables. For example, the dependent variable in the blood glucose drug study would be a measurement called “blood_glucose”. Most of the time the dependent variable is plotted on the ordinate, or y-axis, scale. In statistical notation the dependent variable is usually depicted by the uppercase symbol \\(Y\\). The values that variable can assume are symbolically represented as lowercase symbol \\(y_i\\), where \\(i\\) is the sample size, ranging from 1 to \\(n\\) independent replicates. Similarly, the indepedent variable is usually depicted by uppercase \\(X\\) (or some other letter) and its values are lowercase \\(x_i\\). I’m going to use that convention but with a twist. Independent variables denoted using \\(X\\) will represent continuous scaled variables, whereas independent variables denoted using \\(A\\) or \\(B\\), or \\(C\\), will represent discrete, factoral variables. These will take on values denoted by lowercases, eg, \\(a_i\\), \\(b_i\\), \\(c_i\\)) . To illustrate dependent and independent variables think about a linear relationship between two continuous variables, \\(X\\) and \\(Y\\) . This relationship can be expressed using the model \\(Y=\\beta_0 + \\beta_1 X\\). \\(X\\) would be a variable the researcher manipulates, such as time or the concentration of a substance. \\(Y\\) would be a variable that the researcher measures, such as absorption or binding or fluorescence. The parameters \\(\\beta_0\\) and \\(\\beta_1\\) are constants that modify the relationship between the two variables, which I’m sure you recognize as representing the y-intercept and slope, respectively, of the regression line between the two variables. Thus, \\(Y\\) takes on different values as the researcher manipulates the levels of \\(X\\). Which explains why \\(Y\\) depends on \\(X\\). For example, here’s how the data for a protein standard curve experiment would be depicted. In the R script below the variable \\(X\\) represents known concentrations of an immunoglobulin protein standard in \\(\\mu g/ml\\). The researcher builds this dilution series from a known stock, thus it is the independent variable. The variable \\(Y\\) represents \\(A_{595}\\), light absorption in a spectrophotometer for each of the values of the standard protein. The \\(A_{595}\\) values depend upon the immmunoglobulin concentration. Estimates for \\(\\beta_0\\) and \\(\\beta_1\\) are derived from running a linear regression on the data with the lm(Y~X) script. Thus, for every one unit increment in the value of \\(X\\), there is a 0.02497 increment in the value of \\(Y\\). Again, \\(Y\\) depends upon \\(X\\). #Protein assay data, X units ug/ml, Y units A595. X &lt;- c(0, 1.25, 2.5, 5, 10, 15, 20, 25) Y &lt;- c(0.000, 0.029, 0.060, 0.129, 0.250, 0.371, 0.491, 0.630) #derive the slope and intercept by linear regression lm(Y~X) ## ## Call: ## lm(formula = Y ~ X) ## ## Coefficients: ## (Intercept) X ## -0.0008033 0.0249705 8.1.1 When there is no independent variable This is a course for experimental biologists. In other types of research, particularly in the public health, behavioral and social science fields, studies are often not strictly experimental. Researchers in these fields generally work with data sets lacking true, experimentally-manipulated independent variables as defined above. Yet these researchers are still very interested learning whether certain phenomena cause other phenomena. The problem of drawing causal inference from studies in which all of the variables are observed is beyond the scope of this course. Pearl offers an excellent primer on considerations that must be applied to extract causality from observational data here. 8.2 Discrete or continuous variables At their most fundamental level, the dependent and independent variables of experiments can each be subclassified further into two categories. They are either discrete or continuous. Discrete variables can only take on discrete values, while continuous variables can take on values over a continuous range. If that’s not clear just yet, it should become more clear by reading below. Variables can be subclassified further as either measured, ordered, or sorted. This subdivision fulfills a few purposes. First, it’s alliterative so hopefully easier to remember. It reminds me of Waffle House hashbrowns, which can be either scattered, smothered or covered, and that is just something you’ll never forget once you’ve visited a Waffle House. Second, it covers all types of data and statistical testing, and thus forms the basis for drawing a pragmatic statistical modeling heuristic. knitr::include_graphics(&quot;images/testing_heuristic.jpg&quot;) Figure 8.1: The type of data dictates how it should be modeled. Third, the “measured, ordered, sorted” scheme classifies variables on the basis of their information density, where measured &gt;&gt; ordered &gt;&gt; sorted. Different authors/softwares give these three types of variables different names, which creates some confusion. In SPSS, for example, when setting up variables you can choose to classify it as scalar, ordinal, or nominal, which correspond to measured, ordered and sorted. Another fairly common descriptive set for the three types is interval, ordinal, and categorical. These correspond to measured, ordered, and sorted, too. Though they are named differently, for the most part everybody seems to agree that all variables can be reduced to 3 subtypes, even if they can’t agree on what to name them. 8.2.1 Measured variables Because everything is measured in some sense of the word “measured”\" is probably not an ideal choice to describe what is meant when refering to a continuous variable. My rationale for the choice is that it almost always requires some kind of measuring instrument to grab this type of data. For the present purposes let’s considered the terms measured variables and continuous variables as synonymous. Measured variables are fairly easy to spot. Any derivative of one of the seven base SI units will be a measured variable. knitr::include_graphics(&quot;images/si_units.jpg&quot;) Figure 8.2: The seven SI units Take mass as an example. The masses of physical objects can be measured on a continuous scale of sizes ranging from super-galaxian to subatomic. Variables that are in units of mass take on a smooth continuum of values over this entire range because mass scales are infinitesimily divisible. Here’s a thought experiment for what infinitesimily divisible means. Take an object that weighs a kilogram, cut it in half and measure what’s left. You have two objects that are each one half a kilogram. Now repeat that process again and again. After each split something always remains whose mass can be measured. Even though it gets smaller and smaller. Even when you arrive at the point where only a single atom remains it can be smashed into yet even smaller pieces in a supercollider, yielding trails of subatomic particles….most of which have observable masses. But here’s what’s important about continuous variables: That continuity between gradations means that continuous variables can carry more information than other types of variables. That’s what I meant by information density, in the comment above. On a scale of micrograms, an object weighing one kilogram would have one billion subdivisions. If you have an instrument that can accurately weigh the mass of kilogram-sized objects to the microgram level, and each microgam would be informative, you would say that one kilogram is comprised of a billion bits of information. All of that possible information explains why the distinction between continuous and discrete variables is so important. You’ll see that discrete variables lack this kind of information density between their units. As you read on below as discrete variables are discussed, think about how continuous variables carry more information than discrete variables. More pragmatically, this difference is the basis for why discrete and continuous data behave so differently. And because of this inherent basis for why they behave differently, statisticians have devised statistical models that are more appropriate for one kind of data vs some other. 8.2.2 Discrete categorical and ordinal variables Discrete variables are discontinuous. The units of discrete variables are indivisible. Unlike continuous variables, discrete variables offer no information between their unit scale boundaries. There are two types of discrete variables. These are called ordinal and categorical. I like to call these ordered and sorted, respectively, again for alliterative purposes. 8.2.2.1 Sorted data Categorical variables are a bit easier to understand so let’s start with those. These variables represent objects that are counted. Because they have certain features they are sorted into categories or, as I like to say, buckets. For example, a biostats class might be comprised of 50 students, 15 of whom are men and the rest are women. The name of the variable is sex. The values that the “sex” variable can take on is either male or female. The variable sex is a factoral variable at two levels. If we count all of the men and woman in a class we arrive at another variable called count which represents the discrete counts of people who are sorted into either of the two sex categories. The variable count is an integer varible. It cannot take on any values other than integer values. There cannot be a case that has less than a whole student. A partial biostats student would be absurd! So our data set has two variables. One is sex, a factoral variable that has two levels. The other is count, an integer variable that has $$50 levels. Of course, the categorization of sex is sometimes ambiguous. If it is important to accomodate more, we would add additional categories to account for all possible outcomes. For example, the sex variable could be set to take on values of man, woman, and other. Take a moment to also think about the values of that sex variable. This is to emphasize that man, woman and other are not numeric values. Variables can have non-numeric values. R reads those levels as character values and “coerces” to classify sex as a factoral variable with three levels, man, woman and other. Let’s use R to create a summary table for the composition by sex of the biostats class. Inspection of the code shows the table has two variables, sex as described, and count. The function str(ibs538) reveals that the former variable is a “Factor w/ 3 levels”\" and the later is a integer variable. We used the is.integer function in the code to ensure that count would be an integer variable. Had we not, R wanted to coerce it as a numeric variable. Finally, notice how the variable count is only comprised of discrete integer values. These discrete counts are why sorted data is classified as discrete. sex &lt;- c(&quot;man&quot;, &quot;woman&quot;, &quot;other&quot;) count &lt;- as.integer(c(15, 35, 0)) ibs538 &lt;- data.frame(sex, count); ibs538 ## sex count ## 1 man 15 ## 2 woman 35 ## 3 other 0 str(ibs538) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ sex : Factor w/ 3 levels &quot;man&quot;,&quot;other&quot;,..: 1 3 2 ## $ count: int 15 35 0 Obviously, there’s nothing experimental about counting the sex of biostats students. However, many biomedical experiments generate discrete categorical data, too. Imagine the following: Neurons are poked with an electrode. Counts are recorded of the number of times they depolarize over a certain time period, in response to an depolarizing agent and its control. Cells are stained for expression of a marker protein. The number of cells in which the protein is detected are counted. Counts of a knockdown condition are compared to a control. By some criteria, cells are judged to be either alive or dead and counted as such. The number of alive cells are counted after manipulating expression of a tumor suppressor gene, and compared to a control. By some criteria, mice are judged to either show a disease phenotype or not, and counted as such. Disease incidence is counted in response to levels of a therapeutic agent, or a background genotype, or in response to some stressor. There are an infinite number of examples for experiments that can be performed in which a dependent variable is categorized and each of the replicates are sorted into one or some other level of that category. In the end, sometimes even after some fairly sophisticated instrumentation or biochemical analysis, all the researcher does is count objects that have a characteristic or some other. Unlike continuous variables, discrete variables don’t possess any information between their units. In each of the cases above the replicate either possesses a level of the variable or it does not. It belongs in one bucket or some other. 8.2.2.2 Ordered data Ordered data is, in one sense, a hybrid cross of sorted and measured data. If you’ve ever taken a poll in which you’ve been asked to evaluate something on a scale ranging from something akin to “don’t like at all” to “couldn’t live without”…then you’ve experienced ordinal scaling (such scales are called Likert scales). The precourse survey for this course is chock full of questions that generate data on an ordered scale. Ordered variables are structured to have levels which are quantitatively related to each other. Each experimental replicate is evaluated and then categorized to one of the values of the ordinal variable. It is true there is an element of sorting, but the key difference is these aren’t nominal categories as in sorted data. There are underlying gradations of the variable’s scale. This is not just the absence or presence of an attribute, but rather some amount of the attribute relative to other possible amounts the scale allows for. These gradations within the levels of the ordered variable make them somewhat like measured data. The data strucure has intervals. But ordinal data are discrete because only certain values for the measurement are allowed, depending upon the structure of the scale or scoring system for a given attribute. There is no information between the intervals. Disability status scales represent classic ordinal scales. These are used to assess neurologic abnormalities, for example, those associated with experimental multiple sclerosis. Each replicate in a study is evaluated by trained researchers and assigned the most appropriate value given its condition: 0 for no disease, 1 for limp tail, 2 for mild paraparesis, 3 for moderate paraparesis, 4 for complete hindlimb paralysis, and 5 for moribound. Obviously, in this ordinal scale, as the numeric value increases so to does the severity of the subject’s condition. Here’s what a very small set of ordinal data might look like: genotype &lt;- c(rep(&quot;wt&quot;, 3), rep(&quot;ND4&quot;, 3)) DSS_score &lt;- as.integer(c(0,1,1,5,3,5)) results &lt;- data.frame(genotype, DSS_score); results ## genotype DSS_score ## 1 wt 0 ## 2 wt 1 ## 3 wt 1 ## 4 ND4 5 ## 5 ND4 3 ## 6 ND4 5 str(results) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ genotype : Factor w/ 2 levels &quot;ND4&quot;,&quot;wt&quot;: 2 2 2 1 1 1 ## $ DSS_score: int 0 1 1 5 3 5 genotype is an indepedent, factoral variable that comes in two levels, wt or ND4. DSS_score is a dependent variable that comes in 6 levels, the integer values ranging from 0 to 5. We need to force R to read DSS_score for what it is, an integer rather than as a numeric. One of the key issues of ordinal scales is that they are not necessarily guassian. As a general rule they tend to be skewed (though there is no inherent reason for this to be the case). For example, in my precourse survey I ask students how excited they are, on a scale of 1 to 10, to take a biostats class. The result is decidedly mixed. There’s a bit of a lean towards good enthusiasm, but a fairly pronounced unenthusiastic tail. The data on this ordinal scale are not normally distributed. Any analysis would need to be with a statistical procedure that does not assume normally distributed dependent variables. ggplot(data.frame(score = c(1, 25)), aes(score)) + stat_function(fun = dnorm, args=list(mean=15, sd=1.5)) df &lt;- data.frame(score=c(1:25), y=dpois(1:25, 15)) ggplot(df, aes(score, y))+ geom_col() df &lt;- data.frame(score=c(1:25), y=dbinom(1:25, 25, 0.6)) ggplot(df, aes(score, y))+ geom_col() "],
["jaxwest7.html", "Chapter 9 Reproducible Data Munging in R 9.1 Jaxwest7 glucose data 9.2 Explore the Jaxwest7 data", " Chapter 9 Reproducible Data Munging in R Data munging is the process of taking data from one source(s), working it into a condition where it can be analyzed. Because every data set will differ every data munge will be custom. Data munging is a bit like organic chemistry. You know the chemical you want to create. You know the starting materials that you have on hand. You know the reactions and the intermediates that you’ll need to produce to get that final product. Here’s one example of the munging process. What we want to achieve in this exercise, the final product, is to get some graphical views in order to visualize the data structure along with some summary statistics. We begin with some experimental data that’s a bit unstructured. Our tools include an data import function (datapasta) and a handful of functions in the tidyverse package. Not especially how every transaction with the data is recorded. If you took each of these code chuncks and ran them on your machine, you should get identical results. That’s reproducibility. 9.1 Jaxwest7 glucose data library(datapasta) library(tidyverse) The Jaxwest7 data set is a Jackson Labs experiment conducted in a mouse strain serving as a model for type 2 diabetes. Animals fed a glucose-rich diet develop a type 2 diabetes syndrome. The experiment tests whether the antidiabetic drug rosglitazone suppresses disease development. Half the subjects receive the antidiabetic drug, the other receive vehicle as placebo.The syndrome is assessed by measuring two response variables: body weight and blood glucose concentrations. There are two explanatory variables: day of study and drug treatment. The experimental design is therefore multivariate (weight, blood glucose) two-factor (drug treatment, day) ANOVA with repeated measures from the replicates. The purpose of this chapter is to illustrate how to retrieve and process data in R, focusing only on the blood glucose response variable within the Jaxwest7 data set. Going through this exercise will illustrate how to prepare data for statistical analysis. 9.1.1 Inspect the Jaxwest7 data Download the Jaxwest7.xls file from the mouse phenome database to your machine and open it with Excel (or some other spreadsheet software). Go to the BloodGlucoseGroups tab. This is readable, but what hits you is the sheet’s complexity. In fact, this sheet illustrates what unstructured data looks like. Almost every column has cells containing multiple types of values. The first 8 rows have various descriptor text, including a logo! Rows 9-14 have some other definitions. Scroll way over to the right and some graphs pop up. The data we are interested in are in rows 15 to 42, and in columns F to S. Each of those columns has two column names, a date and a day. Thus, if a column in this array is a group of variable values, then each variable has two names! There should be only 1. Columns T and U have several missing values, because those animals were used for autopsy. We’re going to have to ignore their response values. Columns 43 to 146 are missing entirely! Below the array are some summary statistics on the columns above, which represent different variables. This is not a spreadsheet that can or should be imported whole scale directly into R. Instead, we need to grab only the data we need. Then we’ll use R to structure it properly. 9.1.2 Munge the glucose concentration data into R Let’s get the glucose concentration data into R, and create a “long”\" data frame format, where every column represents a variable and every row is a case (or a subset of a case). We’ll have to create some row and some column variable names. What do we have to work with? What do we need to create? Glucose concentrations were measured twice per day on odd-numbered days plus day 12. Each column represents a blood draw session. This was done on each of 16 animals. Half were in a placebo group, half were in a drug group. I’m going to omit day 15 due to the NA values (those mice were harvested for autopsy, and so day 15 breaks the time series). Let’s make a proper R data frame out of this data. Open up an R script file and follow along. Step 1: Deal with cell F21. It’s value in the excel spreadsheet is “Hi”, a character value rather than a numeric (it must have tested out-of-range for the assay). We have two options: Assign it an NA value, or impute. Since this is a related-measures time series with multiple other glucose measurements for that specific replicate, we’ll impute by using the average of all these other measurements. Calculate the value that will be imputed: #Use datapasta to paste in vector values. Calculate their mean. Then impute value for cell F21 in original data set by exchanging the value &quot;Hi&quot; with the mean produced here. F21 &lt;- mean(c(449L, 525L, 419L, 437L, 476L, 525L, 499L, 516L, 485L, 472L, 535L, 500L, 497L) ); F21 ## [1] 487.3077 Ideally, you’d import the data with the “Hi” value and fix it in R, to have reproducible record. Doing so would involve walking several confusing munging steps into the weeds, which are beyond the scope of this chapter. Step 2: Fix the F21 cell in the spreadsheet, then copy the array F15:S42 to the clipboard. This gives 14 columns and 16 rows of glucose data. All values are numeric and represent the same variable: glucose concentration. Use the datapasta package addin for this procedure. Create an object name, pasting in as a tribble provides the cleanest route. gluc &lt;- tibble::tribble( ~V1, ~V2, ~V3, ~V4, ~V5, ~V6, ~V7, ~V8, ~V9, ~V10, ~V11, ~V12, ~V13, ~V14, 136L, 270L, 162L, 165L, 192L, 397L, 172L, 148L, 291L, 239L, 192L, 172L, 235L, 153L, 345L, 518L, 429L, 413L, 456L, 487L, 468L, 419L, 507L, 559L, 420L, 415L, 511L, 464L, 190L, 301L, 311L, 361L, 398L, 465L, 388L, 392L, 453L, 421L, 355L, 381L, 394L, 444L, 434L, 504L, 453L, 392L, 350L, 400L, 458L, 387L, 342L, 368L, 355L, 429L, 373L, 501L, 424L, 486L, 447L, 417L, 496L, 484L, 468L, 423L, 472L, 507L, 458L, 456L, 519L, 570L, 170L, 208L, 134L, 129L, 147L, 141L, 241L, 128L, 162L, 163L, 222L, 438L, 307L, 252L, 487L, 449L, 525L, 419L, 437L, 476L, 525L, 499L, 516L, 485L, 472L, 535L, 500L, 497L, 218L, 273L, 254L, 265L, 338L, 386L, 287L, 236L, 347L, 235L, 432L, 450L, 509L, 326L, 179L, 184L, 124L, 107L, 108L, 149L, 142L, 143L, 112L, 233L, 113L, 137L, 106L, 150L, 260L, 381L, 174L, 140L, 132L, 138L, 164L, 137L, 122L, 140L, 102L, 174L, 120L, 135L, 115L, 191L, 132L, 132L, 169L, 158L, 129L, 120L, 122L, 157L, 94L, 141L, 120L, 166L, 526L, 517L, 465L, 394L, 310L, 269L, 213L, 185L, 145L, 201L, 131L, 258L, 114L, 160L, 325L, 252L, 203L, 158L, 135L, 162L, 164L, 181L, 150L, 177L, 162L, 192L, 170L, 162L, 329L, 296L, 212L, 159L, 156L, 200L, 139L, 143L, 164L, 150L, 119L, 193L, 148L, 188L, 230L, 414L, 408L, 179L, 432L, 288L, 163L, 240L, 185L, 208L, 138L, 208L, 153L, 140L, 204L, 120L, 138L, 139L, 157L, 122L, 163L, 168L, 164L, 128L, 129L, 218L, 135L, 182L ) Notice how R coerces unique variable names for each column. At this point, they are much cleaner than the column names available in the spreadsheet. Step 3: Create a column for the ID variable. There are 16 replicate mice. We’ll given them each a unique ID name. ID &lt;- LETTERS[1:16] Step 4: Create a column for the treatment variable. The first 8 replicates received vehicle (placebo). The second 8 rosiglitazone. treat &lt;- c(rep(&quot;placebo&quot;, 8), rep(&quot;rosiglit&quot;, 8)) Step 5: Add these columns to the gluc tribble gluc &lt;- add_column(gluc, ID, treat, .before=T) Step 6: Now would be a good time to convert from a wide to a long table format. We use the gather function in the tidyverse package for that. gluc &lt;- gather(gluc, V, glucose, -ID, -treat) Step 7: Replace the variable V with two variables. Note how we created a new variable, V, in this move. As you can see, the gather function successively “gathers” all the glucose values from columns V1, V2,…, V14 into a single column under the variable V. It’s a very, very slick and useful function that’s analogous to an excel pivot table (but much easier to execute). The V variable, however, is ambiguous. The 14 levels of V actually represent two variables. The odd numbered represent the early day blood draw, while the even numbered represent the late day draw. Meanwhile, V1 and V2 represent day one, V3 and V4 represent day three,…, and V13 and V14 represent day eleven. We need a single variable column to represent early or late draw, and another variable column to represent the seven days of the study. There are two levels for the draw variable, and seven levels for the day variable. These repeat in a 16 and 32 unit pattern, respectively. draw &lt;- rep( c(rep(&quot;early&quot;, 16), rep(&quot;late&quot;, 16) ), 7 ) day &lt;- rep( c(1, 3, 5, 7, 9, 11, 12), each =32) Step 8: Update the gluc table to incorporate these new variables. gluc &lt;- add_column(gluc, draw, day, .before=T) gluc ## # A tibble: 224 x 6 ## draw day ID treat V glucose ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 early 1 A placebo V1 136 ## 2 early 1 B placebo V1 345 ## 3 early 1 C placebo V1 190 ## 4 early 1 D placebo V1 434 ## 5 early 1 E placebo V1 424 ## 6 early 1 F placebo V1 170 ## 7 early 1 G placebo V1 487 ## 8 early 1 H placebo V1 218 ## 9 early 1 I rosiglit V1 179 ## 10 early 1 J rosiglit V1 260 ## # ... with 214 more rows Step 9: Remove the V variable column from the tribble. First verify that the correct levels for draws and days correspond to the correct levels of V. After that, the V column has no use so it can be removed. gluc &lt;- select(gluc, -one_of(&quot;V&quot;)) gluc ## # A tibble: 224 x 5 ## draw day ID treat glucose ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 early 1 A placebo 136 ## 2 early 1 B placebo 345 ## 3 early 1 C placebo 190 ## 4 early 1 D placebo 434 ## 5 early 1 E placebo 424 ## 6 early 1 F placebo 170 ## 7 early 1 G placebo 487 ## 8 early 1 H placebo 218 ## 9 early 1 I rosiglit 179 ## 10 early 1 J rosiglit 260 ## # ... with 214 more rows Let’s fix one subtle issue. Each of the variables draw, day, ID and treat should all be interpreted as factor objects. But the data table interprets them as characters (or dbl in the case of day). Some statistical tests need to read these as factors. cols &lt;- c(&quot;draw&quot;, &quot;day&quot;, &quot;ID&quot;, &quot;treat&quot;) gluc[cols] &lt;- lapply(gluc[cols], factor) 9.2 Explore the Jaxwest7 data Histograms are a great way to get a first look at data sets with a reasonably healthy number of values, such as this one. We expect glucose to be a normally-distributed variable. Here it’s clearly bi-modal, perhaps representing two normally-distributed treatment groups? ggplot(gluc)+ geom_histogram(aes(glucose)) By coloring the two groups, the story grows a bit more complex. ggplot(gluc, aes(fill=treat) ) + geom_histogram(aes(glucose)) We can also look at the data as scatter plots, by draw, treatment and time. This view shows the time series for each replicate. Looking at the data atomically, in this way, gives tremendous insights! ggplot(gluc, aes(day, glucose, group=ID, color = treat) ) + facet_wrap(~draw) + geom_point() + geom_line() Now create some summaries. Here’s a plot of the means and standard deviations of the groups. The stat_summary function is a bit quirky to work with, but worth learning. ggplot(gluc, aes(day, glucose, color=treat ) ) + facet_wrap(~draw) + stat_summary(fun.data = &quot;mean_sdl&quot;, fun.args = list(mult = 1), geom =&quot;pointrange&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group=treat) ) Now here’s a summary of all the replicate values in tabular form. gluc %&gt;% group_by(day, treat, draw) %&gt;% dplyr::summarise(mean=mean(glucose), sd=sd(glucose), n=length(glucose)) ## # A tibble: 28 x 6 ## # Groups: day, treat [?] ## day treat draw mean sd n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 placebo early 300. 138. 8 ## 2 1 placebo late 376. 125. 8 ## 3 1 rosiglit early 271 126. 8 ## 4 1 rosiglit late 294. 134. 8 ## 5 3 placebo early 339. 146. 8 ## 6 3 placebo late 320. 118. 8 ## 7 3 rosiglit early 232 131. 8 ## 8 3 rosiglit late 176 90.6 8 ## 9 5 placebo early 352. 125. 8 ## 10 5 placebo late 404. 114. 8 ## # ... with 18 more rows The more you look at the early v late draws, both graphically and in the table above, the more you don’t have a problem treating those as technical or pseudo replicates. Perhaps it was a decision made ahead of time at Jackson? We’ll declare the early and late draws on the same day as not independent. Averaging them leaves will leave us with a tighter estimate of daily blood glucose concentrations, which is nice. The code below averages the early and late draws to produce a single mean glucose value per replicate per time point. gluc %&gt;% group_by(day, treat, ID) %&gt;% dplyr::summarise(mean=mean(glucose), sd=sd(glucose), n=length(glucose) ) ## # A tibble: 112 x 6 ## # Groups: day, treat [?] ## day treat ID mean sd n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 placebo A 203 94.8 2 ## 2 1 placebo B 432. 122. 2 ## 3 1 placebo C 246. 78.5 2 ## 4 1 placebo D 469 49.5 2 ## 5 1 placebo E 455 43.8 2 ## 6 1 placebo F 189 26.9 2 ## 7 1 placebo G 468 26.9 2 ## 8 1 placebo H 246. 38.9 2 ## 9 1 rosiglit I 182. 3.54 2 ## 10 1 rosiglit J 320. 85.6 2 ## # ... with 102 more rows The ‘mean’ column should be used as the final value for the glucose response variable, in statistical analysis. This is the right call. To leave the technical duplicates would be to flood the data set with undeserved degrees of freedom. We’ll create a final, working table of the data now. It catalogs how each replicate behaved on each day, and which treatment it received. You’ll note we’ve : glucFinal &lt;- gluc %&gt;% group_by(day, treat, ID) %&gt;% dplyr::summarise(glucose=mean(glucose) ) glucFinal ## # A tibble: 112 x 4 ## # Groups: day, treat [?] ## day treat ID glucose ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 placebo A 203 ## 2 1 placebo B 432. ## 3 1 placebo C 246. ## 4 1 placebo D 469 ## 5 1 placebo E 455 ## 6 1 placebo F 189 ## 7 1 placebo G 468 ## 8 1 placebo H 246. ## 9 1 rosiglit I 182. ## 10 1 rosiglit J 320. ## # ... with 102 more rows Because it has all of the replicate data, the glucFinal data table is what we would use for statistical testing. If we published a figure from all of this, we’d create it from the glucFinal data table. The following would be the most appropriate because it is based on average technical replicates: ggplot(glucFinal, aes(x = day, y = glucose, color=treat ) ) + stat_summary(fun.data=&quot;mean_sdl&quot;, fun.args = list(mult = 1), geom =&quot;pointrange&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group=treat)) And finally, here’s some summary stats on that final data set: glucFinal %&gt;% group_by(day, treat) %&gt;% dplyr::summarise(n = length(glucose), mean = mean(glucose), median = median(glucose), sd = sd(glucose), sem = sd/sqrt(n), min = min(glucose), max = max(glucose)) ## # A tibble: 14 x 9 ## # Groups: day [?] ## day treat n mean median sd sem min max ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 placebo 8 338. 338. 128. 45.1 189 469 ## 2 1 rosiglit 8 283. 300. 121. 42.8 153 522. ## 3 3 placebo 8 330. 378. 131. 46.2 132. 472 ## 4 3 rosiglit 8 204 169. 106. 37.6 116. 430. ## 5 5 placebo 8 378. 403. 115. 40.6 144 490 ## 6 5 rosiglit 8 193. 156 85.1 30.1 128. 360 ## 7 7 placebo 8 352. 406. 132. 46.7 160 512 ## 8 7 rosiglit 8 162. 158 27.8 9.83 124. 202. ## 9 9 placebo 8 379. 396 132. 46.7 162. 533 ## 10 9 rosiglit 8 160. 160. 21.2 7.48 131 196. ## 11 11 placebo 8 386. 405. 98.5 34.8 182 504. ## 12 11 rosiglit 8 157. 164. 27.5 9.71 118. 194. ## 13 12 placebo 8 410. 428 117. 41.5 194 544. ## 14 12 rosiglit 8 147. 145. 16.0 5.66 128. 168 Phew! "],
["binomial.html", "Chapter 10 The Binomial Distribution 10.1 dbinom 10.2 pbinom 10.3 qbinom 10.4 rbinom", " Chapter 10 The Binomial Distribution library(ggplot2) The binomial probability distribution models the discrete outcomes of dichotomous processes. In other words, events that can be categorized as either a successes or as a failure. 10.1 dbinom The binomial probability mass function in R is dbinom. R’s dbinom function returns p(x), a probability value produced by the binomial probability mass function: \\[p(x)={n\\choose x}(p)^x(1-p)^{(n-x)}\\] Where \\(n\\) is the number of trials, \\(x\\) is the value of the number of successes, \\(p\\) is the probability of a single success, and therefore \\(1-p\\) is the probability of a single failure. The coin toss serves as the classic explainer for binomial events. A fair coin can land as either heads, or tails with equal probabilities. Unless you’re Patriot’s quarterback Tom Brady, for whom it always lands as heads. If Matt Ryan tossed a coin 10 times, what’s the probability of him getting EXACTLY 7 heads? dbinom(x=7, size=10, prob=0.5) ## [1] 0.1171875 That’s almost a 12% chance! Eight heads would be even more unlikely. And so on. Female mice enter estrus one out of five days. This implies that if a female mouse is mated on any random day, the probability of any single mating resulting in pregnancy is 0.2. On a given day, if you set up 12 female mize for mating, what’s the probability that exactly half of them would become pregnant? dbinom(x=6, size=12, prob=0.2) ## [1] 0.01550215 There’s only a 1.55% chance of getting EXACTLY 6 dams out of that mating set up. The script below illustrates the probabilities over a full range of possible pregnancy outcomes, for a trial of size 12 (ie, 12 matings set up) x &lt;- 1:10 size &lt;- 12 prob &lt;- 0.2 df &lt;- data.frame(x, px=dbinom(x, size, prob)) ggplot(df, aes(x, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) It’s evident that binomial distributions where the probabilities of successes and failures are uneven are skewed. The only way to make these appear more normally distributed is to have equal probabilities for successes and failures. 10.2 pbinom R’s pbinom is the cumulative probability distribution function for the binomial. \\[p(x)={\\sum_{i=0}^{x}}{n\\choose i}(p)^i(1-p)^{(n-i)}\\] This function returns the cumulative probability value for a number of successes in n trials. This can be a very useful value to model. For example, if you set up 12 matings of mice, where each had a 0.2 probability of pregnancy, what is the probability that you would have up to 6 pregnant dams? pbinom(6, 12, 0.2, lower.tail=T) ## [1] 0.9960969 There’s is a very high probability of getting UP TO 6 pregnancies from 12 matings! If we turn the lower.tail argument from TRUE to FALSE the pbinom returns a p-value like probability. What’s the probability of getting 6 or more pregnancies from 12 matings where the probability of a single pregnancy is 0.2? pbinom(6, 12, 0.2, lower.tail=F) ## [1] 0.003903132 That’s about 0.39%! Which would be a very rare outcome from the mating trial, indeed! Maybe even scientifically significant were it to occur. Perhaps it’s useful to visualize both the upper and lower tails of this cumulative function: q &lt;- 1:10 size &lt;- 12 prob &lt;- 0.2 df &lt;- data.frame(q, px=pbinom(q, size, prob)) ggplot(df, aes(q, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;pbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;lower.tail=TRUE&quot;,&quot;)&quot;)) df &lt;- data.frame(q, px=pbinom(q, size, prob, lower.tail=F)) ggplot(df, aes(q, px)) + geom_col(fill =&quot;blue&quot;) + xlab(&quot;x, number of successes&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;pbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;lower.tail=FALSE&quot;,&quot;)&quot;)) 10.3 qbinom The quantile binomial distribution function in R is qbinom. qbinom is the inverse of the pbinom function. This predicts the number of successes that might occur given a percentile of the distribution. Assuming 12 matings are set up, where the probability of any one pregnancy success is 0.2, what number of pregnancies would be expected if the group performed at the 90th percentile? qbinom(p=0.90, size=12, prob=0.2, lower.tail=T) ## [1] 4 That’s only 4 litters. That should make sense, since only 1 in 5 would be pregnant on average. To out perform this expectation at the 90th percentile is still not a very large numer The graph below illustrates this. Notice the step-wise distribution, which is diagnostic of discrete functions. #define variables p &lt;- seq(0, .99, 0.03) #cumulative probability quantiles size &lt;- 12 #number of trials prob &lt;- 0.2 #probability of success of one trial df &lt;- data.frame(p, q=qbinom(p, size, prob)) ggplot(df, aes(p, q)) + geom_col(fill=&quot;blue&quot;) + xlab(&quot;p(q)&quot;) + ylab(&quot;q&quot;) + labs(title = paste(&quot;qbinom&quot;,&quot;(&quot;,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) 10.4 rbinom The rbinom function is for random simulation of n binomial trials of a given size and event probability. The output is the number of successful events per trial. Let’s simulate 12 matings 12 times, as if we do one a mating involving 12 females, once per month. How many successes will we see per month? The output below represents the number of litters we would produce on each of those months. The point is, we don’t get the average every month. Some times its more successes, others its fewer. Models are perfect, data are not. rbinom(n=12, size=12, prob=0.2) ## [1] 3 2 2 1 3 2 2 2 3 2 1 2 Here’s a histogram from a very large number of simulations of the same scenario. You can clearly see the binomial distribution for this trial size and probability of success is skewed. You can also clearly see the average of the distribution…which is somewhere between 2 and 3. n &lt;- 10000 #number of simulations size &lt;- 12 #number of trials prob &lt;- 0.2 #probability of success of one trial df &lt;- data.frame(x=rbinom(n, size, prob)) #x=number of successful trials ggplot(data=df, aes(df$x)) + stat_count(fill=&quot;blue&quot;) + xlab(&quot;x&quot;)+ ylab(&quot;count&quot;)+ labs(title = paste(&quot;rbinom&quot;,&quot;(&quot;,&quot;number=&quot;,n,&quot;trial size=&quot;,size,&quot;,&quot;,&quot;p=&quot;,prob,&quot;)&quot;)) "],
["poisson.html", "Chapter 11 The Poisson Distribution 11.1 Poisson Events 11.2 dpois 11.3 ppois 11.4 rpois 11.5 Overdispersion", " Chapter 11 The Poisson Distribution library(tidyverse) 11.1 Poisson Events Counts of random, discrete events that occur in blocks of time or space are said to have the property of frequency. They can be modeled by the Poisson distribution. The values of these events are always integers. For example, the number of times a neuron depolarizes over a fixed period of time would be frequency. The number of cells on which an antigen can be detected that are in a fixed volume of fluid would also be a frequency. The number of facebook friends a random biostats student has would be a frequency. 11.2 dpois dpois is the Poisson probability mass function in R: \\(p(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}\\) dpois takes as arguments i) the scalar \\(x\\), and ii) lambda, an average or expectation of the distribution, and returns the value of the probability, or otherwise known as the probability mass, for that scalar. \\(x\\) can be either a single value, or a vector comprised of many values. We use the latter, conveniently, to produce nice graphs. For example, assume a neuron, on average, can be expected to depolarize spontaneously 8 times per second. What is the probability it would only depolarize half that number of times? Use dpois to calculate the probability that a random number of events would occur in a time or space, given some expected average frequency. dpois(x=4, lambda=8) ## [1] 0.05725229 Therefore, the probability that a randomly selected neuron would depolarize exactly 4 times per second is 5.72%. The probability of some frequency we might expect to see is sometimes useful to calculate. What’s most notable about the the dpois is how it loses symmetry and becomes more skewed as its average (lambda) gets lower. x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=dpois(x, lambda)) ggplot(df, aes(x=x,y=px)) + geom_col(fill = &quot;red&quot;) + xlab(&quot;x&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dpois&quot;,&quot;(&quot;,&quot;lambda=&quot;, lambda,&quot;)&quot;)) 11.3 ppois R’s ppois function is the Poisson cumulative mass function \\[p(x)=\\sum_{i=0}^{x} \\frac{e^{-\\lambda}\\lambda^i}{i!}\\] This calculates a cumulative probability value for a certain frequency, given the average frequency of the distribution. Let’s say, for example, that a neuron depolarizes on average 8 times per second. If you took a random measure of depolarization activity, what is the probability that you’d observe a frequency as high as 4 depolarizations per second? ppois(4, 8, lower.tail=T) ## [1] 0.0996324 The value of almost 10% is higher than what we determined using the dpois because the ppois is a cumulative function! What is the probability that you’d observe a frequency of 16 or more depolarizations per second? To answer this question we have to reverse the function’s default lower.tail argument. As you might suspect, when set with the lower.tail=FALSE argument, as below, the ppois function returns a p-value. ppois(16, 8, lower.tail=F) ## [1] 0.003718021 Thus, the probability of observing a frequency twice as high or higher than the average for this distribution is quite low, at about 0.37%! Here’s the cumulative Poisson distribution for a phenomenon that has an average of 8, over a range of frequency values: x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=ppois(x, lambda, lower.tail=T)) ggplot(df, aes(x, px)) + geom_col(fill=&quot;red&quot;) It’s probably worth reversing the lower.tail argument to visualize the distribution of p-values for a Poisson x &lt;- c(0:25) lambda &lt;- 8 df &lt;- data.frame(x, px=ppois(x, lambda, lower.tail=F)) ggplot(df, aes(x, px)) + geom_col(fill=&quot;red&quot;) ## qpois The qpoisfunction is the inverse of the cumulative Poisson mass function. It takes a probability as an argument and returns a frequency value. What depolarization frequency can we expect as the 90th percentile for a neuron that has an average frequency of 8 depolarizations per second? qpois(0.9, 8) ## [1] 12 Visualized, notice the stair-step pattern, which is diagnostic of discrete probability distributions p &lt;- seq(0, 1, 0.05) lambda &lt;- 8 df &lt;- data.frame(p, frequency=qpois(p, lambda, lower.tail=T)) ggplot(df, aes(p, frequency)) + geom_col(fill=&quot;red&quot;) 11.4 rpois The rpois function in R is used to generate random Poisson data. It takes arguments of lambda, the average frequency of a population, and the number of random counts to generate. To simulate a sample of 10 measurements of a neuron that on average depolarize 8 times per sec: rpois(10, 8) ## [1] 9 9 7 6 6 9 10 11 12 6 Here’s a histogram of a larger sample. Notice how it isn’t as perfect as the Poisson distribution would suggest. That’s because models are perfect, but samples are not. Even computer-generated samples! Also notice the low but detectable frequency of extreme values..frequencies higher than 20, by random chance. df &lt;- data.frame(s=rpois(10000, 8)) ggplot(df, aes(s)) + geom_histogram(binwidth=1, color=&quot;red&quot;) 11.5 Overdispersion It’s very common to observe systems that disobey the Poisson. In other words, these are systems that are discrete counts in time or space, and thus whose very nature is frequency. But yet they are poorly fit by the Poisson distribution model. For example, the two graphs below. The first is a distribution of the counts of facebook friends of biostats students. Their average number of friends is 610. The second is a based upon a Poisson distribution whose lambda is 610. The model obviously is a poor fit for the data. Models are perfect, data are not. The friends distribution is said to be over-dispersed. The reason for this overdispersion is likely that it is a more complex system than what a Poisson would assume. A key assumption for a Poisson model is that the counts occur randomly. It seems quite likely that biostats students probably don’t choose friends randomly. survey &lt;- read.csv( &quot;datasets/precourse.csv&quot;) ggplot(survey, aes(x=friends)) + geom_histogram(aes(y=..density..), color=&quot;red&quot;, binwidth=50) x &lt;- c(0:5000) lambda &lt;- 610 df &lt;- data.frame(x, px=dpois(x, lambda)) ggplot(df, aes(x=x,y=px)) + geom_col(fill = &quot;red&quot;) + xlab(&quot;x&quot;) + ylab(&quot;p(x)&quot;) + labs(title = paste(&quot;dpois&quot;,&quot;(&quot;,&quot;lambda=&quot;, lambda,&quot;)&quot;)) "],
["normal.html", "Chapter 12 The Normal Distribution 12.1 dnorm 12.2 pnorm 12.3 qnorm 12.4 rnorm", " Chapter 12 The Normal Distribution 12.0.1 The Standard Normal If the variable \\(X\\) represents a population of normally distributed values with a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the variable \\(Z\\) \\[Z=\\frac{X-\\mu}{\\sigma}\\] has a standard normal distribution with a mean of 0 and a standard deviation of 1. Converting experimental sample data to standard normal z-scores is very common. Assume we have a sample of 210 peoples heights, whose sample mean = 169.9 and sd = 10.5. The tallest height in the sample is 205. The z-score of the tallest individual is derived using the sample parameters and is (205-169.9)/10.5 = 3.34. The z-score could be interpreted this way: the tallest individual is said to be 3.34 standard deviations taller than the mean height of the students in the sample. 12.1 dnorm dnorm is the normal probability density function in R: \\(p(x)=\\frac{e^\\frac{-(x-\\mu)^2}{2\\sigma^2}}{\\sigma\\sqrt{2\\pi}}\\) dnorm takes i) the scalar \\(x\\), ii) a mean and iii) sd as arguments and returns the value of the probability, or otherwise known as the probability density, for that scalar. \\(x\\) can be either a single value, or a vector comprised of many values. For example, assume the average height of an adult in the US is 168.8 cm with a standard deviation of 7.1 cm. Use dnorm to calculate the probability that a randomly selected US adult will be 176 cm tall: dnorm(x=176, mean=168.8, sd=7.1 ) ## [1] 0.03360041 Therefore, the probability that a randomly selected US adult would be 176 cm tall is 3.36%. The probable height of one specfic individual is usually not very useful or interesting to calculate. What’s a bit more useful is to see how the function operates over a range of values. For example, we can model the distribution of adult heights using dnorm. We might be interested in the fraction of adults whose heights are 176cm and taller compared to the rest of the population. To illustrate the answer with a plot, we pass a range of x values range = c(130, 210) into the dnorm function, rather than a single value. dnorm calculates probabilities over that full range. We can make a plot to visualize the answer, shading anybody as tall or taller than 176 in blue: range &lt;- c(130,210) pop &lt;- list(mean=168.8, sd=7.1) auc &lt;- c(176,210) ggplot(data.frame(x=range), aes(x)) + stat_function(fun=dnorm, args = pop, color = &quot;red&quot;) + stat_function(fun=dnorm, args = pop, xlim = auc, geom=&quot;area&quot;, fill=&quot;blue&quot;) + xlab(&quot;heights, cm&quot;) + ylab(&quot;p(heights)&quot;) Later on, we’ll use other functions to quantify the blue shaded area. Here’s plot of the standard normal distribution over a range of z-values that are 4 SD’s below and above the mean: z &lt;- seq(-4,4,0.1) mean &lt;- 0 sd &lt;- 1 df &lt;- data.frame(z, pz=dnorm(z, mean, sd)) ggplot(df, aes(x=z,y=pz)) + geom_line(color = &quot;red&quot;) + xlab(&quot;z&quot;) + ylab(&quot;p(z)&quot;) + labs(title = paste(&quot;dnorm&quot;,&quot;(&quot;,&quot;mean=&quot;, mean,&quot;,&quot;,&quot;sd=&quot;,sd,&quot;)&quot;)) 12.2 pnorm When a height value is given, calculating the probabilities for heights up to or greater than than that limit can be of considerable interest. pnorm is the R function for that…give it the value of a normally-distributed variable, such as height, and it returns a cumulative probability for the distribution on either side of that value. Thus, pnorm is called the normal cumulative distribution function in R: \\[p(x)=\\int^x_{-\\inf}\\frac{e^\\frac{-x^2}{2}}{\\sqrt{2\\pi}}\\] By default pnorm will return the cumulative value of the normal pdf up to the value of the input scalar. In otherwords, given the value of a variable, pnorm returns the probability of that value or less. However, this can be reversed by changing to the lower.tail=FALSE argument. In this case, pnorm returns something akin to a p-value. Given some value of a variable, pnorm returns the probability of that value or greater. The first line in the script below calculates the probability that a US adult will be less than or equal to 175.9 tall, which in this instance turns out to be 1 sd taller than the mean height. It returns a value of about 84%. The second line calculates the probability that a US adult will be greater than or equal to 175.9 cm tall. It returns a value of about 16%. pnorm(175.9, 168.8, 7.1, lower.tail = T) ## [1] 0.8413447 pnorm(175.9, 168.8, 7.1, lower.tail = F) ## [1] 0.1586553 Subtracting the latter from the form illustrates that about 68% of US adults will be within 1 standard deviation of the average US adult height: pnorm(175.9, 168.8, 7.1, lower.tail = T) - pnorm(175.9, 168.8, 7.1, lower.tail = F) ## [1] 0.6826895 About 95% of the values for any continuous, normally-distributed variable will be between 2 standard deviations of the mean: pnorm(q=2, mean=0, sd=1, lower.tail=T) ## [1] 0.9772499 pnorm(2, 0, 1, lower.tail=F) ## [1] 0.02275013 pnorm(2) - pnorm(2, lower.tail=F) ## [1] 0.9544997 (The script above illustrates a few of the different shorthands that can be taken working with R’s probability functions). Calculating “p-values”\" using pnorm Let’s go back to human heights. What’s the probability of an US adult being as tall or taller than 205 cm? Notice the lower.tail=F argument: pnorm(205, 168.8, 7.1, lower.tail=F) ## [1] 1.71095e-07 That’s a very low probability value, because the height is so extreme. Calculating percentiles using pnorm What is the height percentile of a 205 cm tall US adult? pnorm(205, 168.8, 7.1)*100 ## [1] 99.99998 12.3 qnorm qnorm is the inverse of the cumulative distribution function of a continuous normally-distributed variable. By default, qnorm takes a cumulative probability value (eg, a percentile) as an argument (along with the mean and sd of the variable) and returns a limit value for that continuous random variable. Here’s what the qnorm distribution looks like: p &lt;- seq(0.0, 1.0, 0.01) mean &lt;- 0 sd &lt;- 1 df &lt;- data.frame(p, z=qnorm(p, mean, sd)) ggplot(df, aes(p, z)) + geom_line(color = &quot;red&quot;) + xlab(&quot;p(z)&quot;) + ylab(&quot;z&quot;) + labs(title = paste(&quot;qnorm&quot;,&quot;(&quot;,&quot;mean=&quot;, mean,&quot;,&quot;,&quot;sd=&quot;,sd,&quot;)&quot;)) You’re probably very familiar with quantiles since percentiles are a class of quantiles. For example, if your standardized exam score is in the 90th percentile, then you did as well or better than 90% of test takers. If you forgot your score, but remember your percentile, you could use qnorm to return your specific test score…so long as you also know the mean and sd values of the test scores. Back to heights. What’s the height in cm of a US adult who is at the 99th percentile? What about the 5th percentile? qnorm(0.99, 168.8, 7.1) ## [1] 185.3171 qnorm(0.05, 168.8, 7.1) ## [1] 157.1215 The first script below returns the height of the 84th% quantile of US adults. 84% are about 175.9 cm tall or less. The 84th is the upper 1 SD quantile. The second script returns the height of the complement of the 84th quantile of US adults, by switching the default lower.tail argument. This complement is the lower 1 SD quantile. qnorm(0.84, 168.8, 7.1, lower.tail = T) ## [1] 175.8607 qnorm(0.84, 168.8, 7.1, lower.tail = F) ## [1] 161.7393 About two thirds of US adults are between 161.7 and 175.9 cm tall. 12.3.0.0.1 Confidence interval limits and qnorm The qnorm distributon has pragmatic utility for finding the limits for confidence intervals when using the normal distribution as a model for the data. In a standard normal distribution, the limits for the lower and upper 2.5% of the distribution are about \\(\\pm\\) 1.96 standard deviation units. Thus, the 95% confidence interval for standard normal z-values is -1.96 to 1.96. qnorm(0.025) ## [1] -1.959964 qnorm(0.025, lower.tail=F) ## [1] 1.959964 Based upon the average height and sd of US adults, the 95% confidence interval for US adult height is: paste(round(qnorm(0.025, 168.8, 7.1),1), &quot;to&quot;, round(qnorm(0.025, 168.8, 7.1, lower.tail=F),1)) ## [1] &quot;154.9 to 182.7&quot; The confidence interval means there is a 95% chance the true average US adult is within that range. Rember that height sample from near the very start of this document? It had 210 replicates, a mean = 169.9, sd = 10.5. What is the 95% confidence interval of US adult heights, based upon that sample? paste(round(qnorm(0.025, 169.9, 10.5),1), &quot;to&quot;, round(qnorm(0.025, 169.9, 10.5, lower.tail=F),1)) ## [1] &quot;149.3 to 190.5&quot; Drawing inference to the whole population on the basis of this sample, and assuming a normal model, there is a 95% chance the true US adult height is within this slightly wider range. The truth is, a better model to derive confidence intervals for normally-distributed samples is the t-distribution. 12.4 rnorm R’s rnorm is the random number generator function for the normal distribution. This has a lot of utlity in synthesizing data sets, for example, when running simulations. It returns random normally distributed values given size, mean and standard deviation arguments. For example, here is a randomly generated sample of 10 US adult heights, rounded to 1 significant digit. We’ll use this function a LOT in this course to generate normally-distributed data for various purposes. Here’s 10 random, simulated heights of US adults, rounded to the first digit to make it cleaner looking: round(rnorm(10, 168.8, 7.1), 1) ## [1] 168.8 177.2 174.7 175.9 168.6 177.4 172.1 165.0 164.3 172.1 12.4.1 Plotting histograms of some rnorm samples Histograms are a powerful way to explore the underlying structure in a dataset. They are a descriptive tool. Adjusting the bin parameters provides a way to evaluate the data. A histogram takes one variable, plotting its values on the x-axis while displaying the counts or the density of those values on the y-axis. The distribution of the data in a histogram is controlled by adjusting the number of bins, or by adjusting the binwidth. 12.4.2 Bins and Binwidth Think of bins as compartments. If using, for example 100 bins as a plotting argument, the range of the sample values are split, from the shortest to the tallest heights, into 100 evenly spaced…compartments. For normally distributed variables, the central bins are more dense or have more counts then those on the tails of the histogram, because values near the average are more common. A binwidth argument can be used instead of bin. Setting the binwidth to 10 on the graph below would look the same as setting the number of bins to 6. Adjusting bins or binwidths has the effect of rescaling the y-axis. 12.4.2.1 Histograms of counts vs density Because histograms are so commonggplot2 has a geom_histogram function to simplify creating the plots. The default argument creates a histogram of values of a random variable of interest. To create a histogram of densities, use a density aes argument in the function, instead as you’ll see below. Counts are the number of cases within a given bin. Density is the fraction of all counts within a given bin. Here are 1000 random, simulated heights, plotted as a histogram of counts. set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=168.8, sd=7.1)) ggplot(df, aes(s)) + geom_histogram(bins=100) Same thing, plotted as a probability density: set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=168.8, sd=7.1)) ggplot(df, aes(s)) + geom_histogram(aes(y = ..density..), bins=100) 12.4.2.2 Histogram with Distribution For an example of plotting a model along with the raw data, here’s a density histogram of the rnorm sample, plotted along with a dnorm distribution model, given the population mean and sd. By eyeball, the model seems to be a decent fit for the sample. But it is not a perfect fit. It’s not the model’s fault. Models are perfect, data are not. How does the sample size affect how well the model fits the data? set.seed(1234) df &lt;- data.frame(s=rnorm(n=1000, mean=0, sd=1)) ggplot(df, aes(s)) + geom_histogram(aes(y = ..density..), bins=100) + stat_function(fun = dnorm, args=list(mean=0, sd=1), color = &quot;red&quot;, size = 2) "],
["categorical.html", "Chapter 13 Statistics for Categorical Data 13.1 Types of categorical data 13.2 Exact v Asymptotic Calculations of p-values 13.3 Overview of the types of hypothesis testing 13.4 Comparing proportions 13.5 Exact tests for two proportions 13.6 Goodness of fit Tests 13.7 Contingency Testing 13.8 Doing a priori power analysis for proportion tests 13.9 Power analysis functions for proportion tests 13.10 Graphing Proportions", " Chapter 13 Statistics for Categorical Data library(PropCIs) library(tidyverse) library(binom) library(pwr) library(statmod) library(EMT) Biomedical research is full of studies that count discrete events. A common mistake made by many researchers is to use statistics designed for measured variables on discrete count variables. For example, they transform count data into scaler measures (eg, percents, folds etc) and then apply statistics designed for continuous variables to events that are fundamentally discrete by nature. The problem with that is there are inherent differences in the behaviors of continuous and discrete variables. Therefore, it is important to recognize 13.1 Types of categorical data What proportion of cells express a specific antigen and does an experimental treatment cause that proportion to change? What proportion of rats treated with an anxiolytic drug choose one chamber over others in a maze test? How many people who express a certain marker go on to have cancer? In these three scenarios the primary data are counts. All of the study results have integer values. The counts are categorized with variable attributes, thus they are called categorical data. In fact, the three scenarios above are very different experimental designs. The first represent experiments that compare simple proportions. The second compare freqeuncies, and the third is an association study. The analysis of these require using a common suite of statistical tools in slightly different ways. Broadly, all of these tools boil down to dealing with proportions. A few types of proportions (eg, odds ratio, relative risk) that can be calculated from these datasets are sometimes used as effect size parameters. Other times we’d use a confidence interval as a way of conveying an effect size. Statistical tests are then used to evaluate whether these effect sizes, or frequencies, or simple proportions, are extreme enough relative to null counterparts so that we can conclude the experimental variable had some sort of effect. 13.1.1 Proportions We might… Inactivate a gene hypothesized to function in the reproductive pathway. To test it, mating triangles would be set up to count the number of female mice that become pregnant or not. Implant a tumor in mice, before counting the number of survivors and non-survivors at a given point later. Mutate a protein that we hypothesize moves in and out of an intracellular compartment, before staining cells to count the number of cells where it is and is not located in a compartment of interest. Each of the examples above have binomial outcomes…pregnant vs not, dead vs alive, or inside vs outside. In each case above, both the succesful and the failed events are counted in the experiment. A proportion is a simple ratio of counts of success to counts of failures. 13.1.2 Frequencies Other kinds of counted data occur randomly in space and time. The examples below illustrate this. Note how only the number of events are recorded, rather than categorizing them as successes or failures. These counts therefore have the statistical property of frequency, such as counts per time or counts per volume or counts per area. We can… Expose a neuron to an excitatory neurotransmitter, then count the number of times it depolarizes over a given period of time. In a model of asthma, count the number of immune cells that are washed out in a pulmonary lavage protocol after an immunosuppressive agent. The key difference for these are that their non-event counterparts are meaningless. For example, it is not possible to measure the number of depolarizations that don’t occur, or know the number of immune cells that don’t wash out in the lavage. 13.1.3 Associations Lastly, the examples below illustrate the design of association studies, which are based upon, according to the null hypothesis, independent predictors and outcomes. Here are some examples of association study designs: You might wish to identify causal alleles associated with a specific disease phenotype by counting the number of people with and without the disease, who have or don’t have a particular allelic variants. determine if a history of exposure to certain carcinogens is associated with a higher risk of cancer by counting people with cancer who have been exposed. know if a drug treatment causes a higher than expected frequency of a side-effect by counting the people on the drug with the side-effect. In the simplest (and most general) case, association studies are 2X2 in design: A predictor is either present or absent as the row factor, and an outcome was either a success for a failure as the column factor. Subjects are categorized into groups on the basis of where they fall in the 4 possible combinations that such 2X2’s allow for. It should be noted that higher order association studies are also possible, which can be either symmetric (eg, 3x3) or non-symmetric (eg,) 9X2, 2x3, and so on. 13.1.4 Statistics Covered Here Confidence intervals of proportions One-sample proportions test Two-sample proportions test Goodness of fit tests Tests of associations Power analysis of proportions (including Monte Carlo simulation) Plotting proportions with ggplot2 13.2 Exact v Asymptotic Calculations of p-values The statistical tests for hypotheses on categorical data fall into two broad categories: exact tests (binom.test, fisher.test, multinomial.test) and asymptotic tests (prop.test chisq.test). Exact tests calculate exact p-values. That’s made possible using factorial math. The prop.test and chisq.test generate asymptotic (aka, approximate) p-values. They calculate a \\(\\chi^2\\) test statistic from the data before mapping it to a \\(\\chi^2\\) probability density function. Because that function is continuous, the p-values it generates are asymptotically-estimated, rather than exactly calculated. 13.2.1 Choosing exact or asymptotic As a general rule, given the same datasets and arguments, exact and approximate hypothesis test functions will almost always give you p-values that differ, but only slightly. That’s usually not a problem unless you’re near a threshold value. Typically, an integrity crisis is evoked when that happens: “Which is”right???\" Do you p-hack and choose the favorable one or not? You should use the test you said you’d use when you first designed the experiment. And if you didn’t pre-plan…or at least have some idea about where you are going…recognize that the exact tests are more accurate. Another issue that arises is how well the tests perform with low count numbers. For example, as a rule of thumb, avoid using the chisq.test when the data have counts less than 5 in more than 20% of the cells because the accuracy of the chisq.test is less at low cell counts. Use an exact test instead. 13.3 Overview of the types of hypothesis testing We’ll go through each below in more detail, emphasizing practical experimental design and interpretation principles. 13.3.1 Proportion analysis You can learn a lot about experimental statistics by thinking about proportions. So a lot of time is spent on it. Proportions are derived from events that can be classified as either successes or failures. Sometimes we want to compare simple proportions to decide if they are the same or not. 13.3.2 Goodness of fit testing We do this when we want to compare the frequency distribution we observe in an experiment to the null expectation for that frequency distribution. 13.3.3 Contingency Analysis Contingency analysis, otherwise known as tests of independence, are very different from goodness-of-fit test and simple proportion tests, in design and in purpose. They allow us to ask if two (or more) variables are associated with each other. Unlike a lot of the statistics we’ll deal with, there is a hint of a predictive element associated with these types of studies because the effect sizes we use to explain their results are related to odds and risk and likelihood. Which is not to say that we couldn’t use the same predictive concepts in proportions and goodness of fit testing. Contingency tests are very common in epidemiology and in clinical science. You recognize by their names as cohort studies, case control studies, and so forth. 13.4 Comparing proportions In their simplest use, the tests here can be used to compare one proportion to another. Is the proportion of successes to failures that results from a treatment different from the proportion that results from control? We’ll dive into this further below. 13.4.1 A Mouse T Cell Pilot Experiment: The Cytokine-inducible antigen gradstudin Let’s imagine a small pilot experiment to see how a cytokine affects T cells. This is a very crude experiment designed mostly to illustrate some principles. A cytokine is injected into a single mouse. There is no control injection, just one mouse/one cytokine injection. A time later, blood is harvested from the mouse to measure an antigen on T cells. Let’s call the antigen gradstudin. Assume a method exists to detect T cells in the sample that express gradstudin and don’t express gradstudin. That method implies some nominal criteria are established to categorize T cells as either expressing gradstudin or not. FACS machines are very useful for this. The machine typically produces continuous fluorescent data, where intensity is proportional to gradstudin levels. But we don’t care about the magnitude of the expression level, we just care whether it is there or not. Based upon our scientific expertise, we establish cutoff gating criteria above which fluoresence == gradstudin is present. The machine therefore returns simple counts of both gradstudin-positive and gradstudin-negative cells. 13.4.2 Calculating Proportions Here’s the data, counts of cells expressing and not expressing gradstudin. It is a very simple dataset: pos &lt;- 5042 neg &lt;- 18492 A proportion is the count of a particular outcome relative to the total number of events. It’s customary to use the number of successes as the numerator. Whereas its customary to refer to the total number of events, n, as the trial number rather than as sample size, but they mean the same thing. n &lt;- pos+neg #trial size prop &lt;- pos/n prop ## [1] 0.2142432 13.4.3 What A Proportion Estimates This sample proportion is descriptive statistic. It serves as a point estimate of the true proportion of the population we sampled. The only way to know the true proportion would be to count every T cell in every drop of the subjects blood! This point estimate is statistically valid if our sample meets two conditions. First, that this is a random sample of the T cells in the subject’s blood. Second, if we consider every T cell in the sample as statistically independent of every other T cell. We can safely assume those conditions are met. Strictly, as an estimate this proportion only infers the population of blood borne T cells in that one subject. We really can’t generalize much further than that, including the composition of T cells in sequesterd compartments (thymus, nodes, etc). Which is fine for our purposes now because we’re trying to keep this simple. 13.4.4 Confidence Intervals of Proportions Confidence intervals (CI) have features of both descriptive and inferential statistics. 13.4.4.1 Definition of a 95% CI The 95% CI for a sample proportion represents a range of proportions within which 95% of the time we would expect the true population proportion. There’s a lot going on there. The value of the proportion we measured in the sample is a mathematical fact that is not in dispute. It is what it is. The question is, what does it represent? Although there might be some error associated with measuring it, our single sample offers no real information about what that error might be. As an n=1 sample, there is no variation! What is unknown is the true proportion of gradstudin+ T cells in the population we sampled. CI’s are designed to give us some insights into that unknown. 95% CI’s are a range estimate of what that true population proportion might be. CI’s are calculated in part upon the quality of the point estimate. In the case of proportions, the quality of the point estimate is driven by the size of the sample, the number of counts that are involved in calculating the proportion. As you might imagine intuitively, the more counts we have in the sample, the more confidence we should have that our proportion provides a good estimate of the population’s proportion. 13.4.4.2 Calculating CI with R The PropCIs package offers several ways to calculate a CI. Is one better than the other? Sometimes, yes. For now, let’s not worry about that. Wilson’s score interval with continuity correction] is suggested as the most accurate for proportions. When publishing it is important is to state which CI method is used. Other methods are more commonly used than Wilson’s because they gained traction as being easier to compute by hand, and old habits die slowly. Taking the data on cytokine induced gradstudin+ T cells, the chunk below illustrates how to use PropCIs to derive a Wilscon score interval-based 95% CI: scoreci(pos, n, conf.level=0.95) ## ## ## ## data: ## ## 95 percent confidence interval: ## 0.2090 0.2195 13.4.4.3 Interpretation of a CI The value of our sample proportion, 0.214, falls within this 95% CI. That’s not a big surprise, given the 95% CI was calculated from our proportion! On the basis of the sample proportion, we can conclude from this CI that there is a 95% chance the true proportion of gradstudin positive T cells falls within this very narrow range. 13.4.4.4 Using the CI as a quick test of statistical significance. Let’s say, for example, that we have tremendous experience and great scientific reason to expect to see under normal conditions that only 15% of T cells would be gradstudin-positive normally. Does our sample proportion differ from that expectation? Since a proportion of 0.15 is not within the 95% CI calculated above, we can conclude that the cytokine-induced sample proportion differs from this expectation at the 5% level of statistical significance. We just did a statistical test, without running any software (sorta) or generating any p-values!! And it is perfectly valid inference. 13.4.5 A One-Sample Proportion Test We’ll use prop.test to run a test that generates a p-value to decide if the sample proportion we have above differs from 0.15. 13.4.5.1 Hypothesis Tested in a One-Sample Proportion Test In this test the sample antigen-positive proportion is compared to a theoretical proportion. If the typical proportion of antigen-positive T cells within a blood sample is 15%, is the result after cytokine treatment different from this proportion? Let’s say that our scientific hypothesis going into all this is that the cytokine induces the antigen on T cells. Since we are predicting an increase, we should establish a one-sided alternative (thus using greater as an argument in prop.test below) as our statistical hypothesis. Our statistical hypothesis is the null. We’ll decide whether or not to reject the null on the basis of the test results. Philosophically, we’re using a falsification method. The statistical alternate hypothesis: \\(\\pi&gt;15\\%\\) The statistical null hypothesis: \\(\\pi\\le15\\%\\) We use Greek notation to represent the ‘true’ population proportion. This reminds us that a statistical hypothesis is an inferential test about the population proportion. Again, there is no question that the sample proportion differs from a proportion of 15%. 21% != 15%. That’s a simple numerical fact. Statistical tests are not necessary to make that assertion. On the basis of the sample proportion p, we’d like to draw inference on the composition of all of the T cells in the blood of the subject. Thus, the sample p is only an estimate of a true \\(\\pi\\) (which we notate using Greek letters). Statistical testing allows us to generate some insight into the reliability of our estimate. The chunk below lays out these arguments using R’s prop.test function: #pos and n in the test arguments are objects that were defined above! prop.test( pos, n, p=0.15, alternative = &quot;greater&quot;, conf.level = 0.95, correct = TRUE ) ## ## 1-sample proportions test with continuity correction ## ## data: pos out of n, null probability 0.15 ## X-squared = 761.29, df = 1, p-value &lt; 2.2e-16 ## alternative hypothesis: true p is greater than 0.15 ## 95 percent confidence interval: ## 0.2098559 1.0000000 ## sample estimates: ## p ## 0.2142432 13.4.5.2 Interpreting one-sample prop.test output Like all statistical tests, this one is evaluated under the assumption that the null hypothesis is true. We use the test outcome to decide whether the null hypothesis should be rejected. The prop.test conducts a chi-square analysis. The value of \\(\\chi^2\\) for this sample is very large. The p-value represents the probability of obtaining a \\(\\chi^2\\) value as larger or larger then what is calculated from our sample. If the null hypothesis is true in this case, the probability of a \\(\\chi^2\\) value as large or larger than we obtained is 2.2e-16, which is very, very low. The 95% CI is 0.2098 to 1.0. There is a 95% chance the population proportion is greater than 0.2098. The reason it differs from the Wilson’s CI calculated above is that we used greateras a one-sided hypothesis argument in the prop.test. 13.4.5.3 How to write this up The proportion of gradstudin positive T cells after cytokine treatment in the subject differs from an expected value of 0.15 (one-sided one-sample proportions test, p-value=2.2e-16, 95% CI = 0.209 to 1.0) Notice how I didn’t say “significantly” or “statistically significantly” or some such. Whether an outcome is signficant or not should be a scientific assertion, rather than statistical. 13.4.5.4 An exact test for one proportion R’s binom.test function is an exact test for whether a proportion differs from a theoretical expectation. It compares proportions using an entirely different procedure. As a one-proportion test the binom.test gives an exact p-value derived from the binomial distribution, whereas the prop.test gives approximate p-values because it uses the chi-square distribution. That distinction is hard to see with our examples here, but the differences will become more noticable when analyzing samples with far fewer events. Here’s the binomial test run on two different proportions. In each, the test is comparing the experimental proportion to the proportion value of 0.15. binom.test(pos, n, p=0.15) ## ## Exact binomial test ## ## data: pos and n ## number of successes = 5042, number of trials = 23534, p-value &lt; ## 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.15 ## 95 percent confidence interval: ## 0.2090155 0.2195416 ## sample estimates: ## probability of success ## 0.2142432 binom.test(x=567, n=1778, p=0.15) ## ## Exact binomial test ## ## data: 567 and 1778 ## number of successes = 567, number of trials = 1778, p-value &lt; ## 2.2e-16 ## alternative hypothesis: true probability of success is not equal to 0.15 ## 95 percent confidence interval: ## 0.2972673 0.3411272 ## sample estimates: ## probability of success ## 0.3188976 13.4.6 Comparing Two Proportions We can stick with the T cell-cytokine-gradstudin scenario, but let’s change up the experiment a tad. Let’s imagine we’ve withdrawn a sample of blood from a subject and enriched for T cells. Half of the sample is exposed in vitro to a cytokine for a few hours. The other half is exposed to vehicle as a control. We count the gradstudin-positive and gradstudin-negative T cells in both groups. We now have a predictor group at two levels (treatment = vehicle or cytokine) and an outcome group at two levels (antigen = positive or negative) 13.4.6.1 Hypothesis Tested Let’s test the hypothesis that the proportion of postive T cells in the two samples differ. The choice is not to test whether one proportion is greater than the other. We just want to know if they differ. The statistical hypotheses here differs from the one sample hypotheses in two ways. First, notice how we’re comparing the population proportion of cytokine- to that for vehicle-treatment. Second, we’re making this a two-tailed (two.sided) test instead of one-tailed (greater). The statistical alterate hypothesis: \\(\\pi_c\\ne\\pi_v\\) The statistical null hypothesis: \\(\\pi_c=\\pi_v\\) A second way of writing these hypotheses to say the same thing: Alternate: \\(\\pi_c-\\pi_v\\ne0\\) Null: \\(\\pi_c-\\pi_v=0\\) 13.4.6.2 Running the test Let’s say that here are the outcome data: gradstudin-positive, gradstudin-negative, total Cytokine-treated: 567, 1211, 1778 Vehicle-treated: 412, 1485, 1897 The data can be dumped directly into prop.test. Normally we’d run this at 0.95 confidence level (type1 error of 5%). Let’s say we have good scientific reasons to run this test at a much more stringet threshold level for type1 error, because we’re really hoping to avoid a false positive: prop.test( x=c(567, 412), n=c(1778, 1897), conf.level=0.9999999 ) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: c(567, 412) out of c(1778, 1897) ## X-squared = 48.066, df = 1, p-value = 4.121e-12 ## alternative hypothesis: two.sided ## 99.99999 percent confidence interval: ## 0.02364902 0.17977620 ## sample estimates: ## prop 1 prop 2 ## 0.3188976 0.2171850 13.4.6.3 Interpretion of Two-Sample proportions test output This test is evaluated under the assumption that the null hypothesis is true. The test results helps us decide whether the null hypothesis should be rejected. We’ll do that if the p-value is less than our type1 error threshold. The prop.test conducts a chi-square analysis using the Yates continuity correction. This \\(\\chi^2\\) value is very large; extremely large. The p-value represents the probability of obtaining a \\(\\chi^2\\) value as larger or larger than that by chance. If the null hypothesis is true in this case, the probability of that sized \\(\\chi^2\\) value is 4.12e-12, which is very low, and still much lower than our type1 error threshold. We can reject the null. If we subtract prop2 from prop1 we get a value of about 0.1017, as a point estimate for the difference between the two proportions. The 99.99999% CI for the difference between two proportions does not include the value of 0. Since the 99.99999% CI does not overlap with 0, we can conclude from it alone that there is a difference between the two proportions. Even at this extremely high confidence level!! 13.4.6.4 Write Up The proportion of gradstudin positive T cells after cytokine differs from that in vehicle treated cells (two-sided two-sample proportions test, p-value=4.12e-12, 99.99999% CI for proportion difference = 0.023 to 0.179) Note how we don’t say “statistically significantly different”. 13.5 Exact tests for two proportions An alternative to prop.test to compare two proportions is the fisher.test, which like the binom.test calculates exact p-values. The fisher.test requires that data be input as a matrix or table of the successes and failures. We need to make a matrix of the data first, then perform the fisher.test on that matrix: gradstudin-positive, gradstudin-negative, total Cytokine-treated: 567, 1211, 1778 Vehicle-treated: 412, 1485, 1897 Notice below that here we’re entering the successes and the failures in the matrix. We’re tagging the matrix with some dimension names so it doesn’t get confusing. M &lt;- matrix( c(567, 412, 1211, 1485), nrow=2, dimnames = list( Treat=c(&quot;Cytokine&quot;, &quot;Vehicle&quot;), Antigen=c(&quot;Positive&quot;, &quot;Negative&quot;) ) ) M ## Antigen ## Treat Positive Negative ## Cytokine 567 1211 ## Vehicle 412 1485 fisher.test(M) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: M ## p-value = 3.433e-12 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.451760 1.962188 ## sample estimates: ## odds ratio ## 1.687312 Notice the output differs from the prop.test. In adddition to a p-value, the Fisher test produces an odds ratio and its confidence interval. The p-value leads to the same result and write-up as for the two proportion test. The odds ratio could be interpreted like this: The cytokine increases the odds of gradstudin+ T cells by 1.687 compared to vehicle treatment (Fisher’s Exact Tet for Count Data, p = 3.433e-12, OR 95% CI = 1.45 to 1.96). 13.6 Goodness of fit Tests Goodness-of-fit tests are useful for testing hypotheses about patterns of counts in time or space…whether the distribution of their observed frequencies differs from expectations of a null case. In other words, do they occur in a non-random pattern? There is no independent variable in these tests. The shape of these datasets is either as 1 row or 1 column, where every cell is a time or space and the cell value is the the number of counts that occured in that time or space. Either the multinomial.test (for exact p-values) or the chisq.test (for approximate p-values) can be used for Goodness-of-fit testing. The latter is most commonly used. These designs compare the disribution of events to a hypothetical (or expected) model null distribution of those events. These expected counts are entered in the test script as a prop or p argument. This can be confusing. It’s important to recognize you should enter a vector of null probabilities in p or prop! Don’t enter the counts you hope to see if the test were positive!!! Say we had a spatial memory test in which 28 independent subjects are placed into a maze for testing (one at a time) and we count which of 4 chambers they enter first. They do so at the following frequencies: A=14, B=3, C=7, D=4. Does this frequency distribution differ from the null expectation, A=7, B=7, C=7, D=7, where no chamber is more likely to be entered than another? Failure is not an option in this design! Only successes are counted. Given enough time, a subject will always choose a chamber. If one fails the task, it must be censured. Let’s test this at the 5% type1 error threshold: 13.6.1 An Exact Goodness of Fit test NB:The multinomial.test function requires you to assert the null frequency distribution explicitly and as fractions whose sum is 1. x &lt;- c(A=14, B=3, C=7, D=4) prob &lt;- c(A=.25, B=.25, C=.25, D=.25) multinomial.test( x, prob=prob ) ## ## Exact Multinomial Test, distance measure: p ## ## Events pObs p.value ## 4495 1e-04 0.0235 Note on the multinomial.testoutput: Please see this site for further information on what is represented by events (its a combination result–a metric of the computation it took to do this) and pObs (its a multinomial probability). We’re only intersted in the p-value, since we’re using this function as an exact goodness-of-fit hypothesis test for the null hypothesis. Why use an exact test rather than a chisq.test? Because we have two cells in the dataset with counts &lt; 5! An exact p-value will be more accurate. The test compares our sample frequency distribution to that in the null model. We actually wrote the latter explicitly in the function argument: null is uniform distribution–the subjects are equally likely to enter each chamber. H0: The probability of choice is equal for each chamber. \\(\\pi_A=\\pi_B=\\pi_C=\\pi_D\\) H1: The probability of choice is not equal for each chamber. \\(\\pi_A\\ne\\pi_B\\ne\\pi_C\\ne\\pi_D\\) (Note: this is an omnibus test. It doesn’t explicity tell us which chambers are preferred by the subjects.) Because the p-value is less than 0.05, we reject the null hypothesis and conclude that the chamber choice is not equitable across the four options. 13.6.1.1 Write up In a 4 chamber maze test, the subjects displayed a clear, non-uniform chamber preference (Exact Multinomial Test, n=28, p=0.0235) Note how this implies the null hypothesis. 13.6.2 An Approximate Goodness of Fit test The \\(\\chi^2\\) test of the same data is really simple to execute. It offers the same conclusion, but note how the p-value is very different. Note also that we didn’t enter the null frequency argument. The chisq.testfunction will coerce the null distribution if it is not entered as an argument explicitly, as you can see from the output for the second line. If for some reason to test against a non-uniform null distribution, you’ll need to write that in your argument explicitly (eg, p = c(A=0.5, B=0.25, C &amp; D = 0.125). chisq.test(x) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 10.571, df = 3, p-value = 0.01428 chisq.test(x)$expected ## A B C D ## 7 7 7 7 13.6.2.1 Write up The interpretation is no different than for the exact test. The write up is: In a 4 chamber maze test, the subjects displayed a clear chamber preference (Chi square test for uniform probabilities, \\(\\chi^2\\)=10.571, df=3, p=0.01428) 13.7 Contingency Testing Contingency testing is for deciding whether two or more variables are associated or not. These either explicitly (ie, when using fisher.text) or implicity (ie, when using chisq.test) use ratio’s of proportions–the odds ratio, or relative risk, or the likelihood ratio, or sometimes other proportions–as parameters that express the magnitude of these associations. In other words, the hypothesis test asks whether these ratio’s of proportions are more extreme than the null (which would be 1). Let’s take the cancer marker data from the contingency analysis lecture. As you recall, a marker has been discovered that is hoped to be strongly associated with cancer. 100 people were tested for whether or not they have the marker, and whether or not they go on to have cancer. We’ll create a simple matrix then pass it through the fisher.test function to illustrate the procedure and interpretation. x &lt;- matrix( c(14, 16, 6, 64), ncol=2, dimnames = list( Marker=c(&quot;Present&quot;, &quot;Absent&quot;), Cancer = c(&quot;Present&quot;, &quot;Absent&quot;) ) ) x ## Cancer ## Marker Present Absent ## Present 14 6 ## Absent 16 64 fisher.test(x) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: x ## p-value = 3.934e-05 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 2.762514 33.678765 ## sample estimates: ## odds ratio ## 9.061278 #more argument customization than this is possible 13.7.1 Intepretation of Contingency Results The odds of a person with the marker having cancer are 9.06 times greater than that for those who don’t have the marker. There is a 95% chance the true odds ratio in the population is between 2.76 and 33.68. There is an association between the presence of this marker and the probability that cancer occurs. 13.7.2 Write Up The large OR indicates the presence of this marker is strongly associated with cancer (n=100, OR = 9.06, 95% CI = 2.76 to 33.68, Fisher’s Exact Test for Count Data, p = 3.934e-05). The word “strongly” is used to emphasize the effect size, which is OR, not the smallness of the p-value. Here are the other tests you might use to conduct for a contingency analysis, to illustrate how they differ: ` x &lt;- matrix( c(14, 16, 6, 64), ncol=2, dimnames = list( Marker=c(&quot;Present&quot;, &quot;Absent&quot;), Cancer = c(&quot;Present&quot;, &quot;Absent&quot;) ) ) x ## Cancer ## Marker Present Absent ## Present 14 6 ## Absent 16 64 prop.test(x) ## ## 2-sample test for equality of proportions with continuity ## correction ## ## data: x ## X-squared = 16.741, df = 1, p-value = 4.284e-05 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.2496194 0.7503806 ## sample estimates: ## prop 1 prop 2 ## 0.7 0.2 chisq.test(x) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x ## X-squared = 16.741, df = 1, p-value = 4.284e-05 chisq.test(x, correct=F) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 19.048, df = 1, p-value = 1.275e-05 First, note that the prop.test is just the chisq.test. You get the same \\(\\chi^2\\) value and p-value for each. They just differ in parameter output and input options. Second, note how turning off the Yates continuity correction changes the \\(\\chi^2\\) value and p-value. That’s to be expected, it changes the calculation! Both the prop.test and chisq.test use Yates by default. The best way to think about Yate’s is that it acts as a smoothing function to take off some of the jigger in the calculation of the \\(\\chi^2\\) value. 13.7.3 Interpretation of \\(\\chi^2\\) output There is an association between the presence of this marker and the probability that cancer occurs. We could take the prop test’s calculation of the proportions and their difference, along with the 95% CI of their difference and make some hay out of that (the probability of getting cancer with the marker is 70%, and without the marker is 20%). But it’s more customary to use the odds ratio or relative risk rather than differences between probabilities to make effect size assertions. 13.7.4 Write Up You would want to derive the odds ratio and its 95% CI, even though the \\(\\chi^2\\) test doesn’t produce it for you. The easiest way to do that is with fisher.test. Having that: The large OR indicates the presence of this marker is strongly associated with cancer (n=100, OR = 9.06, 95% CI = 2.76 to 33.68, Pearson’s Chi-square test with Yate’s continuity correction, p = 4.284e-05). As before, the word “strongly” is used to emphasize the effect size, which is the OR, rather than the extremeness of the p-value. 13.7.5 Which contingency test is best? With so many options, the question that always arises is which is best to use for contingency analysis? The answer is, * make this decision in advance and use what you said you would use before you started the experiment. * when it comes to p-values, are you an exactophile or an exactophobe? * for datasets with low cell numbers (eg, counts less than 5 in a cell), exact tests tend to provide more accurate p-values. * the fact that fisher.test generates the OR and its CI is very, very convenient. I prefer the fisher.exact test. However, in R you’ll need to understand how to configure its arguments to get it to work on higher dimension contingency tables (eg, 2x3, 3x2, 3x3, etc). 13.7.6 Higher dimension contingency analysis Not infrequently we have studies with many levels of a predictor variable and two outcomes (eg, 7x2), or two predictors and more than two outcomes (eg, 2x3). For these, you’ll find the chisq.test works fairly automagically. In contrast, you’ll need to customize arguments in fisher.test for it to pass in anything other than a 2x2 matrix. Furthermore, with dimensions higher than 2x2 there is more than a single odds ratio or relative risk or likelihood ratio in higher dimensions to be computed. A step-wise approach for these more complex analyses is to first run an omnibus chisq.test on the intact dimension. If the test is positive (low p-value), analyze 2x2 segments of the grid post hoc using the fisher.test to derive odds ratios and to see which of the proportion ratio’s explain the significance. Such post hoc analyses must include correction for multiple comparisons (eg, the Bonferroni correction) when drawing inference. To do this, pass a vector of p-values into the p.adjust function. 13.7.7 Other experimental designs involving categorical data Imagine an experiment to compare two or more conditions (eg, placebo v drug, wildtype vs mutant) and the outcome variable is discrete (eg, frequency counts or success and failure counts). The experiment involves several independent replications. For example, cell depolarizations are counted both in the absence or presence of a condition on several different occassions. Table 13.1: Hypothetical replicates comparing depolarizations in mutant and wildtype cells should be analyzed using poisson regression. replicate predictor counts one wildtype 87 two wildtype 102 three wildtype 105 one mutant 125 two mutant 126 three mutant 139 Alternately, the fraction of cells in a culture dish that have died in the absence or presence of a condition is repeated a handful of times. Table 13.2: Hypothetical replicates comparing in mutant and wildtype cells should be analyzed using poisson regression. replicate predictor alive dead one wildtype 30 67 two wildtype 33 73 three wildtype 37 76 one mutant 65 38 two mutant 56 36 three mutant 62 42 The key distinction here, compared to what’s been discussed in this chapter up to this point, is that within each replicate, all of the events are intrinsically-linked. Through replication we’re establishing whether the events are repeatable. Logistic (for dichotomous data) or poisson (for frequency data) regression are the appropriate analytical tools for these designs. These involve using the generalized linear model, conducted with the function glm or the function glmer (for so-called mixed models). These are discussed in the logistic regression chapter. 13.8 Doing a priori power analysis for proportion tests Power analysis should be done before starting an experiment. The purpose of a conducting power calculations a priori is to determine the number of trials, or subjects or sample size, to use for the study. This is a two step process. Step 1: Using scientific judgement, decide what is the value of a null proportion and an alternate that you think would be a scientifically meaningful proportion to observe. You need to have some insight into the system you’re studying to make these calls. What’s important is to establish an expectation of what a minimally scientifically significant outcome would look like. Step 2: Calculate the number of subjects (or trials) you’ll need to study, given these proportions (and also given some type1 and type2 error tolerances). There are several options in R for the second step. In the examples below, we’re declaring a 5% difference between the null (0.15) and alternate (0.20) proportions would be a scientifically meaningful. We’re also using 5% for type1 error and 20% for type2 error (80% power) as tolerance thresholds. 13.9 Power analysis functions for proportion tests The function pwr.p.test is for one-sample proportion tests. The calculations below return a sample size n that should be used in the study, given a null and an alternate proportions, in addition to error rates. NB: Since takes a Cohen’s effect size as an argument, you 1st must calculate a Cohen effect size, h, given the alternate and null proportions you expect, using Es.h(). Then plug that effect size into the power calculator. h &lt;- ES.h(0.2, 0.15) h ## [1] 0.1318964 pwr.p.test( h, sig.level=0.05, power=0.8, alternative=&quot;two.sided&quot; ) ## ## proportion power calculation for binomial distribution (arcsine transformation) ## ## h = 0.1318964 ## n = 451.1706 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided binom.power is a function from the binom package. Instead of returning sample size, this function returns power, given sample size. You iterate through (by hand) entering sample sizes until it returns an acceptable power. Then run the experiment at that sample size. Note that it doesn’t give exactly the same result as pwr.p.test. The calculation differs, but the result is close. binom.power( 0.2, n=451, p=0.15, alpha=0.05, alternative = &quot;two.sided&quot;, method=&quot;exact&quot; ) ## [1] 0.7908632 To estimate sample size needed for a two-sample proportion test design, use the power.prop.test function. power.prop.test( p1=0.15, p2=0.2, sig.level = 0.05, power=0.8 ) ## ## Two-sample comparison of proportions power calculation ## ## n = 905.3658 ## p1 = 0.15 ## p2 = 0.2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Finally, the statmod package has the power.fisher.test, which returns the power for a Fisher’s exact test, given arguments of proportion, trial size and type1 error. Note how it is in close but not exact agreement with power.prop.test. power.fisher.test( 0.15, 0.2, 905, 905, 0.05, alternative = &quot;two.sided&quot; ) ## [1] 0.81 13.9.0.1 Monte Carlo power simulations Monte Carlo’s are very simple. The basic gist is to simulate and test a very large number of experiments. Each of these experiments is comprised of a random sample of some size, corresponding to your minimal effect size you define as scientifically meritorious. These are run through the test of significance, to calculate a p-value. The fraction of simulations that are “hits”–for example, that have p-values &lt; 0.05, is the power! Simulations are re-run by adjusting the sample size until a desired power is achieved. That’s the sample size you’d use in a real experiment! The question the script addresses is this: What is the power of an experiment, given this trial size n, the null and alternate proportions evaluated, and the type1 error threshold? If n is too low, the test will return a power below 0.8 meaning it is not adequetely powered to test the difference between the null and alternate proportions. Iterate through some a few sample sizes (n) until you arrive at an acceptable value for power. 13.9.0.1.1 monte carlo simulation for prop.test power #these are the initializers #number of experiments to simulate, each of trial size n sims &lt;- 1000 #expected null proportion null &lt;- 0.15 #expected minimal effect proportion alternate &lt;- 0.20 #binomial trial size, just a guess n &lt;- 450 #type 1 error threshold alpha &lt;- 0.05 #s1 is a random sample vector #each value is the number of #successes observed in a trial #of size n, given the alternate proportion. #it simulates the outcome of one experiment &quot;sims&quot; times s1 &lt;- rbinom(sims, n, alternate) #t1 is a vector of p-values, #derived from a one sample proportion test #on each of the values in s1. #read from inside the function to see the logic t1 &lt;- unlist( lapply( s1, function(s1){ prop.test( s1, n, null, alternative=&quot;two.sided&quot;, conf.level=1-alpha, correct=T)$p.value } ) ) power &lt;- length(which(t1 &lt; alpha))/sims power ## [1] 0.789 13.10 Graphing Proportions (needs improvement…add mosaic plots) Here’s a few ggplot2-based ways of visualizing proportion data. First thing is to create a dataframe of the proportion data since data fed into ggplot2 must be in dataframe format. prop.df &lt;- data.frame( group=c(&quot;positive&quot;, &quot;negative&quot;), value=c(pos, neg) ) prop.df ## group value ## 1 positive 5042 ## 2 negative 18492 13.10.0.0.1 Simple stacked bar chart ggplot( prop.df, (aes(x=&quot;&quot;, y=value, fill=group) ) ) + geom_bar(stat=&quot;identity&quot;) 13.10.0.0.2 Side-by-side bar chart Note: There is no error to report. There’s no variation. Cells were classified as either having or not having the antigen. ggplot( prop.df, (aes( x=group, y=value, fill=group) ) ) + geom_bar(stat=&quot;identity&quot;) .. "],
["chisquare.html", "Chapter 14 The Chi-square Distribution 14.1 Background 14.2 dchisq 14.3 pchisq 14.4 qchisq 14.5 rchisq", " Chapter 14 The Chi-square Distribution library(tidyverse) 14.1 Background The chi-square permeates biostatistics in several important ways. For example, the \\(\\chi^2\\) distribution is used to generate p-values for Pearson’s chi-square test statistic, \\[\\chi^2=\\sum_{i=1}^{n}\\frac{(O_i-E_i)^2}{E_i}\\] which is used in goodness-of-fit and independence tests. The \\(\\chi^2\\) distribution is used to generate p-values for tests of homogeneity and also to calculate the confidence intervals of standard deviations. A good way to think of the chi-square distribution more generally is as a probability model for the sums of squared variables. As such, the \\(\\chi^2\\) test statistic only takes on positive values. If \\[X_1,..,X_k\\] is a set of standard normal variables, then the sum of their squared values is a positive, random variable \\(Q\\), \\[Q=\\sum_{i=1}^{k}{X^2_i}\\] that will take on a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom. The mean of any \\(\\chi^2\\) distribution is \\(k\\) while it’s variance is \\(2k\\). The distributions are skewed (mode = \\(k-2\\)) except that those having very large degrees of freedom, which are approximately normal. 14.2 dchisq dchisq is the \\(\\chi^2\\) probability density function in R. The simplest way to think of dchisq is as the function that gives you the probability distribution of the \\(\\chi^2\\) test statistic. The \\(\\chi^2\\) probability density function is: \\[p(x)=\\frac{e^\\frac{-x}{2}x^{\\frac{k}{2}-1}}{2^\\frac{k}{2}\\Gamma(\\frac{k}{2})}\\] Given a chi-square value and the degrees of freedom of the dataset as input, dchisq returns the probability for a given chi-square value. For example, if a 2X2 test of independence (df=1) returns a \\(\\chi^2\\) value of 4, the probability of obtaining that value (its “density”) is equal to 0.02699: dchisq(x=4, df=1) ## [1] 0.02699548 For continuous distributions like the \\(\\chi^2\\) such point density values are usually not particularly useful. In contrast, inspecting how the function behaves over a range of \\(\\chi^2\\) values and at different df’s is informative. That’s plotted below: df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- dchisq(x, i) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;p(chi-square)&quot;) 14.3 pchisq pchisq is the \\(\\chi^2\\) cumulative distribution function in R. The simplest way to think about the pchisq function is as the probabilities under the \\(\\chi^2\\) curve. The \\(\\chi^2\\) cumulative distribution function is: \\[p(x)=\\frac{\\gamma(\\frac{k}{2}\\times\\frac{x}{2})}{\\Gamma(\\frac{k}{2})}\\] It returns the cumulative probability for an area under the curve up to a given \\(\\chi^2\\) value. For example, the probability that a \\(\\chi^2\\) value with 1 degree of freedom is less than 4 is 0.9544997: pchisq(q=4, df=1, lower.tail = T) ## [1] 0.9544997 df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- pchisq(x, i, lower.tail = T) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;cumulative p(chi-square)&quot;) 14.3.1 Calculating p-values from pchisq The pchisq function is used to calculate a p-value. A p-value is a probability that a \\(\\chi^2\\) value, x, is as large or larger. \\[p-value = P(\\chi^2 \\ge x)\\] This can be calculated using the pchisq function simply by changing lower.tail argument in the function tolower.tail = F pchisq(q=4, df=1, lower.tail = F) ## [1] 0.04550026 Here p-values for the \\(\\chi^2\\) at various df’s: df = 9 x = seq(1,16,0.05) pxd &lt;- matrix(ncol=df, nrow=length(x)) for(i in 1:df){ pxd[,i] &lt;- pchisq(x, i, lower.tail = F) } pxd &lt;- data.frame(pxd) colnames(pxd) &lt;- c(1:df) pxd &lt;- cbind(x, pxd) pxd &lt;- gather(pxd, df, px, -x) ggplot(pxd, aes(x, px, color=df)) + geom_line() + xlab(&quot;chi-square&quot;)+ylab(&quot;p-value&quot;) 14.4 qchisq qchisq is the inverse of the \\(\\chi^2\\) cumulative distribution function. This function takes a probability value as an argument, along with degrees of feedom, and returns a \\(\\chi^2\\) value corresponding to that probability. Here’s the \\(\\chi^2\\) value corresponding to the 95th percentile of a distribution with 3 degrees of freedom: qchisq(0.95, 3) ## [1] 7.814728 Here is what the quantile \\(\\chi^2\\) distribution looks like for several different df’s: df = 9 p = seq(0.01, .99, 0.01) xpd &lt;- matrix(ncol=df, nrow=length(p)) for(i in 1:df){ xpd[,i] &lt;- qchisq(p, i) } xpd &lt;- data.frame(xpd) colnames(xpd) &lt;- c(1:df) xpd &lt;- cbind(p, xpd) xpd &lt;- gather(xpd, df, qchisq, -p) ggplot(xpd, aes(p, qchisq, color=df)) + geom_line() + xlab(&quot;probability&quot;)+ylab(&quot;chi-square&quot;) 14.5 rchisq R’s rchisq function is used to generate random values corresponding to \\(\\chi^2\\)-distributed data. This has utility in simulating data sets with skewed values, for example, to mimic overdispersed Poisson distributions. Here are 10 random values from a \\(\\chi^2\\) distribution with 3 degrees of freedom: rchisq(10, 3) ## [1] 1.7793487 1.6128441 1.9412011 8.1186947 2.5548657 2.8908581 4.7170045 ## [8] 5.9611732 3.7333294 0.7858192 "],
["nonparametrics.html", "Chapter 15 Nonparametric Statistical Tests 15.1 Experiments involving discrete data 15.2 Deviant Data 15.3 Sign Test 15.4 Wilcoxon Sign Rank Test for One Group 15.5 Wilcoxon Mann Whitney Rank Sum Test for 2 independent groups 15.6 Wilcoxon Sign Rank Test for paired groups 15.7 Kruskal-Wallas 15.8 Friedman test 15.9 Summary", " Chapter 15 Nonparametric Statistical Tests Non-parametric statistical tests are versatile with respect to the dependent variables they tolerate. They are typically applied to datasets involving ordered data. One nonparametric test is used to assess simple proportions. They can also be for data on measured, equal interval scales, for which the normality and equal variance assumptions of parametric statistical testing are not satisfied or cannot be assumed. Nonparametric statistics are parameter-less. They don’t compare means, or medians (though people frequently treat nonparametrics as tests of medians), or standard deviations, or variance. They do compare distributions of data, but only after the data has been transformed into a standardized measure of ranks–either signs, sign ranks or rank sums. The tests, essentially, evaluate whether the distribution of ranks in an experimental outcome differs from a null distribution of ranks, given a sample size. That can seem pretty abstract. But it’s actually a simple and elegant way to think about these tests. With the exception of the Sign Test, which has a probability as an effect size, strictly speaking there really isn’t an effect size that describes non-parametric outcomes other than the value of the test statistic. However, it is possible to use confidence interval arguments in R’s tests to coerce them into providing effect size output as estimates of medians. This can be sometimes useful. Non-parametric analogs exist for each of the major parametric statistical tests (t-tests and one-way anova. Which analog to use for a given dataset analysis depends entirely upon the experimental design. Sign Test -&gt; analog to the binomial Test -&gt; when events are categorized as either successes or failures. Wilcoxon Sign Rank Test for one group -&gt; analog to the one sample t-test -&gt; compare a one group dataset to a standard value. Mann Whitney Rank Sum Test for 2 independent groups -&gt; analog to the unpaired t test -&gt; for comparing two groups in a dataset. Wilcoxon Sign Rank Test for paired groups -&gt; analog to the paired t-test -&gt; comparing a group of paired outcomes in a dataset to no effect null. Kruskal-Wallis Test -&gt; analog to one way completely randomized ANOVA -&gt; comparing 3 or more groups Friedman Test -&gt; analog to one way related measures ANOVA -&gt; comparing 2 two factors, each at two or more levels. In R, the wilcox.testfunction is a work horse for non-parametric analysis. By simply changing the function’s arguments it can do either a WSRT, or Mw, or a WSRT for paired groups analysis. 15.1 Experiments involving discrete data Discrete data arises from counting objects or events, as opposed to measuring attributes of the study subjects. Counted objects are easy to spot—they are indivisible. Discrete data can be either sorted or ordered. When planning an experiment ask whether the data will be sorted into categories on the basis of nominal characteristics (eg, dead vs alive, in vs out). Or will the data be categorized on some ordered basis. For example, a score of 1 = the attribute, a score of 2 = more of the attribute, a score of 3= even more of the attribute…and so on. The discrete counts within one category of an ordered scale mean that they have more or less of some feature than do the counts in another category in the ordered group. Thus, compared to nominal data, ordered data have more information. Whereas nominal events are just sorted into one bucket or another, ordered events are inherently categorized by rank. Ordered data are common in survey instruments and polling. Certain experimental designs generate inherently ordered data as well. For example, imagine a test that scores dermal inflammatory responses. Given a subject, * Score 1 if we don’t see any signs of inflammation. * Score 2 if there was a faint red spot. * Score 3 for a raised pustle. * Score 4 for a large swollen area that feels hot to the touch. * Score 5 for anything worse than that, if it is possible! Using that ordered scale system, we’d run experiments, for example, to compare a steroid treatment that might reduce inflammation compared to a vehicle control. Or we’d look at a gene knockout, or CRISP-R fix, or whatever, and score an outcome response. In quantifying effect sizes for such studies, a mistake you often see is parametric analysis. The researcher uses parameters such as means, standard deviations, performs t-tests, and so forth on the score rank values. This isn’t always bad, but it assumes a couple of things. First, the distribution of the data is approximately normal, as is the population that was sampled. Second, the scoring scale is equal interval. That is to say, “the difference between inflammation scores of 1 and 2 is the same as the difference between scores 2 and 3, and so on…”. Suffice to say that researchers should validate whether these assumptions are true before resorting to parametric tests. Or they can just use nonparametric tests and save themselves from all that validation work! Sometimes we take measurements of some variable on a perfectly good measurement scale, one that satisfies these assumptions, but then break the data out to some ordered scale. Take blood pressure, for example. We might measure it’s value for each subject, but on the basis of that measurement sort the subjects into ordered categories of low, medium and high. Our scientific experitise drives what blood pressure values match those categories. And we should have good reasons to throw away perfect good scalar information by doing this. It is on this ordered scale, of discrete events, rather than the original measurements on a continous scale, that we might then run statistical tests. My point is, of course, that not all ordered scales are based upon subjective assessments. 15.2 Deviant Data Any scale can yield deviant data. Deviant data is non-normal, skewed, has unequal variances among groups, has outliers, and is just plain ugly. When data are deviant there are two options: Use recipricol or log transform functions to transform the data distribution into something more normal-like. Run the statistical tests on the transformed values. Run non-parametric statistical tests on the data, which transforms the data into a rank-based distribution, which are normal-like. Tossing outliers is almost always a bad option because it introduces bias! 15.3 Sign Test The Sign Test is a non-parametric way of saying a binomial test. An experiment is conducted on a group of subjects, who are graded in some way for either passing (+) or failing (- ) some test. Did a cell depolarize, or not? Is a stain in the cell nucleus, or not? Did the animal move fast enough, or not? Did the subject meet some other threshold you’ve established as a success, or not? Simply count the number that passed..or received a “+” sign, and the number that failed (received a “-” sign). Using scientific judgement, assume a probability for the frequency of successes under the null hypothesis. For example, the null might be to expect 50% successes. If after analyzing the data and the number of successes differs from this null proportion, you may have a winner! Here’s an analysis of a behavioral test, the latency to exit a dark chamber, as an index of anxiety. Let’s say that exiting a chamber in less than 60 seconds is a threshold for what we’d consider “non-anxious” behavior. Fifteeen subjects are given an anti-anxiety drug. The null probability of exiting the chamber is 0.5. Which is to say there is a 50/50 chance a mouse will, at random, exit the chamber at any given time before or after 60 sec. Or put another way, under the null, neither exiting nor remaining in the chamber by 60 seconds is favored. The results are that twelve exited the chamber in less than 60 seconds, and 5 did not. We have not recorded times. Let’s imagine we have an alarm set to go off 60 seconds after placing the subject in the chamber. When the alarm sounds, we score the subject as either (+) or (-). Scientifically, we predict the subjects on an anti-anxiety drug is are more likely to exit before this mark. This experiment tests the null hypothesis that the probability of successes are less than or equal to 50%. If something is not less than or equal to another, it can only be greater. Thus, we choose the “greater” for the alternative hypothesis argument in the binomial test function. We think on an anti-anxiety drug the probability is greater that the subjects will successfully exit the chamber! binom.test(x=12, n=15, p=0.5, alternative =&quot;greater&quot;, conf.level=0.95 ) ## ## Exact binomial test ## ## data: 12 and 15 ## number of successes = 12, number of trials = 15, p-value = 0.01758 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.5602156 1.0000000 ## sample estimates: ## probability of success ## 0.8 scoreci(x=12, n=15, conf.level = 0.95) ## ## ## ## data: ## ## 95 percent confidence interval: ## 0.5481 0.9295 15.3.1 Interpretation The effect size is 0.8, which represents the fraction of subjects that left the chamber prior to the 60 second threshold we set. The p-value is the probability of observing an effect size this large, if the null hypothesis is actually true. There is a 95% chance the true effect size is within the range of 0.56 to 1. To get a clear sense of what’s going on, here is the distribution of the binomial function for the null hypothesis. # I&#39;ll use the rbinom function to simulate data &lt;- data.frame(x = c(0:15),y=dbinom(c(0:15), 15, prob=0.5)) ggplot(data, aes(x, y))+ geom_col(fill=&quot;blue&quot;) + xlab(&quot;exits before 60s&quot;) + ylab(&quot;prob of that many exits&quot;) + geom_text(aes(x=1, y=.20, label=&quot;H0 distribution&quot;)) And here is the distribution for the alternate hypothesis: data &lt;- data.frame(x = c(0:15),y=dbinom(c(0:15), 15, prob=0.8)) ggplot(data, aes(x, y))+ geom_col(fill=&quot;green&quot;) + xlab(&quot;exits before 60s&quot;) + ylab(&quot;prob of that many exits&quot;) + geom_text(aes(x=1, y=.20, label=&quot;H1 distribution&quot;)) This is to emphasize that the binomial distribution is used here as a model of the experimental effect. Thus, we might also conclude that our data is consistent with a binomial distribution of 15 trials wherein the probability of event success is 80%. In effect, our p-value allows us to conclude this alternate distribution is a better model for the population than is the null distribution. This is subject to a 1.758% chance that this might be a false positive conclusion….an acceptable risk of being wrong. This also is a way to visualize the confidence interval, which says we should expect more than 8 successes 95% of the time…an assertion that covers all but two of the lower bins in this distribution! 15.3.2 Write Up Drug treatment increases fearlessness (one-sided binomial test, p = 0.01759). The fraction exiting the chamber (0.8) is greater than expected for the null of 0.5 (95% CI = 0.55 to 1.0, Wilson’s CI) 15.4 Wilcoxon Sign Rank Test for One Group The test statistic for the Wilcoxon Sign Rank is determined as follows. 1. Calculate the difference between the theoretical median or threshold value and the values recorded for each independent replicate. 2. Rank those differences from lowest (rank = 1) to highest (rank = n). 3. Assign a negative value to the replicate values that are less than the median. 4. The test statistic V is the sum of the postive values. (software other than wilcox.test in R may calculate W, the sum of the positive and negative values). The test statistic V has an approximately normal discrete distribution, whose cumulative function psignrank can be used to compute p-values. 15.4.1 Wilcoxon Sign Rank Experimental Designs This experimental design is similar to the Sign Rank test except in one important detail: We actually measure the time it takes for the subjects to exit the chamber. No alarm sounds to end the game at 60 sec. If subjects dawdle about and take longer than 60 sec to exit, we wait and record that time! Thus, because the dataset is comprised of the actual values for the latency variable, rather than counts of a simple (+) or (-) score, the Wilcoxon Sign Rank design collects more information than does the Sign Rank Test. Let’s say we have a chamber test on 7 subjects who’ve all been given an anti-anxiety drug. After placement in the chamber, their exit times (in seconds) are 3, 5, 8, 15, 19, 21 and 108. Based upon scientific judgement, we think exiting sooner than 60 would represent fearlessness (less anxiety). This test ranks each subject’s performance relative to that reference time and then “signs” it as negative or positive based on whether it’s original value was below or above the 60 second threshold. In our data, only one subject exceeded that value…108 sec. Our prediction is that less anxious subjects should exit the comfort of the dark chamber sooner than would be expected. The null hypothesis is that the “location”\" of the null distribution is greater than or equal to 60 seconds. The alternate is the location is “less” than 60 seconds, since less is everything that greater than or equal to cannot be. We run the Wilcoxon Sign Rank test to test this hypothesis using the arguments below. wilcox.test(x=c(3,5,8,15,19,21,108), mu=60, alternative = &quot;less&quot;, conf.level = 0.95, conf.int = 0.95) ## ## Wilcoxon signed rank test ## ## data: c(3, 5, 8, 15, 19, 21, 108) ## V = 4, p-value = 0.05469 ## alternative hypothesis: true location is less than 60 ## 95 percent confidence interval: ## -Inf 61.5 ## sample estimates: ## (pseudo)median ## 14 15.4.2 Interpretation The value of the test statistic, V is four. How extreme is that? It is pretty far to the left on the test statistic distribution (see below) for this sample size. The p-value is above the threshold of 5%. The evidence is not enough to reject the null hypothesis. Otherwise, the probability of making an error doing so would be 0.05469. That V = 4 means it is the value corresponding to the sum of the positively signed ranks in the sample. The pseudo-median of the latency time is 14 seconds. The one-sided 95% confidence ranges from -infinity to 61.5. Here’s a null signrank distribution for a sample size of 7. The values of the x scale are V, the test statistic. These are all the possible values that V can take on, given the sample size. For example, if all the signed ranks were positive…if every subject took longer than 60 sec to exit)…then V would equal 28. If all subjects exited before 60 sec, then V would equal zero. Which is to say the location of this distribution is, by coincidence, also centered on 14. The value of 4 is less than this location, but not extremely-enough lower to be considered as belonging to some other distribution with a different location! The 95% confidence interval of the location on the V test statistic ranges from -infinity to 62.5. psignrank(q=4, n=7) ## [1] 0.0546875 psignrank(q=1, n=7) ## [1] 0.015625 upper &lt;- 28 n &lt;- 7 df &lt;- data.frame(x=0:upper, y=dsignrank(0:upper, n)) ggplot(df, (aes(x,y)))+ geom_col() + xlab(&quot;V&quot;) + scale_x_continuous(breaks=(seq(0,upper,1))) 15.4.3 Write Up Analysis of the chamber test results indicates the anti-anxiety drug has no effect (Wilcoxon Signed Rank test, V = 4, n = 7, p= 0.0547) 15.5 Wilcoxon Mann Whitney Rank Sum Test for 2 independent groups This nonparametric test, often referred to simply as the Mann-Whitney test, is analogous to the parametric unpaired t-test. It is for comparing two groups that receive either of 2 levels of a predictor variable. For example, in an experiment where one group of m independent replicates is exposed to some control or null condition, while a second group with n independent replicates is exposed to some treatment. More generally, the two groups represent two levels of a predictor variable given to m+nindependent replicates. The rank sum is calculated as follows: The data are collected from any scale, combined into a single list, whose values are ranked from lowest (rank 1) to highest (rank m+n), irrespective of the level of the predictor variable. Let \\(R_1\\) represent the sum of the ranks for the one level of the predictor variable (eg, group2). Let \\(U_1\\) represent the number of times a data value from group2 is less than a data point from group1. \\(U_1=m*n+\\frac{m(m+1)}{2}-R_1\\) And \\(U_2=m*n-U_1\\) The rank sum test computes two test statistics, \\(U_1\\) and \\(U_2\\) that are complementary to each other. Here’s another in the line of the mighty mouse experiments. 55 independent subjects were split into two groups. One group received an anti-anxiety drug and the second a vehicle as control. The subjects were run through the dark chamber test. The scientific prediction is the drug will reduce anxiety levels and so the drug treated mice will exit the chamber more quickly compared to the control mice. Since this is a parameter-less test, the null hypothesis is that location of the distribution of the drug-treated population is greater than or equal to the location of the vehicle distribution. The alternative hypothesis is that the location of the distribution of the drug-treated population is less than that of the vehicle distribution. The alternative is consisent with our scientific prediction and represents an outcome that is exclusive and comprehensive of the null! We choose the “less” option for the alternative argument in the test. mightymouse &lt;- read.csv(&quot;datasets/mightymouse.csv&quot;) wilcox.test(Time ~ Group, data = mightymouse, alternative =&quot;less&quot;, conf.level=0.95, conf.int=T) ## ## Wilcoxon rank sum test ## ## data: Time by Group ## W = 55, p-value = 0.1804 ## alternative hypothesis: true location shift is less than 0 ## 95 percent confidence interval: ## -Inf 6 ## sample estimates: ## difference in location ## -9.8 15.5.1 Interpretation The test statistic you see in the output, W, warrants some discussion. W is equal to \\(U_2\\) as defined above. By default, R produces \\(U_2\\) (labeled W!) as the test statistic. Most other software packages use \\(U_1\\), which in this case would be 88 (easy to compute in the console given \\(U_2\\)). Think of W as a value on the x axis of a rank sum distribution for a sample size of m+n. The rank sum distribution has a function in R called dwilcox. Here it is (note the large value this distribution can take on is m*n and the smallest is zero): df &lt;- data.frame(x=0:143, y=dwilcox(0:143, 11, 13)) ggplot(df, (aes(x,y)))+ geom_col()+ xlab(&quot;W&quot;)+ scale_x_continuous(breaks=c(55, 88)) All this can seem confusing, but it is very elegant. First, the rank sums of samples, like the rank signs of samples, take on symmetrical, normal-like distributions. The greater the sample sizes, the more normal-like they become. Second, the bottom line is the same as for all other statistical tests: test statistic values at either extreme of these null distributions are associated with large effect sizes. The non-extreme-ness of the test statistic value for our sample is illustrated in that plot. Clearly, W=55, it is well within the null distribution. I calculated it’s symmetrical counterpart, \\(U_1\\) = 88, from the relationship above. As you can see, the value of the test statistic and 88 frame the central location of this null ranksum distribution null quite nicely: The p-value for W=55 indicates that the probability of creating a false positive by rejecting the null is 18.04%, well above the 5% type1 error threshold. So we should not reject the null given we’d have a 1 in 5 chance of being wrong if we did! The “effect size” is in the output is the magnitude of the difference between the location parameters (pseudo-medians) of the two groups, on the scale of the original data. The 95% confidence interval indicates there is a 95% chance the difference in locations is between negative infinity and 6. Since the 95% confidence interval includes zero, the possibility exists that there is zero difference between the two locations. That provides additional statistical reasoning not to reject the null. 15.5.2 Write Up There is no difference in performance using the closed chamber test between subjects randomized to anti-anxiety drug (n=11) or to vehicle (n=13) (Mann-Whitney test, W = 55, p = 0.1804). 15.6 Wilcoxon Sign Rank Test for paired groups The classic paired experimental design happens when two measurements are taken from a single independent subject. For example, we take a mouse, give it a sham treatment, and measure it’s latency in the chamber test. Later on we take the same mouse, give it an anti-anxiety drug treatment, and then measure its latency once again. This kind of design can control for confounding factors, like inter-subject variability. But it can also introduce other confounds. For example, what if the mouse “remembers” that there is no real risk of leaving the dark chamber? Pairing can happen in many other ways. A classic pairing paradigm is the use of identical twins. Individuals of inbred mouse strains are all immortal clones. Two littermates would be identical twins and would also be, essentially, clones of their parents and their brothers and sisters from prior litters! Two dishes of cultured cells, passed together and now side-by-side on a bench are intrinsically-linked. All of these can be treated, statistically, as pairs. In this example, we take a pair of mice from each of 6 independent litters produced by mating two heterozygots of a nogo receptor knockout. One of the pair is nogo(-/-). The other is nogo(+/+). We think the nogo receptor causes the animals to be fearful, and predict animals in which the receptor is knocked out will be more fearless. The independent experimental unit in this design is a pair. We have six pairs, Therefore, the sample size is 6 (even though 12 animals will be used!) We’ll measure latency in the dark chamber test. Our random variable will be the difference in latency time between the knockout and the wild type, for each pair. Here’s the data, latency times are in sec units: mmko &lt;- data.frame(knockout=c(19, 24, 4, 22, 15, 18), wildtype=c(99, 81, 70, 62, 120, 55)) #create a long data fram to do formula arguments in wilcox test mmkotidy &lt;- gather(mmko, genotype, latency ) Scientifically, we predict there will be a difference in latency times within the pairs. Specifically, the knockout will have lower times than their paired wild-type. The null hypothesis is that the difference within pairs will be greater than or equal to zero. The alternative hypothesis is the difference will be less than zero. wilcox.test(latency ~ genotype, data=mmkotidy, paired=T, conf.level=0.95, conf.int=T, alternative=&quot;less&quot;) ## ## Wilcoxon signed rank test ## ## data: latency by genotype ## V = 0, p-value = 0.01563 ## alternative hypothesis: true location shift is less than 0 ## 95 percent confidence interval: ## -Inf -40 ## sample estimates: ## (pseudo)median ## -61.5 15.6.1 Interpretation Note that this is not a rank sum test as for the Mann-Whitney, but a signed rank test. So we have seen the V test statistic before. It’s value of 0 is as extreme as can be had on the null distribution, as is evident in the distribution below! That happened because in each of the six pairs, the knockout had a lower latency time than its paired wildtype. All of the signed ranks were negative! In terms of position differences, it is as strong of an effect size as possible. upper &lt;- 21 n &lt;- 6 df &lt;- data.frame(x=0:upper, y=dsignrank(0:upper, n)) ggplot(df, (aes(x,y)))+ geom_col() + xlab(&quot;V&quot;) + scale_x_continuous(breaks=(seq(0,upper,1))) The p-value is exact…and it can never be lower, given this sample size. We can reject the null since it is below our 5% threshold and it says the probably that we are accepting a type1 erro is 1.563%. The pseudo-median is in units of latency time. It represents the median for the differences in latency within the pairs. In other words, there are six values of differences, one difference value for each pair. -61.5 is the median of those 6 differences. There is a 95% chance the true median of the differences lies between negative infinity and -40. Note that the 95% CI does not include the value of zero. 15.6.2 Write up Dark chamber test latency differs markedly within pairs of knockout and wildtype subjects (Wilcoxon Signed Rank Test for pairs, n=6, V = 0, p=0.01563) 15.7 Kruskal-Wallas The kruskal.test is a non-parametric method for comparing 3 or more treatment groups. It serves as an omnibus test for the null hypothesis that each of the treatment groups belong to the same population. If the null is rejected, post hoc comparison tests are then used to determine which groups differ from each other. A post hoc test for this purpose in base R is pairwise.wilcox.test. The PMCMRplus package has others. Documentation within the PMCMRpackage vignette provides excellent background and instructions for these tests. The Kruska-Wallas test statistic is computed as follows. Values of the outcome variables across the groups are first converted into ranks, from high to low. Tied values are rank-averaged. The test can be corrected for large numbers of tied values. The Kruskal-Wallas rank sum test statistic is: \\[H=\\frac{12}{n(n+1)}\\sum_{i=1}^k\\frac{R_{i}^2}{n_i}-3(n+1)\\] \\(n\\) is the total sample size, \\(k\\) is the number of treatment groups, \\(n_i\\) is the sample size in the \\(ith\\) group and \\(R_i^2\\) is the squared rank sum of the \\(ith\\) group. Under the null, \\(\\bar{R_i} = (n+1)/2\\). The H statistic is approximated using the \\(\\chi^2\\) distribution with \\(k-1\\) degrees of freedom to produce p-values. Let’s analyze the InsectSprays dataset, it comes with the PMCMRplus package. This is a multifactorial experiment in which insects were counted in agricultural field plots that had been sprayed with 1 of 6 different insecticides. Each row in the dataset represents an independent field plot. Do the insecticides differ? ggplot(InsectSprays, aes(spray, count))+ geom_violin() The violin plots (modern day versions of box plots) illustrate how the groups have unequal variance. Such data are appropriate for non-parametric analysis. #insectsprays &lt;- read.csv(&quot;insectsprays.csv&quot;) kruskal.test(count ~ spray, data=InsectSprays) ## ## Kruskal-Wallis rank sum test ## ## data: count by spray ## Kruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10 data(insectsprays) pairwise.wilcox.test(InsectSprays$count, InsectSprays$spray, p.adjust.method=&quot;bonferroni&quot;, alternative =&quot;two.sided&quot;) ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: InsectSprays$count and InsectSprays$spray ## ## A B C D E ## B 1.00000 - - - - ## C 0.00058 0.00058 - - - ## D 0.00117 0.00104 0.03977 - - ## E 0.00051 0.00051 0.78860 1.00000 - ## F 1.00000 1.00000 0.00052 0.00105 0.00052 ## ## P value adjustment method: bonferroni We first do a Kruskal-Wallas rank sum omnibus test to test the null hypothesis that the locations of the groups within the dataset are the same. The null is rejected given the large \\(\\chi^2\\) test statistic, which has a p-value well below the threshold. That’s followed by a pairwise Wilcoxon rank sum test…think of it as running a Mann-Whitney test on all possible pairs in the group. The number of pairwise tests for 6 groups is choose(6, 2)= 15. Each pairwise test is a single hypothesis test associated with 5% type1 error risk. If we don’t make a correction for doing the multiple comparisons, the family-wise type1 error would inflate to \\(15 x 5% = 75%\\)! The Bonferroni adjustment is the most conservative and simple to understand. It multiples every unadjusted p-value by 15, the number of comparisons made. Thus, each of the p-values in the grid is 15X larger than had the adjustment not been made. Every p-value less than 0.05 in the grid is therefore cause for rejecting the null hypothesis that the pair does not differ. The highest among these is the comparisons between sprays C and D, which has a p-value of 0.03977. More generally, the Bonferroni correction is \\(adjusted\\ type1\\ threshold = 0.05/C\\) where C is the number of comparisons to make. 15.7.1 Write Up A non-parametric omnibus test establishes that the locations of the insecticide effects of the six sprays differ (Kruskal-Wallas, \\(\\Chi^2\\) = 54.69, df=5, p=1.511e-10). Posthoc pairwise multiple comparisons by the Mann-Whitney test (Bonferroni adjusted p-values) indicate the following sprays differ from each other: A vC(0.00058), D(0.00117), E(0.0051), …and so on 15.8 Friedman test 15.9 Summary If you’re used to comparing means of groups, non-parametrics can be somewhat disorienting. There are no parameters to compare! And the concept of location shifts or differences seems rather abstract. The tests transform the values of experimental outcome variables into either sign rank or into rank sum units. That abstraction can be disorientating, too. But it is important to recognize that sign ranks and rank sum distributions are approximately normal. Therefore, perhaps its best to think of non-parametrics as a way to transform non-normal data into more normal data. The non-parametrics are powerful statistical tests that should be used more widely than they are. "],
["signrank.html", "Chapter 16 Signed Rank Distribution 16.1 Transformation of data into sign ranks 16.2 R’s Four Sign Rank Distribution Functions", " Chapter 16 Signed Rank Distribution The signed rank per se represents a type of data transformation. Experimental data are first transformed into signed ranks (see below). When signed ranks for a group of data are summed that sum serves as a test statistic. The sign rank test has a symmetrical normal-like distribution, which is discrete rather than continuous. In R, when using the wilcox.test to evaluate data from one sample or for paired sample nonparametric designs, the sums of only the postive signed ranks are calculated as the test statistic \\(V\\). The distribution of \\(V\\) is the focus of this document. There is a unique sign rank sum distribution for sample size \\(n\\). These represent the distributions that would be expected under the null hypothesis. In practice, values of \\(V\\) that are on the extreme ends of these distributions can have p-values that are so low that we would reject the null \\(V\\) distribution as a model for our dataset and accept an alternative. The use of \\(V\\) as a test statistic and its null distributions can be a bit confusing if you’ve worked with other software to do nonparametric hypothesis testing. For example, Prism reports \\(W\\) as a test statistic for sign rank-based nonparametric experimental designs. On that platform, \\(W\\) is the sum of all signed ranks, not just the positives. The center location of \\(W\\) as a sign rank test statistic is at or near zero on the \\(W\\) distributions, whereas zero is the lowest possible value for \\(V\\). Although calculated slightly differently, \\(W\\) and \\(V\\) are equivalent ways to express the same relationship between the values within the original dataset. 16.1 Transformation of data into sign ranks 16.1.1 For a one group sample A one group sample represents a single vector of data values. Let a sample of size \\(n\\) for a variable \\(X\\) take on the values \\(x_1, x_2, ... x_n\\). The location of this vector will be compared to a location value \\(x = \\mu\\), such that \\(z_i=x_i-\\mu\\) for \\(i=1\\ to\\ n\\). To transform the sample to a vector of signed ranks, first rank \\(|z_i|\\) from lowest to highest. \\(R_i\\) is the rank value for each value of \\(|z_i|\\). \\(\\psi = 0\\) if \\(z_i &lt; \\mu\\), or \\(\\psi= 1\\) if \\(z_i &gt; \\mu\\). The signed ranks of the original vector values are therefore \\(R_1\\psi, R_2\\psi,...R_n\\psi\\). 16.1.2 For a paired sample The differences between paired groups in an experimental sample also represent a single vector of data values, which explains why a sign rank-based test are used, rather than a rank sum test. A sample of \\(n\\) pairs of the variables \\(X\\) and \\(Y\\) has the values \\((x_1,y_1); (x_2,y_2); ... (x_n,y_n)\\). The difference between the values of \\(X\\) and \\(Y\\) will be compared to zero. For \\(i=1\\ to\\ n\\), the difference is \\(z_i=x_i-y_i\\). The sign rank transformation of \\(z_i\\) for each pair is performed as above. 16.1.3 The sign rank test statistic in R The sign rank test statistic for both the one- and two-sample cases is \\(V = \\sum_i^{n}R_i\\psi_{&gt;0}\\), which has a symmetric distribution ranging from a minimum of \\(V = 0\\) to a maximum of \\(V = \\frac{n(n+1)}{2}\\). This test statistic is produced by the wilcox.test for one-sample and paired-sample expriments. Again, \\(V\\) is the sum of the positive signed-ranks only. Other softer produces a test statistic \\(W\\), which is the sum of all the signed ranks 16.1.3.1 More about the test statistic The expectation of \\(V\\) (ie, its median) when the null hypothesis is true is \\(E_0(V)=\\frac{n(n+1)}{4}\\) and its variance is \\(var_0(V)=\\frac{n(n+1)(2n+1)}{24}\\) The standardized version of \\(V\\) is \\(V^*=\\frac{V-E_0(V)}{var_0(V)}\\). As \\(n\\) gets large, \\(V^*\\) asymptotically approaches a normal distribution \\(N(0,1)\\). Zero and tied values of \\(|Z_i|\\) occur in datasets for both the one group and paired group transformations of raw data. When values of \\(|Z|=0\\) occur, they are discarded in the calculation of \\(V\\) and the \\(n\\) is readjusted. The integer value of ranks for tied \\(|Z&#39;s|\\) are averaged. Although these ties don’t change \\(E_0(V)\\), they do reduce the variance of \\(V\\). \\(var_0(V)=\\frac{n(n+1)(2n+1)-\\frac{1}{2}\\sum_{j=1}^{g}t_j(t_j-1)(t_j+1)}{24}\\) 16.2 R’s Four Sign Rank Distribution Functions 16.2.1 dsignrank The function dsinerank returns a probability value given two arguments: \\(x\\) is an integer value, meant to represent \\(V\\), which is the value of the sign rank test statistic; and \\(n\\) is the sample size of either a one-sample or a paired experiment. The probabilities for individual values of \\(V\\) are sometimes useful to calculate. For example, the probability of obtaining \\(V=3\\) for an \\(n\\)=10 experiment is: dsignrank(3, 10) ## [1] 0.001953125 More ofen it is useful to visualize the null distribution of the sign rank test statistic over a range of values. Here’s a distribution of \\(V\\) for an experiment of sample size 10. n &lt;- 10 #number of independent replicates in experiment max &lt;- n*(n+1)/2 df &lt;- data.frame(pv=dsignrank(c(0:max), n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p(V)&quot;) 16.2.2 psignrank This is the cumulative distribution function for the sign rank test statistic. \\(q\\) is an integer to represent the expected value of \\(V\\), and \\(n\\) is the sample size. Given these arguments, psignrank will return a p-value for a given test statistic value. psignrank returns the sum of the probabilities of \\(V\\) over a range, and is reversible using the lower.tail argument. Here we use psignrank to generate the p-value when \\(V\\)=3 and \\(n\\)=1. Then we show the sum of the dsignrank output for \\(V\\)=0:3 is equal to psignrank. Finally, the symmetry of the distribution is illustrated: psignrank(3, 10, lower.tail=T) ## [1] 0.004882812 sum(dsignrank(c(0:3), 10)) == psignrank(3, 10, lower.tail=T) ## [1] TRUE psignrank(51, 10, lower.tail=F) ## [1] 0.004882812 psignrank(51, 10, lower.tail=F)==psignrank(3, 10, lower.tail=T) ## [1] TRUE The distributions of psignrankfor its lower and upper tails: n &lt;- 10 max &lt;- n*(n+1)/2 df &lt;- data.frame(pv=psignrank(c(0:max), n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p-value for V&quot;) df &lt;- data.frame(pv=psignrank(c(0:max), n, lower.tail=F)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;V&quot;) + ylab(&quot;p-value for V&quot;) 16.2.3 qsignrank This is the quantile signrank function in R. If given a quantile value (eg, 2.5%) and sample size \\(n\\), qsignrank will return the value of the test statistic V. For example, here is how to find the critical limits for a two-sided hypothesis test for an experiment with 10 independent replicates, when the type1 error of 5% is evenly distributed to both sides: qsignrank(0.025, 10, lower.tail=T) ## [1] 9 qsignrank(0.025, 10, lower.tail=F) ## [1] 46 Interpretation of critical limits output: When the null two-sided hypothesis is true, \\(9\\le V\\le46\\). In other words, the null hypothesis would not be rejected if an experiment generated a \\(V\\) between 9 and 46. Alternative interpretation: \\(p&lt;0.05\\) when \\(9&gt;V&gt;46\\). In other words, a two sided null hypothesis would be rejected if it generated a \\(V\\) below 9 or greater than 46. And here are the critical limits for one-sided hypothesis tests for an experiment with 10 independent replicates, when the type1 error is 5%. Notice how the critical limits differ between one-sided and two-sided tests: qsignrank(0.05, 10, lower.tail=T) ## [1] 11 qsignrank(0.05, 10, lower.tail=F) ## [1] 44 Standard interpretion: For one-sided type1 error of 5%, When the null hypothesis is true, \\(V\\ge11\\), or \\(V\\le44\\). Alternative interpretation: \\(p &lt; 0.05\\) when \\(V&lt; 11\\) or \\(V&gt;44\\) Here is a graphical representation of the qsignrank function. Note the stairstep pattern, characteristic of discrete distributions: n &lt;- 10 x &lt;- seq(0,1,0.01) df &lt;- data.frame(v=qsignrank(x, n, lower.tail=T)) ggplot(df, aes(x=x, v)) + geom_point() + xlab(&quot;p-value&quot;) + ylab(&quot;V&quot;) 16.2.4 rsignrank This function is used to simulate random values of the test statistic for null distributions for \\(V\\). Here’s a group of 5 random values from a distribution for a sample size of 10. The output values represent the sum of the positive sign ranks, aka the \\(V\\) test statistic values that would be generated from doing 10 different random experiments of this sample size…if the null hypothesis were true: rsignrank(5, 10) ## [1] 32 22 41 30 24 Let’s simulate 1,000,000 such experiments. And then let’s count the number of extreme values of \\(V\\) that would randomly occur in that number of experiments. We know from the qsignrank function that 11 is the critical value for a one-sided test at \\(\\alpha=0.05\\) for a sample size of 10. Thus,our “significance test” for each of the 1,000,000 random experiments is to ask whether \\(V\\) is below the critical value of 11 for that sample size. Given we’ve set a 5% false positive rate, we’d expect to see around 50000 false positives (5% of 1,000,000): sim &lt;- rsignrank(1000000, 10) test &lt;- sim&lt;11 length(which(test==TRUE)) ## [1] 42031 If you ran that chunk repeatedly you’d come up with something around 4.2% each time. Why is it not exactly 5%? What do you think that means? Are there experimental sample-sizes where the type1 error would be even further or closer to 5%? "],
["ranksum.html", "Chapter 17 Rank Sum Distribution 17.1 R’s Four Sign Rank Distribution Functions", " Chapter 17 Rank Sum Distribution The Wilcoxon rank sum per se represents a type of data transformation. The transformation is then used to calculate the nonparametric test statistic \\(W\\) for the Wilcoxon Test for independent two group samples, which is equivalent to the Mann-Whitney test. The distribution of \\(W\\) is discrete but normal-like. Given two groups for comparison, such as a control population, \\(X\\) vs a treatment population, \\(Y\\). Under the null hypothesis, the distributions of the values of \\(X\\) and \\(Y\\) are equal. Any effect due to treatment can be defined as \\(\\Delta = E(Y)-E(X)\\), where \\(E(Y)\\) and \\(E(X)\\) are the averages for the treatment and control effects, respectively. Under the null hypothesis, \\(\\Delta = 0\\). 17.0.1 Transformation of data into rank summs \\(X\\) has a sample size \\(m\\) and \\(Y\\) has a sample size \\(n\\) and the total sample size is \\(N=m+n\\). Combine all values of \\(X\\) and \\(Y\\) and rank them from smallest to largest. If \\(S_1\\) is the rank of \\(y_1,..,S_n\\) then W is the sum of the ranks assigned to the \\(Y\\) values: \\(W=\\sum_{j=1}^nS_j\\) 17.0.2 The sign rank test statistic in R When the null is true, the average of W is \\(E_0(W)=\\frac{n(m+n+1)}{2}\\) and the variance is \\(var_0(W)=\\frac{mn(m+n+1)}{12}\\). \\(W\\) can take on the values from zero to \\(mn\\). 17.1 R’s Four Sign Rank Distribution Functions 17.1.1 dwilcox Given a value of \\(W\\) and group sample sizes \\(m\\) and \\(n\\), dwilcox will return the value of the probability for that \\(W\\). For example, here is the probability of getting a \\(W\\) of exactly 10 with group sizes of 5 and 6: dwilcox(10, 5, 6) ## [1] 0.04978355 That probability is NOT a p-value. The p-value would be the sum of the probabilities returned from the dwilcox function for the range of \\(W\\) from 0 to 10. Given the range of possible values for \\(W\\) its distribution can be plotted for any combination of group sizes. We can see that a value of 10 for \\(W\\) is left-shifted, but not too extreme. m &lt;- 5 #number of independent of group 1 n &lt;- 6 #number of independent replicates of group 2 max &lt;- m*n df &lt;- data.frame(pv=dwilcox(c(0:max), m, n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p(W)&quot;) 17.1.2 pwilcox The pwilcox function returns a p-value when given \\(W\\) along with the sample sizes \\(m\\) and \\(n\\) corresponding to the two groups. Thus, the probability of obtaining a \\(W\\) value of 10 or less with group sample sizes of 5 and 6 is: pwilcox(10, 5, 6, lower.tail=T) ## [1] 0.2142857 The probability of obtaining a \\(W\\) value of 10 or more with group sample sizes of 5 and 6 is: pwilcox(10, 5, 6, lower.tail=F) ## [1] 0.7857143 The relationship of pwilcox to dwilcox is as depicted here there is a slight inequality between the two functions somewhere beyond 10 significant digits, thus the rounding: round(sum(dwilcox(c(0:10), 5, 6)), 10) == round(pwilcox(10, 5, 6), 10) ## [1] TRUE Here are the lower and upper tailed cumulative distributions of the Wilcoxon distribution: m &lt;- 5 n &lt;- 6 max &lt;- m*n df &lt;- data.frame(pv=pwilcox(c(0:max), m, n)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p-value for W&quot;) df &lt;- data.frame(pv=pwilcox(c(0:max), m, n, lower.tail=F)) ggplot(df, aes(x=c(0:max), pv)) + geom_col() + xlab(&quot;W&quot;) + ylab(&quot;p-value for W&quot;) 17.1.3 qwilcox This is the quantile signrank function in R. If given a quantile value (eg, 2.5%) and sample sizes \\(m\\) and \\(n\\), qwilcox will return the value of the corresponding test statistic \\(W\\). For example, this can be used to find the critical limits for a two-sided hypothesis test for an experiment with 10 independent replicates, when the type1 error of 5% is evenly distributed to both sides: qwilcox(0.025, 5, 6, lower.tail=T) ## [1] 4 qwilcox(0.025, 5, 6, lower.tail=F) ## [1] 26 Interpretation of critical limits output: When the null two-sided hypothesis is true, the values of \\(W\\) are \\(4\\le V\\le26\\). In other words, the null hypothesis would not be rejected if an experiment generated a \\(W\\) between 4 and 26. Alternative interpretation: \\(p&lt;0.05\\) when \\(4&gt;V&gt;26\\). In other words, a two sided null hypothesis would be rejected if it generated a \\(W\\) below 4 or greater than 26. And here are the critical limits for one-sided hypothesis tests for an experiment with 10 independent replicates, when the type1 error is 5%. Notice how the critical limits differ between one-sided and two-sided tests: qwilcox(0.05, 5, 6, lower.tail =T) ## [1] 6 qwilcox(0.05, 5, 6, lower.tail = F) ## [1] 24 Thus, having obtained a \\(W\\) of less than 6 or 24 and greater we would reject the null hypothesis. Do notice the distribution for these two sample sizes lacks perfect symmetry: pwilcox(6, 5, 6) ## [1] 0.06277056 pwilcox(24, 5, 6, lower.tail=F) ## [1] 0.04112554 The quantile distribution of \\(W\\) is depicted below: m &lt;- 5 n &lt;- 6 x &lt;- seq(0,1,0.01) df &lt;- data.frame(w=qwilcox(x, m, n, lower.tail=T)) ggplot(df, aes(x=x, w)) + geom_point() + xlab(&quot;p-value&quot;) + ylab(&quot;W&quot;) 17.1.4 rwilcox We would use rwilcox to generated random values of \\(W\\) from null distributions. Here are 7 random values of \\(W\\) for experiments involving sample sizes of \\(m=5\\) and \\(n=6\\) rwilcox(7, 5, 6) ## [1] 16 11 21 6 9 4 30 Let’s simulate 1,000,000 such experiments. And then let’s count the number of extreme values of \\(W\\) that would randomly occur in that number of experiments. We know from the qwilcox function that 24 is the critical value for a one-sided test at \\(\\alpha=0.05\\). Thus, our “significance test” for each of the 1,000,000 random experiments is to ask whether \\(W\\) is equal to or greater than 24. Recall, the cumulative distribution function indicates the p-value for a one-sided test of a \\(W=24\\) is: pwilcox(24, 5, 6, lower.tail=F) ## [1] 0.04112554 Given we’ve set a 5% false positive rate, we’d expect to see around 50000 false positives (5% of 1,000,000) by simulation of \\(W=24\\) under that scenario: sim &lt;- rwilcox(1000000, 5, 6) test &lt;- sim&gt;=24 length(which(test==TRUE)) ## [1] 62924 In this instance, we see a much higher number of false positive 6.2% than the expected number of 5%! Why? "],
["ttests.html", "Chapter 18 The t-tests 18.1 Data assumptions for t-tests 18.2 The t Statistic 18.3 t Test Hypotheses 18.4 Confidence Intervals of Means 18.5 t Tests: Running the analysis 18.6 Plotting t Tests 18.7 t Test Power", " Chapter 18 The t-tests The t-tests, otherwise known as “Student’s t tests” are for experimental designs that involve testing hypotheses comparing one group to a standard or two groups two each other, and where the outcome variable is continuous and on an equal interval scale. In other words, t-tests are for measured data in two or fewer groups. To recall, continuous data are on scales that have values between their intervals. For example, we might use a mass balance to measure the mass of individual mice, and then record how their weight change in response to some factor at two different levels. Here, mass would be an equal interval measurement. Mass is continuous, in so far as a theoretically infite number of values can exist between two units on the scale, depending upon the sensitivity of the measurements. Derivative continuous variables are common in t-testing. For example, growth rate, in grams/week, is a derivative of the mass and time measurement scales. Like mass and time, growth rate is also continuous. Just as importantly, the statistics derived from continuous, equal interval data also have the properties of continuous, equal interval data. For example, the means and standard deviations, the standard errors of the mean, and the t-test statistic itself are also scaled continuous. 18.1 Data assumptions for t-tests The following assumptions should be checked to use t-tests properly. 18.1.0.1 Strong assumptions, t tests are invalid if these not met: The replicate data arise from a randomized process. Each sample replicate is independent of every other sample replicate. 18.1.0.2 Weak assumptions: t-tests are less robust when these are not met: The sampled population for each group is normally distributed. The variances of the compared groups are equivalent. The smaller the sample size, the more difficult it is to validate these latter two assumptions. Unless you’re dealing with sample sizes of 30 or greater, it is probably futile to try to make these assessments. There are two equally valid ways to approach this. First, a property of continuous data is that it will be normally distributed when it is measured within a linear range on its scale. The same holds for the variance assumption. There are a normality tests Alternately, the researcher can conduct hypothesis testing using nonparametric procedures. The performance of t-tests and their nonparametric counterparts are virtually identical when the normality and homoscedasticity assumptions are met. Whereas nonparametrics statistics are an appropriate option when these conditions are unmet. 18.2 The t Statistic The t statistic is a ratio signifying the signal to noise ratio in a sample. The numerator would be a measure for the magnitude of the average effect, whereas the denominator would represent the precision by which those mean differences are determined. There are three t-tests, reflecting 3 very different experimental designs. The decision to use one or the other t-test is entirely scientific and based upon how the experiment is conducted. If you understand your experimental design, you’ll know which t test is applicable. One sample tests inspect the difference between a sample mean and a hypothetical mean. Unpaired tests inspect the difference between the means of two groups. *The paired t test inspects the mean difference between paired observations and the value zero. A common mistake is to default to an unpaired t-test if an uneven number of measurements are in the data set. That is only correct when the measurements are not intrinsically-linked. 18.2.1 One sample t tests For comparing the mean response \\(\\bar y\\) to a single level of a predictor variable against a hypothetical population mean, \\(\\mu\\). A group comprised of \\(n\\) independent observations, whose \\(\\bar y\\) The one sample t test has \\(n-1\\) degrees of freedom. \\[t=\\frac{\\bar y-\\mu}{\\frac{sd}{\\sqrt n}}\\] 18.2.2 Unpaired t tests This test is used for experiments in which the measurements are all independent from each other. Th unpaired t-test compares the difference between the mean responses to each of two levels, A and B, of a predictor variable. The design is comprised of a total of \\(n_A + n_B\\) observations and has \\((n_A + n_B)-2\\) degrees of freedom. \\(s^2_p\\) is the pooled variance of the sample. \\[t=\\frac{\\bar y_A-\\bar y_B}{\\sqrt{\\frac{s^2_p}{n_A}+\\frac{s^2_p}{n_B}}}\\] The pooled variance is calculated using the sum of the squared deviates \\(SS_A\\ and\\ SS_B\\) from each group as follows: \\[s^2_p=\\frac{SS_A+SS_B}{df_A+df_B}\\], where \\(df_A=n_A-1\\) and \\(df_B=n_B-1\\). The denominator of the \\(t\\) ratio is the standard error of the test. The standard error represents the test’s precision. Inspection of the pooled variance equation and how it factors into the calculation of the test statistic should give you some indication for why its not a good idea to have unequal sample sizes in unpaired experimental designs. If the sample size for one group is much larger than the other, it’s variance will dominate the \\(t\\) calculation. That becomes a more significant problem the more unequal are the group variances. 18.2.3 Paired t tests This test is used when measurements are intrinsically-linked. Each of \\(n\\) replicates is exposed to both levels, A and B, of a predictor variable. There are \\(n\\) pairs of measurements in the design. The mean difference \\(\\bar d\\) of the paired responses is compared to the value of zero. \\(sd_d\\) is the standard deviation of the \\(n\\) differences. The experiment has \\(n-1\\) degrees of freedom. \\[t=\\frac{\\bar d}{\\frac{sd_d}{\\sqrt n}}\\] 18.3 t Test Hypotheses As for whether to choose a paired or unpaired analysis, the choice of t-test hypotheses depends upon the experimental design, and the scientific question at hand. Resist the temptation to o toggle between two-sided and one-sided options. That’s p-hacking, which is a bias. Since t-tests are parametric, hypotheses are stated on the basis of the statistical parameters. In this case, the means of the samples are meant to infer the sampled population, so we revert to greek notation. To put this another way, that the two samples differ is a mathematical fact. We don’t need a statistical test to tell us that. The test helps us infer whether the populations that were sampled differ. One-sided hypotheses predict the direction of an effect. For example, “the response to treatment will be greater than control.” Or, “the response to treatment will be less than control.” Two-sided hypothesis do not predict the direction of an effect: “The response to treatment will differ from control, either higher or lower.” Therefore, use a one-sided hypothesis if you think your treatment will go in a specific direction. Choose a two-sided test when you’re not willing to bet on an effect’s direction. This matters because at the 95% confidence level the threshold value of the t statistic will be lower for a one-sided test ( eg,qt(0.05, 2) than for a two-sided test (qt(0.025, 2)) given the same data. Put another way, the “significance” threshold will always be a higher bar to cross for a two-sided hypothesis. For a one-sided test, all of the 5% cumulative probability is on one side of the distribution. For a two-sided test, that 5% is evenly split to both sides. Therefore, two-sided tests are slightly more stringent. 18.3.1 One sample hypotheses Two sided: Use when, going into an experiment, you are not sure which direction the predictor variable will change the outcome variable relative to the population mean. H_0: \\(\\bar x = \\mu\\) H_1: \\(\\bar \\ne \\mu\\) One sided: Use when, going into an experiment, you are confident the predictor variable will cause the outcome response to be higher than the population mean. H_0: \\(\\bar x \\le \\mu\\) H_1: \\(\\bar x&gt;\\mu\\) Or you are confident the predictor variable will cause the outcome response to be lower than the population mean. H_0: \\(\\bar x \\ge \\mu\\) H_1: \\(\\bar x&lt;\\mu\\) 18.3.2 Unpaired hypotheses Two sided: Use when, going into an experiment, you are not sure whether level A or B of the predictor will cause a higher outcome response. H_0: \\(\\bar x_A = \\bar x_B\\) H_1: \\(\\bar x_A \\ne \\bar x_B\\) One sided: Use when, going into an experiment, you are confident level A of the predictor variable will cause the outcome response to be higher than that for level B. H_0: \\(\\bar x_A \\le \\bar x_B\\) H_1: \\(\\bar x_A &gt; \\bar x_B\\) Or when you are confident the level A of the predictor variable will cause the outcome response to be lower than that for level B. H_0: \\(\\bar x_A \\ge \\bar x_B\\) H_1: \\(\\bar x_A &lt; \\bar x_B\\) 18.3.3 Paired hypotheses Two sided: Use when, going into an experiment, you are not sure whether the mean difference between levels of the predictor variable will be less than or greater than zero. H_0: \\(\\bar x = 0\\) H_1: \\(\\bar x \\ne 0\\) One sided: Use when, going into an experiment, you are confident the mean difference between levels of the predictor will be greater than zero. H_0: \\(\\bar x \\le 0\\) H_1: \\(\\bar x&gt;0\\) Or when, going into an experiment, you are confident the mean difference between levels of the predictor will be less than zero. H_0: \\(\\bar x \\ge 0\\) H_1: \\(\\bar x&lt;0\\) 18.4 Confidence Intervals of Means A 95% confidence interval is an inferential statistic that allows for estimating the accuracy of a sample. The 95% CI is the range of values in which there is a 95% chance the true mean exists. A 95% CI can be calculated as follows: \\(mean\\pm qt(0.975, df)*\\frac{sd}{\\sqrt n}\\) For example, given a sample size of n=3, that has a mean of 100. and a standard deviation of 25 upper.limit &lt;- 100+qt(0.975, 2)*(25/sqrt(3)) lower.limit &lt;- 100-qt(0.975, 2)*(25/sqrt(3)) upper.limit ## [1] 162.1034 lower.limit ## [1] 37.89656 18.5 t Tests: Running the analysis In R t.test represents a single function by which each of the three t test experimental designs can be analyzed. 18.5.1 One sample t test Let’s say a standard to measure against is the value of 100. We can ask if a random sample that is 2-fold greater is different than 100, less than 100, or greater than 100: set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25) t.test(a, mu=100, alternative=&quot;two.sided&quot; ) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.02635 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 129.1056 273.4744 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.9868 ## alternative hypothesis: true mean is less than 100 ## 95 percent confidence interval: ## -Inf 250.2778 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative =&quot;greater&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.01318 ## alternative hypothesis: true mean is greater than 100 ## 95 percent confidence interval: ## 152.3023 Inf ## sample estimates: ## mean of x ## 201.29 18.5.1.1 Interpretation t is a descriptive statistic. Notice how the t-value doesn’t change. The same sample will give the same signal to noise ratio, irrespective of the hypothesis tested. The p-value is the probability of making a type1 error by rejecting the null hypothesis. The p-value differs, given the hypothesis argument. The p-value is an inferential statistic. Notice how the p-value for a two sided test is larger than for a one sided test. Notice how the alternative output corresponds to the alternative input. Notice how the 95% confidence level is the default output. If you want a differnt confidence level, enter an argument for it. The 95% Confidence Interval differs for each hypothesis. Like the p-value, that indicates the 95% CI is an inferential statistic. The simplest way to get a typical 95%CI of a mean in R is to run a two.sided t.test and pull it out of that. 18.5.2 Unpaired t test Now will pull two random samples: one from a normal distribution that has a mean of 200, and the second from a distribution that has a mean of 100. Both have standard deviations of 25. Will these random samples differ? set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25) b &lt;- rnorm(3, mean=100, sd=25) t.test(a, mu=100, alternative=&quot;two.sided&quot; ) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.02635 ## alternative hypothesis: true mean is not equal to 100 ## 95 percent confidence interval: ## 129.1056 273.4744 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.9868 ## alternative hypothesis: true mean is less than 100 ## 95 percent confidence interval: ## -Inf 250.2778 ## sample estimates: ## mean of x ## 201.29 t.test(a, mu=100, alternative =&quot;greater&quot;) ## ## One Sample t-test ## ## data: a ## t = 6.0375, df = 2, p-value = 0.01318 ## alternative hypothesis: true mean is greater than 100 ## 95 percent confidence interval: ## 152.3023 Inf ## sample estimates: ## mean of x ## 201.29 18.5.2.1 Interpretation The interpretation is is similar to the one sample case. The “mean of x” corresponds to the a sample, or the first group in the argument. In the unpaired t test, good practice is to put the ‘treatment’ as the first data argument, and the ‘control’ as the second argument. 18.5.3 Paired t Test The structure of the data input matters. If entered as two vectors, the first value of the first vector will be paired with the first value of the second vector, and so on. set.seed(1234) a &lt;- rnorm(3, mean=200, sd=25);a ## [1] 169.8234 206.9357 227.1110 b &lt;- rnorm(3, mean=100, sd=25);b ## [1] 41.35756 110.72812 112.65140 t.test(a, b, alternative=&quot;two.sided&quot;, paired=T ) ## ## Paired t-test ## ## data: a and b ## t = 12.105, df = 2, p-value = 0.006756 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 72.86194 153.22676 ## sample estimates: ## mean of the differences ## 113.0443 t.test(a, b, alternative=&quot;less&quot;, paired=T) ## ## Paired t-test ## ## data: a and b ## t = 12.105, df = 2, p-value = 0.9966 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 140.314 ## sample estimates: ## mean of the differences ## 113.0443 t.test(a, b, alternative =&quot;greater&quot;, paired=T) ## ## Paired t-test ## ## data: a and b ## t = 12.105, df = 2, p-value = 0.003378 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 85.77465 Inf ## sample estimates: ## mean of the differences ## 113.0443 18.5.3.1 Intepretation Notice how, given the same data, the paired computes a different value for t compaired to the unpaired. The sample estimate for the paired analysis is the mean of the differences. The 95% CI is for the mean of the differences. There is a 95% chance the true mean of the differences in the population sampled is within the CI’s range of values. 18.6 Plotting t Tests 18.6.1 Unpaired A t test compares two means, so the plot should illustrate that comparison. There’s a growing movement to show all the data as points. Tidy the data, create a descriptive statistics summary table #data munge data.u &lt;- data.frame(control=b, treatment=a) %&gt;% gather(key=Predictor, value=Response) %&gt;% add_column(id=LETTERS[1:6]) #summary statistics table to have data.y &lt;- data.u%&gt;% group_by(Predictor) %&gt;% summarise( mean=mean(Response), sd=sd(Response), n=length(Response), sem=mean(Response)/sqrt(n) ) Plot the data points with mean +/- standard error of the mean. Since the dotplot produces the actual data, the variability of the sample can be ascertained. In this case, showing the SEM is appropriate. One of the more straight forward ways to add error bars is by use of the stat_summary function: ggplot(data.u, aes(Predictor, Response)) + geom_dotplot(binaxis=&#39;y&#39;, dotsize=1, stackdir=&quot;center&quot;) + stat_summary(fun.data=&quot;mean_se&quot;, fun.args=list(mult=1), geom=&quot;errorbar&quot;, color=&quot;red&quot;, width=0.2) + stat_summary(fun.y=mean, geom=&quot;point&quot;, color=&quot;red&quot;) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Computation failed in `stat_summary()`: ## &#39;what&#39; must be a function or character string 18.6.2 Paired Paired t test data should always be plotted with point-to-point lines to illustrate the paired experimental design. In the paired design, each replicate is a pair of observations. The means of the observations for the two groups are irrelevant. The design tests for the difference the treatment causes within each pair, which the slope of the line illustrates. For example, a horizontal line connect a replicate would indicate no effect of the predictor variable! Munge the data into a long data frame format. Adding an ID for each replicate allows using it as a grouping variable for the plot data.w &lt;- data.frame(control=b, treatment=a, id=LETTERS[1:3]) %&gt;% gather(key=Predictor, value=Response, -id) ggplot(data.w, aes(Predictor, Response, group=id)) + geom_point(size=2) + geom_line(color=&quot;red&quot;) 18.7 t Test Power A power analysis should precede every planned experiment to decide upon an optimal sample size. The principle output of a power function is the sample size that would be necessary to conduct a well-powered experiment. The pwr library has the pwr.t.test function that can be used for this purpose and is very simple to execute. This function requires that several decisions be made in advance. First, it takes arguments for acceptable type1 error and for intended power. The standard for these is 5% and 80%, respectively, but you can use any level you deem appropriate. Two other arguments are type, which is the experimental design, and alternative, which is the hypothesis. Finally, there is the d argument, which is Cohen’s \\(\\delta\\). This is less obvious, but very important and simple to understand. Cohen’s \\(\\delta\\) is the signal to noise for the effect size you anticipate. For example, imagine you anticipate measurements that will have a standard deviation of 25 units. You estimate this value on the basis of your own preliminary data or published information. You also believe the average response to a control level of the predictor varible will be around 50 units. In your mind, a minimally scientifically valid treatment effect will have a 100 unit response. The signal will be 100-50=50, andn the noise will be 25. Cohen’s \\(\\delta\\) will therefore be 50/25=2. A power calculation for a one-sided hypothesis is illustrated for a unpaired design: pwr.t.test(d=2, sig.level=0.05, power=0.8, type=&quot;two.sample&quot;, alternative=&quot;greater&quot;) ## ## Two-sample t test power calculation ## ## n = 3.986998 ## d = 2 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater ## ## NOTE: n is number in *each* group 18.7.1 Interpretation The experiment should have a total sample size total of 8. Randomly allocate each of two levels of the predictor variable to 4 replicates each. Notice how this function produces a bizarra, fractional sample size. There is no such critter as a partial replicate! People seem to have difficulty with Cohen’s delta. It’s nothing more complicated than a simple signal-to-noise estimate. Don’t make the mistake of putting too fine of a point on Cohen’s delta. Scientific judgement is required to calculate the values to use to calculate Cohen’s delta. Use your best scientific guess for standard deviation and the response magnitude, and err on the conservative side. "],
["ttestmc.html", "Chapter 19 Statistical design of t-tests 19.1 About this chapter 19.2 One sample t-test Monte Carlo 19.3 Unpaired t-test Monte Carlo 19.4 Paired t-test Monte Carlo", " Chapter 19 Statistical design of t-tests library(datapasta) library(tidyverse) 19.1 About this chapter This chapter walks through a statistical design for three different types of t-test experiments; the one-sample, the unpaired and the paired t-tests. Each of these examples use Monte Carlo simulation to assess experimental power at given sample and effect sizes, so you’d know many replicates are needed for a real life experiment. For preliminary data, these all use information in a mouse diabetes study conducted by the Jackson Labs. This is done to illustrate how existing data can be used to refine predictions about the values we might expect in our own experiment. In a single cycle of a Monte Carlo t-test simulation, a random distribution function is used to simulate a random sample. This mimics how the values for replicates might be generated in a real life experiment. The key distinction between real life and simulated samples is that we know the true parameters of the sampled population for the latter…because we code them in! In these t-test cases, we’ll use the rnorm function to generate sample replicates. That’s because, ideally, t-tests are only used on dependent variables that are continuous and normally distributed. In this case, we’ll be simulating experiments that involve measuring glucose concentration, which is a continuous, normally distributed variable. Having a simulated sample of a proper sample size, a t-test is next configured to test that sample, and then the p-value for it is collected. A p-value less than 0.05 would be counted as a “hit”. A cycle of random sampling, t-testing, and p-value evaluation is repeated anywhere from 100 to 10000 times, depending upon how accurate you’d like to be. The fraction of ‘hits’ relative to the number of simulations is the power of the test. The higher the number of simulations, the more accurate the Monte Carlo in terms of predicting “hits” and power. If the result is not an acceptable power, change the sample size n in the rnorm function until a Monte Carlo simulation run gives you an acceptable fraction of ‘hits’. There’s your sample size! Furthermore, any other assumptions or conditions can be changed, too. Need to re-evaluate the predicted standard deviation? Change it! Will the effect size be larger or smaller than you think? Simulate that! Want to compare a one-tailed to a two-tailed hypothesis? Switch it up! The time to do p-hacking and HARKing is during a Monte Carlo, before running the real life experiment. 19.1.1 Scenario Let’s imagine we have developed a new drug we hope will be useful for treating type II diabetes. Our role is to generate pre-clinical data in support of an FDA application. The planning is based on some mouse phenome data in a diet-induced obesity study in a mouse strain, which is a common model for type II diabetes. On the basis of expertise in the field, we make the judgment that a 50% reduction in blood glucose caused by the drug in this diet-induced obesity model would be a scientifically meaningful outcome. We need to design an experiment capable of detecting that 50% effect size. We’ve already compiled the phenome data into summary format [link to chapter]. We’ll use that to guide our estimates in these Monte Carlo. These data show that under diabetogenic conditions the animals have an average blood glucose of 368 and SD = 119 (mg glucose/dl). Since this is an exercise in prediction and estimation, we’ll round those values to 370 and 120, if only to make the point that highly precise numbers misses the point of what we’re doing here. A 50% reduction would yield a target value of 185 mg/dl. Finally, we’d like to run the real life experiments at 90% power. Why? Let’s imagine that these are a pretty important test: a “go” vs “no go” inflection point for a novel drug candidate. The stakes are high. 19.2 One sample t-test Monte Carlo In this single arm design, all C57Bl/6J mice are enrolled in an diabetogenic protocol would receive the drug treatment. Blood glucose levels are taken at the end of a proscribed period. The statistical test evaluates the null hypothesis, that mean blood glucose with drug treatment does not differ from the mean blood glucose that would be expected in animals that undergo the diabetogenic protocol. There is no formal placebo control group. Step 1: Enter mean and sd parameter estimates for the expected effect of the new drug. The sd estimate is a judgment call to think through and to model out. The entry below is conservative. It assumes the drug-treated group has the same sd as an untreated group. The Jaxwest7 data suggest a lower sd might happen with drug (the sd was 80 for the rosiglitazone group). Also enter an estimate for the theoretical mean of the population it will be evaluated against. Finally, enter a value for the sample size of the drug-treated group. meanDrug &lt;- 180 sdDrug &lt;- 120 muCon &lt;- 370 nDrug &lt;- 5 Step 2: Declare relevant arguments for the t-test function: alt =&quot;two.sided&quot; pairing = FALSE var = FALSE alpha=0.05 Step 3: Declare the number of simulations for the experiment and set up an empty vector to collect p-values generated by the function. nSims &lt;- 1000 p &lt;- c() Step 4: Run the simulation function. Notice how with each loop it simulates a new random sample then runs a on-sample t-test on that sample, then stores the p-value in a vector that grows with each loop. for(i in 1:nSims){ x&lt;-rnorm(n = nDrug, mean = meanDrug, sd = sdDrug) z&lt;-t.test(x, alternative = alt, paired = pairing, mu=muCon, var.equal = var, conf.level = 1-alpha) p[i]&lt;-z$p.value #get the p-value and store it } Step 5: Calculate and show “hits” and power. Remember, a “hit” is a simulation with a p-value &lt; 0.05. Power is the fraction of all simulations that meet this hit critria. # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 764 power &lt;- hits/nSims; power ## [1] 0.764 Step 6: Visualize the p-value output with a histogram. Because it’s pretty. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) Next steps: The returned power for the estimates above is about 75%. That’s a bit lower than a power of 90%, which we’d like here. Change the value of the nDrug term to a higher sample size, before re-running the simulation loops, until a power of ~90% is achieved. Change the estimate for sdDrug, too. How does lowering that influence sample size for 90% power? Why 90%? That’s both a scientific and strategic call. In this instance a positive result will have important implications for committing further to a costly drug development process. For that reason, the study should be run at a higher power than what might be chosen for a test that is more exploratory in nature. 19.3 Unpaired t-test Monte Carlo This is an alternative experimental design to the one above. A group is used to directly measure glucose concentration in placebo control for comparison to the drug effect, rather than assume what the glucose concentration would be in the placebo control This design therefore involves two groups of animals. All animals would be subjected to the diabetes-inducing diet. In the control arm, the group would receive a placebo. In the experimental arm, the group would receive the new drug. Each animal would be assumed as statistically independent of every other animal. The objective is to test the null hypothesis that the means of the blood glucose concentrations do not differ between the two groups. Step 1: Let’s call the “A” group the placebo, and the “B” group the drug treated. We’ll use standard deviation and the mean estimates for blood glucose levels as described above. We’ll design for equal sample sizes, though this test can tolerate differences. #Sampled population paramemters # sample A placebo meanA &lt;- 380 sdA &lt;- 120 nA &lt;- 5 # sample B new drug meanB &lt;- 190 sdB &lt;- 120 nB &lt;- 5 Step 2: Set the t-test function arguments as initializers, rather than down in the loop function, so they are easy to read and to modify. #t-test function arguments alt&lt;- &quot;two.sided&quot; pairing &lt;- FALSE var &lt;- TRUE alpha &lt;- 0.05 Step 3: Declare the number of simulations. The larger the number of simulations, the more accurate will be the power calculation. Also set up an empty vector for the following function to fill as it cycles through simulations and generates p-values. nSims &lt;- 10000 #number of simulated experiments p &lt;- c() Step 4: Run the simulation function. # the monte carlo function for(i in 1:nSims){ #for each simulated experiment x&lt;-rnorm(n = nA, mean = meanA, sd = sdA) #produce n simulated participants #with mean and SD y&lt;-rnorm(n = nB, mean = meanB, sd = sdB) #produce n simulated participants #with mean and SD z&lt;-t.test(x,y, alternative = alt, paired = pairing, var.equal = var, conf.level = 1-alpha) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } Step 5: Print out the power, which is the number of “significant” results divided by the total number of simulations. # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 5973 power &lt;- hits/nSims; power ## [1] 0.5973 Step 6: Plot out the distribution of p-values. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) Next steps: This configuration with a sample size of 5 in each group is a bit underpowered. Adjust these sample sizes to dervied a power of about 90%. Also experiment with adjusting other features of the test. What happens if the SD for the drug-treated group is lower? What about a one-tailed hypothesis instead of a two-sided? Monte Carlo is the time for p-hacking and harking. 19.4 Paired t-test Monte Carlo Another way to test whether the new drug can reduce blood sugar concentrations is by using a paired design. Paired designs offer some level of control over random variability by assigning some of it to the variation within subjects. For example, individuals may differ wildly in their absolute blood glucose concentrations, but the magnitude of change due to drug from subject to another will be fairly consistent. The paired t-test challenges the null hypothesis that the average change in blood glucose caused by the drug is zero. Step 1: When simulating data for a paired design it is important to account for the expected correlation between variables. That’s best accomplished on the basis of some data. We can use the serial glucose measurements from individual subjects in the Jaxwest7 data set to extract this information. There are two daily blood glucose measurements taken on days 1, 3, 5, 7, 9, 11 and 12 of a study, from each of 16 different subjects. We can think of each blood collection as a variable, for which 16 independent replicate measurements are taken. Across the blood collections we expect to see high correlation within the replicates. In other words, animals with high values should be consistently high across the study period, and animals with low values should be consistently low across the same time frame. #Copying cells F14:S32 of the Jaxwest7 table (the value at F21 was imputed as the average of its row before pasting) using the datapasta package. bloodGlucose &lt;- data.frame( day01 = c(136L, 345L, 190L, 434L, 424L, 170L, 487L, 218L, 179L, 260L, 115L, 526L, 325L, 329L, 230L, 204L), day01 = c(270L, 518L, 301L, 504L, 486L, 208L, 449L, 273L, 184L, 381L, 191L, 517L, 252L, 296L, 414L, 120L), day03 = c(162L, 429L, 311L, 453L, 447L, 134L, 525L, 254L, 124L, 174L, 132L, 465L, 203L, 212L, 408L, 138L), day03 = c(165L, 413L, 361L, 392L, 417L, 129L, 419L, 265L, 107L, 140L, 132L, 394L, 158L, 159L, 179L, 139L), day05 = c(192L, 456L, 398L, 350L, 496L, 147L, 437L, 338L, 108L, 132L, 169L, 310L, 135L, 156L, 432L, 157L), day05 = c(397L, 487L, 465L, 400L, 484L, 141L, 476L, 386L, 149L, 138L, 158L, 269L, 162L, 200L, 288L, 122L), day07 = c(172L, 468L, 388L, 458L, 468L, 241L, 525L, 287L, 142L, 164L, 129L, 213L, 164L, 139L, 163L, 163L), day07 = c(148L, 419L, 392L, 387L, 423L, 128L, 499L, 236L, 143L, 137L, 120L, 185L, 181L, 143L, 240L, 168L), day09 = c(291L, 507L, 453L, 342L, 472L, 162L, 516L, 347L, 112L, 122L, 122L, 145L, 150L, 164L, 185L, 164L), day09 = c(239L, 559L, 421L, 368L, 507L, 163L, 485L, 235L, 233L, 140L, 157L, 201L, 177L, 150L, 208L, 128L), day11 = c(192L, 420L, 355L, 355L, 458L, 222L, 472L, 432L, 113L, 102L, 94L, 131L, 162L, 119L, 138L, 129L), day11 = c(172L, 415L, 381L, 429L, 456L, 438L, 535L, 450L, 137L, 174L, 141L, 258L, 192L, 193L, 208L, 218L), day12 = c(235L, 511L, 394L, 373L, 519L, 307L, 500L, 509L, 106L, 120L, 120L, 114L, 170L, 148L, 153L, 135L), day12 = c(153L, 464L, 444L, 501L, 570L, 252L, 497L, 326L, 150L, 135L, 166L, 160L, 162L, 188L, 140L, 182L) ) We calculate the correlation between any two daily sets of values. In fact, we can calculate the correlation between all possible pairs of daily values. This leaves us with a large number of unique correlation coefficients. We then derive an overall average correlation coefficient. #create a full correlation matrix cormat &lt;- cor(bloodGlucose) #remove lower half of matrix cormat[lower.tri(cormat)] &lt;- NA #remove matrix diagonal cormat[cormat==1.0000000] &lt;- NA How correlated are the glucose levels in the Jaxwest7 data set? #calculate the average correlation coefficient among all the correlations in the Jaxwest7 glucose level data set mean(cormat, na.rm=T) ## [1] 0.7665732 #phew! What does this value mean? First, when the value of the correlation coefficient between the variables \\(X,Y\\) is \\(r\\), then the relationship between each pair of \\(x_i, y_i\\) values in the set is \\[y_i=x_i\\times r+y_i\\sqrt{1-r^2}\\] (Intuitively, this makes sense. You can see, when \\(r=0\\), then \\(y_i=y_i\\). When \\(r=1\\), then, \\(y_i=x_i\\)) So the correlation coefficeint from the Jaxwest7 data set means that within each subject in our experiment the expected correlation between pre-drug glucose concentrations and post-drug glucose concentrations is 0.7666. Step 2: Initialize the Monte Carlo with estimates for the measurement values. We start with the mean and sd values for the pre-drug blood glucose measurements. Their estimates are derived from the placebo group in the Jaxwest7 data set, rounded to 380 and 120, respectively. A scientifically-meaningful effect of the drug would be a 50% reduction in glucose. We want to set up an experiment that can detect that effect. The expected correlation between pairs of measures is 0.7666, rounded to 0.75. #Sampled population paramemters # pre-drug measurements mean1 &lt;- 380 sd1 &lt;- 120 # post-drug response effect &lt;- 0.5 r &lt;- 0.75 k &lt;- sqrt(1-r^2) # number of paired measures pairs &lt;- 10 Step 3: This step sets the arguments in the t-test function. Even though we predict a reduction in glucose, we’ll test this as a two-tailed hypothesis. It’s a little more stringent. The t.test function needs to be set for paired=TRUE so that it runs the appropriate test. #t-test function arguments alt&lt;- &quot;two.sided&quot; pairing &lt;- TRUE var &lt;- TRUE alpha &lt;- 0.05 Step 4: Declare the number of simulations. The larger the number of simulations, the more accurate will be the power calculation. Also set up an empty vector to fill with p-values, as they are generated each cycle. nSims &lt;- 10000 #number of simulated experiments p &lt;- c() Step 5: Re-simulate and re-run the t-test nSims times. The x and y vectors are each a set of randomly generated values for the pre- and post-drug measurements, respectively. Both measures need to be simulated as random variables. But the y vector needs to be corrected for its correlation with x. That is accomplished with the w vector. The values of x and y are correlated and then adjusted for the expected size of the drug effect (effect). for(i in 1:nSims){ #for each simulated experiment x&lt;-rnorm(n = pairs, mean = mean1, sd = sd1) #produce n simulated participants #with mean and SD y&lt;-rnorm(n = pairs, mean = mean1, sd = sd1) #produce n simulated participants #with mean and SD #correlated w &lt;- (x*r+y*k)*effect z&lt;-t.test(x,w, alternative=alt, paired=pairing, var.equal=var, conf.level=1-alpha) #perform the t-test p[i]&lt;-z$p.value #get the p-value and store it } Step 6: Calculate power as the fraction of p-values less than 0.05. # the output hits &lt;- length(which(p &lt; alpha)); hits ## [1] 9588 power &lt;- hits/nSims; power ## [1] 0.9588 Step 7: Visualize the p-value distribution. #now plot the histogram ggplot(data.frame(p))+ geom_histogram(aes(p), color=&quot;#f2a900&quot;, fill=&quot;#012169&quot;, bins=20) library(pwr) pwr.t.test(n=10, d=1.58, sig.level=0.05, type=&quot;paired&quot;, alternative=&quot;two.sided&quot;) ## ## Paired t test power calculation ## ## n = 10 ## d = 1.58 ## sig.level = 0.05 ## power = 0.9929412 ## alternative = two.sided ## ## NOTE: n is number of *pairs* pwr.t.test(n=9, d=1.58, sig.level=0.05, type=&quot;two.sample&quot;, alternative=&quot;two.sided&quot;) ## ## Two-sample t test power calculation ## ## n = 9 ## d = 1.58 ## sig.level = 0.05 ## power = 0.8819622 ## alternative = two.sided ## ## NOTE: n is number in *each* group "]
]
