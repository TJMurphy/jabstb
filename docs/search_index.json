[
["anovamc.html", "Chapter 34 ANOVA power using Monte Carlo 34.1 Alternatives to Monte Carlo 34.2 What is Monte Carlo 34.3 One-way completely randomized ANOVA Monte Carlo 34.4 One-way related measures ANOVA Monte Carlo 34.5 Directions", " Chapter 34 ANOVA power using Monte Carlo library(ez) library(tidyverse) library(viridis) Monte Carlo simulation is a great tool for experimental design. Monte Carlo forces you to do several things that, taken together, we can characterize as good statistical hygiene. Explicitly define predictor variables and their levels Explicitly consider expected values and variation of the response variable Select scientifically meaningful groups to compare Consider which hypotheses is to be tested in advance Define a minimal scientifically meaningful effect size Establish a sample size a priori Establish the data munging and statistical analysis a priori Establish feasibility of the experiment a priori If you want to do unbiased research (and you should) Monte Carlo simulation helps you do exactly that. Design the experiment in advance. Analyze a simulation of the experiment, in advance. Figure out what you are going to do, in advance. Then go and conduct and analyze the experiment you designed. The primary output of the Monte Carlo script below is experimental power at given sample sizes. You run the script primarily to know the number of independent replicates that you’ll need to observe for the effect size that you anticipate. But conducting a Monte Carlo offers much more than just sample size. An ability to ‘see’ experimental outcomes prior to running an experiment is the heartbeat of turning predictions into strong testable hypotheses. Poking around with simulated data before running an experiment helps you evaluate many of your assumptions. Which helps you see what might be missing. It also helps you practice an analysis. When working with simulated data, there is nothing unethical about p-hacking, harking and data snooping. Design the analysis in a way that gives you the best chance of observing the result you expect! Figuring that out before the real data set is in front of you helps you avoid ethical gray areas after the real data set is in front of you. 34.1 Alternatives to Monte Carlo Using R you can run the power.anova.testfunction in the base stats package or pwr.anova.test in the pwr package. To produce a sample size outpu t, both of these require you to enter number of groups, type1 error and type2 error tolerance. The former requires estimates for model (between) and residual (within) variance. These can be entered as relative values. The latter requires an entry for Cohen’s \\(f\\). \\[f=\\sqrt{\\frac{\\eta^2}{1-\\eta^2}}\\] where \\(\\eta^2=\\frac{SS_{model}}{SS_{total}}\\) for a completely randomized factor, whereas for a related measures factor it is \\(\\eta_p^2=\\frac{SS_{model}}{SS_{model}+SS_{residual}}\\) 34.2 What is Monte Carlo As the name implies, Monte Carlo is based on random chance. In a single cycle of a Monte Carlo you simulate a random sample for an experiment, running the statistical analysis and deciding whether that experiment work or not? Then you repeat that random sampling and analysis. Many times. Hundreds or even thousands of times. What fraction of these repetitions worked? That fraction is the experimental power. If the power is too low, re-run the simulated repetitions of the experiment on a larger sample size. Or re-design the experiment entirely. Once the Monte Carlo simulations result gives you an acceptable power, you have a sample size by which to go and conduct the real experiment. The process of creating a Monte Carlo script forces you to consider, explicitly, the variables in the experiment. What are their expected values? What effect size do you predict or think will be minimally scientifically significant? How many groups are in the experiment? Do these groups help you answer the question you really want answered? Should you add more groups or fewer groups? Is the experiment under- or over-designed, or even feasible? Is there a better way to answer the question? Spending an hour or two going through this process in silico, optimizing and validating the experimental design, can save months of futile effort doing the wrong experiment in real life. The first series of scripts below are for a one-way completely randomized ANOVA design. After that, a one-way related measures ANOVA Monte Carlo is shown. Their simulations differ in a couple of key respects. 34.3 One-way completely randomized ANOVA Monte Carlo The Outcome variable is random, continuous, and measured on an equal, or nearly equal interval, scale. The experiment is comprised of \\(k\\ge3\\) groups representing different levels of the Predictor variable. The group variances are equivalent. Every replicate (n) is independent of every other replicate. 34.3.1 Directions Create initial values. Visualize one random sample. Run the Power Simulator Change n and repeat until a suitable power is achieved. 34.3.2 Step 1: Create initial values This is where you predict results. I like making these Monte Carlo projects using a stand-alone initializer section. The whole point of the script is to play around with different initial parameters! Using an initializer section means you change these in only one place. Here’s the logic: First, provide an estimate for the expected mean of the outcome variable under basal conditions. Next, predict the fold-to-basal effect sizes for the positive control and also the treatment we’re planning to test. Next, estimate the value of the standard deviation of the Outcome variable. Finally, select the number of replicates per group (n per k) that we might run. b = 100 #expected basal outcome value a = 1.5 #expected fold-to-basal effect of our positive control f = 1.25 #minimal scientifically relevant fold-to-basal effect of our treatment sd = 25 #expected standard deviation of Outcome variable n = 3 # number of independent replicates per group sims = 100 #number of Monte Carlo simulations to run. 34.3.3 Step 2: Visualize one random sample This produces one graph illustrating an example of the type of random samples we’re generating in the Monte Carlo. No two samples will be the same, because they’re each random! So this figure will change every time it is run, even with the same initial parameter values. And because variances can differ a lot between groups with small sample sizes (n&lt;35). # Use this script to simulate your custom sample. CRdataMaker &lt;- function(n, b, a, f, sd) { a1 &lt;- rnorm(n, b, sd) #basal or negative ctrl a2 &lt;- rnorm(n, (b*a), sd) #positive control or some other treatment a3 &lt;- rnorm(n, (b*f), sd) #treatment effect Outcome &lt;- c(a1, a2, a3) Predictor &lt;- c(rep(c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;), each = n)) ID &lt;- as.factor(c(1:length(Predictor))) df &lt;-data.frame(ID, Predictor, Outcome) } dat &lt;- CRdataMaker(n,b,a,f,sd) ggplot(dat, aes(Predictor, Outcome))+ geom_jitter(width=0.1,size = 4, alpha=0.5) Is that what you expect to see? Should there be more variation? Is the effect size larger or smaller than what you anticipate? Should you add more or fewer groups? Edit the code in the initializer to make it as real life as possible. 34.3.4 Step 3: Run The Power Simulator Returns power (as a percentage value) for an experiment conducted given the values of the initializer variables. Change n in the initializer above to see what it takes to get 80% or better power. Carefully look at this code. It creates an ezANOVA output object, and then goes in to collect the p-value of the ANOVA F test from the ANOVA table. pval &lt;- replicate( sims, { sample.df &lt;- CRdataMaker(n, b, a, f, sd) sim.ezaov &lt;- ezANOVA( data = sample.df, wid = ID, dv = Outcome, between = Predictor, type = 2 ) pval &lt;- sim.ezaov$ANOVA[1,5] } ) pwr.pct &lt;- sum(pval&lt;0.05)/sims*100 paste(pwr.pct, sep=&quot;&quot;, &quot;% power. Change &#39;n&#39; in your initializer for higher or lower power.&quot;) ## [1] &quot;38% power. Change &#39;n&#39; in your initializer for higher or lower power.&quot; ggplot(data.frame(pval))+ geom_histogram(aes(pval), color=&quot;#d28e00&quot;)+ labs(x=&quot;p-value&quot;) This could be modified. For example, here it collects the post hoc comparison between the test group and the negative control. Why is the power so much lower for this comparison compared to the power for a positive ANOVA? pval &lt;- replicate( sims, { sample.df &lt;- CRdataMaker(n, b, a, f, sd) pval &lt;- pairwise.t.test(sample.df$Outcome, sample.df$Predictor)$p.value[2,1] } ) pwr.pct &lt;- sum(pval&lt;0.05)/sims*100 paste(pwr.pct, sep=&quot;&quot;, &quot;% power. Change &#39;n&#39; in your initializer for higher or lower power.&quot;) ## [1] &quot;11% power. Change &#39;n&#39; in your initializer for higher or lower power.&quot; ggplot(data.frame(pval))+ geom_histogram(aes(pval), color=&quot;#d28e00&quot;)+ labs(x=&quot;p-value&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 34.3.5 Step 4: Optimize for suitable power The initializer is preconfigured with a fairly crappy experiment, comprised of a low sample size and not particularly strong effect size. As you can see the power calculation is also pretty crappy. About a third of experiments run in this way are likely to yield positive results. Iteratively increase n until you get a more defensible level of power. Or maybe you need to change the estimates for the effect sizes? 34.3.6 Notes And Considerations At k = 3 this is the minimal ANOVA design (any fewer groups and we’d just do a t test). This k = 3 experiment represents a minimal defensible scientific experiment: It has a negative control, a positive control, and the experimental. The example above is also a simple one-way ANOVA completely randomized. Modifying the code to two-way and even three-way ANOVA designs is fairly trivial. You just add more groups. For example, if you have a second factor, Factor B and two levels, estimate the b1 and b2 results in much the same way the a1, a2 and a3 results are estimated. You’ll need to adjust how the data frame is created, adding a column for the levels of Factor B. Further modification will likely be necessary to collect a run of p-values for a specific group comparison. You may wish to grab the p-value for the interaction F test. Alternately, you may wish to go for a key posthoc comparison. 34.4 One-way related measures ANOVA Monte Carlo The script below is a modification of that above, so that one way related measures ANOVA experiments can be simulated. There are two key differences. The first is adding a correlation coefficient to the initializer. The second is in the arguments for the ANOVA test function (from classifying the predictor variable from between to within). The intrinsically-linked measurements that are a hallmark of related measures ANOVA tend to have high inherent correlation. This correlation should be taken into account when simulating related/repeated measure ANOVA experiments. 34.5 Directions Create initial values. Visualize one random sample. Run the Power Simulator Change n and repeat until a suitable power is achieved. 34.5.1 Step 1: Initial values Here’s our initializers. Note the added terms r and k. Read this to understand what they represent and how you can estimate them for your own system. b &lt;- 100 #expected basal outcome value a &lt;- 1.25 #expected fold-to-basal effect of our positive control f &lt;- 1.4 #minimal scientifically relevant fold-to-basal effect of our treatment sd &lt;- 25 #expected standard deviation of Outcome variable n &lt;- 3 # number of independent replicates per group r &lt;- 0.9 #correlation between groups of outcome values across replicates k &lt;- sqrt(1-r^2) #a conversion factor sims = 100 #number of Monte Carlo simulations to run. Keep it low to start. 34.5.2 Step 2: Visualize one sample Make sure you understand how the a2 and a3 variables were each correlated to the a1 variable. Let’s call a1 the pivot variable. When simulating correlated values, your pivot variable should be the lowest expected outcome value in the data set. # Use this script to simulate your sample. RMdataMaker &lt;- function(n, b, a, f, sd, r, k) { a1 &lt;- rnorm(n, b, sd) #basal or negative ctrl a2 &lt;- a1*r+k*rnorm(n, (b*a), sd) #positive control or some other treatment a3 &lt;- a1*r+k*rnorm(n, (b*f), sd) #treatment effect Outcome &lt;- c(a1, a2, a3) Predictor &lt;- c(rep(c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;), each = n)) ID &lt;- rep(as.factor(c(1:n))) df &lt;-data.frame(ID, Predictor, Outcome) } dat &lt;- RMdataMaker(n,b,a,f,sd,r,k) ggplot(dat, aes(Predictor, Outcome, color=ID, group=ID) )+ geom_line()+ geom_point(size = 4, shape=5)+ scale_color_viridis(discrete=T)+ theme_classic() 34.5.3 Step 3: Run the power simulator Now we can simulate a long run of random samples like above, capturing how many of them yielded a positive result! pval &lt;- replicate( sims, { sample.df &lt;- RMdataMaker(n, b, a, f, sd, r, k) sim.ezaov &lt;- ezANOVA( data = sample.df, wid = ID, dv = Outcome, within = Predictor, type = 2 ) pval &lt;- sim.ezaov$ANOVA[1,5] } ) pwr.pct &lt;- sum(pval&lt;0.05)/sims*100 paste(pwr.pct, sep=&quot;&quot;, &quot;% power. Change &#39;n&#39; in your initializer for higher or lower power.&quot;) ## [1] &quot;99% power. Change &#39;n&#39; in your initializer for higher or lower power.&quot; ggplot(data.frame(pval))+ geom_histogram(aes(pval), color=&quot;#d28e00&quot;)+ labs(x=&quot;p-value&quot;, title=&quot;p-value distribution&quot;) 34.5.4 Step 4: Should anything be changed? The experiment simulated above has an n=3 and power of 99%! Notice how it has the same estimated group values as for the completely randomized simulation above, which yields only ~30% power. That’s the efficiency of related measures! You might want to experiment with changing correlation coefficient and sample size to see how the results are affect. As for the completely randomized scripts, this one can be easily extended by adding more groups, or even more factors. You’ll need to make changes in all three script stages to customize your needs. "]
]
