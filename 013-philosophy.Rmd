# What is your philosophy? {#philos}

As a new PhD student you are in a significant transition state. 

You've been a really good knowledge consumer. Otherwise you wouldn't have been accepted into a highly competitive Ph.D. program. 

Now things are different. An important condition of your Ph.D. will be to create an original contribution to knowledge. In other words, your graduate program training is mostly about the transition from a knowledge consumer into a knowledge producer. 

The two require different toolkits.

For example, a knowledge consumer might use statistics as only rote process...that there are right ways and wrong ways to do things. They might say, "Tell me the rules and procedures and I'll just follow them." Or they might ask, "tell me about when one test works better than some other?" 

A knowledge producer will want to go deeper to understand why things are done the way they are. To question their validity and wonder if they can be improved. They may even change the process, adapting rules and procedures that fit her philosophy better. Or they will crack open the computer to see how functions work and simulate data sets to see how tests perform under different assumptions. 

A knowledge producer makes adjustments and is able to defend their approach.

You'll find this course is mostly about pointing out the various statistical decisions you'll need to make as you go about creating new knowledge. In this course I am going to hammer home the point, over and over, that you have a responsibility to think critically about your statistical practice. Hopefully you'll see that how we go about collecting and analyzing data is more important than the data itself. 

In a statistical design framework, our intentions and our data generation process matter. How we think about the analysis matters. We have a lot of judgments to make.

To a large extent, the experimental design and analysis procedures we choose are driven by our philosophical approach to science. From time to time we need to lean on philosophy when called to defend our decisions (for example, when interpreting results, or in rebutting a reviewer, or when deciding to move forward or to stop some project).

When I say philosophy I'm thinking in terms of the really fundamental ideas. Do you think truth is absolute and attainable, or provisional? What is the nature of evidence? What are the merits of inductive and deductive reasoning and how much weight should you give to each in your problem solving process? How should you test hypotheses, through affirmation or falsification? Are you more Popperian (deduction, falsification) than Carnapian (induction, confirmation)? 

What are your biases? Do you and your workflow have integrity? 

Are you a perfectionist or a dontgiveafuctionist or somewhere in between?

It is a big, diverse world. Reasonable people operate on either side of such philosophical and behavioral divides. These questions have a lot of gray areas. 

My sense is that most of us biomedical scientists are hybrids. We go about discovery through a mixture of inductive and deductive practices. The process of making observations then establishing models before seeking confirmation in related experiments is both inductive and empirical. Whereas experiments that test predictions based upon some model generate observations that require deductive reasoning. We often use deduction when formulating a logic for deciding to open up a new thread of research.

## Exploration and uncertainty

What everyone seems to agree upon is that creating knowledge occurs through exploration. And to explore means we have to deal with uncertainty. 

By what criteria will you ensure you are not making mistakes? By what criteria will you validate an observation? How will you ensure a hypothesis has been well-tested? The answers to this group of questions are found in inferential statistics, which in turn use probability.

Probability is the mathematics of uncertainty. Probability provides a way to keep score when we're working near the forefront of knowledge, confronted by improbable ideas and uncertain data. 

It turns out that there are a couple of statistical frameworks that people tend to apply to derive probability.

One decision you'll have to make is to choose which of these frameworks to apply for your work: Bayesian statistics or the Fisher/Neyman-Pearson based error statistics, aka "frequentism"?

In the Bayesian framework, the rules of probability are used to represent the plausibility of a hypothesis under the observed data. In the Fisher/Neyman-Pearson framework, probability is used to assess the plausibility of the data under a null hypothesis. 

Equally reasonable people choose either framework. If you search for "Bayesian v frequentism" you will run smack dab into what are called the statistics wars. In one or two clicks you are sure to stumble into impassioned arguments on each side.  

```{r, fig.cap="When you wander into a chat between a frequentist and a Bayesian."}
knitr::include_graphics("images/statistics_wars.gif")
```


Which is better? I'm honestly agnostic. I learned, practice and teach error statistics, or something along the lines of the Fisher/Neyman-Pearson school. But I definitely appreciate the validity of Bayesian approaches in many circumstances. 

That sounds wishy-washy so here is a hill that I will stand on. It is less important that one framework might be better than the other. What's more important is to operate within a framework, for which you fully understand its strengths and limitations.

This course advocates adopting a framework that speaks to replicable experimental practices, to variables and models, to the implications of evidence, and also to a reproducible workflow. The inferential statistics you will learn are mostly frequentist. That choice is motivated for several reasons that I won't go into here, except for a nod towards a philosophical reason that makes a lot of sense to me, as spoken by Deborah Mayo:  

*"In the severe testing view, probability arises in scientific contexts to assess and control how capable methods are at uncovering and avoiding erroneous interpretations of data. That is what it means to view statistical inference as severe testing. A claim is severely tested to the extent it has been subjected to and passes a test that probably would have found flaws, were they present. The probability that a method commits an erroneous interpretation of data is an error probability. Statistical methods based on error probabilities I call error statistics. It is not probabilism or performance we seek to quantify, but how severely probed claims are."* [-Mayo](https://blog.apaonline.org/2019/03/07/interview-with-deborah-mayo/)


