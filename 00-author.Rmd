# About the author and book {#author}

I first learned this material as a graduate student at Mizzou. The stats courses that impacted me the most were taught by the late Gary Krause, then a professor and statistician in Mizzou's agricultural college.

The light turned on for me during Gary's *Experimental Design* course. That's when the fog of mathematical statistics cleared enough so I could finally "get" the pragmatic value of statistics for the researcher. Why it didn't hit me earlier, I don't know. I'd been involved in plenty of research and data collection by then.

What dawned on me is that experimental design is a statistical framework for conducting unbiased research.

That concept permeates my course and this book and it is the one thing I most want my students to take away from suffering through me for a full semester.

Here's how it hit me.

I was working on my PhD in pharmacology within the medical school. But most of my classmates in Gary's courses were from the other side of campus, working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science.

The problem my classmates shared, which was not a problem that really affected me, is having only a single growing or mating season by which to run a fully replicated experiment.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="One shot!"}
knitr::include_graphics("images/my_shot.jpg")
```

They only had one shot.

Which changes everything.

Planning was a priority for them. They needed to map out their experimental design well in advance and with all the statistical thought on the front end.

Once the experiment began, they didn't have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups.

This planning was based upon statistical design principles, often in consultation with Gary. Defining variables and endpoints and comparisions and sample sizes and more. For them, statistics were mostly *a priori* planning. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the script they wrote up the previous winter.

That is how unbiased research is conducted.

In contrast, it is fair to say that a lot of biomedical scientists fail to incorporate statistical design into their plans at all, yet run statistical tests and inference as if they did. Failure to write and follow a script opens up a whole can of worms that, kindly, characterized as using statistics in ways our statistics were never meant to be run.

The biomedical researcher who takes a more "fly by the seat of their pants" approach to running experiments and collecting data is too common. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation of what the data obviously show.

Oops.

Unfortunately, that approach is riddled with biases.

A lot has been written in recent years about the replication and reproducibility crisis in biomedical research. When reproducibility is defined as achieving the same analytic result given identical raw data, then any failures are mostly a breadcrumb problem. That's relatively easy to solve. We just need to change our data handling and analysis process and be more explicit about how we arrive at our numbers statistical solution.

When replication is defined as an independent repeat of the same general result, then any failures are indicative of a bias. Bias is more complicated to fix then just a simple breadcrumb problem.

Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made.

Therefore, the focus of this course is in teaching statistics not as tests to run after the fact, but as an experimental design framework.

The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance, to follow it (and to feel really, really guilty when they don't) and to appreciate why this is so important to reduce bias.

That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs and data types. Most of which she will forget immediately.

And since I emphasize pre-planning, this book is full of simulations. These mostly are simulations designed to determine a sample size necessary to run an experiment. However, they are based on a method in which the experimental groups are all pre-specified. This provides a way to map out and even visualize potential results in advance. Which is a great way to check assumptions.

I'm not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work "under the hood". When I do, it is mostly for stuff I think should be helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don't believe I'm the person who should be teaching it.

Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That's why operating within a statistical design framework is so important.

Perhaps most importantly, for the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers the potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments that prolonged time to degrees.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```
