# About the author {#author}

I learned this material as a graduate student at Mizzou. There I took several stats courses as electives. The ones that impacted me the most were taught by the late Gary Krause, then a professor and statistician in Mizzou's agricultural college.

The light turned on for me during Gary's *Experimental Design* course. That's when the fog of mathematical statistics cleared enough so I could finally "get" the pragmatic value of statistics for the researcher. Why it didn't hit me earlier, I don't know. I'd been involved in plenty of research and data collection by then.

What became most clear to me is that experimental design is a statistical framework for conducting unbiased research. 

That concept permeates my course and this book and it is the one thing I most want my students to take away from this.

Here's how it hit me. I was working on my PhD in pharmacology within the medical school. But most of my classmates in Gary's courses were from the other side of campus, working on a PhD in one of the agriculture programs, usually in some area of agronomy or in animal science. 

The problem my classmates shared, which was not a problem that really affected me, is having only a single growing or mating season by which to run a fully replicated experiment.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="One shot!"}
knitr::include_graphics("images/my_shot.jpg")
```

They only had one shot. 

Which changes everything about the approach to research.

Planning was a priority for them. They needed to map out their experimental design well in advance and with a lot of statistical thought on the front end. 

Once the experiment began, any new wrinkles or oversights would have to wait until the next growing season. They didn't have the luxury of running out to the field to plant another row of the crop, or to arrange additional breeding groups, to tweak it. 

They don't have the luxury we have in the biomedical sciences with our typically much easier access to biological material.

Their planning was based upon statistical design principles, often in consultation with Gary. For them, statistics were *a priori* planning and *post-hoc* tests. At the end of the season the samples were harvested. After all the biochemistry was completed at their lab benches, the final statistical analysis was performed according to the planned approach.

An experiment set in motion according to a plan could only follow the plan.

In contrast, it is fair to say that most biomedical scientists fail to incorporate statistical design into their plans at all.  That failure opens up a whole can of worms that can generally be characterized as running statistics in ways our statistics were never meant to be run.

All too common is the biomedical researchers who takes a more "fly by the seat of their pants" approach to running experiments and collecting data. In this approach, bunches of near and partial replicates are munged together before looking at the results and making a decision about what statistical analysis would be most appropriate to confirm their inclined interpretation of what the data obviously show. 

Oops.

Unfortunately, that approach is riddled with biases, if not flat out lacking in integrity.

A lot has been written in recent years about the replication and reproducibility crisis in biomedical research. The reproducibility problem is a problem of process, and fairly simple to solve. Just be more explicit about how you arrived at your statistical solution, and I should be able to arrive at the same value, given your data.

I'm completely agnostic about whether a given type of statistics (Bayesian vs frequentist) has anything to do with the replication problem or offers a better approach.

What I do think has been left largely unsaid is the failure of most biomedical researchers to embrace a statistical design framework in their experimental planning. It doesn't make any sense to use the final stage tools of hypothesis testing, no matter if it is a posterior or p-value, if the hypothesis is being stated for the first time only **AFTER** all the data are in and have been inspected for meaning! 

And that's pretty much what's happening out there, by and large.

Experimental statistics was invented by the founders as a means of instilling some structure into the planning, discovery and inference process so that unbiased interpretations can be made.

Therefore, the focus of this course is in teaching statistics as an experimental design framework. 

The ideal learner will finish the course knowing how to map out the statistical plan for an experiment in advance, to follow it (and to feel really, really guilty when they don't) and to appreciate why this is so important to reduce bias.  

That same learner will also know how to analyze, interpret, visualize, and write up the results for a wide array of experimental designs and data types. Most of which she will forget immediately.

But she will no where to go to relearn it and what to do when she gets there.

And since I emphasize pre-planning, this book is full of simulations. Other than that it serves as a reproducible environment, that's the really great advantage of using R to teach biostats, in my view.

I'm not a mathematician so I only offer enough theoretical and mathematical statistics to provide a glimpse of how how things work "under the hood". When I do, it is mostly for stuff I think should be helpful to interpret statistical output, or illustrate why a test works in a specific way. I very much believe there is an important place for mathematical statistics, I just don't believe I'm the person who should be teaching it.  

Scientists have a lot of overt biases and are the last to realize it. Data frequently has a lot of hidden biases we fail to see. That's why operating within a statistical design framework is so important. 

For the biomedical PhD student hoping to graduate while still young, a statistical design framework also offers potential to keep things rolling downhill for you. Statistical thinking should help you avoid the time-sucking rabbit holes that are associated with sloppy, inconclusive or uninterpretable experiments and prolonged time to degrees.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

