# Survival Analysis {#surv}

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(survival)
library(survminer)
library(knitr)
library(kableExtra)
library(coin)
```

Survival analysis derives its name from experiments designed to study factors that influence the time until discrete death events occur, such as deaths due to cancer or heart disease. When survival is plotted as a function of time, the resulting lines drawn between the data points are called survival curves. The slopes of these curves are called the hazard. The hazard is the rate of death during some period within that curve. For example, if 80% of mice implanted with a tumor die within 1 month the tumor's hazard is 80% per month.

These ghoulish terms persist even though survival analysis is very useful more generally for studies that involve tracking time to any kind of event, deadly or not. 

The two common statistical techniques associated with survival analysis that we will cover are Kaplan-Meier (KM) estimation (including the log rank test to compare two curves) and Cox proportional hazards regression. 

KM estimation mostly offers a descriptive way to quantify time to event and survival. KM plots have the characteristic stair step survival curves, where the abscissa is time and the ordinate is survival probability. A KM estimate is the value on these y-axes at a given time. A KM estimate represents the probability of surviving beyond a specific unit of time. For example, when the analysis declares a median survival time for a cancer of 23 months, that implies half of the subjects can be expected to survive beyond 23 months.

KM analysis also allows for testing hypotheses about whether two treatments cause any difference in survival times. The log rank tests employed for this, in effect, test whether the distributions of two survival curves differ from each other. Although these tests report out median survival parameters, they are calculated on the basis of all of the data within a survival curve and are non-parametric.

Cox proportional hazards regression offers a related inferential procedure. The name has three origins. It was invented by Sir David Cox. It is a method to calculate and compare hazard coefficients. It is based upon the assumption that the hazard rates of the two curves being compared are proportional and constant through their full extent. When this latter condition is true, two survival curves can be compared in terms of relative risk.

Cox proportional hazards regression is the basis for making assertions such as, "A person with that cancer has a 20% risk of dying per year." That statement comes from a survival curve with a hazard rate of 0.2 per year. There may be some study that reports a new treatment, which reduces the hazard rate for that cancer by 50%. The treated hazard rate is now 0.10 per year. Therefore, the treatment reduces the risk of death by half.

## Data records

For a moment I want to distinguish a data frame from a data record. The might be something the researcher keeps while the survival experiment is on-going.

A data frame for a typical survival analysis study is comprised of many cases. Each case represents an independent replicate. The minimal number of variables recorded for each case are as follows: 

* The event status
* The duration in time until the event occurred
* A grouping variable (when testing hypotheses)

That dataframe will be minimally necessary to run survival analysis and statistical functions.

In practice, cases come and go throughout an enrollment and follow up period. Our pre-processed records have an ID for the case, an enrollment date or time, a date or time when the event status changed, the event status and the grouping variable, and perhaps some notes. 

```{r}
id <- c(LETTERS[1:4])
start_date <- c("2020-01-14", "2020-01-12", "2020-01-30", "2020-01-29")
end_date <- c("2020-03-29", "2020-01-19", "2020-04-04", "")
status <- c(1,1,0, "")
treatment <- c("placebo","placebo","placebo","placebo" )
notes <- c("", "", "left area, unenrolled", "" )

study <- data.frame(id, start_date, end_date, status, treatment, notes)

kable(study) %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

The event status and end date are intimately linked. An event status is typically entered as a discrete variable code for what occurred on the end date. For example, a status of $1$ would indicate the event (eg, death) occurred for that case. An event status of $0$ would indicate the case was censored after that amount of time. The concept of censor is discussed below.

A grouping variable would be something like a treatment variable. For example, a variable named `treatment` might have two levels, placebo and drug. There may be additional grouping variables.

__Given a record of start and end dates/times, we'll either have to first calculate their difference before passing such data into survival functions, or argue a function so that it will do this given this information.__

### Deriving times from recorded dates

The duration to an event is something that is calculated during data processing. It is important to pass into the `Surv` function (below) values in units of time. They should not be simple numeric values. 

Survival analysis can be conducted over any time scale. 

Scales of days, weeks, months and even years are common. In such designs calendar date variables are commonly used. For example, in a cancer trial a date is recorded for the date a mouse is implanted with a tumor. Subsequently, the date an event (eg, death or censure) occurs is also recorded. The time between these days is then calculated during data processing.

It turns out that dates can be a tricky variable in any computer language due to their imprecision. Shifts such as time zones, the number of days in a month, daylight savings, leap years and even leap seconds occur. Strictly, unlike for seconds (which always have a fixed duration) time variables in units of minutes, hours, days, weeks, months and years (and more) might be approximate unless we account for these shifts. See `?POSIXct` for more information.

The `lubridate` package has several utilities to deal with date/time data. Our main concern is converting date or date/time entries into time values on whatever scale useful for our survival analysis algorithms.

Let's image the simple case where we record the start dates for enrollment for each case in a study, and then record the date of the event or censure entry. 

Note how each vector is that of character strings

```{r}
start_date <- c("2020-01-14", "2020-01-12", "2020-01-30", "2020-01-29")

end_date <- c("2020-07-29", "2021-01-19", "2020-08-04", "2020-02-29")

```

The simplest way to calculate the time interval is to first convert the character strings into date objects using the `ymd` function. Then subtract. Note how, by default, time units of days are the output. Note this produces a `difftime` object.

```{r}
days <- ymd(end_date) - ymd(start_date)
days
class(days)

```

When the need is for numeric values as output rather than difftime, or when calcuating intervals greater than week (difftime does not have units of months or years), the method below takes character strings and produces numeric values in the sought for units.

```{r}

# illustrating how to calculate four different time scales
# usually there is only one scale of interest
# The divisor can be set to any length: days(7) == week(1)

dateSet <- tibble(start_date, end_date) %>% 
  mutate(days = interval(start_date, end_date)/days(1),
         weeks = interval(start_date, end_date)/weeks(1),
         months = interval(start_date, end_date)/months(1), 
         years = interval(start_date, end_date)/years(1)
         )

dateSet

```

### Censor

The concept of censor is important in survival studies. Censoring occurs in either of two ways:

* The study period ends without an event having occurred for that case.
* The case is de-enrolled prematurely from an active study for reasons other than meeting the event criterion. 

For example, imagine a survival study in mice implanted with experimental tumors where death is the study event. One day, a live mouse jumps out of a cage and escapes from our care, never to be seen again. That mouse should be censored on the date it was lost. A score of $0$ is entered for the event variable and the date the case was lost is also recorded. It's gone. Since we don't know if it died or when it died we have no choice but to record its event as censored.

Censored cases should not be counted as events nor should they be ignored completely once enrolled in a trial. In either circumstance they would skew results if not censored. Censoring reduces the number of subjects at risk that remain in the study, which influences the survival probability calculation (see below).

If a study is designed to last for a limited period, or a decision is made to end the study, then all remaining survivors are censored on that end date. 

## Kaplan-Meier estimator

Everyone will recognize the familiar step-down Kaplan-Meier "survival" curve, even though we might not know its name or function. These plot an updated survival probability each time the number at risk ($N_t$) in the study changes. The number at risk can change either due to an event ($D_t$) or due to censoring ($C_t$). $S_{t+1}$ is the surival probability we are computing due to an event or censoring, while $S_t$ is the survival probility just before this.  \[S_{t+1}=S_t \frac{N_{t+1}-D_{t+1}}{N_{t+1}}\]

Perhaps this is easier to understand through illustration. We begin a study with 10 subjects. We have only one group (no comparisons are involved) and everyone is enrolled simultaneously, to keep it simple. Thus, 10 subjects are at risk of not surviving the protocol. At the intial time, 100% are survivors. 

```{r}
months <- c(0,5,10, 15, 20, 25)
Nt <- c(10,10,8, 7, 6, 5)
Dt <- c("", 1, 1, 1, "", 2)
Ct <- c("", 1, "", "", 1, "")
Stplus1<- c(1, 0.9, 0.788, .675, .675, .404)

kable(data.frame(months, Nt, Dt, Ct, Stplus1))
```

Exactly five months later one subject dies while a second subject is censored. At the time of these two events there were still 10 subjects at risk. Only the previous survival probability (1), the number of deaths at 5 months (1) and the numbers at risk (10) before this event happened factor into how the survival probability is changed. $S_{t+1}=(1)\frac{10-1}{10}=0.9$.

The study protocol continues with 8 subjects at risk.

Another five months later a second death occurs, meaning we have to update the survival probability. The number at risk is now 8, since there were previously 1 death and 1 censor. At 10 months, the updated survival probability is $S_{t+1}=(0.9)\frac{8-1}{8}=0.788$.

The study protocol continues with 7 subjects at risk, awaiting the next event.

Fifteen months after starting the third death occurs. The number at risk prior to this death is 7, due to the loss at the 8 month time point. $S_{t+1}=(0.788)\frac{7-1}{7}=0.675$.

There are now 6 subjects at risk moving forward.

Twenty months after starting a subject is censored. Nobody dies. The number at risk at this time point was 6 due to the previous death. Since there has been no death, the survival probability remains unchanged. $S_{t+1}=(0.675)\frac{6-0}{6}=0.675$

But moving forward there are now 5 subjects at risk in the protocol.

Twenty-five months after starting two subjects die. To update the survival probability the number at risk is down to 5 due to the 1 censor at the prior time. $S_{t+1}=(0.675)\frac{5-2}{5}=0.404$

Moving forward 3 subjects are at risk. Their survival probability is 0.404.

The R survival functions will accurately calculate these survival probabilities. Hopefully it is apparent from this example that each updated survival probability is forward looking and applies to the remaining subjects at risk. Furthermore, hopefully it is evident the effect of censoring is to reduce the numbers at risk without affecting survival probability.

More generally, $S_{t}$ serves as an estimator of the survival function, which predicts the probability of surviving longer than time t. 

Median survival times are a very common use of the KM estimator as a descriptive statistic for assessing the influence of some condition on time-to-event. In this paradigm median survival is when $S_{t}$ is 0.5. Median survival times can be devined from the intercept of the survival curve and a horizontal line extending from $S_{t}$ = 0.5 on ordinate. The time point corresponding to that intercept is the median survival time.   

## Glioma data

Let's look at some data for exploring statistical inference.

The glioma data set below comes from the `coin` package. Note it contains 37 cases listing the survival times for people with either Grade3 or GBM gliomas. Notice also that it contains a few grouping factors, including histology, sex, and group.

We'll just focus on survival associated with histology, which you'll recognize as two different types of glioma.

The first step is to create a survival model of the data. The `Surv` function from the `survival` package in conjunction with the `survfit` function summarizes the events while calculating the survival probability for each event and its associated time points (which are in months), along with some useful summary statistics. 

Hopefully the formula `Surv(time, event) ~ 1` below strikes as reminiscent of that which is used in regression. Here, the `survfit` function calculates the numbers at risk, the numbers of events and the survival, along with standard error and confidence interval for each time point. **This is how to regress the data WITHOUT accounting for the effects of any of the predictors.** This is being done to illustrate a couple of points.

```{r}
modelAll <- survfit(Surv(time, event) ~ 1, data = glioma)

summary(modelAll)

modelAll
```

The overall median survival calculated for these 37 cases is 31 months (95% CI is 15 to NA).

```{r}
modelAll
```

Now let's focus on comparing the two groups under the histology variable to ask whether the type of tumor influences survival. We are NOT going to factor in the effects of age, sex or the group variable, in order to keep things simple.

First we change the formula from above:

```{r}
modelHis <- survfit(Surv(time, event) ~ histology, data = glioma)
summary(modelHis)
```

The two groups differ in median survival. For the cases with GBM it is 14 months (95%CI is 11 to 31 months). The median survival for Grade3 cases is undefined. That's because the survival rate is greater than 0.5...they have not reached the median point!

```{r}
modelHis
```

## Kaplan-Meier plots

The `survival` package has base plotting functions, but we use functions in the `survminer` package to generate ggplots of survival curves.

First a bare bones plot of the unsegmented data. Notice how the median surival line is drawn. Also notice the discrete nature of the 95% confidence intervals (shaded).

```{r}
ggsurvplot(modelAll,
           surv.median.line = "hv", # Specify median survival
           palette = c("#002878"))
```

Next we plot the survival curves based upon the histology variable. Note how Grade3 survival never reaches 0.5, thus a median survival time is not possible to calculate.

```{r}
ggsurvplot(modelHis,
          conf.int = TRUE,
          surv.median.line = "hv", # Specify median survival
          ggtheme = theme_bw(), # Change ggplot2 theme
          palette = c("#002878", "#d28e00"))
```

## Log rank tests

The key question we address with log rank testing is whether survival differs between GBM and Grade3.

It is a very simple question. If true, it implies the two conditions differ. The experimental researcher is typically only interested in survival curve differences and uninterested in hazard ratios or relative risk. For example, in an implanted tumor model, the researcher wishes to manipulate the immune system in some way to test if it alters survival. Period.

Log rank tests (there are several) are nonparametric tests for comparing survival distributions and thus answering this question.  

The differences between the various log rank tests is beyond the intended scope. See `coin::?logrank_test` for an entry into the broader universe of the various tests, their features and arguments and use cases.

In the case of the glioma data, we test the null hypothesis that the survival distributions for GBM and Grade3 are equivalent. 

$H_0$: The GBM and Grade3 survival distributions are the same.
$H_1$: The GBM and Grade3 survival distributions differ.

The parameterless wishy-washiness of the null hypothesis should remind us of nonparametric hypothesis tests. There is no parameter here. The log rank test is a nonparametric test, which only evaluate the relative locations of the two distributions without consideration of their central tendency or any other parameter. 

It is very common to summarize the differences between survival curves through describing their relative median survival times. Therefore, median survival times serve as a useful pseudo-parameter. However, the log rank test is definitely not comparing medians.

One can imagine instances where the values of median are roughly the same (if not identical) yet the distributions differ markedly, because survivals diverge at times beyond a common median survival time point. The log rank tests are likely to pick up those differences in survival, because they factor into their calculations the full breadth of the survival curves.

The default mode for the `survdiff` function in the `survival` package runs a Mantel-Haenszel log rank test producing a $\chi^2$ test statistic with 1 degree of freedom. \[\chi^2=\frac{(\sum O_{1t}-\sum E_{1t})^2}{\sum Var(E_{1t})}\]

Here $\sum O_{it}$ and $\sum E_{it}$ are the sum of the observed and expected number of events in group 1 over all event times. The denominator is the sum of the variances for all events, \[Var(E_{1t})=\frac{N_{1t}\times N_{2t}\times (N_t-O_t)}{N_t^2\times(N_t-1)}\]  Here $N$ represents the total numbers at risk at a given time point, while $N_1$ and $N_2$ are the at risk within each group.

Here is how to run the test.

```{r}
survdiff(Surv(time, event) ~ histology, data = glioma)
```

Note that the column labeled `(O-E)^2/E` reports the $\chi^2$ values for each of the two groups. When summed, they become the classic Mantel-Cox log rank test statistic, which can be used as an alternative for inference rather than the default. 

The column sum is $\chi^2=12.28$ with 1 degree of freedom.  Use the `chisq` distribution to compute its p-value:

```{r}
pchisq(12.28, df=1, lower.tail=F)
```

Although the test statistic and p-values for these two log rank tests are similar, they are not identical. The values should agree in well-powered experiments. To avoid p-hacking temptations, make the decision to use one or the other for inference in advance.

Nevertheless, both tests generate an extreme $\chi^2$ test statistic and corresponding low p-value, below the typical type1 error threshold of 0.05. On this basis the null hypothesis is rejected.

### Write up of log rank test

__Survival with Grade3 (median survival = undetermined months, 95%CI = 53 to undetermined) differs from survival with GBM (median survival = 14 months, 95%CI = 11 to 31 months; Mantel-Haenszel log rank test, chisq= 13.4, p=2e-4).__

## Cox proportional hazards analysis

Log rank testing only leaves us with a test statistic. 

Cox proportional hazards analysis is invoked when the researcher seeks to describe the treatment effect by deriving a quantitative parameter from the survival data. Specifically, for when one wants to describe quantitatively how survival rates are influenced by treatment conditions. 

The Cox model is otherwise known as the hazard function. 

The jargon used to define the survival rate of a treatment condition is the hazard, $\lambda(t)$. The baseline hazard, $\lambda_0(t)$, is the survival rate in the control setting, in the absence of any treatment condition. The Cox model defines the hazard as a function of a linear combination of predictor variables, $X$:  \[\lambda(t) = \lambda_0(t)\times e^{\beta_1 X_1+\beta_2 X_2..+beta_p X_p}\] 
In the simple case where the predictor variable $X$ is discrete at two levels, such as a control or a treatment, for the control $X=0$ then \[\lambda(t) = \lambda_0(t)\], or is the baseline hazard. 

When treatment is present, $X=1$ then \[\lambda(t) = \lambda_0(t)\times e^{\beta X}\] or \[\frac{\lambda(t)}{\lambda_0(t)}=e^{\beta}\] Finally, natural log transformation creates a linear form of the equation \[log(\frac{\lambda(t)}{\lambda_0(t)})=\beta\]

* $e^{\beta}$ equals the hazard ratio or the relative risk
* $\beta$ equals the log hazard ratio or log relative risk

This (and a bit more mathematical proofing of the Cox model) implies that the ratio of the two hazards are a constant and independent of time, from whence the term 'proportional' is derived. In other words, the procedure is based upon the assumption that the hazard ratio is constant across all times of the study period.

### Cox proportional hazards regression of glioma

In the code below Cox regression is executed on the glioma data, testing only the histology variable for the difference in survival between the GBM and Grade3 gliomas. Again, we ignore the other predictors.

```{r}
modelHis.cox <- coxph(Surv(time, event) ~ histology, data = glioma)
modelHis.cox

print("/////////////////////////////////////")
print("/////////////////////////////////////")
summary(modelHis.cox)
```

### Interpretation of Cox regression output

From the output above, printing the regression model alone is comprehensive except for confidence intervals. Passing the model into the `summary` function pulls out this and some additional detail

#### Log hazard ratio/relative risk

The `coef` value corresponds to $\beta$ from the hazard equation above, which equals $log(\frac{\lambda(t)}{\lambda_0(t)})$. Thus we can say the log hazard ratio is -1.6401. 

But what does that actually mean?

Remember that $\lambda$ symbolizes survival rates. And there are two survival rates of interest in this case: one for GBM and another for Grade3. Furthermore, we can bloody obvious tell from the plot above that the survival rate for Grade3 is larger than that for GBM. Think about that carefully for a moment.

Log fractions can confuse. By algebra $log(\lambda(t))-log(\lambda_0(t))=-1.6401$. The negative value of this difference means that $\lambda_0(t)$ must be greater than than $\lambda(t)$. Therefore we can deduce that this R function has coerced Grade3 as baseline, or \[coef=log \ hazard \ ratio=log(\frac{\lambda_{GBM}(t)}{\lambda _{Grade3}(t)})=-1.6401\]

The z statistic and p-value for `coef` correspond to a Wald test (a ratio of the coefficient value to its SE). This tests the null hypothesis that `coef`= 0. If the null is true it would indicate there is no evidence that factor associated with that coefficient affects the log hazard ratio.

#### Hazard ratio/relative risk

Now for the `exp(coef)`. This is the **hazard ratio**, otherwise known as the **relative risk**. Either of these two terms are used commonly and interchangeably. They also have greater descriptive efficacy than the log hazard ratio because logs tend to confuse people.

It is worth repeating is that the hazard ratio nothing more complicated than a ratio of survival rates. 
\[\frac{\lambda(t)}{\lambda_0(t)}=e^{\beta}=\frac{\lambda_{GBM}(t)}{\lambda _{Grade3}(t)}=0.1940\]

Since the value of the hazard ratio is below 1, and $\lambda$ symbolizes survival rates, this result shows that the survival rate with GBM is lower than the survival with Grade3 tumors. That is bloody obvious from inspection of the plot above.

In fact, the GBM survival rate is 19.4% the survival rate for Grade3. Or we can say the relative risk of death due to Grade3 tumors is 19.4% of GBM tumors. or we can say hazard associated with Grade3 tumors is 19.4% of that for GBM.

Now please note the `exp(-coef)` column. This is merely the reciprocal of the hazard ratio. A personal preference might be to use this. Is so, one would say that the hazard rate of GBM is 5.156 times greater than that for Grade3. Alternately the relative risk of GBM to Grade3 glioma is 5.156.

#### Confidence interval

We can say that there is a 95% chance the hazard ratio (or relative risk) of Grade3 to GBM tumor has the values between 0.07452 and 0.5049.

It takes professional judgement to decide whether that range is scientifically meaningful. Is is reasonable to think Grade3 tumors are anywhere from a tenth to half as deadly as GBM. An expert in the field could assess that conclusion. Importantly the confidence interval does not include the value of 1, which for a ratio would indicate the two tumors have the same hazard.

#### Statistical tests

Finally on to the statistical tests. 

This function by default generates 3 inferential tests. In well-powered experiments all three should converge to a similar conclusion but won't have the exact same values. It is up to the researcher to decide in advance of testing which one to use.

The log rank test is handled in the section above. The Wald test won't be discussed here in detail. It is an extension of the Wald test used above for the single coefficient value. The main difference is that the Wald test for the overall regression handles many coefficients simultaneously. That's not an issue in this simple case, but it would be if other predictor variables were added to the regression model.

Likelihood ratio tests are most commonly used. They always compare the fits of two different models, usually nested, to a common data set. 

The likelihood function first calculates parameter values for each model that are most likely to fit the data the best. The test statistic is then the ratio of these two likelihoods:  \[LR=-2log\frac{L(resricted \ model)}{L(full \ model)}=2(loglike(full \ model)-loglike(restricted \ model)\]
LR is distributed approximately $\chi^2$ with degrees of freedom equal to the number of restricted parameters. 

In this case the full model is that which includes the histology variable and corresponding $\beta$ coefficient as a parameter. The restricted model lacks any predictor variable altogether. 

### Write up

We can conclude that survival with GBM is approximately one-fifth that for Grade3 glioma (hazard ratio = 0.194, hazard ratio 95%CI 0.07 to 0.509, Cox proportional hazards regression, LR = 13.24, df=1, p=0.00027, n =37 with 23 events).

Or we could conclude that survival with GBM is approximately one-fifth that for Grade3 glioma (relative risk = 0.194, relative risk 95%CI 0.07 to 0.509, Cox proportional hazards regression, LR = 13.24, df=1, p=0.00027, n =37 with 23 events).

## Summary

* Thinking of survival analysis as time-to-event can broaden the experiments we might apply this design to.
* Log rank tests are a straight forward nonparametric method of asking whether two survival curves differ.
* Cox proportional hazards regression is used to obtain quantified parameters associated with a treatment model.
* We covered a simple binary event scenario, but events can be more complex than this (eg, two or more outcome events other than censor).

This chapter is intended to serve as an introduction to survival analysis for the researcher. The Cox proportional hazards model is flexible and can accommodate considerably more intricate statistical designs then listed here, including multiple variables and interaction tests between them. None of this is covered here.

This is probably one of those areas of statistics where a reliable textbook should be consulted when diving into more sophisticated experiments than shown here. [Here's a good discussion on the different books available and which might best suite you](https://stats.stackexchange.com/questions/1053/references-for-survival-analysis).
