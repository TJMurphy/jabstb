# One-way ANOVA CR

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(magrittr)
library(tidyverse)
library(ggformula)
library(DescTools)
library(ez)
library(lsr)
```

## Using `ezANOVA`

R has several functions to run ANOVA. In this course we're going to use `ezANOVA` from the `ez` package because I feel it is a better alternative for someone just learning ANOVA.

In this document, we'll run through an analysis of a one-way completely randomized ANOVA data set. 

## The chickwt data set 

This data set in R's base. It compares the influence of 6 different types of food sources on chick weight. There is one predictor factor (feed) at 6 different levels (the various food sources). There is one continuous outcome variable (weight). We can assume the chicks have been randomly assigned to a level of feed, and have been weighed after a period of time.  

This is a classic one-way completely randomized ANOVA design.

### Inspect the data

The next few scripts involve inspecting the data set, which should be done prior to running any statistical tests.

```{r}
data(chickwts)
#take a look at the data structure, depending upon how you like to view data
str(chickwts)
chickwts
```

Calculate some descriptive stats for inspection:

```{r}
cw1 <- chickwts %>%
  group_by(feed) %>%
  summarise(
    mean= mean(weight),
    median=median(weight),
    sd= sd(weight),
    n = n(),
    var=var(weight)
    )
cw1
```

Plot the data for a look. Simply looking at the data graphically goes a long way to ensuring this is a one-way ANOVA design. 

```{r}
ggplot(chickwts, aes(feed, weight))+
  geom_jitter(width = 0.2, size=2) +
  stat_summary(fun.data = mean_sdl, 
               fun.args = list(mult=1), 
               geom="crossbar", 
               width=0.2, 
               color="red")

```

### Run the ANOVA

We can imagine designing an experiment like this with either of 3 experimental objectives in mind.

1) Perhaps we're interested mostly in whether any feeds are better than others for achieving weight gain? We could answer that by making all possible pairwise comparisons. Since there are 6 levels of the factor feed, that would involve $C=\frac{6(6-1)}{2}=30$ comparisons(!!)

2) Imagine casein is the standard feed, and we wish to know if any of the other feeds differ from this standard? We would compare casein to every feed. That would involve only 5 comparisons.

3) Perhaps we just want to know if any of the feeds differ in causing weight gain, but we aren't interested in which feeds differ? We could answer that question using the F-test result.

Other than, perhaps, how we order our data in the data set, which of these objectives is true doesn't influence how we run the `ezANOVA` function per se. However, the objective will influence which post hoc analysis we conduct.

### Run the chickwts One Way ANOVA

First, ezANOVA requires a 'wid', which is a unique ID variable for each independent replicate. We need to add one to the chickwts data set (while it's in our environment). Since all the measures are independent, we'll just do that by row number. At the same time we'll convert the integer to a factor so ezANOVA won't bark at us. 

```{r}
chickwts$ID <- as.factor(1:nrow(chickwts))
```


You should look at R's help for ezANOVA `?ezANOVA` to understand these test arguments. The help page is pretty clear for most of these. 

Since we don't use the term 'dependent variable' much in this course, to be clear, 'dv' is the outcome response variable.

If the factor is completely randomized, list it after the 'between' argument. If the factor is related/repeated measures, list it after the 'within' argument.

Notice that `ezANOVA` is a function. Use it to create a list object called `my.ezaov`, which has all of the output information. The object name could have been `foo`. We call specific information from the `my.ezaov` object to see the results.

```{r}
my.ezaov <- ezANOVA(
            data = chickwts, 
            wid = ID, 
            dv = weight, 
            between = feed,
            type = 2, 
            return_aov = T, 
            detailed = T)

my.ezaov$ANOVA
my.ezaov$Levene
my.ezaov$aov
```

### Interpreting the One-Way CR ANOVA Output

The ezANOVA output prints 3 list objects by default:

* $ANOVA (which is the first data frame)
* $`Levene's Test for Homogeneity of Variance` (which is the 2nd data frame)
* $aov (which is the end of the console output)

In fact, there is a great deal more computed that is not printed, which you can visualize in the console by typing `str(my.ezaov)`.

#### $ANOVA: The ANOVA table

For a CR one way ANOVA design, the SS are partitioned as follows: $SS_{total}=SS_{model}+SS_{residual}$. 

The $SS_{model}$ has only one component: $SS_{feed}$.

Thus, the ANOVA table only summarizes the model components. 

The DFn = 5 corresponds to the 6 groups, less 1 degree of freedom (one is lost to calculate mean of groups) for the model source of variance.

The DFn = 65 corresponds to the degrees of freedom for the residuals (one df is lost per group to calculate group means). 

Therefore, this ANOVA tests a null F distribution with 5 and 65 degrees of freedom.

$F=MS_{feed}/MS_{residual}=15.3648$, where $MS = SS/df$. The SS can be found in the $aov output.

* ges = [generalized eta-squared](https://link.springer.com/article/10.3758/BF03192707). 

`ges` is an effect size parameter for ANOVA. For this particular experimental design, $ges=\frac{SS_n}{SS_n+SS_d}$. In other words, `ges` summarizes the variation associated with the model as a fraction of the total variation in the data. 

Thus, 54.16% of the variation in weight is attributable to the different levels of feed in the experiment.

Think of eta-squared, partial eta-squared, and generalized eta-squared as all related to the more commonly understood $R^2$ of regression. They are each calculated differently, but all related to $R^2$ in so far as they serve as estimates for how much of the variation is due to the model. `ges` takes on values from 0 to 1. Higher values indicate a greater degree of the overall variation is due to the factor tested in the experiment.

#### $aov

This table provides the accounting for the sum of squares and degrees of freedom, while calculating the residual standard error.

* DFn=degrees freedom for numerator. k-1, where k = levels of factor.
* DFd=degrees freedom for denominator. n-k, where n = number of independent replicates.
* SSn & SSd = sum of squares for model and residual, respectively
* Residual standard error is a parameter that estimates the precision by which the data fit the model, and is in units of the outcome variable, weight. $SE$ is the square root of the residual variance: $S_{y.x}=\sqrt{\frac{SS_{residual}}{df_{residual}}}$

If $S_{y.x}$ were zero, the data points would all rest at the value of the group means. The data would fit perfectly to a model of 6 group means at their observed values. $S_{y.x}$ therefore is a descriptive statistic that declares how much error, or the degree by which the data is unexplained by the model. It has some utility for confidence intervals and power analysis.

#### The Hypothesis Tested by this ANOVA

The scientific prediction for this experiment is that chick weights will vary depending upon the type of feed they are grown on. The null is that their weights will be roughly the same, irrespective of food source.

ANOVA tests this hypothesis through the variance parameter. The question is whether the variance associated with the model, one of 6 different feed group means, is fractionally greater than the residual variance in the sample.

The null statistical hypothesis is that the variance associated with the different levels of feed is less than or equal to the residual variance. Therefore, the alternate hypothesis is the variance associated with feed is greater than residual variance.

$H_0: MS_{feed}\le MS_{residual}$, $H_1: MS_{feed}>MS_{residual}$

It is just as valid to express the null hypothesis in terms of the group means:

$H_0: \mu_a=\mu_b=\mu_c=\mu_d=\mu_e=\mu_f$ Though, strictly, rejecting the null doesn't mean that all group means differ from each other, it just means that some of them differ. $H_1: \mu_a\ne\mu_b\ne\mu_c\ne\mu_d\ne\mu_e\ne\mu_f$

The F statistic of 15.3648 is extreme for a null F distribution of 5 and 65 degrees of freedom. The very low p-value illustrates this extremeness. 

The probability of erroneously rejecting the null hypothesis is about 5e-10. We can reject the null and conclude that differences in effect on chick weights exist between this group of feeds.

#### Levene's test for homogeneity of variance

Levene's test determines whether there is a substantial level of differences in variance *between* groups. Levene's test is run as a check to determine if the groups variance is homogeneous, as homoskedasticity is one of the validity assumptions of ANOVA.

Levene's test statistic is calculated as follows:

\[W=\frac{(n-k)}{(k-1)}\frac{\sum\limits_{i=1}^{k}n_i(\bar Z_i-\bar Z)^2}{\sum\limits_{i=1}^{k}\sum\limits_{j=1}^{n_i}(Z_{ij}-\bar Z_i)^2}\]

where $Z_{ij}=|x_{ij}-\bar x_i|$ and $Z_i$ are the group means and $\bar Z$ is the overall mean of $Z_{ij}$. 

The null hypothesis of the Levene test is rejected when $W>F_{(\alpha,\ k-1,\ n-k)}$, where the F is the critical value.

Levene's test output is a 2nd ANOVA table, and can easily be confused with the ANOVA output. Levene's test lacks a $ges$ parameter, nor does it have a column that lists the factor name.

If the Levene's F value is low and the p-values is high, as is the case here, the variance homogeneity assumption is validated.

If this were not the case, you have two options. 

Option 1: Simply ignore the result. The luck of the draw with small samples can explain group differences in variance, where none really exists. The impact on your inference will be modest. What's more important is that the population you are sampling is normally distributed. Do you have reason to think that is not the case? 

Option 2:  Transform the data to homogenize outliers and variance, or switch the analysis to a Kruskal-Wallis nonparametric test. 

### Post hoc pairwise comparisons

If the ANOVA F test for the factor is extreme the next step is post hoc analysis to determine which group means actually differ. 

In other words, post hoc involves testing several hypotheses. This should be done in a way that keeps the cumulative, family-wise type1 error rate (FWER) below the type 1 error threshold that was set for the experiment; usually 5%.  

#### Overview of options

Sometimes we do these experiments without any clear plans of what to compare. In those cases, we compare every group to all other groups. That "burns" a lot of alpha needlessly, but that's the price paid for not planning. The tests for that include the Bonferroni (aka Dunn's) and its cousins, Holm, Hommel, and Hochberg. They differ slightly.

Step-down tests operate a bit differently but do the same thing in terms of making all comparisons, these including the Tukey HSD and Newman-Keuls.

When we intend to compare all groups back to one level of the factor, thus making only a subset of all possible comparisons, we use Dunnett's or the FisherLSD. But it's just as valid to use any other of these methods.

#### Two imperatives in pairwise testing

1) Since every p-value represents a hypothesis, when simultaneously testing many hypotheses always make adjustments to keep the FWER < 0.05.

2) Let your scientific questions drive the comparisons you'll make, and thus your choice of post hoc test.

I suggest you worry less about the specific post hoc test, or about gamification of p-values, and the like. 

But there are a couple of provisos: 1) Bonferroni (or its cousins) works just fine if you don't have too many means to compare, 2) Dunnett's is for comparing two or more means back to a control mean but should not be used in related measures factors, 3) Fishers LSD will give you output to make those one or few key comparison you set out to make, even though you have many other groups in the experiment (ie, several of the other groups are internal controls and not particularly scientifically important).

#### Here are some post hoc options

**No p-value adjustment**: Sometimes it's useful to generate p-values that are not corrected for multiple comparisons. For example, although you've done a simple ANOVA, the intention all along was to test only one hypothesis between two groups. In that case, I recommend using the FisherLSD because it will generate confidence intervals for you.

The script below will generate a matrix of p-values for t tests of all possible comparisons, none of which are adjusted for multiple comparisons. When running the other scripts, come back to this to see how adjusted p-value methods changes these results 

```{r}
pairwise.t.test(chickwts$weight, chickwts$feed, p.adjust= "none")
```

**Fisher LSD** This also doesn't make adjustments for multiple comparisons, making it the most liberal post hoc test. Notice the p-values are no different than above. Fisher LSD is better than 'none' because it provides CI's on the differences. Note how it's among the pairwise.t.test arguments. So we have to run a base R function. Note: diff = difference between diet means.

```{r}
#from the DescTools package
PostHocTest(my.ezaov$aov, method = "lsd")
```

All of the following scripts do adjust p-values for multiple comparisons. Therefore, notice how they increase the p-values we see in the output above. Frequently, "significant" unadjusted p-values become "nonsignificant" after these adjustments.

**Bonferroni**: This is the easiest to understand. Let c be the number of possible comparisons for k factor levels. An adjusted type 1 error threshold for each comparison is:
\[p_k=\frac{\alpha}{c}\]

For example:

```{r}
pairwise.t.test(chickwts$weight, chickwts$feed, alternative = "two.sided", p.adjust = "bonferroni")
```

The optional methods for this function, "holm", "hochberg" and "hommel", each calculates p-value adjustments in slightly different ways. They are considered slightly more liberal versions of the "bonferroni". Which is the best? For all practical purposes, none. Choose one early in your career and stick with it forever.

**Dunnett's**: This post hoc method differs from above because it only does dependent t tests, on a subset of the means. For example, all means are compared to the negative control mean. The fewer comparisons don't spread the allowed FWER as thin as the other options. The following script is configured to compare the means at each level of feed to that for the first listed feed. Note: diff = the difference between diet means.


```{r}
DunnettTest(weight ~ feed, data = chickwts)
```

**Tukey HSD**: A method based upon the Studentized range, doing all comparisons, and providing adjusted p-values along with confidence intervals. Used when the question is, "Are there any group differences here at all?"

```{r}
TukeyHSD(my.ezaov$aov, "feed", ordered = T)
```

**Other**: "BH" is Benjamini-Hochberg, an FDR-based tool that will gives you more positives at the risk of higher false positives. Use this when you have an excessive number of comparisons to make. For example, over-designed ANOVA, 3 way ANOVA, multivariate experiments. The BH procedure first ranks the differences between paired groups from largest to smallest, and then performs the tests, unadjusted, running down the list. It debits each p-value against a starting value of 0.05 (or whatever). Once that 0.05 is exhausted, it stops testing. In this way all of the type1 experimentwise error is spent on the biggest differences.  

## Reporting the result

Note how the R way is to subtract the 2nd group from the 1st. For this reason, the diff values are all negative, even though the feed sources caused higher growth than casein.

*The effect of feed source accounted for 54% of the experiment's observed variation in chick weights (one-way completely randomized ANOVA, type=2, F(5,65) = 15.365, p = 5.9e-10, ges=0.54. Pairwise group analysis using Dunnett's test shows that horsebean (p=6.3e-9), linseed(p=8.6e-5) and soybean (p=0.0032) each differ from casein.  (In methods: ANOVA analysis was performed using R v4.3, ez package).* (Note, it is better to put CI's and p-values in a table)

