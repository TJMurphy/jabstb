# Linear discriminant analysis {#lda}

```{r}
library(car)
library(ez)
library(MASS)
library(tidyverse)
```

Linear discriminant analysis (LDA) is identical to MANOVA, mathematically. The focus of MANOVA is inferential, to test whether independent variables have any effects on a latent composite variables constructed from all of the dependent variables. The focus of LDA is prediction and explanatory, usually to define groupings based upon the latent composite variable.

## Flipping the general linear model

Let's think about it this way.

What if we flipped the general linear model? \[Y=\beta X+\epsilon\] Here, we measure dependent variables $Y$ given different levels $X$. The $\beta$ coeffiecient represents the slope of the linear relationship between $X$ and $Y$ while $\epsilon$ is the residual error, which is normally distributed. This is from middle school. 

We fit that model to experimental data to derive estimates for the regression coefficient, $\beta$. We're left with an equation to predict values of $Y$ for any imagined levels of $X$. 

For example, if $Y$ was weight and $X$ was some level of diet, knowing the regression coefficient $\beta$ would allow for predicting the weight while on that diet! That could be very useful for making decisions about what to eat. \[weight = \beta \times diet + \epsilon\]

Even $\epsilon$ is important, because it can provide a sense of the range of weight losses or gains to expect from the diet.

We could add a second predictor variable and do the same \[Y=\beta_1 X_1 +\beta_2X_2 +\epsilon\] Here, $Y$ might represent weight, $X_1$ might be diet, while $X_2$ might be exercise. Knowing the coefficients and the levels of diet and exercise predicts weight. That's even more useful than knowing what to eat, especially if diet and exercise interact. 

Which we could test for using ANOVA \[weight = \beta_{diet} \times diet + \beta_{exercise} \times exercise + \epsilon\] 

Now let's apply that linear-model-way-of-thinking to the multivariate state. What if we flipped the general linear model and turned our dependent variables into predictors? In general, \[V=\beta_1 Y_1+\beta_2Y_2+\epsilon\] where $Y_1$ and $Y_2$ represent two dependent variables, while $V$ is some latent variate. 

In statistical jargon, $V$ is known as a discriminate function variate, but we'll get to that soon. Continuing with the example, here, $Y_1$ and $Y_2$ are weights and heights that we've measured in response to different levels of diets and exercises. $beta_1$ and $beta_2$ are regression coefficients with values that are estimated from the same experiments (they aren't the same coefficients as the ones above which you can tell by the different subscripts). \[V=\beta_{weight}\times weight+ \beta_{height} \times height\] 

What is $V$?

$V$ is a latent variable. It's completely imaginary and weird...which you can tell just by looking at its units. The units are a combination of weight and height. What this is, conceptually, is a new dimension of the data. $V$ represents a linear combination of the weight and height variables, which in turn are each predicted by diet and exercise.





**Example 1: Influence of therapy on obsessive-compulsive actions and thoughts**
OCD is characterized by repetitive behaviors and thoughtful impulses to start a reptitive behavior. For example, the obsessive impulse to wash ones hands. 

30 subjects were enrolled in the study. 10 subjects were randomized to each of either 3 treatment interventions: either cognitive behavioral therapy (cbt), behavioral therapy (bt), or no therapy (nt). 

In a subsequent testing period they recorded the number of times the subjects engaged in OCD actions (a) and also the number of times they had OCD thoughts (t). Thus, two dependent variables (a and t) are recorded.

Construct an ocd data set. 3 treatments (cbt, bt, nt) followed by 2 outcomes (actions, thoughts). This dataset is derived from [Discovering Statistics Through SPSS, 3rd edition, Stanley Field, Chapt 16](https://www.discoveringstatistics.com/books/dsus/)
```{r}
# create vectors for the outcome responses following each of the therapies
a.cbt <- c(5,5,4,4,5,3,7,6,6,4)
a.bt <- c(4,4,1,1,4,6,5,5,2,5)
a.nt <- c(4,5,5,4,6,4,7,4,6,5)
t.cbt <- c(14,11,16,13,12,14,12,15,16,11)
t.bt <- c(14,15,13,14,15,19,13,18,14,17)
t.nt <- c(13,15,14,14,13,20,13,16,14,18)

# synthesize a data frame
ID <- as.factor(c(rep(1:30)))
#type <- c(rep(c("actions", "thoughts"), each = 30))
treat <- c(rep(c("cbt", "bt", "nt"), each = 10))
#outcome <- c(a.cbt, a.bt, a.nt, t.cbt, t.bt, t.nt)
actions <- c(a.cbt, a.bt, a.nt)
thoughts <- c(t.cbt, t.bt, t.nt)
ocd <- data.frame(ID, actions, thoughts, treat)

#check the dataframe to ensure it is correct
ocd
```
### graph

```{r}
ggplot(ocd, aes(x=treat))+
  geom_jitter(aes(y=actions, color="actions"), width=0.1)+
  geom_jitter(aes(y=thoughts, color="thoughts"), width=0.1)+
  labs(y="Score", y="Treatment")
```

```{r}
ocd.1 <- ocd%>%gather(symptoms, score, -ID, -treat)

ggplot(ocd.1, (aes(x=symptoms, y=score)))+
  geom_jitter(aes(color=treat), size=3, width=0.025)+
  geom_line(aes(group=ID))
```


Do manova on data directly



```{r}
ocd.man <- manova(lm(cbind(actions, thoughts)~treat, ocd))
#ocd.man
ocd.man

summary(ocd.man)

# a couple of ways
#summary.aov(ocd.man)
#summary(Anova(ocd.man), univariate = T, multivariate = F, p.adjust.method = T)

```

Or do a linear model

```{r}
locd.man <- lm(cbind(actions, thoughts) ~ treat, data = ocd)
summary(locd.man)
#summary(locd.man)
#summary(Anova(locd.man), univariate = T, multivariate = T, p.adjust.methods=T)
```



+, : and * for main effects, interactions, main effects plus interactions




```{r}
ocd.lda <- lda(treat~actions +thoughts, data=ocd)
ocd.lda

prcomp(ocd[,c(-1, -4)])
```

```{r}
plot(ocd.lda)
```


```{r}
ocd.man <- manova(cbind(actions, thoughts) ~ treat, ocd)
```

Here, ocd.man is an object of class "manova" 
```{r}
class(ocd.man)
ocd.man
```


```{r}
summary(ocd.man, test = "Roy")
```


```{r}
summary.aov(ocd.man)
```



