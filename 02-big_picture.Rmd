# The Big Picture {#bigpic}

*To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher*
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(Hmisc)
```

Let's start by listing out some key characteristics that most biomedical experiments share in common.

They...

* tend to involve the use of relatively small sample sizes. 
*	are usually highly exploratory in nature. 
*	generate data that are either discrete counts or measurements of continuous scalars.
* are structured by a small group of fairly common experimental designs.
* are usually interpreted in a binary way; as having “worked”, or not.
* test hypotheses (though too often these are unstated).
* aspire for rigor, replicability and reproducibility.
* aspire to be unbiased.

The stakes of our work can be pretty high. These include the higher ideals such as the validation of novel scientific paradigms, the steady advancement of knowledge, and opening the door to create impactful solutions, particularly in the realm of human diseases and suffering. 

But no less motivating are the issues more related to the professional practice of science. These include ego, the completely natural impulse to seek out validation for an idea, publication and/or commercialization, time to degree, career viability, scientific reputations, and coveted research/investment funds. 

The point is that the process of scientific discovery is driven both by ideals and by biases. This is nothing new. 

The one big concept that I hope you embrace is that the statistical design and analysis of experiments serves as a working framework within which the biomedical researcher can conduct reasonably unbiased work. The statistical approaches covered in this course, it turns out, were invented long ago with all of these drivers in mind. 

## What are experimental statistics?

Experimental statistics are used 

* to summarize data into simpler descriptive models.
* as procedures to draw inferences from samples. 
* as procedures that guide the design of experiments.
* to serve as framework for conducting unbiased research.

Chances are you thought biostats was just one or two of those bullets, and probably not the latter two.

### Descriptive modeling

Statistical models are ways of simplifying or summarizing data so that they can be more readily described and interpreted. 

For example, if we have a sample in which blood glucose levels are measured in each of many subjects, clarity demands we explain those results in terms of summary statistics. Thus, we use parameters like the sample mean and standard deviation, or median and ranges or percentiles. The alternative is unthinkable today (but common long ago), which is to discuss each replicate individually.

Statistical tests also have a descriptive element in that they convey information about the experimental design. If you say, "I'm working on analyzing a two-tailed paired t-test," say no more. From that alone I know something about your hypothesis, how replication was handled, the number of groups, and the type of data you're measuring. 

Regression models also describe data. For example, here is the well-known Michaelis-Menten model that describes product formation as a function of substrate concentration.  
\[[P]=\frac{[S][Vmax]}{[S]+Km}\]

That's a model we might fit to certain kinds of enzyme kinetic data, because we use it to estimate scientifically meaningful parameters, like $V_{max}$ and $K_m$. 

In fact, mathematical statistics is actually just modeling. Modeling is the process of simplifying data into something more coherent. 

Take a simple example of two groups shown here. Each group has been fit to a simple model: that for the mean and standard deviation. Clearly, that model fits the control group much better than it fits the treatment group. 

```{r echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="How good of a descriptive model is the mean for each these groups?"}

Control <- c(111, 113, 92, 103, 91, 101, 90, 93, 105, 91)
Treatment <- c(111, 113, 92, 103, 91, 101, 90, 93, 1, 2)
data <- data.frame(Control, Treatment)
data <- gather(data, Group, Serum_glucose)

ggplot(data, aes(Group, Serum_glucose))+
  geom_jitter(width=0.1, size=4, color="blue", alpha = 0.2) +
  stat_summary(fun.data="mean_sdl", fun.args =list(mult=1))

```

Why do I say that? The treatment group data are much more skewed. Most of the data values are greater than the mean of the group. Sure, a mean can be calculated for that group, but it serves as a fairly crappy summary. Perhaps some other model (or group of statistical parameters) would better convey how these data behave?

This is to point out that learning statistics is about learning to make judgments about which models are best for describing a given data set.

### Statistical inference

There are two types of inference researchers make. 

One type is to infer whether an experiment “worked” or not. This familiar process involves calculating a test statistic from the data (eg, t-test, F-tests, etc) and then applying a threshold rule to its value. If the test passes the rule, we conclude the experiment worked. 

A second type of inference is to extrapolate from a sample some **estimate** for the values of the variables within the population that was sampled. 

Both types of inference are subject to error. By random chance alone our sample could be way off the mark, even with perfectly calibrated instrumentation. In those cases our inferences would be wrong. The real difficulty with inference is we can never know for certain whether we are right or wrong. 

### Experimental design

Experimental planning that involves dealing with statistical issues is referred here as experimental design.

This involves stating a testable statistical hypothesis and establishing a series of decision rules in advance of data collection. These rules range from subject selection and arrangement, predetermination of sample size using *a priori* power analysis, setting some data exclusion criteria, defining error tolerance, specifying how the data will be transformed and analyzed, declaring a primary outcome, on up to what statistical analysis will be performed on the data.

Experimental design is very common in [prospective clinical research](https://clinicaltrials.gov/ct2/home). Unfortunately, very few basic biomedical scientists practice anything remotely like this. Most biomedical researchers begin experiments with only vague ideas about the statistical analysis, which is usually settled on after the fact. Much of the published work today is therefore retrospective, rather than prospective. Yet, most researchers tend to use statistics that are largely intended for prospective designs. That's a problem. 

### Statistics as an anti-bias framework

If you are ever asked (for example, in an examination) what purpose is served by a given statistical procedure, and you’re not exactly sure, you would be wise to simply offer that it exists to prevent bias. That may not be the answer the grader was hunting for, but it is almost surely correct.

The main purpose of “doing” statistical design and analysis of experiments is to control for bias. Humans are intrinsically prone to bias and scientists are as human as anybody else. Holding or working on a PhD degree doesn't provide us a magic woo-woo cloak to protect us from our biases. 

Therefore, whether we choose to admit it or not, bias infects everything we do as scientists. This happens in subtle and in not so subtle ways. We work hard on our brilliant ideas and, sometimes, desperately wishing to see them realized, we open the door to all manner of bias.

Here are some of the more important biases.

####Cognitive biases

From a statistical point of view biases can be classified into two major groupings. The first are [Cognitive biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases). These are how we think (or fail to think) about our experiments and our data. 

These frequently cause us to make assumptions that we would not if we only knew better or were wired differently. If you ever find yourself declaring, "how could this not work!" you are in the throes of a pretty deep cognitive bias.  In bench research, cognitive biases can prevent us from building adequate controls into experiments or lead us to draw the wrong interpretation of results, or prevent us from spotting confounding variables or recognizing telling glitches in the data as meaningful.

####Systematic biases

The second are systematic biases. Systematic biases are inherent to our experimental protocols, the equipment and materials we use, the timing and order by which tasks are done, the subjects we select and, yes (metaphorically), even whether the data are collected left-handed or right-handed, and how data is handled or transformed. 

Systematic biases can yield the full gamut of unintended outcomes, ranging between nuisance artifacts to false negatives or false positives. For example, poorly calibrated equipment will bias data towards taking inaccurate values. Working forever on an observed phenomenon using only one strain of mouse or cell line may blind us from realizing it might be a phenomenon that only occurs in that strain of mouse or cell line.

####Scientific misconduct

More malicious biases exist, too. These include [forbidden practices such as data fabrication and falsification](https://grants.nih.gov/policy/research_integrity/index.htm). This is obviously a problem of integrity. Very few scientists working today are immune from the high stakes issues that pose threats to our sense of integrity. 

In the big picture, particularly for the biomedical PhD student, I like to call bias the event horizon of rabbit holes. A rabbit hole is that place in a scientific career where it is easy to get lost for a long, long time. You want to avoid them.

The application of statistical principles to experimental design provides some structure to avoid making many of the mistakes that are associated with these biases. Following a well-considered, statistically designed protocol enforces some integrity onto the process of experimentation. Most scientists find a statistical framework quite livable. 

If you give it some thought, the only thing worse than a negative result from a statistically rigorous experiment is a negative result from a statistically weak experiment. With the former at least you know you've given it your best shot. That is hard to conclude when the latter occurs.


