# Variability, Accuracy and Precision {#dispersion}

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(ggformula)
```


One of my favorite sayings is "models are perfect, data are not", because it's a simple way to express the statistical way of thinking. In statistics, the population parameters that interest us are held to have true, fixed values. The models we conjure up to estimate these parameters are also perfect. Unfortunately, the data that we generate and fit these models to are usually pretty crappy and messy. 

Therefore, a major focus of statistics is to evaluate how well our perfect models fit these messy data. Quantifying that fit is the basis for asserting how reliable our parameter estimates are. Which in turn means how well we think we understand a system. 

All of this points to the fact that it is not possible to understand statistical analysis without wrapping our heads around the concepts of variability, accuracy and precision.

The reason data are messy is that sample data possess inherent variability. This means there is always some 'error' between the true values of the population parameters and their estimated values within the model. That error is held to be random. In statistics, random error is defined as the variation that we can't account for in our model.

In running experiments on biological systems we think about two main contributors to this error. The first is biological variation. There are confounding factors inherent within the sampled specimens that are driving the observed variation, but we don't understand them and are not controlling for them.  The second source is so-called systematic error, having to do with our technique and equipment, the way we measure the sample.

As you recall from middle school, a straight line drawn between two variables is described by the linear model $Y=\beta_0+\beta_1 X$. This model has two parameters, $\beta_0$ represents the y-intercept while $\beta_1$ represents the slope of the line. The predictor variable is $X$. Thus, passing values of $X$ into this model generates values for the response variable, $Y$.

I mention this so I can illustrate randomness, perfect models, and all of that. 

Here's a simulation of response data from two linear models, $Y1$ and $Y2$. 

Reading the code, you notice that one of the models, $Y1$ has no error term, so it is perfect, which is to say the values of $Y1$ are perfectly predicted by the model's intercept and slope parameters. A linear regression goes through every data point and the output of that regression yields the exact same parameter values that were input (a bit of a circular proof, if you will). The model perfectly fits its data. 

The other model ($Y2$) has an error term tacked onto it. As a result it yields imperfect values of $Y2$ even when given the exact same model parameters as for $Y1$. Applying a linear regression to this data yields an imperfect fit...the line doesn't go through all of the data points.  Error is associated with the estimate (the gray shade), and the intercept and slope parameters estimated through linear regression differ somewhat from the parameter input values.

```{r fig.height=4, fig.width=5, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap = "Models are perfect, data are not."}

#intital model parameter values

b0 <- 0
b1 <- 5
X <- seq(1,10,1)

error <- rnorm(length(X), mean=0, sd=10)

#models

Y1 <- b0+b1*X
Y2 <- b0+b1*X + error

#put the simulations into a data frame so it can be plotted

df <- data.frame(X, Y1, Y2) %>%
  gather(model, response, -X)

#plotting function

ggplot(df, aes(X, response))+
  geom_point()+
  geom_smooth(aes(X, response), method=lm)+
  facet_grid(cols=vars(model))
```

You can appreciate how the second model, with the random error term, is more real life. What's even more interesting is that every time you run the second model you'll generate a completely different result. Copy and paste that code chunk into R and see for yourself. That error term is just a simulation of a random process. And that's just how random processes behave!

(Play with that code if you don't understand how it works. Run it on your machine. Change the initializers)

In real life, unlike in this simulation, we wouldn't know the true values of the parameters (as we know them in this simulation). In real life we sample and fit a perfect model to imperfect data. 

The resulting output provides estimates of the true parameter values. But the output also includes several bits of information that summarizes all of the variability in the data. Interpreting that tells us whether or not the model is a good way to explain the data. 

At this point in the course it's not important to interpret what all of this error analysis means. 

The point I'm making right now, and that hopefully you'll grow to appreciate, is that to a large extent, statistics is mostly about residual error analysis. It's actually sketchy if there is no residual error. I mean, look at the automagic warning message R barks out below for the summary of the first of these linear regressions...the perfect model:

```{r}
summary(lm(Y1~X)) 
summary(lm(Y2~X))
```

In some respects, you can say that what we do as researchers is iterate through discovery problems. Each time we improve our understanding of a system, in essence, we're explaining just a little bit more of what we previously classified as residual error. 

Is there a biological cause for the variation I see in some assay? Can I explain it by testing a new predictor variable? If I use better technique or different equipment can I "clean up" some of the variation in the results, so that I can get a better handle on some ideas I have about what causes that biological variation?

In big picture words, the signals that we are looking for as biological researchers are actually within all of that noise. 

Irrespective of the statistical variables and tests that you will use, every null hypothesis that you will ever test is, basically, stating "This data can only be explained by random noise." 

## Variance: Quantifying variation by least squares

The two primary methods to quantify variation are termed "ordinary least squares" and "maximum likelihood estimation". Mathematically they are distinct, and that difference is beyond the scope of this course. It would require teaching math.

What is important is that, in most cases, either method will arrive at roughly the same solution.

Ordinary least squares is the method used for quantifying variation for most of the statistics discussed in this course. The only exception is when we get to generalized linear models. So we'll omit consideration of maximum likelihood estimation until then.

Ordinary least squares arises from the theoretical basis of variance, and is a deceptively simple concept.

It can be shown (by mathematicians) that the variance of a random variable, $Y$, with an expected value equivalent to its mean, $E(Y) = \mu$, is the difference between it's squared expected value and its expected value squared. \[Var(Y) = E(Y^2)-E(Y)^2 \]


Imagine a sample of size $n$ replicates drawn from a population of the continuous random variable, $Y$. The replicate sample values for $Y$ are $y_1, y_2, ...y_n$. 

The mean of these replicate values provides an estimate for the mean of the sampled population, $\mu$, and is $$\bar y = \frac{{\sum_{i=1}^n}y_i}{n}$$

The value by which each replicate within the sample varies from the mean is described alternately as deviate or as a residual. Each represents the same thing: $y_i-\bar y$. A vector of sample replicates will have values that are smaller than and larger than the mean. Thus, some variate values are positive, while others are negative.

In a random sample we would expect that the values for replicates are roughly equally dispersed above and below a mean. Thus, if we were to sum up the deviates (or residuals) we'd expect that that sum's value to approach zero.  $$E(\sum_{i=1}^n(yi-\bar y))=0$$

By squaring the deviates, negative values are removed, providing a parameter that paves the way to more capably describe the variation within the sample than can the sum of the deviates. This parameter is called the "sum of squares": \[SS=\sum_{i=1}^n(yi-\bar y)^2\]

A sample's variance is the sum of the squared deviates divided by the sample degrees of freedom, $df=n-1$. \[s^2=\frac{\sum_{i=1}^n(yi-\bar y)^2}{n-1}\]

A sample's variance, $s^2$ is an estimate of the variance, $\sigma^2$, of the population that was sampled, in the same way $y_bar$ estimates $\mu$.

You can think of variance as an approximate average of $SS$. The reason $s^2$ is arrived at through dividing by degrees of freedom, $n-1$, rather than by $n$ is because doing so produces a better estimate of the population variance, $\sigma^2$. [Mathematical proofs of that assertion](https://stats.stackexchange.com/questions/100041/how-exactly-did-statisticians-agree-to-using-n-1-as-the-unbiased-estimator-for) can be found all over the internet.

Variance is a hard parameter to wrap the brain around. It describes the variability within a data set, but geometrically: The units of $s^2$ are squared. For example, if your response variable is measured by the $gram$ mass of objects, then the variance units are in $grams^2$. 
That's weird.

Later in the course, we'll discuss statistical testing using analysis of variance (ANOVA) procedures. The fundamental idea of ANOVA is to test for group effects by "partitioning the error". That's done with $SS$. When a factor causes some effect, the $SS$ associated with that factor get larger...by the square of the variation. Statistical testing is then done on the variance in groups, which in ANOVA jargon is called the "mean square", or $MS$. $MS$ is just another way for saying $s^2$.   

## Standard deviation

The sample standard deviation solves the problem of working with an unintuitive squared parameter. The standard deviation is a more pragmatic descriptive statistic than is variance. 

The standard deviation is the square root of the sample variance: \[sd=\sqrt{\frac{\sum_{i=1}^n(y_i-\bar y)^2}{n-1}}\] 

### What does the standard deviation tell us

The sample standard deviation is two things at once. It is 
* a statistical parameter that expresses the variability within the sample. 
* an estimate of the variability within the population that was sampled.

There aren't many factoids in statistics worth committing to memory, but this one on the standard deviation is one of them: A bit over two thirds of the values for a normally-distributed variable will lie between one standard deviation below and above the mean of that variable.

Here's one way to calculate that using R's `pnorm` function, the cumulative distribution function:
```{r}
pnorm(-1, lower.tail = T) #Calculates the AUC below a zscore of -1.
pnorm(1, lower.tail = F) #Calculates the AUC above a zscore of 1
1-pnorm(-1)-pnorm(1, lower.tail = F) #The middle range
```

Explore: Use `pnorm` to calculate the AUC between z scores of -2 and 2.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="About 86% of the values for a normally distributed variable are within +/- one standard deviation from the mean."}
ggplot(data.frame(zscore = c(-5, 5)), 
       aes(zscore)) +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                xlim= c(-5, 5),
                geom = "area", fill="red")+
  stat_function(fun = dnorm,
                xlim= c(-1, 1),
                geom = "area", fill="blue")+
  ylab("p(z)")+
  scale_x_continuous(breaks=seq(-5,5,1))+
  annotate("text", x=0, y=0.2, 
           label="~68%", 
           color= "white",
           size = 10)+
  annotate("text", x=-3, y=0.1, 
           label="pnorm(-1, lower.tail=T)")+
  annotate("text", x=3, y=0.1, 
           label="pnorm(1, lower.tail = F)")
```

No matter the scale for the variable, the relative proportion of values within 1 standard deviation for normally distributed variables will always behave this way. Here's the distribution of serum glucose concentration values, where the average is 100 mg/dl and the standard deviation is 10 mg/dl:

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE,fig.cap="Modeling the distribution of a blood glucose variable."}
ggplot(data.frame(glucose=seq(50, 150, 1)), aes(glucose)) +
  stat_function(fun = dnorm, args=list(mean=100, sd=10)) +
  stat_function(fun = dnorm, args=list(mean=100, sd=10),
                xlim= c(50, 150),
                geom = "area", fill="red")+
  stat_function(fun = dnorm, args=list(mean=100, sd=10),
                xlim= c(90, 110),
                geom = "area", fill="blue")+
  ylab("p(glucose)")+xlab("glucose, mg/dl")+
  scale_x_continuous(breaks=seq(50,150,10))
  
```

## Other ways of describing variability

*Just show all of the data as scatter plots! There's no need to hide the variability in bar plots with "error bars".

*Violin plots are pretty ways to illustrate the spread and density of variation graphically.

*The coefficient of variation, $cv=\frac{sd}{mean}$, is a dimensionless index. If you tell me, "My cv is 60%," then my response would be, "Mmmm, that sounds pretty noisy." That is, the more cv's you pay attention to, the better you appreciate the physical implications of a given value.

*Percentiles and ranges. In particular, the innerquartile range. This is usually reserved for non-normal data, particularly discrete data. The IQR illustrates the spread of the middle 50% of data values, from the 25th to the 75th percentiles, and is usually accompanied by the use of the median as the centrality parameter.

## Precision and Accuracy

Even with well-behaved subjects, state-of-the-art equipment, and the best of technique and intentions, samples can yield wildly inaccurate estimates, even while measuring something precisely. My favorite illustration of this is how [estimates for the masses of subatomic particles have evolved over time](https://unbiasedresearch.blogspot.com/2018/02/accuracy-v-precision.html). We can probably assume that the real masses of these particles have remained constant. Yet, note all the quantum jumps, pun intended, in their estimated values. 

Ways of measuring things change. What seems very accurate today could prove to be wildly inaccurate tomorrow. And just because you've measured something precisely doesn't mean you know its true value.

If there is one clear take away from this it's that all such statistical estimates are provisional.

## Standard error

This will sound counter-intuitive, but we can actually know how precise our estimate of some parameter is without knowing the true value. That's because precision is the repeatability of a measurement, and it is possible to repeat something very, very reliably but inaccurately. 

**The standard error is the statistical parameter used to express sample precision. Standard error is calculated from standard deviation**
\[precision:\ SE = \frac{sd}{\sqrt n}\]

In contrast, as the history of subatomic particle estimates illustrates, we can never know for sure whether an estimate derived from a sample accurately estimates the sampled population. We will always be uncertain.

### What exactly does the standard error represent?

The central limit theorem predicts a few important things: 
1) A distribution of many sample means will be normally distributed, even if a non-normal distribution is sampled.
2) A distribution of sample means will have less dispersion with larger sample sizes.
3) If a sample population has a mean $\mu$ and standard deviation $\sigma$, the distribution of sample means of sample size $n$ drawn from that population will have a mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt n}$.

These features are illustrated below, as a prelude to further defining standard error. 

The first graph showing a big black box simply plots out a uniform distribution of the random variable $Y$ ranging in values from 0 to 1, using the `dunif`function. It just means that any value, $y$, plucked from that distribution is as equally likely as all other values.

```{r message=FALSE, warning=FALSE, fig.cap="A variable with a uniform distribution."}
ggplot(
  data.frame(
    x=seq(0, 1, 0.01)),
  aes(x))+
  stat_function(
    fun=dunif, 
    args=list(min=0, max=1),
    xlim=c(0, 1),
    geom="area", 
    fill="black")+
  labs(y="p(y)", x="Y")
```

The probability of sampling any one of these values is equivalent to that for all other values. Uniform distributions arise from time-to-time. For example, important actually, the distribution of p-values from null hypothesis tests is uniform.

As you might imagine, the mean of this particular uniform distribution is 0.5 (because it is halfway between the value limits 0 and 1). Probably less obvious is the standard deviation for a uniform distribution with limits $a$ and $b$ is $\frac{b-a}{\sqrt{12}}$. So the standard deviation for this particular uniform distribution $\sigma$ = 0.2887.

Just so you trust me, these assertions pass the following simulation test:

```{r}
mean(runif(n=100000, min=0, max=1))
sd(runif(n=100000, min=0, max=1))
```

The next graph illustrates the behavior of the central limit theorem.

In the following script, random samples are taken from this uniform distribution many times. It takes either 1000 small random samples (n=3) or 1000 large random samples (n=30). 

It then calculates the means for each one of those samples before finally plotting out a histogram showing the distribution of all of those sample means.

Notice how the distributions of sample means are normally distributed, even though they comes from a uniform distribution! That validates the first prediction of the CLT, made above.

Note also that the distribution of means corresponding to the larger sample sizes has much less dispersion than that for the smaller sample sizes. That validates the second CLT prediction made above. 

```{r message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="The central limit theorem in action, distributions of sample means of small (red) and large (blue) sample sizes."}

meanMaker <- function(n){
 mean(runif(n))
}
small.sample <- replicate(1000,meanMaker(3))
large.sample <- replicate(1000, meanMaker(30))

ggplot(data.frame(small.sample, large.sample))+
  geom_histogram(aes(small.sample), fill="red")+
  geom_histogram(aes(large.sample), fill="blue", alpha=0.5 )+
  xlab("n=3 (red) or n=30 (blue")
```

As for the third point, the code below calculates a mean of all these means, and the SD of all these means, for each of the groups. 

```{r}
mean(small.sample)
sd(small.sample)
mean(large.sample)
sd(large.sample)
```

Irrespective of sample size, the mean of the means is a great estimator of the mean of the uniform distribution.

But passing the sample means into the `sd` function shows that neither provides a good estimate of the $\sigma$ for the population sampled, which we know for a fact has a value of 0.2887.

Obviously, the standard deviation **of a group of means** is not an estimator of the population standard deviation. So what is it?  The standard deviation of the distribution of sample means is what is known as the standard error of the mean.

**This goes a long way to illustrate what the standard error of the mean of a sample, SEM, actually represents. It is an estimator of the theoretical standard deviation of the distribution of sample means,**$\sigma_{\bar y}$. 

Now what are the implications of THAT?

There are a few.

First, SEM is not a measure of dispersion. It is a measure that describes the precision by which a mean has been estimated. The SEM for the large sample size group above is much lower than that for the small sample size group. Meaning, $\mu$ is estimated more precisely by using large sample sizes.

Second, statistically naive researchers use SEM to illustrate dispersion in their data (eg, using SEM as "error bars"). Ugh. They do this because, invariably, the SEM will be lower than the SD. And that looks cleaner. 

But they shouldn't do this, because SEM is not an estimator of $\sigma$. Rather, SEM estimates $\sigma_{\bar y}$. Those are two very different things.

I suggest you use SEM if it is important to illustrate the precision by which something is measured. That's usually only important for parameters that are physical constants. 

Third, when we do experiments we sample only once (with $n$ independent replicates) and draw inferences about the population under study on that bases of that one sample. 

We don't have the luxury or resources to re-sample again and again, as we can in simulations. However, these simulations illustrate that, due to the central limit theorem, a long run of sampling is predictably well-behaved. This predictability is actually the foundation of statistical sampling methodology.

The SEM estimates a theoretical parameter ($\sigma_{\bar y}$ that we would rarely, if ever, attempt to validate. Yet, on the basis of one sample it serves a purpose by providing an estimator of precision.

## Confidence intervals

Confidence intervals are the range of values within which we predict is the true value of a parameter, with some confidence. 

We just need to pick a level within which to be confident. 90%? 95%? 99%? Once that is decided, we can calculate the range within which that true value likely lies. 

**The confidence interval serves as the statistical parameter used to express sample accuracy.**

Standard errors and confidence intervals are calculated from the same sample data. So there is a bit of a circular argument at play here. We use the variability within the sample to generate two related parameters, standard error and confidence intervals. The former tells us about the precision and the latter tells us about the accuracy.

### Simulations

The random sampler is a script that **generates a random sample from a known distribution** before calculating several sample parameters, including the confidence interval. 

It's particularly useful to illustrate the relationships between confidence intervals, sample sizes, standard deviations, and confidence levels.

When the confidence interval is accurate, a blue-colored bar appears. When the confidence interval is not accurate, you get a red bar. 

The confidence interval for a sample mean is calculated as follows:

\[CI=\bar x \pm t_{df(n-1)}\cdot\frac{sd}{\sqrt{n}}\]

```{r message=FALSE, warning=FALSE, fig.cap="Confidence interval illustrator"}
n <- 5
pop.mean <- 100
pop.sd <- 25
sig.dig <- 2
conf.level <- 0.95
x <- c(seq(1, 200, 0.1))
#draw sample and calculate its descriptive stats
mysample <- rnorm(n, pop.mean, pop.sd)
mean <- round(mean(mysample), sig.dig)
sd <- round(sd(mysample), sig.dig)
sem <- round(sd/sqrt(n), sig.dig)
ll <- round(mean-qt((1+conf.level)/2, 
                    n-1)*sem, sig.dig)
ul <- round(mean+qt((1+conf.level)/2, 
                    n-1)*sem, sig.dig)
#print to console
print(c(round(mysample, sig.dig), 
        paste(
          "mean=", mean, "sd=", sd, 
          "sem=", sem, "CIll=", ll, 
          "CIul=", ul)))
# graph the sample confidence interval on the population

pretty <- ifelse(ll > pop.mean | ul < pop.mean, "red", "blue")

#graph (note: using ggformula package)
gf_line(dnorm(x, pop.mean, pop.sd)~x)%>%
  gf_segment(0 + 0 ~ ll + ul, 
             size = 2, 
             color = pretty)%>%
  gf_labs(subtitle = 
            paste(100*conf.level,
                  "% CI =",ll,
                  "to",ul))
```

Here's a script that compares the confidence intervals of two means, from random **samples of known distributions**. 

When two CI's don't overlap, they remain colored blue and green, and the two means they correspond to are unequal. It's the same as a test of significance without the formal hypothesis.

If the two CI's overlap, one of them turns red. It's a sign the two samples are indistinguishable, since the 95% CI of one sample includes values in the 95% CI of the other. It's also the same as a test of significance, for a null outcome.

What's should be done with this simulator is to use it to gain an intuitive understanding about how confidence intervals operate. Practice changing the sample size (n), the population means and standard deviations, and even the confidence level. Under what conditions are overlapping intervals diminished? What factors influence narrower intervals?

```{r message=FALSE, warning=FALSE, fig.cap="Comparing two samples using confidence intervals. Red-colored indicates the two CI overlap, meaning the two groups for that sample test out as no different."}

n <- 5
m <- 100
conf.level=0.95
t <- qt((1+ conf.level)/2, n-1)
pop.mean.A <- 125
pop.mean.B <- 200
pop.sd.A <- 25 
pop.sd.B <- 25
x <- c(seq(1, 300, 0.1))
y <- seq(0.0005, m*0.0005, 0.0005)

#simulate  
mydat.A <- replicate(
  m, rnorm(
    n, pop.mean.A, pop.sd.A
    )
  )
ldat.A <- apply(
  mydat.A, 
  2, 
  function(x) mean(x)-t*sd(x)/sqrt(n)
  )
udat.A <- apply(
  mydat.A, 
  2, 
  function(x) mean(x)+t*sd(x)/sqrt(n)
  )

mydat.B <- replicate(
  m, rnorm(
    n, pop.mean.B, pop.sd.B
    )
  )
ldat.B <- apply(
  mydat.B, 
  2, 
  function(x) mean(x)-t*sd(x)/sqrt(n)
  )
udat.B <- apply(
  mydat.B, 
  2, 
  function(x) mean(x)+t*sd(x)/sqrt(n)
  )  

ci <- data.frame(
  y, ldat.A, udat.A, ldat.B, udat.B
  )

alt <- ifelse(
  udat.A >= ldat.B, "red", "blue"
  )

#plots made with ggformula package

gf_line(dnorm(
  x, pop.mean.A, pop.sd.A)~x, 
  color = "dark green"
  )%>%
  
  gf_line(dnorm(
    x, pop.mean.B, pop.sd.B)~x, 
    color = "blue"
    )%>%
  
  gf_segment(
    y+y ~ ldat.A + udat.A, 
    data = ci, 
    color = "dark green"
    )%>%
  
  gf_segment(
    y+y ~ ldat.B + udat.B, 
    data = ci, 
    color = alt
    )%>%
  gf_labs(
    y = "dnorm(x, pop.mean, sd.mean"
    )

```

# Key take aways

* Variability is inherent in biological data. The two main sources are intrinsic biological variation--which has so many causes, and variability associated with our measurements.
* Statistics operates on the presumption that the values of parameters in the populations we sample are fixed. Residual error is unexplained deviation from those fixed values.
* About two-thirds of the values of a normally distributed variable will lay, symmetrically, within one standard deviation on either side of the variable's mean.
* Variance expresses the variability within a sample, but geometrically, in squared units.
* The standard deviation, the square root of variance, estimates the variability of a sampled population.
* The standard error of the mean estimates the precision by which a sample mean estimates a population mean.
* A confidence interval estimates the accuracy by which a parameter, such as a mean, has been estimated.
* If the confidence intervals of two samples do not overlap, the sampled distributions likely differ. 
* The central limit theorem saves the day, because even if the variable you are measuring is not normally-distributed, the means of your samples are.

